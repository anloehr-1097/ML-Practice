{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 2., 3.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.Tensor([1,2,2,3])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnc = None \n",
    "\n",
    "membank = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 20])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_loc: int = 10  # N\n",
    "dim_loc: int = 20  # M \n",
    "\n",
    "memmat = torch.rand(size=(n_loc, dim_loc))\n",
    "memmat.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_head -> w_t vec weights with N entries \n",
    "# read_vector r_t = convex combination of mem elements with weights = w_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1])\n",
      "tensor([[0, 0],\n",
      "        [6, 8],\n",
      "        [6, 5],\n",
      "        [9, 2],\n",
      "        [7, 5]])\n",
      "tensor([[0.1000],\n",
      "        [0.2000],\n",
      "        [0.3000],\n",
      "        [0.4000],\n",
      "        [0.5000]])\n",
      "tensor([[0.0000, 0.0000],\n",
      "        [1.2000, 1.6000],\n",
      "        [1.8000, 1.5000],\n",
      "        [3.6000, 0.8000],\n",
      "        [3.5000, 2.5000]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([10.1000,  6.4000])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.arange(0.1,.6,0.1)\n",
    "w = w[:, None]\n",
    "print(w.size())\n",
    "memmat = torch.randint(10, (5,2))\n",
    "print(memmat)\n",
    "print(w)\n",
    "print(w * memmat)\n",
    "torch.sum(w * memmat, dim=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadHead(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def forward(self, mem_mat: torch.Tensor):\n",
    "        \"\"\"Emit read vector r_t\"\"\"\n",
    "        \n",
    "        # product weights w_t\n",
    "        # emit reaad vector r_t\n",
    "        weights = torch.rand(mem_mat.size()[0])\n",
    "        weights = weights[:, None]\n",
    "        return torch.sum(weights * mem_mat, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rh = ReadHead()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [6, 8],\n",
       "        [6, 5],\n",
       "        [9, 2],\n",
       "        [7, 5]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'gradient'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rh\u001b[38;5;241m.\u001b[39mforward(memmat)\u001b[38;5;241m.\u001b[39mgradient()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'gradient'"
     ]
    }
   ],
   "source": [
    "\n",
    "rh.forward(memmat).gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WriteHead(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    def forward(self, memmat: torch.Tensor):\n",
    "        \"\"\"Write head adjusts memory matrix.\"\"\"\n",
    "\n",
    "        weights = torch.rand(memmat.size()[0])\n",
    "        v_erase =  torch.rand(memmat.size()[1])\n",
    "        v_add =  torch.rand(memmat.size()[1])\n",
    "        weights = weights[:, None]\n",
    "        ones = torch.ones(memmat.size()[1])\n",
    "        for i in range(len(memmat.size()[0])):\n",
    "            memmat[i, :] = memmat[i, :] * (ones - weights[i] * v_erase)\n",
    "            memmat[i, :] = memmat[i, :] * (ones - weights[i] * v_add)\n",
    "\n",
    "        return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2607, 0.6224, 0.4122],\n",
       "        [0.2607, 0.6224, 0.4122],\n",
       "        [0.2607, 0.6224, 0.4122],\n",
       "        [0.2607, 0.6224, 0.4122]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_mat = e.repeat(4,1)\n",
    "e_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free params controller\n",
    "SIZE_OF_MEM = 10\n",
    "NUM_READ_HEADS = 5\n",
    "NUM_WRITE_HEADS = 5\n",
    "LOC_SHIFT_RANGE = list(range(1,6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "class MemoryBank(torch.nn.Module):\n",
    "    def __init__(self, num_vectors: int, vec_dim: int):\n",
    "        super(MemoryBank, self).__init__()\n",
    "        self.num_vec = num_vectors\n",
    "        self.vec_dim = vec_dim\n",
    "        self.batch_size: Optional[int] = None\n",
    "        self.data: torch.Tensor | None = None\n",
    "\n",
    "    def init_state(self, batch_size, device):\n",
    "        self.batch_size = batch_size\n",
    "        self.data = torch.zeros(batch_size, self.num_vec, self.vec_dim).to(device)\n",
    "\n",
    "    def update(self, weight: torch.Tensor, erase_vec: torch.Tensor, add_vec: torch.Tensor):\n",
    "        # make sure that batch_dim of tensor is indeed self.batch_dim\n",
    "        # TODO check if dims are ok and batch dim is considered correctly\n",
    "        erase_row_stack = erase_vec.repeat(erase_vec.shape[0], 1, self.num_vec).reshape(erase_vec.shape[0], erase_vec.shape[1], -1)\n",
    "        erase_row_stack *= weight\n",
    "        self.data -= erase_row_stack\n",
    "        self.data += weight*add_vec.repeat(add_vec.shape[0], add_vec.shape[1], self.num_vec).reshape(add_vec.shape[0], add_vec.shape[1], -1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.ones([2,3])\n",
    "t.repeat(1, 3, 1).reshape(t.shape[0], t.shape[1], -1)\n",
    "# torch.ones(size=(3,2))\n",
    "# t, t.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetController(torch.nn.Module):\n",
    "    \"\"\"Some kind of Recurrent net or feedforward net.\"\"\"\n",
    "    # typically LSTM\n",
    "    def __init__(self, h_size=20, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.hidden_size = h_size\n",
    "        self.lstm_cell = torch.nn.LSTM(input_size=1, hidden_size=h_size, batch_first=True)\n",
    "        self.memory_bank: MemoryBank = MemoryBank(10, h_size)\n",
    "\n",
    "    \n",
    "    def forward(self):\n",
    "        # outputs k_t, beta_t, g_t, s_t, \\gamma_t\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Combinator(torch.nn.Module):\n",
    "    \"\"\"Combine output of controller and memory access to make final prediction.\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim: int, out_dim: int=10):\n",
    "        \"\"\"Default out dim for copy task: 0 through 9.\"\"\"\n",
    "        super.__init__()\n",
    "        self.input_dim: int = in_dim\n",
    "        layer_1: torch.nn.Linear = torch.nn.Linear(in_dim, out_dim)\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return torch.argmax(torch.nn.Softmax(self.layer_1(x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "from typing import Tuple, Callable\n",
    "from dataclasses import dataclass\n",
    "\n",
    "class Head(torch.nn.Module):\n",
    "    \"\"\"produce key, key_strength, interpolation_gate, shift_weighting, sharpening_factor.\n",
    "\n",
    "    key in mathbb{R}^{mem_size}\n",
    "    key_strength in mathbb{R}_{+} how sharp the attention to memory locations should be\n",
    "    interpolation_gate in (0,1) how much of last weight should be retained\n",
    "    shift_weighting in mathbb{R}^{num_mem_locations} prob distribution over the num locations\n",
    "    sharpening_factor in [1, infty)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim: int,\n",
    "                 membank: MemoryBank,\n",
    "                 sim_func: Callable[[torch.Tensor, torch.Tensor], torch.Tensor]\n",
    "                ):\n",
    "\n",
    "        self.membank = membank\n",
    "        self.num_mem_locations = self.membank.num_vec\n",
    "        self.sim_func = sim_func\n",
    "        self.mem_size = self.membank.vec_dim\n",
    "        self.current_weight: torch.Tensor = torch.ones(self.num_mem_locations) / self.num_mem_locations # size = num_mem_locations\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_params(self):\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        ...\n",
    "\n",
    "    def get_weight(\n",
    "        self,\n",
    "        k: torch.Tensor,\n",
    "        ks: torch.Tensor,\n",
    "        ig: torch.Tensor,\n",
    "        sw: torch.Tensor,\n",
    "        sf: torch.Tensor\n",
    "        ):\n",
    "        loc_similarity: torch.Tensor = self.sim_func(k, self.membank.data)\n",
    "        loc_weight: torch.Tensor = torch.nn.functional.softmax(ks * loc_similarity, dim=-1)\n",
    "        gated_weighting: torch.Tensor = (1 - ig) * self.current_weight + ig * loc_weight\n",
    "        compound_weight: torch.Tensor = torch.zeros(self.num_mem_locations)\n",
    "\n",
    "        # TODO implement convolution without for loops\n",
    "        for i in range(self.num_mem_locations):\n",
    "            for j in range(self.num_mem_locations):\n",
    "                compound_weight[i] += gated_weighting[j] * sw[(i-j)%self.num_mem_locations] \n",
    "\n",
    "        sharpened_weight: torch.Tensor = torch.pow(compound_weight, sf)\n",
    "        weight = sharpened_weight / torch.sum(sharpened_weight)\n",
    "        self.current_weight = weight\n",
    "        return weight\n",
    "\n",
    "\n",
    "class ReadHead(Head):\n",
    "    def __init__(self, in_dim, membank, sim_func):\n",
    "        super().__init__(in_dim, membank, sim_func)\n",
    "        self.out_size = self.mem_size + 1 + 1 + self.num_mem_locations + 1\n",
    "        self.layer_1 = torch.nn.Linear(in_features=in_dim, out_features=self.out_size)\n",
    "\n",
    "    def get_params(self, h: torch.Tensor) -> Tuple[torch.Tensor, ...]:\n",
    "        out: torch.Tensor = self.layer_1(h)\n",
    "        read_key: torch.Tensor = out[:, 0 : self.mem_size]\n",
    "        key_strenght: torch.Tensor = torch.exp(out[:, self.mem_size])\n",
    "        interpolation_gate: torch.Tensor = torch.nn.Sigmoid(out[:, self.mem_size + 1])\n",
    "        shift_weighting: torch.Tensor = torch.softmax(out[:, (self.mem_size + 2): -1], dim=-1)\n",
    "        sharp_factor: torch.Tensor = 1 + torch.exp(out[:, -1])\n",
    "        return (read_key, key_strenght, interpolation_gate, shift_weighting, sharp_factor)\n",
    "\n",
    "    \n",
    "    def forward(self, h: torch.Tensor):\n",
    "        q_key, key_strength, interpolation_gate, shift_weighting, sharp_factor = self.get_params(h)\n",
    "        weight: torch.Tensor = self.get_weight(q_key, key_strength, interpolation_gate, shift_weighting, sharp_factor)\n",
    "        read_vec: torch.Tensor = torch.sum(weight * self.membank.data, dim=0)\n",
    "        return read_vec\n",
    "\n",
    "\n",
    "class WriteHead(Head):\n",
    "    def __init__(self, in_dim, membank, sim_func):\n",
    "        super().__init__(in_dim, membank, sim_func)\n",
    "        self.out_size: int = self.mem_size * 3 + 1 + 1 + self.num_mem_locations + 1\n",
    "        self.layer_1 = torch.nn.Linear(in_features=in_dim, out_features=self.out_size)\n",
    "\n",
    "    def forward(self, h: torch.Tensor):\n",
    "        params: Tuple[torch.Tensor, ...] = self.get_params(h)\n",
    "        erase_vec: torch.Tensor = params[0]\n",
    "        add_vec: torch.Tensor = params[1]\n",
    "        q_key: torch.Tensor = params[2]\n",
    "        key_strength: torch.Tensor = params[3]\n",
    "        interpolation_gate: torch.Tensor  = params[4]\n",
    "        shift_weighting: torch.Tensor  = params[5]\n",
    "        sharp_factor: torch.Tensor = params[6]\n",
    "        weight: torch.Tensor = self.get_weight(q_key, key_strength, interpolation_gate, shift_weighting, sharp_factor)\n",
    "        self.membank.update(weight, erase_vec, add_vec)\n",
    "        return None\n",
    "\n",
    "    def get_params(self, h: torch.Tensor) -> Tuple[torch.Tensor, ...]:\n",
    "        out: torch.Tensor = self.layer_1(h)\n",
    "        erase_vec: torch.Tensor = torch.nn.functional.sigmoid(out[:, :self.mem_size])\n",
    "        add_vec: torch.Tensor = out[:, self.mem_size: 2*self.mem_size]\n",
    "        qkey: torch.Tensor = out[:, self.mem_size: 3*self.mem_size]\n",
    "        key_strength: torch.Tensor = torch.exp(out[:, 3*self.mem_size])\n",
    "        interpolation_gate: torch.Tensor = torch.nn.Sigmoid(out[:, 3*self.mem_size + 1])\n",
    "        shift_weighting: torch.Tensor = torch.softmax(out[:, (3*self.mem_size + 2): -1], dim=-1)\n",
    "        sharp_factor: torch.Tensor = 1 + torch.exp(out[:, -1])\n",
    "        return (erase_vec, add_vec, qkey, key_strength, interpolation_gate, shift_weighting, sharp_factor)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "copy_dataset = None\n",
    "sort_dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# copy dataset \n",
    "copy_vecs = torch.randint(2, (1000, 8))\n",
    "copy_vecs[1, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class CopyDataset(Dataset):\n",
    "    def __init__(self, len, delim: int=-1):\n",
    "        super().__init__()\n",
    "        self.delim = delim\n",
    "        self.data = torch.randint(2, (len, 8))\n",
    "        self.data = torch.column_stack([self.data, torch.ones(self.data.size()[0]) * self.delim])\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        return (self.data[idx, :], self.data[idx, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.,  0.,  0.,  0.,  1.,  1.,  1.,  0., -1.]),\n",
       " tensor([0., 0., 0., 0., 1., 1., 1., 0.]))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cop_data = CopyDataset(10)\n",
    "cop_data[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for each head, be it a read or write head, the addressing mechanism is implemented\n",
    "=> for each head, the controller needs to produce\n",
    "  + key vector $k_t$\n",
    "  + key strength $\\beta_t$\n",
    "  + interpolation gate $g_t$\n",
    "  + shift weighting $s_t$\n",
    "  + sharpening factor $\\gamma_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"Tensor\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m3\u001b[39m, (\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m3\u001b[39m, (\u001b[38;5;241m3\u001b[39m,))\n\u001b[0;32m----> 3\u001b[0m x, y, torch\u001b[38;5;241m.\u001b[39mcat(\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"Tensor\") to list"
     ]
    }
   ],
   "source": [
    "x = torch.randint(3, (5,3))\n",
    "y = torch.randint(3, (3,))\n",
    "x, y, torch.cat([x] + y, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class A:\n",
    "    def __init__(self, a,b,c):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "        self.ls = [a,b,c]\n",
    "\n",
    "class B:\n",
    "    def __init__(self, candA: A):\n",
    "        self.candA = candA\n",
    "    \n",
    "    def print(self):\n",
    "        print(self.candA.ls)\n",
    "\n",
    "    def mod(self, v):\n",
    "        self.candA.a = v\n",
    "        self.candA.ls.append(self.candA.a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[1, 2, 3]\n",
      "[1, 2, 3, 234]\n",
      "[1, 2, 3, 234]\n",
      "[1, 2, 3, 234, 888]\n",
      "[1, 2, 3, 234, 888]\n"
     ]
    }
   ],
   "source": [
    "a = A(1,2,3)\n",
    "b = B(a)\n",
    "c = B(a)\n",
    "\n",
    "b.print()\n",
    "c.print()\n",
    "\n",
    "b.mod(234)\n",
    "b.print()\n",
    "c.print()\n",
    "c.mod(888)\n",
    "b.print()\n",
    "c.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[6.1253, 8.4934, 8.1074, 7.6174, 6.4826],\n",
       "         [1.5601, 2.4506, 4.0441, 3.1744, 2.9900],\n",
       "         [6.9845, 0.4665, 0.5360, 4.0491, 7.3489],\n",
       "         [2.4286, 5.8532, 2.7234, 1.9773, 5.1179],\n",
       "         [0.7154, 6.6040, 1.2617, 8.1936, 1.4594],\n",
       "         [9.3102, 0.7271, 2.4330, 6.2105, 4.5922],\n",
       "         [4.8239, 6.0186, 2.6772, 4.6056, 7.0927],\n",
       "         [9.7810, 4.5565, 0.1035, 1.3065, 8.4828],\n",
       "         [4.5577, 1.2025, 5.8060, 4.7937, 5.3212],\n",
       "         [2.6435, 1.2668, 8.5653, 2.6699, 6.4882]]),\n",
       " tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=torch.rand(size=(10,5)) * 10\n",
    "t, torch.nn.functional.softmax(t, dim=-1).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-2 % 10\n",
    "\n",
    "torch.Tensor([2]) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
