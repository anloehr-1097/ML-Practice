{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 2., 3.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "t = torch.Tensor([1,2,2,3])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_head -> w_t vec weights with N entries \n",
    "# read_vector r_t = convex combination of mem elements with weights = w_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ReadHead(torch.nn.Module):\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "    \n",
    "#     def forward(self, mem_mat: torch.Tensor):\n",
    "#         \"\"\"Emit read vector r_t\"\"\"\n",
    "        \n",
    "#         # product weights w_t\n",
    "#         # emit reaad vector r_t\n",
    "#         weights = torch.rand(mem_mat.size()[0])\n",
    "#         weights = weights[:, None]\n",
    "#         return torch.sum(weights * mem_mat, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class WriteHead(torch.nn.Module):\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#     def forward(self, memmat: torch.Tensor):\n",
    "#         \"\"\"Write head adjusts memory matrix.\"\"\"\n",
    "\n",
    "#         weights = torch.rand(memmat.size()[0])\n",
    "#         v_erase =  torch.rand(memmat.size()[1])\n",
    "#         v_add =  torch.rand(memmat.size()[1])\n",
    "#         weights = weights[:, None]\n",
    "#         ones = torch.ones(memmat.size()[1])\n",
    "#         for i in range(len(memmat.size()[0])):\n",
    "#             memmat[i, :] = memmat[i, :] * (ones - weights[i] * v_erase)\n",
    "#             memmat[i, :] = memmat[i, :] * (ones - weights[i] * v_add)\n",
    "\n",
    "#         return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free params controller\n",
    "SIZE_OF_MEM = 10\n",
    "NUM_READ_HEADS = 5\n",
    "NUM_WRITE_HEADS = 5\n",
    "LOC_SHIFT_RANGE = list(range(1,6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "class MemoryBank(torch.nn.Module):\n",
    "    def __init__(self, num_vectors: int, vec_dim: int):\n",
    "        super(MemoryBank, self).__init__()\n",
    "        self.num_vec = num_vectors\n",
    "        self.vec_dim = vec_dim\n",
    "        self.batch_size: Optional[int] = None\n",
    "        self.data: torch.Tensor | None = None\n",
    "\n",
    "    def init_state(self, batch_size, device):\n",
    "        self.batch_size = batch_size\n",
    "        self.data = torch.zeros(batch_size, self.num_vec, self.vec_dim).to(device)\n",
    "\n",
    "    def update(self, weight: torch.Tensor, erase_vec: torch.Tensor, add_vec: torch.Tensor):\n",
    "        # make sure that batch_dim of tensor is indeed self.batch_dim\n",
    "        # TODO check if dims are ok and batch dim is considered correctly\n",
    "        erase_row_stack = erase_vec.repeat(erase_vec.shape[0], 1, self.num_vec).reshape(erase_vec.shape[0], erase_vec.shape[1], -1)\n",
    "        erase_row_stack *= weight\n",
    "        self.data -= erase_row_stack\n",
    "        self.data += weight*add_vec.repeat(add_vec.shape[0], add_vec.shape[1], self.num_vec).reshape(add_vec.shape[0], add_vec.shape[1], -1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.ones([2,3])\n",
    "t.repeat(1, 3, 1).reshape(t.shape[0], t.shape[1], -1)\n",
    "# torch.ones(size=(3,2))\n",
    "# t, t.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "class NeuralNetController(torch.nn.Module):\n",
    "    \"\"\"Some kind of Recurrent net or feedforward net.\"\"\"\n",
    "    # typically LSTM\n",
    "    def __init__(self, in_size:int=50, h_size: int=20, num_layers: int=1, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.hidden_size: int = h_size\n",
    "        self.inp_size: int = in_size\n",
    "        self.num_lstm_cells: int = num_layers\n",
    "        self.lstm_cell_0 = torch.nn.LSTMCell(\n",
    "            input_size=self.inp_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            )\n",
    "        # self.memory_bank: MemoryBank = MemoryBank(10, h_size)\n",
    "\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, hidden_state: torch.Tensor, cell_state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"product hidden vector of size self.hidden_size from input with size self.inp_size.\"\"\"\n",
    "        # outputs k_t, beta_t, g_t, s_t, \\gamma_t\n",
    "        return self.lstm_cell_0(x, (hidden_state, cell_state))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Combinator(torch.nn.Module):\n",
    "    \"\"\"Combine output of controller and memory access to make final prediction.\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim: int, read_vec_dim: int=10, num_read_heads: int=1, out_dim: int=10):\n",
    "        \"\"\"Default out dim for copy task: 0 through 9.\"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_dim: int = hidden_dim\n",
    "        self.read_vec_dim: int = read_vec_dim\n",
    "        self.num_read_heads: int = num_read_heads\n",
    "        self.layer_1: torch.nn.Linear = torch.nn.Linear(\n",
    "            self.hidden_dim + self.read_vec_dim * self.num_read_heads,\n",
    "            out_dim\n",
    "            )\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return torch.argmax(torch.nn.Softmax(self.layer_1(x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable\n",
    "class NeuralTuringMachine(torch.nn.Module):\n",
    "    \"\"\"Neural Turing Machine.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_size: int,\n",
    "            out_size: int,\n",
    "            hidden_size: int,\n",
    "            num_mem_vectors: int,\n",
    "            mem_vect_dim: int,\n",
    "            num_read_heads: int,\n",
    "            num_write_heads: int,\n",
    "            num_lstm_layers: int,\n",
    "            sim_func: Callable\n",
    "            ):\n",
    "        super().__init__()\n",
    "        self.inp_size: int = in_size  # input seq dim of each element\n",
    "        self.out_dim: int = out_size # output seq dim of each element\n",
    "        self.hidden_size: int = hidden_size\n",
    "        self.num_mem_vectors: int = num_mem_vectors\n",
    "        self.vect_dim: int = mem_vect_dim\n",
    "        self.num_read_heads: int = num_read_heads\n",
    "        self.num_write_heads: int = num_write_heads\n",
    "        self.membank: MemoryBank = MemoryBank(self.num_mem_vectors, self.vect_dim)\n",
    "        self.controller: NeuralNetController = NeuralNetController(\n",
    "            self.inp_size,\n",
    "            self.hidden_size,\n",
    "            num_layers=num_lstm_layers\n",
    "            )\n",
    "        self.sim_func: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] = sim_func\n",
    "        self.combinator: Combinator = Combinator(\n",
    "            self.hidden_size,\n",
    "            self.vect_dim,\n",
    "            num_read_heads=self.num_read_heads,\n",
    "            out_dim=self.out_dim\n",
    "            )\n",
    "\n",
    "        self.read_heads: List[ReadHead] = [ReadHead(self.hidden_size, self.membank, sim_func=self.sim_func)]\n",
    "        self.write_heads: List[WriteHead] = [WriteHead(self.hidden_size, self.membank, sim_func=self.sim_func)]\n",
    "\n",
    "\n",
    "    def read(self):\n",
    "        pass\n",
    "\n",
    "    def write(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"Tensor of t entries, each entry with dim inp_size.\"\"\"\n",
    "\n",
    "        h_t: torch.Tensor = torch.randn(x.shape[0], self.hidden_size)\n",
    "        c_t: torch.Tensor = torch.randn(x.shape[0], self.hidden_size)\n",
    "        read_vecs: List[torch.Tensor]\n",
    "        out_vals: List = []\n",
    "        # read_vecs: List[torch.Tensor] = [read_head.forward(h_t) for read_head in self.read_heads]\n",
    "\n",
    "        for i in range(x.shape[1]):\n",
    "            # always expect batch dim\n",
    "            h_t, c_t = self.controller.forward(x[:, i, :], h_t, c_t)\n",
    "\n",
    "            read_vecs = [read_head.forward(h_t) for read_head in self.read_heads]\n",
    "            read_vec = torch.concat(read_vecs)\n",
    "            out_vals.append(self.combinator.forward(torch.concat([h_t, read_vec])))\n",
    "            # update current weights for writing\n",
    "            for write_head in  self.write_heads:\n",
    "                write_head.get_weight()\n",
    "\n",
    "            # update memory \n",
    "            for write_head in  self.write_heads:\n",
    "                write_head.forward(h_t)\n",
    "\n",
    "        return torch.Tensor(out_vals)\n",
    "            \n",
    "\n",
    "    def __call__(self, x: torch.Tensor):\n",
    "        return self.forward(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "from typing import Tuple, Callable\n",
    "from dataclasses import dataclass\n",
    "\n",
    "class Head(torch.nn.Module):\n",
    "    \"\"\"produce key, key_strength, interpolation_gate, shift_weighting, sharpening_factor.\n",
    "\n",
    "    key in mathbb{R}^{mem_size}\n",
    "    key_strength in mathbb{R}_{+} how sharp the attention to memory locations should be\n",
    "    interpolation_gate in (0,1) how much of last weight should be retained\n",
    "    shift_weighting in mathbb{R}^{num_mem_locations} prob distribution over the num locations\n",
    "    sharpening_factor in [1, infty)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim: int,\n",
    "                 membank: MemoryBank,\n",
    "                 sim_func: Callable[[torch.Tensor, torch.Tensor], torch.Tensor]\n",
    "                ):\n",
    "\n",
    "        self.membank = membank\n",
    "        self.num_mem_locations = self.membank.num_vec\n",
    "        self.sim_func = sim_func\n",
    "        self.mem_size = self.membank.vec_dim\n",
    "        self.current_weight: torch.Tensor = torch.ones(self.num_mem_locations) / self.num_mem_locations # size = num_mem_locations\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_params(self):\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        ...\n",
    "\n",
    "    def get_weight(\n",
    "        self,\n",
    "        k: torch.Tensor,\n",
    "        ks: torch.Tensor,\n",
    "        ig: torch.Tensor,\n",
    "        sw: torch.Tensor,\n",
    "        sf: torch.Tensor\n",
    "        ):\n",
    "        loc_similarity: torch.Tensor = self.sim_func(k, self.membank.data)\n",
    "        loc_weight: torch.Tensor = torch.nn.functional.softmax(ks * loc_similarity, dim=-1)\n",
    "        gated_weighting: torch.Tensor = (1 - ig) * self.current_weight + ig * loc_weight\n",
    "        compound_weight: torch.Tensor = torch.zeros(self.num_mem_locations)\n",
    "\n",
    "        # TODO implement convolution without for loops\n",
    "        for i in range(self.num_mem_locations):\n",
    "            for j in range(self.num_mem_locations):\n",
    "                compound_weight[i] += gated_weighting[j] * sw[(i-j)%self.num_mem_locations] \n",
    "\n",
    "        sharpened_weight: torch.Tensor = torch.pow(compound_weight, sf)\n",
    "        weight = sharpened_weight / torch.sum(sharpened_weight)\n",
    "        self.current_weight = weight\n",
    "        return weight\n",
    "\n",
    "\n",
    "class ReadHead(Head):\n",
    "    def __init__(self, in_dim, membank, sim_func):\n",
    "        super().__init__(in_dim, membank, sim_func)\n",
    "        self.out_size = self.mem_size + 1 + 1 + self.num_mem_locations + 1\n",
    "        self.layer_1 = torch.nn.Linear(in_features=in_dim, out_features=self.out_size)\n",
    "\n",
    "    def get_params(self, h: torch.Tensor) -> Tuple[torch.Tensor, ...]:\n",
    "        out: torch.Tensor = self.layer_1(h)\n",
    "        read_key: torch.Tensor = out[:, 0 : self.mem_size]\n",
    "        key_strenght: torch.Tensor = torch.exp(out[:, self.mem_size])\n",
    "        interpolation_gate: torch.Tensor = torch.nn.Sigmoid(out[:, self.mem_size + 1])\n",
    "        shift_weighting: torch.Tensor = torch.softmax(out[:, (self.mem_size + 2): -1], dim=-1)\n",
    "        sharp_factor: torch.Tensor = 1 + torch.exp(out[:, -1])\n",
    "        return (read_key, key_strenght, interpolation_gate, shift_weighting, sharp_factor)\n",
    "\n",
    "    \n",
    "    def forward(self, h: torch.Tensor):\n",
    "        q_key, key_strength, interpolation_gate, shift_weighting, sharp_factor = self.get_params(h)\n",
    "        weight: torch.Tensor = self.get_weight(q_key, key_strength, interpolation_gate, shift_weighting, sharp_factor)\n",
    "        read_vec: torch.Tensor = torch.sum(weight * self.membank.data, dim=0)\n",
    "        return read_vec\n",
    "\n",
    "\n",
    "class WriteHead(Head):\n",
    "    def __init__(self, in_dim, membank, sim_func):\n",
    "        super().__init__(in_dim, membank, sim_func)\n",
    "        self.out_size: int = self.mem_size * 3 + 1 + 1 + self.num_mem_locations + 1\n",
    "        self.layer_1 = torch.nn.Linear(in_features=in_dim, out_features=self.out_size)\n",
    "\n",
    "    def forward(self, h: torch.Tensor):\n",
    "        params: Tuple[torch.Tensor, ...] = self.get_params(h)\n",
    "        erase_vec: torch.Tensor = params[0]\n",
    "        add_vec: torch.Tensor = params[1]\n",
    "        q_key: torch.Tensor = params[2]\n",
    "        key_strength: torch.Tensor = params[3]\n",
    "        interpolation_gate: torch.Tensor  = params[4]\n",
    "        shift_weighting: torch.Tensor  = params[5]\n",
    "        sharp_factor: torch.Tensor = params[6]\n",
    "        # weight: torch.Tensor = self.get_weight(q_key, key_strength, interpolation_gate, shift_weighting, sharp_factor)\n",
    "        self.membank.update(self.current_weight, erase_vec, add_vec)\n",
    "        return None\n",
    "\n",
    "    def get_params(self, h: torch.Tensor) -> Tuple[torch.Tensor, ...]:\n",
    "        out: torch.Tensor = self.layer_1(h)\n",
    "        erase_vec: torch.Tensor = torch.nn.functional.sigmoid(out[:, :self.mem_size])\n",
    "        add_vec: torch.Tensor = out[:, self.mem_size: 2*self.mem_size]\n",
    "        qkey: torch.Tensor = out[:, self.mem_size: 3*self.mem_size]\n",
    "        key_strength: torch.Tensor = torch.exp(out[:, 3*self.mem_size])\n",
    "        interpolation_gate: torch.Tensor = torch.nn.Sigmoid(out[:, 3*self.mem_size + 1])\n",
    "        shift_weighting: torch.Tensor = torch.softmax(out[:, (3*self.mem_size + 2): -1], dim=-1)\n",
    "        sharp_factor: torch.Tensor = 1 + torch.exp(out[:, -1])\n",
    "        return (erase_vec, add_vec, qkey, key_strength, interpolation_gate, shift_weighting, sharp_factor)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "copy_dataset = None\n",
    "sort_dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 0, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# copy dataset \n",
    "copy_vecs = torch.randint(2, (1000, 8))\n",
    "copy_vecs[1, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class CopyDataset(Dataset):\n",
    "    def __init__(self, len, delim: int=-1):\n",
    "        super().__init__()\n",
    "        self.delim = delim\n",
    "        self.data = torch.randint(2, (len, 8))\n",
    "        self.data = torch.column_stack([self.data, torch.ones(self.data.size()[0]) * self.delim])\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        return (self.data[idx, :], self.data[idx, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.,  0.,  1.,  0.,  1.,  1.,  0.,  0., -1.]),\n",
       " tensor([0., 0., 1., 0., 1., 1., 0., 0.]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cop_data = CopyDataset(10)\n",
    "cop_data[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for each head, be it a read or write head, the addressing mechanism is implemented\n",
    "=> for each head, the controller needs to produce\n",
    "  + key vector $k_t$\n",
    "  + key strength $\\beta_t$\n",
    "  + interpolation gate $g_t$\n",
    "  + shift weighting $s_t$\n",
    "  + sharpening factor $\\gamma_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"Tensor\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m3\u001b[39m, (\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m3\u001b[39m, (\u001b[38;5;241m3\u001b[39m,))\n\u001b[0;32m----> 3\u001b[0m x, y, torch\u001b[38;5;241m.\u001b[39mcat(\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"Tensor\") to list"
     ]
    }
   ],
   "source": [
    "x = torch.randint(3, (5,3))\n",
    "y = torch.randint(3, (3,))\n",
    "x, y, torch.cat([x] + y, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class A:\n",
    "    def __init__(self, a,b,c):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "        self.ls = [a,b,c]\n",
    "\n",
    "class B:\n",
    "    def __init__(self, candA: A):\n",
    "        self.candA = candA\n",
    "    \n",
    "    def print(self):\n",
    "        print(self.candA.ls)\n",
    "\n",
    "    def mod(self, v):\n",
    "        self.candA.a = v\n",
    "        self.candA.ls.append(self.candA.a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[1, 2, 3]\n",
      "[1, 2, 3, 234]\n",
      "[1, 2, 3, 234]\n",
      "[1, 2, 3, 234, 888]\n",
      "[1, 2, 3, 234, 888]\n"
     ]
    }
   ],
   "source": [
    "a = A(1,2,3)\n",
    "b = B(a)\n",
    "c = B(a)\n",
    "\n",
    "b.print()\n",
    "c.print()\n",
    "\n",
    "b.mod(234)\n",
    "b.print()\n",
    "c.print()\n",
    "c.mod(888)\n",
    "b.print()\n",
    "c.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5.2178, 1.9274, 1.1395, 6.4742, 5.1977],\n",
       "         [2.7616, 3.0651, 1.0750, 6.6164, 2.5470],\n",
       "         [3.1665, 6.1073, 1.9395, 6.3281, 9.4515],\n",
       "         [7.6308, 5.6517, 2.6815, 1.7194, 8.4667],\n",
       "         [0.6252, 2.7191, 8.2259, 3.9783, 2.7316],\n",
       "         [1.6349, 4.1056, 2.0810, 3.2715, 4.2814],\n",
       "         [7.0307, 6.4599, 4.1481, 4.1009, 9.3166],\n",
       "         [2.1597, 5.4950, 1.7843, 8.0205, 2.7984],\n",
       "         [1.7626, 6.8268, 3.1274, 4.3092, 9.9576],\n",
       "         [2.6819, 1.9981, 8.2281, 0.5521, 2.0147]]),\n",
       " tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=torch.rand(size=(10,5)) * 10\n",
    "t, torch.nn.functional.softmax(t, dim=-1).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-2 % 10\n",
    "\n",
    "torch.Tensor([2]) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
