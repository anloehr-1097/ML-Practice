{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 9,
>>>>>>> f633652cb673a0ee980853e0855a256bcab7b7ee
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Andy/.local/share/virtualenvs/KompAKI-Backend-CwZxqLfI/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/Andy/.local/share/virtualenvs/KompAKI-Backend-CwZxqLfI/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple, Optional, Dict, List\n",
    "import random\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torchvision.transforms.functional import pil_to_tensor\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 10,
>>>>>>> f633652cb673a0ee980853e0855a256bcab7b7ee
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose([\n",
    "    # v2.ToTensor(),\n",
    "    v2.ToImage(), \n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    #v2.Lambda(lambda x: x.view(-1) - 0.5),\n",
    "])\n",
    "\n",
    "mnist_data =  MNIST(root=\".\", download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 11,
>>>>>>> f633652cb673a0ee980853e0855a256bcab7b7ee
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
<<<<<<< HEAD
     "execution_count": 5,
=======
     "execution_count": 11,
>>>>>>> f633652cb673a0ee980853e0855a256bcab7b7ee
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist_data)\n",
    "sample = mnist_data[0]\n",
    "MNIST_SIZE = sample[0].size()[1:]\n",
    "\n",
    "MNIST_SIZE\n",
    "# display(sample[0]), print(sample[1]), MNIST_SIZE"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
=======
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128 \n",
>>>>>>> f633652cb673a0ee980853e0855a256bcab7b7ee
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    mnist_data, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_img(img: \"Image\"):\n",
    "    intermediate_img: Tensor = pil_to_tensor(img)\n",
    "    return intermediate_img / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger = logging.getLogger(\"This\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# TODO change all layer to torch.nn.LAYERTYPE\n",
    "\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.flatten_inp: torch.nn.Flatte = torch.nn.Flatten()\n",
    "        # self.inp_layer: Tensor =  torch.rand(size=(MNIST_SIZE[0] * MNIST_SIZE[1], 20)).to(float)  # hidden dim=20\n",
    "        self.inp_layer: torch.nn.Linear =  torch.nn.Linear(in_features=MNIST_SIZE[0] * MNIST_SIZE[1], out_features=20)\n",
    "        # self.bias1: Tensor = torch.rand(20)\n",
    "        # self.layer2_mu: Tensor = torch.rand(size=(20, 5)).to(float)\n",
    "        self.layer2_mu: torch.nn.Linear = torch.nn.Linear(in_features=20, out_features=5)\n",
    "        # self.layer2_sigma: Tensor = torch.rand(size=(20, 5)).to(float)\n",
    "        self.layer2_sigma: torch.nn.Linear = torch.nn.Linear(in_features=20, out_features=5)\n",
    "        # self.bias2_mu: Tensor = torch.rand(size=(1, 5))\n",
    "        # self.bias2_sigma: Tensor = torch.rand(size=(1, 5))\n",
    "        self.distr: Optional[torch.distributions.Normal] = None\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        return None\n",
    "\n",
    "    def __forward__(self, x: Tensor):\n",
    "        # encoder returns mu and log sigma^2\n",
    "        x = self.flatten_inp(x)\n",
    "        x = self.inp_layer(x)\n",
    "        x = self.tanh(x)\n",
    "        mu = self.layer2_mu(x)\n",
    "        log_sigma_square = self.layer2_sigma(x)\n",
    "        return (mu, log_sigma_square)\n",
    "\n",
    "    def __call__(self, x: Tensor):\n",
    "        return self.__forward__(x)\n",
    "\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        # 5 -> 100\n",
    "        # 100 -> 784\n",
    "        self.inp_layer: torch.nn.Linear = torch.nn.Linear(in_features=5, out_features=100)\n",
    "        self.layer_1_mu: torch.nn.Linear = torch.nn.Linear(in_features=100, out_features=784)\n",
    "        self.layer_1_sigma: torch.nn.Linear = torch.nn.Linear(in_features=100, out_features=784)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        return None\n",
    "        \n",
    "    def __forward__(self, x: Tensor):\n",
    "        logger.info(x.size())\n",
    "        x = self.inp_layer(x)\n",
    "        logger.info(x.size())\n",
    "        x = self.tanh(x)\n",
    "        logger.info(x.size())\n",
    "        mu = self.layer_1_mu(x)\n",
    "        log_sigma_square = self.layer_1_sigma(x)\n",
    "        logger.info((mu.size(), log_sigma_square.size()))\n",
    "        return (mu, log_sigma_square)\n",
    "\n",
    "    def __call__(self, x: Tensor):\n",
    "        return self.__forward__(x)\n",
    "\n",
    "class VAE(torch.nn.Module):\n",
    "    def __init__(self, enc: Encoder, dec: Decoder):\n",
    "        super(VAE, self).__init__()\n",
    "        self.enc = enc\n",
    "        self.dec = dec\n",
    "\n",
    "    def __forward__(self, x: Tensor) -> Tensor:\n",
    "        mu_enc: Tensor\n",
    "        log_sigma_square_enc: Tensor\n",
    "        mu_dec: Tensor\n",
    "        log_sigma_square_dec: Tensor\n",
    "\n",
    "        mu_enc, log_sigma_square_enc=  self.enc(x)\n",
    "        # latent_norm_enc: Dict[str, Tensor] = {\"loc\": x_enc[0], \"scale\": torch.exp(x_enc[1])}\n",
    "        latent_distr: torch.distributions.MultivariateNormal = torch.distributions.MultivariateNormal(\n",
    "            loc=mu_enc,\n",
    "            covariance_matrix=torch.diag_embed(torch.exp(log_sigma_square_enc))\n",
    "            # covariance_matrix=torch.eye(mu_enc.numel())*torch.exp(log_sigma_square_enc)\n",
    "        )\n",
    "        z: Tensor = latent_distr.rsample()\n",
    "        \n",
    "        mu_dec, log_sigma_square_dec = self.dec(z)\n",
    "        reconstr_distr: torch.distributions.MultivariateNormal = torch.distributions.MultivariateNormal(\n",
    "            loc=mu_dec,\n",
    "            # covariance_matrix=torch.eye(mu_dec.numel())*torch.exp(log_sigma_square_dec)\n",
    "            covariance_matrix=torch.diag_embed(torch.exp(log_sigma_square_dec))\n",
    "        )\n",
    "        reconstr_var: Tensor = reconstr_distr.rsample()\n",
    "        return reconstr_var\n",
    "\n",
    "    def encode(self, x: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        # TODO rework\n",
    "        mu: Tensor\n",
    "        sigma: Tensor\n",
    "        mu, sigma = self.enc(x)\n",
    "        distr: torch.distributions.MultivariateNormal = \\\n",
    "            torch.distributions.MultivariateNormal(\n",
    "                loc=mu,\n",
    "                covariance_matrix=torch.eye(mu.numel())*sigma.pow(2)\n",
    "                )\n",
    "        return distr.rsample()\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        # TODO rework\n",
    "        mu: Tensor\n",
    "        sigma: Tensor\n",
    "        mu, sigma = self.dec(z)\n",
    "        distr: torch.distributions.MultivariateNormal = \\\n",
    "            torch.distributions.MultivariateNormal(\n",
    "                loc=mu,\n",
    "                covariance_matrix=torch.eye(mu.numel())*sigma.pow(2)\n",
    "                )\n",
    "        return distr.rsample()\n",
    "\n",
    "    def __call__(self, x: Tensor):\n",
    "        return self.__forward__(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = Encoder()\n",
    "some_index: int = random.randint(0, len(mnist_data))\n",
    "img_tens = list(iter(mnist_data))[some_index][0]\n",
    "img_tens.dtype, img_tens.size()\n",
    "\n",
    "\n",
    "data_iter = iter(train_loader)\n",
    "sample_batch = next(data_iter)\n",
    "sample_batch[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = enc(sample_batch[0])\n",
    "mu, sigma = y\n",
    "# z = torch.distributions.MultivariateNormal(mu, torch.eye(mu.numel())*sigma.pow(2)).rsample()\n",
    "z = torch.distributions.MultivariateNormal(mu, torch.diag_embed(torch.exp(sigma))).rsample()\n",
    "z.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3171, -0.4104, -0.0928, -0.0110,  0.0233]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y  = enc(img_tens)\n",
    "mu, sigma = y\n",
    "z = torch.distributions.MultivariateNormal(mu, torch.eye(mu.numel())*sigma.pow(2)).rsample()\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 784]) torch.Size([1, 784])\n"
     ]
    }
   ],
   "source": [
    "dec = Decoder()\n",
    "mu_x, sigma_x = dec(z)\n",
    "print(mu_x.size(), sigma_x.size())\n",
    "rec_sample = torch.distributions.MultivariateNormal(mu_x, torch.eye(mu_x.numel())*sigma_x.pow(2)).rsample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(enc, dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12728dee0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# !conda install -y matplotlib\n",
    "# display(rec_sample)\n",
    "plt.imshow(img_tens[0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12748f1c0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.imshow(torch.reshape(rec_sample, (28,28)).detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbo(x_i, mu: Tensor, log_sigma_square: Tensor, log_prob: Tensor):\n",
    "    # mu is mu parametrized by encoder\n",
    "    # sigma is variance as parametrized by encoder \n",
    "    # log prob is log p_{\\theta}(x^{i} \\mid z^{i}) or a Monte Carlo estimate based on this\n",
    "    # where log p ... is just the pdf evald\n",
    "    loss: Tensor = Tensor([0])\n",
    "    ones: Tensor = torch.ones(log_sigma_square.size())\n",
    "    mu_square: Tensor = mu.pow(2)\n",
    "    intermed: Tensor = ones + log_sigma_square - mu_square - log_sigma_square.exp()\n",
    "    intermed = intermed.sum(dim=1)\n",
    "    final = torch.mean(- 1/2 * intermed + log_prob)\n",
    "\n",
    "    print(f\"Loss size: {loss.size()}\")\n",
    "    print(f\"log_sigma_square size: {log_sigma_square.size()}\")\n",
    "    print(f\"mu_square size: {mu_square.size()}\")\n",
    "    print(f\"log_prob size: {log_prob.size()}\")\n",
    "    print(f\"intermed size: {intermed.size()}\")\n",
    "    print(f\"final size: {final.size()}\")\n",
    "    print(f\"final: {final}\")\n",
    "\n",
    "    # print(loss.size(), log_sigma_square.size(), mu_square.size(), log_prob.size(), intermed.size(), final.size())\n",
    "    # sigma_square: Tensor = torch.square(sigma)\n",
    "    # return 1/2 * torch.mean((torch.sum(ones + log_sigma_square - mu_square - torch.exp(log_sigma_square)))+ log_prob)\n",
    "    # return 1/2 * torch.mean(intermed + log_prob)\n",
    "    return final\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(x, x_hat, mu: Tensor, log_sigma_square: Tensor):\n",
    "    # mu is mu parametrized by encoder\n",
    "    # sigma is variance as parametrized by encoder \n",
    "    # log prob is log p_{\\theta}(x^{i} \\mid z^{i}) or a Monte Carlo estimate based on this\n",
    "    # where log p ... is just the pdf evald\n",
    "    # rep_loss = torch.nn.functional.binary_cross_entropy(x_hat, x, reduction='sum')\n",
    "    rep_loss = torch.nn.functional.mse_loss(x, x_hat, reduce='sum')\n",
    "\n",
    "    loss: Tensor = Tensor([0])\n",
    "    ones: Tensor = torch.ones(log_sigma_square.size())\n",
    "    mu_square: Tensor = mu.pow(2)\n",
    "    # print(loss.size(), log_sigma_square.size(), mu_square.size(), log_prob.size())\n",
    "    # sigma_square: Tensor = torch.square(sigma)\n",
    "    return torch.mean(- 1/2 * torch.sum(ones + log_sigma_square - mu_square - log_sigma_square.exp())) + rep_loss\n",
    "    \n",
    "\n",
    "# reproduction_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='sum')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder()\n",
    "dec = Decoder()\n",
    "vae = VAE(enc, dec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old training function step by step\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-2\n",
    "def train(epochs: int, vae: VAE, xs: List[Tensor]):\n",
    "    optim: torch.optim.Adam = torch.optim.Adam(params=vae.parameters(),\n",
    "                                              lr=LEARNING_RATE,\n",
    "                                              weight_decay=WEIGHT_DECAY)\n",
    "    enc: Encoder = vae.enc\n",
    "    dec: Decoder = vae.dec\n",
    "    # NUM_SAMPLES: int = 10\n",
    "    for epoch in range(epochs):\n",
    "        for i, x in enumerate(xs):\n",
    "            optim.zero_grad()\n",
    "            # posterior params\n",
    "            enc_mu_sigma: Tuple[Tensor, Tensor] = enc.__forward__(x)\n",
    "            print(f\"@sample {i}: {enc_mu_sigma[0]}, {enc_mu_sigma[1]}\")\n",
    "            # z = torch.distributions.Normal(loc=enc_mu_sigma[0], scale=torch.exp(enc_mu_sigma[1])).rsample()\n",
    "            z = torch.distributions.MultivariateNormal(loc=enc_mu_sigma[0], covariance_matrix=torch.eye(5)*torch.exp(enc_mu_sigma[1])).rsample()\n",
    "            # reconstr distr params\n",
    "            dec_mu_sigma: Tuple[Tensor, Tensor] = dec.__forward__(z)\n",
    "            rec_distr: torch.distributions.Distribution = torch.distributions.MultivariateNormal(\n",
    "                loc=dec_mu_sigma[0], covariance_matrix=torch.eye(x.size()[0])*torch.exp(dec_mu_sigma[1]))\n",
    "            log_prob: Tensor = rec_distr.log_prob(x)\n",
    "            loss: Tensor = elbo(x, enc_mu_sigma[0], enc_mu_sigma[1], log_prob)\n",
    "            print(f\"Loss @ this stage: {loss}\")\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        print(f\"Finished Iteration no {epoch}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Successful trainign with this training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "def train(epochs: int, vae: VAE, dataloader: torch.utils.data.DataLoader):\n",
    "    optim: torch.optim.Adam = torch.optim.Adam(params=vae.parameters(),\n",
    "                                              lr=LEARNING_RATE,\n",
    "                                              weight_decay=WEIGHT_DECAY)\n",
    "    vae.train()\n",
    "    enc: Encoder = vae.enc\n",
    "    dec: Decoder = vae.dec\n",
    "    # NUM_SAMPLES: int = 10\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (x, target) in enumerate(dataloader):\n",
    "            optim.zero_grad()\n",
    "            # posterior params\n",
    "            enc_mu_sigma: Tuple[Tensor, Tensor] = enc.__forward__(x)\n",
    "            print(f\"@sample {batch_idx}: {enc_mu_sigma[0]}, {enc_mu_sigma[1]}\")\n",
    "            # z = torch.distributions.Normal(loc=enc_mu_sigma[0], scale=torch.exp(enc_mu_sigma[1])).rsample()\n",
    "            z = torch.distributions.MultivariateNormal(\n",
    "                loc=enc_mu_sigma[0], \n",
    "                covariance_matrix=torch.diag_embed(torch.exp(enc_mu_sigma[1]))).rsample()\n",
    "                # covariance_matrix=torch.eye(5)*torch.exp(enc_mu_sigma[1])).rsample()\n",
    "            # reconstr distr params\n",
    "            dec_mu_sigma: Tuple[Tensor, Tensor] = dec.__forward__(z)\n",
    "            rec_distr: torch.distributions.Distribution = torch.distributions.MultivariateNormal(\n",
    "                loc=dec_mu_sigma[0], covariance_matrix=torch.diag_embed(torch.exp(dec_mu_sigma[1])))\n",
    "                # loc=dec_mu_sigma[0], covariance_matrix=torch.eye(x.size()[0])*torch.exp(dec_mu_sigma[1]))\n",
    "            # log_prob: Tensor = rec_distr.log_prob(torch.flatten(x, start_dim=1))\n",
    "            # loss: Tensor = elbo(x, enc_mu_sigma[0], enc_mu_sigma[1], log_prob)\n",
    "            x_hat = torch.sigmoid(rec_distr.rsample())\n",
    "            print(f\"x_hat: {x.min(), x.max(), x_hat.min(), x_hat.max()} \")\n",
    "            loss: Tensor = criterion(x.flatten(start_dim=1), x_hat, enc_mu_sigma[0], enc_mu_sigma[1])\n",
    "\n",
    "            print(f\"Loss @ this stage: {loss}\")\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(vae.parameters(), 1.0)    \n",
    "            optim.step()\n",
    "        print(f\"Finished Iteration no {epoch}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trying with actual reconstruction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 0\n",
    "def train(epochs: int, vae: VAE, dataloader: torch.utils.data.DataLoader):\n",
    "    optim: torch.optim.Adam = torch.optim.Adam(params=vae.parameters(),\n",
    "                                              lr=LEARNING_RATE,\n",
    "                                              weight_decay=WEIGHT_DECAY)\n",
    "    vae.train()\n",
    "    enc: Encoder = vae.enc\n",
    "    dec: Decoder = vae.dec\n",
    "    # NUM_SAMPLES: int = 10\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (x, target) in enumerate(dataloader):\n",
    "            optim.zero_grad()\n",
    "            # posterior params\n",
    "            enc_mu_sigma: Tuple[Tensor, Tensor] = enc.__forward__(x)\n",
    "            print(f\"@sample {batch_idx}: {enc_mu_sigma[0]}, {enc_mu_sigma[1]}\")\n",
    "            # z = torch.distributions.Normal(loc=enc_mu_sigma[0], scale=torch.exp(enc_mu_sigma[1])).rsample()\n",
    "            z = torch.distributions.MultivariateNormal(\n",
    "                loc=enc_mu_sigma[0], \n",
    "                covariance_matrix=torch.diag_embed(torch.exp(enc_mu_sigma[1]))).rsample(sample_shape=torch.Size([10]))\n",
    "                # covariance_matrix=torch.eye(5)*torch.exp(enc_mu_sigma[1])).rsample()\n",
    "            # reconstr distr params\n",
    "            dec_mu_sigma: Tuple[Tensor, Tensor] = dec.__forward__(z)\n",
    "            rec_distr: torch.distributions.Distribution = torch.distributions.MultivariateNormal(\n",
    "                loc=dec_mu_sigma[0], covariance_matrix=torch.diag_embed(torch.exp(dec_mu_sigma[1])))\n",
    "                # loc=dec_mu_sigma[0], covariance_matrix=torch.eye(x.size()[0])*torch.exp(dec_mu_sigma[1]))\n",
    "            log_prob: Tensor = - rec_distr.log_prob(torch.flatten(x, start_dim=1)).mean(dim=0)\n",
    "            loss: Tensor = elbo(x, enc_mu_sigma[0], enc_mu_sigma[1], log_prob)\n",
    "            # x_hat = torch.sigmoid(rec_distr.rsample())\n",
    "            # loss: Tensor = criterion(x.flatten(start_dim=1), x_hat, enc_mu_sigma[0], enc_mu_sigma[1])\n",
    "            # print(f\"x_hat: {x.min(), x.max(), x_hat.min(), x_hat.max()} \")\n",
    "\n",
    "            print(f\"Loss @ this stage: {loss}\")\n",
    "            loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(vae.parameters(), 1.0)    \n",
    "            optim.step()\n",
    "        print(f\"Finished Iteration no {epoch}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def train(model, dataloader, optimizer, prev_updates, writer=None):\n",
    "    \"\"\"\n",
    "    Trains the model on the given data.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        dataloader (torch.utils.data.DataLoader): The data loader.\n",
    "        loss_fn: The loss function.\n",
    "        optimizer: The optimizer.\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(tqdm(dataloader)):\n",
    "        n_upd = prev_updates + batch_idx\n",
    "        \n",
    "        data = data.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        \n",
    "        output = model(data)  # Forward pass\n",
    "        loss = output.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if n_upd % 100 == 0:\n",
    "            # Calculate and log gradient norms\n",
    "            total_norm = 0.0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** (1. / 2)\n",
    "        \n",
    "            print(f'Step {n_upd:,} (N samples: {n_upd*batch_size:,}), Loss: {loss.item():.4f} (Recon: {output.loss_recon.item():.4f}, KL: {output.loss_kl.item():.4f}) Grad: {total_norm:.4f}')\n",
    "\n",
    "            if writer is not None:\n",
    "                global_step = n_upd\n",
    "                writer.add_scalar('Loss/Train', loss.item(), global_step)\n",
    "                writer.add_scalar('Loss/Train/BCE', output.loss_recon.item(), global_step)\n",
    "                writer.add_scalar('Loss/Train/KLD', output.loss_kl.item(), global_step)\n",
    "                writer.add_scalar('GradNorm/Train', total_norm, global_step)\n",
    "            \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)    \n",
    "        \n",
    "        optimizer.step()  # Update the model parameters\n",
    "        \n",
    "    return prev_updates + len(dataloader) "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 31,
=======
   "execution_count": 14,
>>>>>>> f633652cb673a0ee980853e0855a256bcab7b7ee
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "@sample 0: tensor([[ 0.0199,  0.1733, -0.0223, -0.1281,  0.0567],\n",
      "        [ 0.0309,  0.2149,  0.0824, -0.1010,  0.1452],\n",
      "        [ 0.0742,  0.0488, -0.1255, -0.2891,  0.0373],\n",
      "        [ 0.1035,  0.1441, -0.1533, -0.1980,  0.0280],\n",
      "        [ 0.1502,  0.0986,  0.0482, -0.2320, -0.0493],\n",
      "        [ 0.0399,  0.1343,  0.1247, -0.1726,  0.0690],\n",
      "        [ 0.0615,  0.1085,  0.1568, -0.2106,  0.0927],\n",
      "        [-0.0574,  0.2317,  0.0856, -0.2829,  0.1223]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.1344, -0.1967, -0.2767,  0.3053, -0.0076],\n",
      "        [-0.0044, -0.0873, -0.1950,  0.1935, -0.0320],\n",
      "        [-0.1468, -0.3180, -0.0566,  0.1327, -0.0716],\n",
      "        [-0.1272, -0.2262, -0.1708,  0.2358,  0.0293],\n",
      "        [-0.2028, -0.2999,  0.0048,  0.1323, -0.1009],\n",
      "        [-0.2242, -0.1413, -0.1156, -0.0083, -0.0594],\n",
      "        [-0.1716, -0.2306, -0.0875,  0.0783,  0.0364],\n",
      "        [-0.1186, -0.2842, -0.1318,  0.1648,  0.0528]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0243, grad_fn=<MinBackward1>), tensor(0.9762, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.9218151569366455\n",
      "@sample 1: tensor([[ 0.0992,  0.3300, -0.0324, -0.0055,  0.0936],\n",
      "        [ 0.0050,  0.1207,  0.1681, -0.2165,  0.1999],\n",
      "        [-0.0256,  0.1097,  0.0531, -0.1798,  0.0788],\n",
      "        [ 0.1026,  0.0922,  0.0628,  0.0165,  0.0725],\n",
      "        [-0.0349,  0.1903,  0.0558, -0.1203,  0.1571],\n",
      "        [ 0.0204,  0.0790,  0.1209, -0.0672,  0.0981],\n",
      "        [ 0.0016,  0.3132,  0.0082, -0.0081,  0.0818],\n",
      "        [ 0.2091,  0.1487, -0.0336,  0.0727,  0.0588]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0625, -0.0052, -0.0222,  0.0074, -0.0613],\n",
      "        [-0.1011, -0.1194, -0.0077, -0.0099, -0.0304],\n",
      "        [-0.0818, -0.2831, -0.1863,  0.2269,  0.0305],\n",
      "        [-0.2093,  0.0359, -0.2479,  0.1991, -0.0805],\n",
      "        [-0.0767, -0.0841, -0.2535,  0.1686,  0.0282],\n",
      "        [-0.1943,  0.0222, -0.2717,  0.1501, -0.0478],\n",
      "        [ 0.0225, -0.0357, -0.1869,  0.1295, -0.0105],\n",
      "        [ 0.0407,  0.1187, -0.0224,  0.0976, -0.1136]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0207, grad_fn=<MinBackward1>), tensor(0.9741, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.7658071517944336\n",
      "@sample 2: tensor([[ 0.0167,  0.0833,  0.1949, -0.0987,  0.0965],\n",
      "        [ 0.1663,  0.2237, -0.1336, -0.0314, -0.0662],\n",
      "        [ 0.0905,  0.1479,  0.1065, -0.1188,  0.0043],\n",
      "        [ 0.0775,  0.2722,  0.0408, -0.1221,  0.1083],\n",
      "        [ 0.0029,  0.2552,  0.0512, -0.0871,  0.1535],\n",
      "        [ 0.0158,  0.2677,  0.0903, -0.0156,  0.0550],\n",
      "        [ 0.0755,  0.1602, -0.0542, -0.1890,  0.0986],\n",
      "        [-0.0666,  0.2089,  0.0777, -0.2468,  0.1157]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0278, -0.1347, -0.1680,  0.1434, -0.0354],\n",
      "        [-0.1278, -0.0108, -0.1295,  0.2147, -0.1553],\n",
      "        [-0.0194, -0.1811, -0.0617,  0.1504, -0.0806],\n",
      "        [-0.0063, -0.1293, -0.0488,  0.0509, -0.0342],\n",
      "        [ 0.0840,  0.0523, -0.0318,  0.0047, -0.1223],\n",
      "        [-0.1896, -0.0914, -0.2075,  0.0273,  0.0124],\n",
      "        [ 0.0078, -0.1199, -0.0553,  0.1786, -0.0252],\n",
      "        [-0.1125, -0.2567, -0.1335,  0.1934,  0.0354]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0173, grad_fn=<MinBackward1>), tensor(0.9905, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.7749624252319336\n",
      "@sample 3: tensor([[ 0.0687,  0.1139, -0.0142, -0.2160,  0.0697],\n",
      "        [ 0.1680,  0.2138,  0.0213, -0.0777,  0.0848],\n",
      "        [ 0.0010,  0.0870,  0.1207, -0.2119,  0.0331],\n",
      "        [ 0.0673,  0.1827,  0.0977, -0.0913,  0.0865],\n",
      "        [ 0.1202, -0.0194,  0.1307, -0.0688, -0.0466],\n",
      "        [ 0.1334,  0.1766,  0.0196, -0.0494, -0.0703],\n",
      "        [ 0.0827,  0.1034,  0.1096,  0.0471,  0.2410],\n",
      "        [ 0.0756,  0.2253,  0.0618, -0.1328,  0.1661]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0816, -0.2494, -0.1869,  0.2378,  0.0393],\n",
      "        [-0.0890, -0.0946,  0.0376,  0.0952, -0.1891],\n",
      "        [-0.1487, -0.2139,  0.0480,  0.1202, -0.0582],\n",
      "        [ 0.0038, -0.1021, -0.1143,  0.0985,  0.0252],\n",
      "        [-0.1295, -0.2985,  0.0369,  0.1278, -0.0888],\n",
      "        [-0.0204, -0.0075, -0.0028,  0.0802, -0.1163],\n",
      "        [-0.0524,  0.0301, -0.2149,  0.1675, -0.0262],\n",
      "        [-0.0311, -0.1551, -0.2224,  0.0523,  0.0609]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0165, grad_fn=<MinBackward1>), tensor(0.9856, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.7249314785003662\n",
      "@sample 4: tensor([[ 0.0701,  0.0942, -0.0928, -0.1150,  0.0514],\n",
      "        [ 0.0730,  0.2664,  0.0341, -0.1390,  0.0325],\n",
      "        [ 0.0420,  0.1545, -0.0119, -0.1497,  0.0477],\n",
      "        [ 0.0683,  0.1757,  0.0316,  0.1027,  0.0815],\n",
      "        [ 0.0493,  0.1973,  0.0438, -0.1932,  0.0341],\n",
      "        [ 0.0786,  0.1720,  0.0707, -0.1470,  0.0585],\n",
      "        [ 0.0047,  0.0502, -0.0145, -0.0079,  0.0513],\n",
      "        [ 0.0519,  0.1195,  0.1799,  0.0448,  0.1128]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.1466, -0.2025, -0.1983,  0.2094,  0.0724],\n",
      "        [-0.1423, -0.0767, -0.1616,  0.1364,  0.0890],\n",
      "        [-0.2025, -0.1784,  0.0480,  0.0715, -0.1110],\n",
      "        [-0.1325,  0.1556, -0.2827,  0.2017,  0.0603],\n",
      "        [-0.0876, -0.1492,  0.0029,  0.0984, -0.0505],\n",
      "        [-0.0526, -0.0122, -0.1152,  0.1226, -0.0098],\n",
      "        [-0.0920, -0.0438, -0.1743,  0.0842,  0.0723],\n",
      "        [-0.0721,  0.0780, -0.1953,  0.1057, -0.2280]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0109, grad_fn=<MinBackward1>), tensor(0.9794, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.6807721853256226\n",
      "@sample 5: tensor([[-0.0427,  0.1720,  0.0835, -0.0211,  0.1474],\n",
      "        [ 0.1478,  0.1827,  0.0077, -0.0323,  0.0623],\n",
      "        [ 0.0761,  0.1631, -0.0683, -0.1374,  0.0512],\n",
      "        [ 0.1301,  0.0960,  0.0319,  0.0689, -0.1119],\n",
      "        [ 0.0671,  0.2093, -0.0495, -0.0690,  0.0843],\n",
      "        [-0.0320,  0.0372,  0.0161, -0.0253,  0.1396],\n",
      "        [ 0.0193,  0.1873,  0.0238, -0.1936,  0.0412],\n",
      "        [ 0.0727,  0.1656, -0.0076, -0.0309,  0.0056]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.1174, -0.0718, -0.4469,  0.2925,  0.2382],\n",
      "        [-0.0251, -0.0687, -0.2455,  0.1825, -0.1006],\n",
      "        [ 0.0087, -0.0974, -0.0413,  0.0983, -0.0334],\n",
      "        [-0.1882, -0.0530, -0.1851,  0.2135,  0.0290],\n",
      "        [-0.0847, -0.0341, -0.1862,  0.1084,  0.0134],\n",
      "        [-0.1004, -0.0775, -0.2511,  0.1207, -0.0524],\n",
      "        [-0.1440, -0.1813, -0.0987,  0.1431, -0.0533],\n",
      "        [-0.1704,  0.0104, -0.1786,  0.1770, -0.0526]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0115, grad_fn=<MinBackward1>), tensor(0.9930, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.7114083766937256\n",
      "@sample 6: tensor([[ 0.0680,  0.1719, -0.0828, -0.1595,  0.0218],\n",
      "        [ 0.1169,  0.1436,  0.0368,  0.0637, -0.1531],\n",
      "        [ 0.0501,  0.1569,  0.0567, -0.0853, -0.0034],\n",
      "        [ 0.0214,  0.2152,  0.0264, -0.2139,  0.0895],\n",
      "        [ 0.0986,  0.1140, -0.0174, -0.0200, -0.0888],\n",
      "        [ 0.0513,  0.1479, -0.0245, -0.1764, -0.0010],\n",
      "        [ 0.0831,  0.1375,  0.0818, -0.0664,  0.0147],\n",
      "        [ 0.0432,  0.0520,  0.1040, -0.0299,  0.0600]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0454, -0.1416, -0.0586,  0.1284, -0.0337],\n",
      "        [-0.2482, -0.0438, -0.1534,  0.1529, -0.1890],\n",
      "        [-0.0051, -0.1253, -0.1029,  0.1117, -0.0133],\n",
      "        [-0.0500, -0.1843, -0.0443,  0.0732, -0.0380],\n",
      "        [-0.0944, -0.0816, -0.1992,  0.1052,  0.0339],\n",
      "        [-0.0842, -0.1530, -0.0102,  0.1077, -0.0454],\n",
      "        [-0.0100, -0.1108, -0.0417,  0.0614, -0.0704],\n",
      "        [-0.1185, -0.1364, -0.1659,  0.1297,  0.0055]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0104, grad_fn=<MinBackward1>), tensor(0.9783, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.607586145401001\n",
      "@sample 7: tensor([[ 0.0407,  0.2856, -0.0268, -0.0051,  0.0431],\n",
      "        [ 0.0602,  0.1252, -0.0269, -0.1356, -0.0095],\n",
      "        [ 0.0340,  0.1200,  0.1063,  0.0958,  0.1096],\n",
      "        [ 0.0562,  0.1107, -0.0914, -0.1718,  0.0364],\n",
      "        [ 0.0549,  0.1887, -0.0393, -0.2545,  0.1115],\n",
      "        [ 0.0142,  0.1246,  0.1161,  0.0247,  0.0390],\n",
      "        [ 0.0870,  0.0472,  0.1437,  0.0329,  0.0836],\n",
      "        [ 0.1899,  0.0482, -0.0134,  0.0320,  0.0036]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0109, -0.1091, -0.1690,  0.0321,  0.0419],\n",
      "        [-0.0480, -0.1331, -0.0400,  0.0998, -0.0648],\n",
      "        [-0.0906,  0.1838, -0.3295,  0.1745,  0.1953],\n",
      "        [-0.1079, -0.1366, -0.1255,  0.1912, -0.0236],\n",
      "        [-0.1058, -0.2271, -0.0438,  0.0841, -0.0684],\n",
      "        [-0.1397,  0.1202, -0.2910,  0.2012,  0.0581],\n",
      "        [-0.1459, -0.0574, -0.0777,  0.1829, -0.1994],\n",
      "        [ 0.0412,  0.0732, -0.0735,  0.2318, -0.1221]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0130, grad_fn=<MinBackward1>), tensor(0.9896, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.7000815868377686\n",
      "@sample 8: tensor([[ 0.1827,  0.0967, -0.0120, -0.1343,  0.0063],\n",
      "        [ 0.1289,  0.0904,  0.1053, -0.0682, -0.0771],\n",
      "        [ 0.0029,  0.1173,  0.0243, -0.0870,  0.0402],\n",
      "        [ 0.0867,  0.0588, -0.0183,  0.0251,  0.0296],\n",
      "        [ 0.2214,  0.0213, -0.1424, -0.0279, -0.0696],\n",
      "        [ 0.0574,  0.2270,  0.0806, -0.0894,  0.0311],\n",
      "        [ 0.0880,  0.1541,  0.0330, -0.0819,  0.0614],\n",
      "        [ 0.0976,  0.1816, -0.0832, -0.0899, -0.0322]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0926, -0.1637,  0.1140,  0.0730, -0.0828],\n",
      "        [-0.0870, -0.1336, -0.1173,  0.1456, -0.1091],\n",
      "        [-0.0350, -0.1410, -0.2776,  0.2189,  0.1262],\n",
      "        [-0.0353, -0.0513,  0.0129,  0.1036, -0.0839],\n",
      "        [-0.1889, -0.0950, -0.0560,  0.1871, -0.0039],\n",
      "        [-0.0474, -0.1336, -0.1570,  0.0319, -0.0398],\n",
      "        [-0.1321, -0.0811, -0.3613,  0.2590,  0.0392],\n",
      "        [-0.0455,  0.0253,  0.0091,  0.0838, -0.0222]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0102, grad_fn=<MinBackward1>), tensor(0.9944, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.6421114206314087\n",
      "@sample 9: tensor([[-0.0254,  0.1280,  0.0426,  0.0364,  0.0414],\n",
      "        [ 0.0022,  0.1781,  0.0590, -0.0415,  0.0866],\n",
      "        [ 0.1175,  0.0342,  0.0101, -0.1046,  0.1033],\n",
      "        [-0.0095,  0.2077, -0.0093, -0.1687,  0.0760],\n",
      "        [ 0.0452,  0.0464,  0.1816,  0.0167,  0.0036],\n",
      "        [ 0.1174, -0.0793,  0.0024, -0.0310, -0.1037],\n",
      "        [ 0.0213,  0.0939,  0.0320,  0.0530, -0.0546],\n",
      "        [ 0.1929,  0.2009, -0.0188,  0.1784, -0.0864]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0290, -0.0494, -0.2245,  0.1777,  0.0314],\n",
      "        [ 0.0039, -0.0507, -0.1430, -0.0173, -0.0404],\n",
      "        [-0.1691, -0.2460, -0.0196,  0.0949,  0.0050],\n",
      "        [-0.0788, -0.1872, -0.1704,  0.1828,  0.0225],\n",
      "        [-0.2062,  0.0206, -0.2358,  0.0717, -0.1405],\n",
      "        [-0.1511, -0.1882, -0.1242,  0.2060, -0.0226],\n",
      "        [ 0.0047,  0.0063, -0.1621,  0.2233,  0.0289],\n",
      "        [-0.0241,  0.0519, -0.2012,  0.0863, -0.0950]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0062, grad_fn=<MinBackward1>), tensor(0.9932, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.6414787769317627\n",
      "@sample 10: tensor([[ 0.1610, -0.0215,  0.0320,  0.2070,  0.1291],\n",
      "        [ 0.1109,  0.0679,  0.0387, -0.0727, -0.0115],\n",
      "        [ 0.0738, -0.0162,  0.1074, -0.0379,  0.0091],\n",
      "        [ 0.1167,  0.2336, -0.0192,  0.0626,  0.0964],\n",
      "        [ 0.0651,  0.0765, -0.0794, -0.1751,  0.0642],\n",
      "        [ 0.0873,  0.1761,  0.0503, -0.0190,  0.0019],\n",
      "        [ 0.0713, -0.0287,  0.1202, -0.0477,  0.1605],\n",
      "        [ 0.1157,  0.2343, -0.1267, -0.0149,  0.0123]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0343,  0.2327, -0.3680,  0.1998,  0.0711],\n",
      "        [-0.0967, -0.0678, -0.0736,  0.1025,  0.0223],\n",
      "        [-0.0974, -0.2390, -0.0443,  0.0316, -0.0431],\n",
      "        [ 0.0781, -0.0197, -0.1213,  0.0794, -0.0528],\n",
      "        [-0.0553, -0.2701, -0.1664,  0.1512,  0.0163],\n",
      "        [ 0.0529, -0.0183, -0.0831,  0.0665, -0.0510],\n",
      "        [-0.0491, -0.1261, -0.0989,  0.0088, -0.1326],\n",
      "        [ 0.0598, -0.1309, -0.0660,  0.0132, -0.0069]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0217, grad_fn=<MinBackward1>), tensor(0.9920, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.6344302892684937\n",
      "@sample 11: tensor([[ 0.0952,  0.1435,  0.0150, -0.0289, -0.0484],\n",
      "        [ 0.1638,  0.0910, -0.0665, -0.1424, -0.0861],\n",
      "        [ 0.0883, -0.0312,  0.0582,  0.1330,  0.1171],\n",
      "        [ 0.0912, -0.0959,  0.0969, -0.0795,  0.0221],\n",
      "        [-0.0026,  0.1207, -0.0014, -0.1828,  0.0605],\n",
      "        [ 0.0890,  0.1825,  0.0082,  0.0319, -0.0192],\n",
      "        [-0.0094,  0.1297,  0.0854, -0.0463,  0.0816],\n",
      "        [ 0.0431,  0.1822, -0.0591, -0.0574,  0.1323]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0256,  0.0102, -0.0626,  0.0531, -0.0285],\n",
      "        [-0.0659, -0.2072, -0.0329,  0.1432, -0.0613],\n",
      "        [-0.0273,  0.1676, -0.2293,  0.1781, -0.0571],\n",
      "        [-0.1645, -0.2875,  0.0418,  0.1372, -0.0386],\n",
      "        [-0.0829, -0.1645, -0.0766,  0.1363, -0.1286],\n",
      "        [ 0.0715,  0.0194, -0.0005,  0.0180, -0.0461],\n",
      "        [ 0.0151, -0.1287, -0.1877,  0.1585, -0.0638],\n",
      "        [ 0.0224,  0.0588, -0.1564,  0.1397, -0.0981]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0085, grad_fn=<MinBackward1>), tensor(0.9907, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.5869375467300415\n",
      "@sample 12: tensor([[ 0.0598,  0.2443,  0.0045, -0.0196,  0.0173],\n",
      "        [ 0.0796,  0.1906, -0.0292, -0.0017,  0.0119],\n",
      "        [ 0.0874,  0.0189,  0.0337,  0.1905,  0.0428],\n",
      "        [ 0.0201,  0.1667,  0.0513, -0.0464,  0.0232],\n",
      "        [ 0.1980,  0.1087, -0.0435,  0.1286,  0.0489],\n",
      "        [ 0.0593,  0.0582,  0.1243,  0.0581,  0.0714],\n",
      "        [ 0.0308,  0.1419, -0.0364, -0.1767, -0.0070],\n",
      "        [ 0.1098,  0.0447, -0.0870, -0.0157,  0.0252]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0430,  0.0146, -0.1092,  0.0911,  0.0497],\n",
      "        [-0.0352, -0.0240, -0.2463,  0.0781,  0.0363],\n",
      "        [-0.0207,  0.2353, -0.2824,  0.2367,  0.0263],\n",
      "        [ 0.0672, -0.0766, -0.1364,  0.1897,  0.0190],\n",
      "        [ 0.1372,  0.0710,  0.0109, -0.0303, -0.1004],\n",
      "        [-0.0872, -0.0341, -0.2171,  0.1701, -0.0768],\n",
      "        [-0.0565, -0.2347,  0.0822,  0.0512, -0.0765],\n",
      "        [ 0.0345, -0.0769, -0.1017,  0.2346, -0.0100]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0080, grad_fn=<MinBackward1>), tensor(0.9841, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.6167869567871094\n",
      "@sample 13: tensor([[ 0.0122,  0.0433,  0.1350,  0.0385,  0.1509],\n",
      "        [ 0.0863,  0.0456, -0.0372, -0.1118, -0.0070],\n",
      "        [ 0.0397,  0.1412,  0.0075,  0.0318,  0.0110],\n",
      "        [ 0.1008,  0.2232, -0.0531,  0.0592, -0.0655],\n",
      "        [ 0.0617,  0.1142, -0.0182, -0.1110,  0.0083],\n",
      "        [-0.0004,  0.1921, -0.0061, -0.1925,  0.0470],\n",
      "        [ 0.0954,  0.1460, -0.0463,  0.0420, -0.0260],\n",
      "        [ 0.0752,  0.0899,  0.1065,  0.0153,  0.0549]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0365, -0.0081, -0.0149,  0.0142, -0.1579],\n",
      "        [-0.0366, -0.1384, -0.0568,  0.1660, -0.0026],\n",
      "        [ 0.0260, -0.0798, -0.1187,  0.1384, -0.0860],\n",
      "        [-0.1169,  0.0186, -0.0968,  0.0470, -0.1097],\n",
      "        [-0.1290, -0.1798, -0.1349,  0.1435, -0.0978],\n",
      "        [-0.0976, -0.1740, -0.0680,  0.1339, -0.0090],\n",
      "        [-0.0488,  0.1001, -0.1757,  0.2231, -0.0510],\n",
      "        [-0.0088,  0.0201, -0.1886,  0.1302, -0.0455]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0168, grad_fn=<MinBackward1>), tensor(0.9779, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.5651327967643738\n",
      "@sample 14: tensor([[ 0.0515,  0.0751, -0.0014, -0.1806,  0.0161],\n",
      "        [ 0.1177,  0.0665,  0.0518, -0.0154, -0.0045],\n",
      "        [ 0.1032, -0.1831,  0.1597,  0.0959,  0.0035],\n",
      "        [ 0.0555, -0.0108,  0.0855, -0.0410, -0.0322],\n",
      "        [ 0.1637,  0.1016, -0.1309,  0.1802,  0.0357],\n",
      "        [ 0.1117,  0.0454, -0.0368, -0.0009, -0.0796],\n",
      "        [ 0.2239,  0.0804, -0.0133,  0.1660,  0.0080],\n",
      "        [ 0.0959,  0.1050,  0.0800,  0.0438,  0.1334]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0895, -0.1669, -0.0238,  0.1260, -0.1054],\n",
      "        [ 0.0747,  0.0501,  0.0161,  0.0952, -0.0753],\n",
      "        [-0.1187, -0.0889,  0.0322,  0.0930, -0.1241],\n",
      "        [-0.0721, -0.2246, -0.0633,  0.0980, -0.0124],\n",
      "        [ 0.0470,  0.1538, -0.0879,  0.1071, -0.0649],\n",
      "        [-0.0790, -0.0136, -0.0910,  0.0773, -0.0776],\n",
      "        [ 0.0815,  0.0465,  0.0510,  0.0812, -0.0726],\n",
      "        [ 0.0181,  0.0170, -0.1617,  0.1616, -0.0965]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0134, grad_fn=<MinBackward1>), tensor(0.9782, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.5593485832214355\n",
      "@sample 15: tensor([[ 0.0609,  0.0029, -0.0202, -0.1643, -0.0337],\n",
      "        [ 0.0087, -0.0475, -0.0007,  0.0601,  0.0095],\n",
      "        [ 0.0576,  0.0547,  0.0764,  0.0091,  0.0213],\n",
      "        [ 0.0927, -0.0948,  0.1084, -0.0319, -0.0348],\n",
      "        [ 0.0205,  0.2088,  0.0313,  0.0512,  0.0805],\n",
      "        [ 0.1557,  0.1605, -0.1146, -0.0660, -0.0744],\n",
      "        [ 0.0505,  0.0786, -0.0253, -0.0749,  0.1024],\n",
      "        [ 0.1383, -0.1983,  0.0065,  0.0365, -0.0856]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.1439, -0.2367, -0.0205,  0.0829, -0.0887],\n",
      "        [-0.0128, -0.0670, -0.1185,  0.1167, -0.0331],\n",
      "        [-0.0368,  0.0652,  0.0084,  0.1057, -0.1089],\n",
      "        [-0.1740, -0.2598, -0.0425,  0.1301, -0.0603],\n",
      "        [ 0.0358,  0.1323, -0.2564,  0.1627,  0.1315],\n",
      "        [-0.0702, -0.1156, -0.1216,  0.1827, -0.0832],\n",
      "        [ 0.0126, -0.0937, -0.1164,  0.0558, -0.0124],\n",
      "        [-0.1669, -0.3001,  0.0383,  0.1575, -0.0199]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0125, grad_fn=<MinBackward1>), tensor(0.9949, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.5835626721382141\n",
      "@sample 16: tensor([[ 0.1409, -0.0600, -0.0883, -0.0508,  0.0802],\n",
      "        [-0.0446,  0.0899,  0.1504, -0.1685,  0.0813],\n",
      "        [ 0.0625,  0.1303,  0.0662, -0.1439,  0.0870],\n",
      "        [ 0.1231,  0.0747,  0.0048,  0.0854, -0.0075],\n",
      "        [ 0.0891,  0.1387, -0.0541,  0.0099,  0.0977],\n",
      "        [ 0.0667,  0.2387,  0.0617,  0.0425,  0.0395],\n",
      "        [ 0.0433,  0.0137,  0.1922, -0.0295, -0.0310],\n",
      "        [ 0.0166,  0.1849,  0.0131, -0.0816, -0.0035]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0101, -0.1567,  0.0468,  0.1849, -0.1033],\n",
      "        [-0.0546, -0.1621, -0.0562,  0.1039, -0.0970],\n",
      "        [-0.0287, -0.1732, -0.0901,  0.0373, -0.0687],\n",
      "        [-0.1240, -0.0376, -0.0107,  0.0194, -0.0781],\n",
      "        [ 0.0425,  0.0631, -0.1502,  0.1308, -0.1275],\n",
      "        [ 0.0359,  0.0319, -0.0360, -0.0008, -0.0171],\n",
      "        [-0.1371, -0.1218, -0.1019,  0.1240, -0.1055],\n",
      "        [-0.1214, -0.1867, -0.1991,  0.0907,  0.0800]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0064, grad_fn=<MinBackward1>), tensor(0.9807, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.5694973468780518\n",
      "@sample 17: tensor([[ 0.0139,  0.1030,  0.0303,  0.0017,  0.0839],\n",
      "        [ 0.0708,  0.0267,  0.0287, -0.0683, -0.0329],\n",
      "        [ 0.0451,  0.1736, -0.0015, -0.1374,  0.0418],\n",
      "        [ 0.0456,  0.0504, -0.0332, -0.0321,  0.0333],\n",
      "        [-0.0002,  0.1451, -0.0147, -0.1649,  0.0278],\n",
      "        [-0.0299,  0.1621,  0.0405, -0.1778,  0.0491],\n",
      "        [-0.0220,  0.1852,  0.1643, -0.0428,  0.1177],\n",
      "        [ 0.0306,  0.0058, -0.0610, -0.0530, -0.0403]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0073, -0.0674, -0.1951,  0.0604, -0.0715],\n",
      "        [ 0.0679, -0.1209, -0.0822,  0.0971,  0.0056],\n",
      "        [-0.0357, -0.0959, -0.0389,  0.1254, -0.0503],\n",
      "        [ 0.0480,  0.0026, -0.0561,  0.0674, -0.0687],\n",
      "        [-0.0723, -0.2009, -0.0793,  0.1032,  0.0108],\n",
      "        [-0.0715, -0.2282, -0.1318,  0.1234,  0.0376],\n",
      "        [-0.0184, -0.0085, -0.2098,  0.0824,  0.0470],\n",
      "        [ 0.0042, -0.1916,  0.0163,  0.0713, -0.0852]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0210, grad_fn=<MinBackward1>), tensor(0.9968, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.5187593698501587\n",
      "@sample 18: tensor([[ 0.0372,  0.1456, -0.0190,  0.0316,  0.1888],\n",
      "        [ 0.1021,  0.0688,  0.0890, -0.0963,  0.0823],\n",
      "        [ 0.1351, -0.0484, -0.1391,  0.0138,  0.0395],\n",
      "        [-0.0371,  0.1705,  0.0349, -0.1653,  0.0297],\n",
      "        [ 0.0487,  0.1822, -0.0868, -0.1020,  0.0075],\n",
      "        [ 0.0728,  0.0113, -0.0359,  0.0087, -0.0039],\n",
      "        [ 0.0529,  0.0393, -0.0830, -0.1729,  0.0763],\n",
      "        [ 0.0105,  0.1258, -0.0405,  0.0275,  0.1193]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0239,  0.0467, -0.1976, -0.0408, -0.0949],\n",
      "        [ 0.0375, -0.1650,  0.0375,  0.0327, -0.0107],\n",
      "        [ 0.1169, -0.1466,  0.0426,  0.1441, -0.1395],\n",
      "        [-0.0778, -0.1951, -0.0898,  0.1336,  0.0147],\n",
      "        [-0.0022, -0.1102, -0.0122,  0.1196, -0.0332],\n",
      "        [ 0.0537, -0.1173, -0.0868,  0.1460, -0.0466],\n",
      "        [-0.0626, -0.1708,  0.0129,  0.0467, -0.0899],\n",
      "        [-0.0059,  0.0500, -0.2522,  0.2217,  0.0578]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0143, grad_fn=<MinBackward1>), tensor(0.9818, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.5615293979644775\n",
      "@sample 19: tensor([[ 0.1009, -0.1112,  0.0465,  0.1617, -0.0948],\n",
      "        [ 0.1095,  0.0931,  0.1007, -0.1560,  0.1186],\n",
      "        [ 0.0284,  0.1125,  0.1346,  0.0352,  0.0286],\n",
      "        [ 0.0692,  0.0573,  0.0634, -0.1052, -0.0211],\n",
      "        [ 0.0493,  0.1257, -0.0322, -0.1103,  0.0170],\n",
      "        [ 0.0682, -0.0172,  0.0700, -0.0385,  0.0285],\n",
      "        [ 0.0439,  0.0592, -0.0465, -0.0332, -0.0947],\n",
      "        [ 0.1251, -0.0113, -0.1162, -0.1142, -0.0932]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0109,  0.1061, -0.0575,  0.1054,  0.0563],\n",
      "        [ 0.0144, -0.1112,  0.0592,  0.0790,  0.0113],\n",
      "        [-0.0893,  0.0572,  0.0017, -0.0220, -0.1107],\n",
      "        [-0.0271, -0.1298,  0.0904,  0.0389, -0.0917],\n",
      "        [-0.0492, -0.0858, -0.0854,  0.0848, -0.0385],\n",
      "        [-0.0362, -0.2281,  0.0168,  0.0933, -0.1049],\n",
      "        [-0.0353, -0.1113,  0.0066,  0.0050, -0.1802],\n",
      "        [-0.1087, -0.2216,  0.0058,  0.0812, -0.0048]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0174, grad_fn=<MinBackward1>), tensor(0.9768, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.5043361186981201\n",
      "@sample 20: tensor([[ 1.5068e-01, -1.2488e-02,  1.5956e-02, -1.2552e-01,  1.5978e-02],\n",
      "        [ 1.0856e-01, -2.7748e-02,  1.6001e-01,  4.2873e-02,  1.6512e-01],\n",
      "        [ 6.6801e-02,  3.8948e-02, -3.7530e-02, -6.3103e-02, -5.9608e-02],\n",
      "        [ 7.9467e-02, -1.0997e-02,  2.6861e-02, -1.0463e-01, -1.3440e-02],\n",
      "        [ 7.6807e-04,  6.9393e-02,  9.3786e-02,  2.5495e-03,  2.6944e-01],\n",
      "        [ 1.1060e-01,  1.8810e-04,  2.7303e-02,  1.6991e-03, -1.2170e-02],\n",
      "        [ 6.3411e-02, -2.1387e-01,  1.0684e-01,  1.1935e-01, -1.1837e-01],\n",
      "        [ 9.4530e-02,  1.6451e-01,  4.6716e-02, -9.8959e-03,  4.7272e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0211, -0.1734,  0.0243,  0.0319,  0.0004],\n",
      "        [-0.1089,  0.1040, -0.1338,  0.0849,  0.0212],\n",
      "        [ 0.0268, -0.0874,  0.0543,  0.0552, -0.0051],\n",
      "        [ 0.0095, -0.1989,  0.0620,  0.0438, -0.0258],\n",
      "        [ 0.0090,  0.0469, -0.1274,  0.0591, -0.0259],\n",
      "        [-0.1529,  0.0211, -0.3249,  0.2666,  0.0721],\n",
      "        [-0.1248,  0.0357, -0.0562,  0.1038,  0.0383],\n",
      "        [-0.0050, -0.0247,  0.0077,  0.0170, -0.0195]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0120, grad_fn=<MinBackward1>), tensor(0.9823, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.5542724132537842\n",
      "@sample 21: tensor([[ 0.0607,  0.0920,  0.0247, -0.1321,  0.0118],\n",
      "        [ 0.1060,  0.1027,  0.0953, -0.1152,  0.0095],\n",
      "        [ 0.1633,  0.0388, -0.0164,  0.1698, -0.1330],\n",
      "        [ 0.0051, -0.0083,  0.1544, -0.0565, -0.0877],\n",
      "        [ 0.0688,  0.0636, -0.0921, -0.0719, -0.0335],\n",
      "        [-0.0123,  0.1112, -0.0103, -0.1160, -0.0223],\n",
      "        [ 0.2036,  0.0694, -0.1067, -0.0678,  0.0248],\n",
      "        [-0.0265,  0.0293,  0.0891,  0.0414, -0.0670]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0314, -0.2027,  0.0471,  0.0265, -0.0848],\n",
      "        [ 0.0401, -0.1351,  0.1321, -0.0116, -0.0735],\n",
      "        [ 0.0884,  0.0640,  0.0291, -0.0320, -0.0920],\n",
      "        [-0.1467, -0.1744, -0.0730,  0.0379, -0.1060],\n",
      "        [-0.0238, -0.0743, -0.0609,  0.1106,  0.0078],\n",
      "        [-0.0357, -0.2237, -0.1008,  0.1378,  0.0553],\n",
      "        [ 0.0084, -0.0717,  0.0241,  0.0018, -0.0451],\n",
      "        [-0.0272,  0.0231, -0.0978,  0.2049, -0.0523]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0164, grad_fn=<MinBackward1>), tensor(0.9859, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.5158900022506714\n",
      "@sample 22: tensor([[ 0.1337, -0.0337, -0.0456,  0.1618, -0.1067],\n",
      "        [ 0.1087,  0.1015, -0.0775, -0.0799, -0.0202],\n",
      "        [ 0.1055, -0.0352, -0.0494, -0.0039, -0.0484],\n",
      "        [ 0.0025,  0.1230, -0.0207, -0.1457,  0.0311],\n",
      "        [ 0.1290, -0.0125,  0.0155, -0.0338, -0.0815],\n",
      "        [ 0.1399, -0.0636,  0.0716, -0.0060, -0.0384],\n",
      "        [ 0.0896,  0.0344,  0.0997, -0.1081,  0.0325],\n",
      "        [-0.0197,  0.0394,  0.0871,  0.0056, -0.0418]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.1130,  0.1309,  0.0223,  0.1253, -0.1020],\n",
      "        [-0.0158, -0.0745,  0.0096,  0.1217, -0.0555],\n",
      "        [-0.0432, -0.0804, -0.1376,  0.0781, -0.0580],\n",
      "        [-0.0160, -0.2028, -0.1870,  0.2142,  0.1073],\n",
      "        [-0.1219, -0.0646, -0.0332,  0.0795, -0.0672],\n",
      "        [-0.0606, -0.1530,  0.0899,  0.0937, -0.1782],\n",
      "        [-0.0644, -0.1981, -0.1703,  0.0563, -0.0685],\n",
      "        [-0.1375,  0.0158, -0.2636,  0.1829,  0.0549]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0305, grad_fn=<MinBackward1>), tensor(0.9797, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.5406761169433594\n",
      "@sample 23: tensor([[ 0.0468,  0.0476,  0.0684, -0.1028, -0.0104],\n",
      "        [ 0.0707,  0.0168, -0.1604, -0.0863, -0.0033],\n",
      "        [ 0.0294,  0.0776, -0.0096, -0.0497, -0.0429],\n",
      "        [ 0.1328, -0.0149, -0.0752, -0.0541, -0.0610],\n",
      "        [ 0.0395,  0.1492,  0.0334, -0.0940, -0.0400],\n",
      "        [ 0.0117,  0.1216, -0.0385, -0.1089,  0.0101],\n",
      "        [ 0.0839,  0.0318, -0.0681, -0.0437, -0.0133],\n",
      "        [ 0.0593, -0.0379,  0.0504,  0.0103, -0.0655]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.1742, -0.1796, -0.0531,  0.0839, -0.1526],\n",
      "        [-0.0167, -0.1486, -0.1059,  0.1630,  0.0461],\n",
      "        [-0.0420, -0.0956, -0.0968,  0.0775, -0.0718],\n",
      "        [-0.0737, -0.1264,  0.0679,  0.0568, -0.0432],\n",
      "        [-0.0005, -0.0123, -0.0904,  0.0951,  0.0443],\n",
      "        [-0.0967, -0.1791, -0.2285,  0.1743,  0.0704],\n",
      "        [ 0.0420, -0.0060,  0.0738,  0.0145, -0.0537],\n",
      "        [-0.0702, -0.1063,  0.0180,  0.1014, -0.1582]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0128, grad_fn=<MinBackward1>), tensor(0.9815, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.4726046621799469\n",
      "@sample 24: tensor([[ 0.0868,  0.0166, -0.0457,  0.0872,  0.0286],\n",
      "        [ 0.0193, -0.0441,  0.1314,  0.0912,  0.1122],\n",
      "        [ 0.0064,  0.0447,  0.0675, -0.1472,  0.1024],\n",
      "        [ 0.0559,  0.0583, -0.1039,  0.0505, -0.0523],\n",
      "        [ 0.1178, -0.1456, -0.0414, -0.0943, -0.1026],\n",
      "        [ 0.0427,  0.1040, -0.0279, -0.0230,  0.1081],\n",
      "        [ 0.1028, -0.0387, -0.0443,  0.1402, -0.0887],\n",
      "        [ 0.0296,  0.0354,  0.0626,  0.0066,  0.0181]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0562, -0.0127, -0.0932,  0.1856, -0.0603],\n",
      "        [ 0.1093,  0.0378,  0.0428,  0.1322, -0.0386],\n",
      "        [-0.0518, -0.2046, -0.2054,  0.0789, -0.1020],\n",
      "        [-0.1209, -0.0463, -0.1675,  0.2124, -0.0244],\n",
      "        [-0.0911, -0.1921,  0.1260,  0.0744, -0.0275],\n",
      "        [ 0.0448, -0.0992, -0.1412,  0.1322,  0.0214],\n",
      "        [ 0.0615, -0.0121,  0.0585,  0.1950, -0.0665],\n",
      "        [-0.0278,  0.0296, -0.1852,  0.1551, -0.0190]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0105, grad_fn=<MinBackward1>), tensor(0.9807, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.5194903612136841\n",
      "@sample 25: tensor([[ 0.1399,  0.0385,  0.0469,  0.0383, -0.0049],\n",
      "        [ 0.1074, -0.0317,  0.0765,  0.0306,  0.0324],\n",
      "        [ 0.1174, -0.0762, -0.0439,  0.0782, -0.0534],\n",
      "        [ 0.0083, -0.0448,  0.0416, -0.0722,  0.0945],\n",
      "        [ 0.0393, -0.0065,  0.0439, -0.0024, -0.0025],\n",
      "        [ 0.0436, -0.0566,  0.0721, -0.0821,  0.0918],\n",
      "        [ 0.0582, -0.1110,  0.0293,  0.0111, -0.0345],\n",
      "        [-0.0090,  0.0815,  0.0723, -0.0186,  0.0565]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0250, -0.0212, -0.0382,  0.1051, -0.1134],\n",
      "        [ 0.0182, -0.0805,  0.0194,  0.0024,  0.0374],\n",
      "        [ 0.1420, -0.0293, -0.0033,  0.1520,  0.0232],\n",
      "        [ 0.0105, -0.0297, -0.0345,  0.0427,  0.0815],\n",
      "        [ 0.0163, -0.1533, -0.0553,  0.0805, -0.0451],\n",
      "        [-0.1040, -0.1400, -0.0197,  0.0808, -0.0789],\n",
      "        [-0.0342, -0.1097,  0.0055,  0.1489, -0.0543],\n",
      "        [ 0.0296, -0.0872, -0.1552,  0.1113, -0.0467]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0174, grad_fn=<MinBackward1>), tensor(0.9962, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.4110777676105499\n",
      "@sample 26: tensor([[ 0.1166, -0.0008, -0.1149, -0.0425, -0.0574],\n",
      "        [ 0.0703,  0.0375, -0.0544, -0.0772,  0.0005],\n",
      "        [ 0.0840, -0.0428,  0.0616, -0.0346,  0.0332],\n",
      "        [-0.0603,  0.1616,  0.0706, -0.1552,  0.0831],\n",
      "        [ 0.0185,  0.0402,  0.0683,  0.0434,  0.0018],\n",
      "        [ 0.0796,  0.0923, -0.0318, -0.0678,  0.0239],\n",
      "        [-0.0192,  0.1392, -0.0158,  0.0491,  0.0982],\n",
      "        [ 0.0221,  0.1536, -0.0453, -0.0514,  0.0671]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0515, -0.0386, -0.0750,  0.1801,  0.0251],\n",
      "        [-0.0245, -0.1046, -0.0716,  0.1141,  0.0269],\n",
      "        [-0.0645, -0.2027,  0.0129, -0.0048, -0.1333],\n",
      "        [-0.0542, -0.1489, -0.1238,  0.1604,  0.0422],\n",
      "        [ 0.0342, -0.0467, -0.1086,  0.1462, -0.0221],\n",
      "        [ 0.0215, -0.0300, -0.0175,  0.0562, -0.0161],\n",
      "        [ 0.1440,  0.0343, -0.0600,  0.0866, -0.0617],\n",
      "        [ 0.0087, -0.1183, -0.1974,  0.0480,  0.1071]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0197, grad_fn=<MinBackward1>), tensor(0.9836, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.47306451201438904\n",
      "@sample 27: tensor([[-0.0116,  0.0851,  0.0261, -0.1349,  0.0338],\n",
      "        [ 0.1253,  0.0152, -0.0216, -0.0208, -0.0559],\n",
      "        [ 0.1067,  0.0696,  0.0253, -0.1045, -0.0038],\n",
      "        [-0.0493, -0.0412,  0.1566, -0.0042, -0.0017],\n",
      "        [ 0.0792,  0.0561, -0.0315, -0.0516,  0.0739],\n",
      "        [-0.0326,  0.1769,  0.0977, -0.0376,  0.0469],\n",
      "        [ 0.0647,  0.0966, -0.0603, -0.1022, -0.0193],\n",
      "        [ 0.0275, -0.0204,  0.0599, -0.0182, -0.0355]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0806, -0.1968, -0.2338,  0.1964,  0.0580],\n",
      "        [ 0.0266, -0.0135,  0.1039,  0.0622, -0.1961],\n",
      "        [ 0.0346, -0.1379,  0.0946, -0.0066, -0.0876],\n",
      "        [-0.1156, -0.1461, -0.0896,  0.0892, -0.0096],\n",
      "        [ 0.0062, -0.1442, -0.1470,  0.0517,  0.0930],\n",
      "        [ 0.0373, -0.0009, -0.1750,  0.1038, -0.0162],\n",
      "        [-0.0865, -0.1251, -0.0595,  0.1036, -0.0986],\n",
      "        [-0.0141, -0.1518,  0.1338,  0.1126, -0.1025]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0190, grad_fn=<MinBackward1>), tensor(0.9771, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.4913591742515564\n",
      "@sample 28: tensor([[-0.0215,  0.1386,  0.0114, -0.1260,  0.0592],\n",
      "        [ 0.0668, -0.0255,  0.0193, -0.0342, -0.0797],\n",
      "        [ 0.0436,  0.0530, -0.0661, -0.0929,  0.0199],\n",
      "        [ 0.0531,  0.0941, -0.0571, -0.0022,  0.0769],\n",
      "        [ 0.0289, -0.0897,  0.0496,  0.0953,  0.0409],\n",
      "        [ 0.0959, -0.0679,  0.0628, -0.0637, -0.0710],\n",
      "        [ 0.0326,  0.1543, -0.0379, -0.0474, -0.0331],\n",
      "        [-0.0674, -0.0109,  0.1151,  0.0497,  0.0504]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0762, -0.1696, -0.1628,  0.1360,  0.0396],\n",
      "        [-0.0529, -0.1422,  0.0363,  0.0290,  0.0177],\n",
      "        [-0.0114, -0.0981, -0.1278,  0.1158,  0.0312],\n",
      "        [ 0.0227, -0.1017, -0.1861, -0.0116, -0.0083],\n",
      "        [ 0.0159,  0.0681, -0.1898,  0.1198,  0.1259],\n",
      "        [-0.1170, -0.0561, -0.0352,  0.0186, -0.0475],\n",
      "        [-0.0279, -0.1165, -0.1198,  0.0164,  0.0837],\n",
      "        [-0.0543,  0.0571, -0.1948,  0.1408,  0.1495]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0124, grad_fn=<MinBackward1>), tensor(0.9691, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.46821358799934387\n",
      "@sample 29: tensor([[ 0.0625,  0.0968, -0.0087,  0.0241,  0.0999],\n",
      "        [ 0.0688,  0.1163,  0.0287, -0.0343,  0.0337],\n",
      "        [ 0.0705,  0.1431,  0.0159,  0.1077,  0.0394],\n",
      "        [ 0.0619,  0.1189, -0.0043,  0.0477,  0.0931],\n",
      "        [ 0.0061,  0.0368, -0.0434, -0.0685,  0.0123],\n",
      "        [ 0.0895,  0.0238, -0.0287,  0.0307,  0.0335],\n",
      "        [ 0.0047,  0.0464, -0.0320, -0.1666,  0.0936],\n",
      "        [ 0.1106,  0.0218, -0.0169, -0.0965, -0.0045]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.1264,  0.1450,  0.0734,  0.0080, -0.0786],\n",
      "        [ 0.0635, -0.0302, -0.2064,  0.0581,  0.0358],\n",
      "        [ 0.1227,  0.1502, -0.0778, -0.0143,  0.0878],\n",
      "        [ 0.0543,  0.0063, -0.1731,  0.0557,  0.0532],\n",
      "        [-0.0770, -0.0703, -0.1734,  0.1493,  0.0106],\n",
      "        [-0.0279, -0.0330, -0.0448, -0.0144,  0.0467],\n",
      "        [-0.0171, -0.1186, -0.1505,  0.0940,  0.0095],\n",
      "        [-0.0200, -0.1780,  0.0577,  0.0738, -0.0087]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0161, grad_fn=<MinBackward1>), tensor(0.9846, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.4568113088607788\n",
      "@sample 30: tensor([[ 0.0327,  0.0554,  0.0036,  0.1303,  0.0238],\n",
      "        [ 0.0820,  0.0037, -0.0139,  0.0858, -0.0546],\n",
      "        [ 0.0460,  0.0594,  0.0890, -0.0373, -0.0364],\n",
      "        [ 0.0733,  0.0343, -0.0374, -0.0402,  0.0340],\n",
      "        [ 0.1047,  0.0065, -0.0225,  0.0632, -0.0829],\n",
      "        [ 0.0018,  0.1125, -0.0272, -0.1628,  0.0779],\n",
      "        [ 0.1236,  0.0122,  0.0103,  0.1680,  0.0268],\n",
      "        [ 0.0585,  0.0648, -0.1916, -0.1446, -0.0511]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0712,  0.1543, -0.0708,  0.0335, -0.0268],\n",
      "        [-0.0074,  0.0597, -0.1860,  0.2001,  0.0535],\n",
      "        [-0.1107, -0.1798, -0.0266,  0.0560, -0.0213],\n",
      "        [-0.0506, -0.0558, -0.0915,  0.0129,  0.1011],\n",
      "        [ 0.0901, -0.0744, -0.0021, -0.0165, -0.0081],\n",
      "        [-0.0053, -0.1662, -0.0940,  0.1214, -0.1201],\n",
      "        [ 0.1123,  0.1035, -0.2318,  0.1485,  0.1417],\n",
      "        [-0.0987, -0.1331,  0.0057,  0.1177, -0.0577]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0086, grad_fn=<MinBackward1>), tensor(0.9830, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.4990209937095642\n",
      "@sample 31: tensor([[ 0.0911,  0.1169, -0.0256, -0.0125, -0.1014],\n",
      "        [ 0.0694,  0.0405, -0.0561,  0.0804, -0.0822],\n",
      "        [ 0.0600,  0.1027,  0.0361, -0.0659,  0.0406],\n",
      "        [-0.0124,  0.0108,  0.0560,  0.0111, -0.0052],\n",
      "        [ 0.1601,  0.1330, -0.0997, -0.0623, -0.0735],\n",
      "        [-0.0021,  0.1450,  0.0980, -0.0440,  0.1098],\n",
      "        [ 0.0566,  0.0362,  0.0375, -0.0033, -0.0593],\n",
      "        [-0.0399,  0.1170,  0.0363, -0.0942,  0.0160]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0879, -0.0573, -0.0347,  0.0963,  0.0147],\n",
      "        [ 0.0458,  0.0033, -0.0377,  0.1212,  0.0156],\n",
      "        [ 0.0841, -0.0409, -0.0055, -0.0065, -0.0740],\n",
      "        [-0.0541, -0.1448, -0.0110,  0.0551,  0.0138],\n",
      "        [-0.0188, -0.0939, -0.0029,  0.0463, -0.0636],\n",
      "        [-0.0530,  0.0534, -0.1794,  0.1045,  0.0064],\n",
      "        [ 0.0284, -0.0695, -0.0578,  0.0215, -0.0849],\n",
      "        [-0.0354, -0.0192,  0.1216, -0.0316, -0.0561]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0178, grad_fn=<MinBackward1>), tensor(0.9893, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.4296324849128723\n",
      "@sample 32: tensor([[ 1.2239e-02,  8.4772e-03,  3.4798e-02, -7.0606e-02,  1.7960e-03],\n",
      "        [ 1.9741e-02,  9.0148e-02,  1.0349e-01,  8.2169e-02,  1.1037e-01],\n",
      "        [-3.0579e-02,  1.3897e-01,  2.3273e-02, -1.3390e-01,  8.9037e-02],\n",
      "        [ 6.1326e-02,  1.3703e-01, -2.8793e-02, -6.5622e-02,  3.0885e-02],\n",
      "        [ 5.5632e-02, -1.0945e-01,  4.5397e-02,  5.0392e-02, -8.3166e-02],\n",
      "        [-4.0977e-05, -2.4606e-02, -2.3494e-02,  3.5531e-02, -1.0049e-01],\n",
      "        [ 9.6918e-02,  3.0688e-02,  1.0962e-02,  5.0334e-02, -6.9472e-02],\n",
      "        [ 7.1946e-02,  5.3033e-02,  9.8674e-02,  1.5556e-01,  2.7639e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0937, -0.0892,  0.0139, -0.0135, -0.0517],\n",
      "        [ 0.0607,  0.0716, -0.1781,  0.0425, -0.0448],\n",
      "        [-0.0598, -0.1712, -0.1777,  0.1275,  0.0290],\n",
      "        [-0.0021, -0.0654, -0.0191,  0.0294, -0.0464],\n",
      "        [-0.1119,  0.0595, -0.0192,  0.0701,  0.0272],\n",
      "        [ 0.0362,  0.0123,  0.1911, -0.0139, -0.0853],\n",
      "        [-0.0025, -0.0095, -0.1893,  0.1105,  0.0425],\n",
      "        [ 0.1014,  0.2525, -0.2053,  0.0775, -0.0149]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0043, grad_fn=<MinBackward1>), tensor(0.9883, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.48232367634773254\n",
      "@sample 33: tensor([[ 0.1648,  0.0469,  0.0237,  0.0563,  0.0785],\n",
      "        [-0.0173, -0.0229,  0.0293,  0.0401,  0.0593],\n",
      "        [ 0.1764,  0.0446, -0.1321,  0.0015, -0.0611],\n",
      "        [ 0.0108,  0.1239,  0.0264, -0.0694,  0.1170],\n",
      "        [ 0.0506,  0.1042,  0.1073,  0.0631,  0.0312],\n",
      "        [ 0.0320, -0.0294,  0.0175,  0.0608,  0.1073],\n",
      "        [ 0.0347,  0.0173, -0.0514, -0.0447,  0.1395],\n",
      "        [ 0.0201,  0.1365, -0.0405, -0.1083,  0.0217]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.1528,  0.0156,  0.1183, -0.0430, -0.0705],\n",
      "        [-0.0024,  0.0647, -0.0393,  0.0380,  0.0449],\n",
      "        [ 0.0157, -0.0550,  0.0920, -0.0291, -0.0316],\n",
      "        [-0.0239, -0.0906, -0.2238,  0.1495,  0.0610],\n",
      "        [ 0.0063,  0.1455, -0.1140,  0.1330, -0.0717],\n",
      "        [-0.0769,  0.0087, -0.0503,  0.0176,  0.0347],\n",
      "        [ 0.1580, -0.0565,  0.1457,  0.0139, -0.1364],\n",
      "        [-0.0526, -0.1174, -0.0792,  0.0991, -0.0008]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0225, grad_fn=<MinBackward1>), tensor(0.9756, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.47349825501441956\n",
      "@sample 34: tensor([[ 0.0311,  0.0677,  0.0328, -0.0124,  0.0008],\n",
      "        [ 0.0771,  0.0667,  0.0422, -0.0613,  0.0602],\n",
      "        [-0.0817,  0.0237, -0.0601, -0.0719,  0.0461],\n",
      "        [ 0.0522,  0.1283, -0.0156, -0.0587,  0.0115],\n",
      "        [ 0.0554, -0.1036,  0.1150,  0.0172,  0.0248],\n",
      "        [ 0.0938,  0.0432, -0.0483, -0.0398, -0.0029],\n",
      "        [-0.0364, -0.0167, -0.0126, -0.2387,  0.1963],\n",
      "        [-0.0708, -0.0593,  0.0191, -0.0798,  0.0220]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0233, -0.0279, -0.0172,  0.0188, -0.0906],\n",
      "        [-0.0034, -0.0221, -0.1160,  0.1959, -0.0635],\n",
      "        [ 0.0105, -0.1069,  0.1916, -0.0600,  0.0026],\n",
      "        [-0.0044, -0.0404, -0.0089,  0.0822, -0.0621],\n",
      "        [ 0.1048, -0.0307,  0.0613, -0.0081,  0.0101],\n",
      "        [-0.1072, -0.0896, -0.0759, -0.1218, -0.0403],\n",
      "        [-0.1043, -0.1372,  0.0451,  0.0478, -0.0855],\n",
      "        [-0.0288, -0.0481,  0.0657, -0.0031,  0.0281]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0111, grad_fn=<MinBackward1>), tensor(0.9781, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.4426102936267853\n",
      "@sample 35: tensor([[ 0.0321, -0.0353,  0.0138,  0.0966,  0.0132],\n",
      "        [ 0.0093,  0.1463, -0.0155, -0.1138,  0.0488],\n",
      "        [ 0.1137,  0.0459, -0.0399,  0.0047, -0.0374],\n",
      "        [ 0.0914,  0.0455, -0.0474, -0.0820,  0.0267],\n",
      "        [ 0.0322, -0.0158,  0.1166, -0.0752,  0.0431],\n",
      "        [ 0.0051,  0.1047,  0.0050,  0.0125,  0.0915],\n",
      "        [-0.0086,  0.1177,  0.2119,  0.1572, -0.0109],\n",
      "        [ 0.0524,  0.0206,  0.0831,  0.0137,  0.0599]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.1080,  0.0220, -0.0241,  0.0418, -0.0157],\n",
      "        [-0.0463, -0.1036, -0.0772,  0.1072, -0.0122],\n",
      "        [-0.0156, -0.0404, -0.1176,  0.2189, -0.0740],\n",
      "        [-0.0182, -0.1856, -0.1271,  0.0755, -0.0365],\n",
      "        [-0.0212, -0.2032,  0.0225,  0.0913, -0.0377],\n",
      "        [ 0.0436, -0.0289, -0.0425, -0.0073, -0.0144],\n",
      "        [ 0.0643,  0.2645, -0.0103, -0.0317,  0.0329],\n",
      "        [ 0.0411, -0.0333, -0.0580, -0.0226, -0.0237]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0227, grad_fn=<MinBackward1>), tensor(0.9772, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.45814788341522217\n",
      "@sample 36: tensor([[-0.0034, -0.0253, -0.0603,  0.1185, -0.0032],\n",
      "        [ 0.0478,  0.0725,  0.0282,  0.0048, -0.0350],\n",
      "        [-0.0294,  0.1440,  0.0726, -0.0999,  0.0303],\n",
      "        [ 0.0476, -0.0127, -0.0385,  0.1486, -0.0281],\n",
      "        [-0.0195,  0.0289,  0.0920, -0.0854,  0.0662],\n",
      "        [ 0.0973,  0.1031,  0.0766, -0.0666, -0.0170],\n",
      "        [ 0.0212,  0.0837,  0.0737,  0.1690,  0.0385],\n",
      "        [ 0.0580,  0.0245,  0.0041, -0.1941,  0.1701]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0314,  0.1130, -0.0989, -0.0122,  0.1359],\n",
      "        [-0.0037, -0.0235, -0.0380,  0.0562, -0.0964],\n",
      "        [-0.0912, -0.0842, -0.0860,  0.0828,  0.0271],\n",
      "        [ 0.0678,  0.1422,  0.0185, -0.0107,  0.0583],\n",
      "        [-0.0218, -0.1651, -0.1404,  0.0904, -0.1134],\n",
      "        [ 0.0211, -0.1644,  0.0744,  0.0402, -0.0460],\n",
      "        [ 0.0796,  0.2120,  0.0327, -0.0450, -0.0300],\n",
      "        [ 0.0113, -0.1687, -0.0318,  0.0524, -0.0290]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0063, grad_fn=<MinBackward1>), tensor(0.9836, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.47985097765922546\n",
      "@sample 37: tensor([[ 0.1549,  0.0164, -0.0781,  0.0945, -0.0457],\n",
      "        [ 0.0128,  0.0310,  0.0602, -0.1572,  0.0323],\n",
      "        [-0.0476,  0.1439,  0.0328, -0.1285,  0.0763],\n",
      "        [-0.0341,  0.0946,  0.0146, -0.0915,  0.0579],\n",
      "        [ 0.0203, -0.0635,  0.0176,  0.0522,  0.0294],\n",
      "        [ 0.0510,  0.1026, -0.0783, -0.0261, -0.0482],\n",
      "        [ 0.0506,  0.1188, -0.0854, -0.0194,  0.0106],\n",
      "        [ 0.0379,  0.1236, -0.0354, -0.1111, -0.0452]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0646, -0.0934, -0.0460, -0.0298, -0.0258],\n",
      "        [-0.0351, -0.2339, -0.1019,  0.0708,  0.0010],\n",
      "        [-0.0453, -0.1531, -0.1420,  0.0815, -0.0025],\n",
      "        [-0.0231, -0.1259, -0.2029,  0.1471,  0.0810],\n",
      "        [-0.0394, -0.0027, -0.1223,  0.1026,  0.0873],\n",
      "        [-0.0466, -0.0589,  0.0010,  0.0764, -0.0766],\n",
      "        [ 0.0265, -0.0228, -0.0221,  0.0582, -0.0182],\n",
      "        [-0.0129, -0.0780,  0.0474,  0.0522, -0.0804]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0220, grad_fn=<MinBackward1>), tensor(0.9892, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.4652581512928009\n",
      "@sample 38: tensor([[ 0.0218, -0.0183,  0.0745,  0.0380, -0.0777],\n",
      "        [ 0.0094,  0.0610, -0.0436,  0.1136,  0.0478],\n",
      "        [ 0.0201,  0.0297,  0.0206, -0.0332, -0.0184],\n",
      "        [ 0.1029,  0.0019,  0.0052, -0.0288,  0.1235],\n",
      "        [ 0.0559,  0.1033, -0.0805, -0.0460,  0.0300],\n",
      "        [ 0.0515, -0.0016, -0.0738, -0.0444,  0.0705],\n",
      "        [-0.0376,  0.1221,  0.0283,  0.0658,  0.0858],\n",
      "        [-0.0126, -0.0382,  0.0779,  0.0312, -0.0436]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-6.7575e-02, -5.6525e-03,  3.0224e-02, -9.3198e-03, -1.5495e-01],\n",
      "        [ 1.2802e-01,  9.1930e-02, -3.7171e-02,  8.5497e-02, -1.1933e-02],\n",
      "        [-3.1526e-03, -1.3148e-01, -4.3250e-02,  6.0451e-02, -3.7871e-02],\n",
      "        [ 1.2112e-01, -2.9339e-02,  2.6661e-02, -3.9077e-03, -9.2816e-03],\n",
      "        [ 6.9483e-02, -1.8907e-03,  1.5656e-02,  1.1895e-02, -6.9510e-02],\n",
      "        [ 3.2438e-02, -2.3846e-02, -9.3941e-02,  1.4220e-02,  4.7454e-02],\n",
      "        [ 1.8843e-01,  1.9529e-01, -1.8159e-04,  5.9813e-02, -8.2023e-04],\n",
      "        [-1.7911e-01, -1.0807e-01, -1.6761e-02, -8.0778e-03, -1.1865e-01]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0186, grad_fn=<MinBackward1>), tensor(0.9828, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.41014358401298523\n",
      "@sample 39: tensor([[ 0.0558,  0.0780, -0.0243, -0.0653, -0.0176],\n",
      "        [ 0.0178,  0.0498,  0.0340, -0.1030,  0.0419],\n",
      "        [ 0.0437,  0.0772, -0.1097, -0.0332,  0.0198],\n",
      "        [-0.0749, -0.0150,  0.0053, -0.1248,  0.0724],\n",
      "        [-0.0127,  0.0920, -0.0553, -0.0946,  0.0177],\n",
      "        [ 0.0315,  0.1257, -0.0330, -0.0332,  0.0049],\n",
      "        [-0.0355,  0.1177,  0.0282, -0.0745,  0.0413],\n",
      "        [ 0.0768, -0.1014, -0.0039,  0.1016, -0.0187]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0477, -0.0883,  0.0423, -0.0152, -0.1426],\n",
      "        [-0.0925, -0.1552, -0.0322,  0.0306, -0.0727],\n",
      "        [ 0.0705,  0.0048,  0.0562,  0.0474, -0.0525],\n",
      "        [-0.1549, -0.1801, -0.0221,  0.0145, -0.1585],\n",
      "        [-0.0434, -0.1086, -0.0523,  0.0792, -0.0873],\n",
      "        [-0.0147, -0.0320, -0.0020,  0.0513, -0.0292],\n",
      "        [-0.0470, -0.1071, -0.1098,  0.1109,  0.0162],\n",
      "        [ 0.0945, -0.0268,  0.1976, -0.0860, -0.0473]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0156, grad_fn=<MinBackward1>), tensor(0.9781, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.4321865737438202\n",
      "@sample 40: tensor([[ 0.0193,  0.0070,  0.0439, -0.1106,  0.0696],\n",
      "        [ 0.0361, -0.1541,  0.0045,  0.0352, -0.0295],\n",
      "        [ 0.0624,  0.0250,  0.0544, -0.1051,  0.0181],\n",
      "        [-0.0506,  0.1071,  0.0767, -0.1683,  0.0771],\n",
      "        [ 0.0500,  0.1732, -0.1566, -0.0242, -0.0098],\n",
      "        [ 0.0296,  0.0741,  0.0462, -0.0953, -0.0103],\n",
      "        [-0.0820, -0.1182,  0.0914,  0.0051, -0.1008],\n",
      "        [ 0.0532,  0.0020, -0.0436,  0.0159, -0.1480]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0168, -0.1265, -0.1024,  0.0695,  0.0255],\n",
      "        [-0.0785, -0.0681, -0.0158, -0.0209,  0.0270],\n",
      "        [ 0.0137, -0.0823,  0.1689, -0.0505, -0.1035],\n",
      "        [-0.0643, -0.1866, -0.0970,  0.0573, -0.0341],\n",
      "        [ 0.0617, -0.0315,  0.0838, -0.0562, -0.0909],\n",
      "        [-0.0114, -0.0905,  0.0417,  0.0097, -0.0491],\n",
      "        [-0.1335, -0.1113, -0.0370,  0.0379, -0.0420],\n",
      "        [-0.0388, -0.1062,  0.1679, -0.0391, -0.1142]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0131, grad_fn=<MinBackward1>), tensor(0.9817, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.4730919599533081\n",
      "@sample 41: tensor([[ 0.0010,  0.0230, -0.0022,  0.0777, -0.0121],\n",
      "        [-0.0344,  0.0466, -0.0206, -0.0158, -0.0576],\n",
      "        [-0.0037, -0.0204,  0.0174, -0.0184,  0.0309],\n",
      "        [ 0.0817, -0.0412,  0.0220, -0.0250, -0.0221],\n",
      "        [-0.0003, -0.0578,  0.0980, -0.0643,  0.0732],\n",
      "        [-0.0584,  0.0326,  0.0140, -0.0530, -0.0394],\n",
      "        [-0.0223,  0.1122,  0.0763, -0.1570,  0.0799],\n",
      "        [-0.0205,  0.1506,  0.0402, -0.1178,  0.0408]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.1448,  0.0503, -0.0234,  0.0542,  0.0276],\n",
      "        [ 0.0220, -0.0984, -0.0438,  0.0621,  0.0186],\n",
      "        [-0.1576, -0.0740, -0.0120, -0.0508,  0.0889],\n",
      "        [ 0.0728, -0.0106,  0.0566,  0.0209, -0.0135],\n",
      "        [-0.0140, -0.0698, -0.0157, -0.0993, -0.0089],\n",
      "        [-0.1725, -0.1271, -0.0076, -0.0868, -0.0901],\n",
      "        [-0.0540, -0.0851, -0.2136,  0.0778,  0.0681],\n",
      "        [-0.0277, -0.1152, -0.0668,  0.0558,  0.0064]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0162, grad_fn=<MinBackward1>), tensor(0.9880, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.40698209404945374\n",
      "@sample 42: tensor([[ 0.0659, -0.0126, -0.0931,  0.0935, -0.0419],\n",
      "        [-0.0064, -0.0880,  0.0591,  0.1960,  0.0665],\n",
      "        [ 0.0215, -0.0367, -0.0448,  0.0764,  0.1056],\n",
      "        [-0.0161,  0.0587, -0.0044,  0.0625,  0.0603],\n",
      "        [ 0.0892,  0.0260, -0.0543,  0.0050,  0.0550],\n",
      "        [ 0.0601,  0.1045,  0.0384,  0.0641,  0.0252],\n",
      "        [ 0.0952, -0.2131, -0.1676,  0.2621, -0.0899],\n",
      "        [-0.0584, -0.0181,  0.0140,  0.0362, -0.0777]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0112,  0.0409, -0.0312,  0.0550, -0.0769],\n",
      "        [ 0.1692,  0.1027, -0.0338,  0.0287, -0.1075],\n",
      "        [-0.0293, -0.0329, -0.1207, -0.0118, -0.0637],\n",
      "        [ 0.1769,  0.0577,  0.0521, -0.0236, -0.0815],\n",
      "        [ 0.1137,  0.0538, -0.0511, -0.0239,  0.0430],\n",
      "        [-0.0209,  0.0927, -0.0238, -0.0232,  0.0013],\n",
      "        [ 0.1889,  0.1309,  0.2967, -0.1289, -0.0383],\n",
      "        [ 0.0272,  0.1109, -0.0004,  0.0309, -0.0310]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0114, grad_fn=<MinBackward1>), tensor(0.9753, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.5072332620620728\n",
      "@sample 43: tensor([[ 0.0304,  0.0991, -0.0052, -0.0602,  0.0067],\n",
      "        [ 0.0442,  0.0047,  0.0181,  0.0215, -0.0909],\n",
      "        [-0.0311,  0.0152, -0.0651, -0.0491,  0.0189],\n",
      "        [ 0.0418, -0.0571,  0.0669, -0.0230, -0.0884],\n",
      "        [ 0.0330, -0.2039,  0.0563,  0.0109, -0.0057],\n",
      "        [ 0.0203, -0.0069, -0.0380, -0.0354,  0.0976],\n",
      "        [ 0.0173,  0.0012,  0.0210,  0.0392, -0.0777],\n",
      "        [ 0.0243, -0.0323,  0.0512,  0.0252, -0.1217]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0128, -0.0525,  0.0271, -0.0063, -0.0511],\n",
      "        [ 0.0125, -0.1094, -0.0478,  0.0117,  0.0422],\n",
      "        [ 0.0128,  0.0026,  0.0516,  0.0730,  0.0236],\n",
      "        [-0.0241, -0.0940,  0.1044, -0.0352,  0.0015],\n",
      "        [-0.0696,  0.0183,  0.0320,  0.0041, -0.1216],\n",
      "        [ 0.0125, -0.0655,  0.1118, -0.0621,  0.0697],\n",
      "        [ 0.0242, -0.1272,  0.0883, -0.0197, -0.0584],\n",
      "        [-0.0490, -0.0968,  0.0524, -0.0138, -0.0909]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0182, grad_fn=<MinBackward1>), tensor(0.9885, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3771814703941345\n",
      "@sample 44: tensor([[ 0.0105,  0.0163,  0.0136,  0.0205,  0.0045],\n",
      "        [ 0.0031,  0.1217,  0.0685, -0.0630,  0.0118],\n",
      "        [-0.0206,  0.0253, -0.0167, -0.0607, -0.0712],\n",
      "        [-0.0301,  0.1192,  0.0331, -0.0042,  0.0225],\n",
      "        [-0.0239,  0.0580, -0.0510,  0.0006,  0.0056],\n",
      "        [ 0.0620, -0.0706,  0.0131,  0.0082, -0.0057],\n",
      "        [ 0.0843, -0.1242, -0.1282,  0.1506, -0.1428],\n",
      "        [ 0.0136,  0.0246, -0.0828,  0.0594, -0.0231]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0864, -0.0140, -0.0653,  0.0920,  0.0067],\n",
      "        [-0.0736,  0.0494,  0.0140,  0.0832, -0.0137],\n",
      "        [-0.0638, -0.0958, -0.0577,  0.1061,  0.0503],\n",
      "        [-0.0079, -0.0346, -0.0304, -0.0104,  0.1057],\n",
      "        [-0.1317, -0.0533, -0.0681,  0.1371, -0.0726],\n",
      "        [ 0.0332, -0.0625,  0.0909, -0.0005,  0.0226],\n",
      "        [ 0.0857,  0.0385,  0.1329, -0.0371, -0.0422],\n",
      "        [ 0.0173, -0.0419, -0.1214,  0.0514,  0.0189]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0033, grad_fn=<MinBackward1>), tensor(0.9801, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3973143994808197\n",
      "@sample 45: tensor([[-0.0197,  0.0753, -0.0086, -0.0493, -0.0615],\n",
      "        [-0.0385, -0.1673,  0.0525,  0.0133, -0.1092],\n",
      "        [ 0.0904, -0.0456,  0.0195, -0.0346, -0.0177],\n",
      "        [ 0.0002, -0.0135,  0.0877, -0.0096, -0.0812],\n",
      "        [-0.0486, -0.0842,  0.0348, -0.0002, -0.0317],\n",
      "        [ 0.0030, -0.1009, -0.1350,  0.1014, -0.0397],\n",
      "        [ 0.0402, -0.0536,  0.0616, -0.0223, -0.1118],\n",
      "        [ 0.0281,  0.0944,  0.0090,  0.0760,  0.0691]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0371, -0.0111,  0.0067, -0.0297, -0.0897],\n",
      "        [-0.0844, -0.0361,  0.0824,  0.0181,  0.0456],\n",
      "        [ 0.0030,  0.0329,  0.0467,  0.0608, -0.0714],\n",
      "        [-0.0288, -0.1737, -0.0335,  0.0269,  0.0112],\n",
      "        [-0.1807, -0.0590, -0.0142,  0.0321, -0.0223],\n",
      "        [-0.0024,  0.0806, -0.0644,  0.0252, -0.0171],\n",
      "        [-0.0172, -0.0844,  0.0864,  0.0150, -0.0968],\n",
      "        [ 0.0418, -0.0228, -0.0757,  0.0857, -0.0490]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0186, grad_fn=<MinBackward1>), tensor(0.9913, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.39832061529159546\n",
      "@sample 46: tensor([[ 0.0259,  0.0264, -0.0267, -0.0070,  0.0249],\n",
      "        [-0.0720,  0.0943,  0.0204, -0.1197,  0.0648],\n",
      "        [ 0.0158,  0.0729,  0.0499, -0.1003,  0.0260],\n",
      "        [-0.0335,  0.0073,  0.0762,  0.0109, -0.0262],\n",
      "        [ 0.0482, -0.0239, -0.0029,  0.0063, -0.1131],\n",
      "        [ 0.0010,  0.0242,  0.0244,  0.0322,  0.0355],\n",
      "        [-0.0035, -0.0219, -0.0169, -0.0549,  0.0304],\n",
      "        [ 0.0639,  0.0492,  0.0076,  0.0072,  0.0945]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-3.7017e-02, -5.4412e-03, -1.8760e-02,  9.8339e-02, -8.4979e-02],\n",
      "        [-1.7546e-01, -1.0624e-01, -1.3710e-01, -1.9707e-02, -4.2001e-02],\n",
      "        [-1.8519e-02, -3.5905e-02, -6.1963e-02,  1.0835e-01, -7.3354e-02],\n",
      "        [-8.5219e-02,  3.7483e-02,  5.6133e-02, -4.6473e-02,  9.5358e-02],\n",
      "        [ 1.0040e-02, -5.5097e-02,  4.3255e-02, -1.7227e-04, -9.2322e-02],\n",
      "        [ 2.9580e-02, -6.4757e-03,  3.2298e-02, -5.1418e-02,  4.3790e-03],\n",
      "        [ 1.7301e-02, -1.3486e-01,  9.3847e-02, -2.0592e-02, -2.8705e-02],\n",
      "        [ 4.6844e-02,  7.9875e-02, -1.4586e-02, -2.9662e-02, -3.1721e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0208, grad_fn=<MinBackward1>), tensor(0.9824, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3613661229610443\n",
      "@sample 47: tensor([[-0.0125,  0.0871,  0.0495, -0.0292,  0.0887],\n",
      "        [-0.0072,  0.0391, -0.0919,  0.1122, -0.0952],\n",
      "        [-0.1156,  0.0245, -0.0760, -0.0839,  0.0500],\n",
      "        [ 0.1053, -0.1135,  0.0339,  0.1289, -0.1521],\n",
      "        [ 0.0096,  0.0540, -0.0059,  0.0028, -0.0582],\n",
      "        [-0.0734, -0.0941, -0.0246,  0.1123, -0.0442],\n",
      "        [-0.0571, -0.0153,  0.0314,  0.0409,  0.0581],\n",
      "        [ 0.0098,  0.0948,  0.0450, -0.0082, -0.0247]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0973, -0.0464, -0.0282,  0.0898,  0.0028],\n",
      "        [ 0.0845,  0.0164,  0.0231,  0.0366,  0.0551],\n",
      "        [ 0.0660, -0.0600,  0.0066,  0.0260,  0.0762],\n",
      "        [ 0.0155, -0.0065,  0.1617, -0.1308, -0.0268],\n",
      "        [-0.0304, -0.0882, -0.1071,  0.1012,  0.0931],\n",
      "        [-0.0041,  0.1151, -0.0321,  0.0559, -0.1051],\n",
      "        [-0.0131,  0.0533, -0.0784, -0.0302, -0.0437],\n",
      "        [-0.0189,  0.0609, -0.0681, -0.0138, -0.0689]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0170, grad_fn=<MinBackward1>), tensor(0.9773, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.41366758942604065\n",
      "@sample 48: tensor([[ 0.0465, -0.0255,  0.0770,  0.0016, -0.0109],\n",
      "        [-0.0213, -0.1040, -0.0452, -0.0146, -0.0489],\n",
      "        [ 0.0137, -0.0092,  0.0228, -0.0209,  0.1476],\n",
      "        [ 0.0718, -0.0180, -0.0925,  0.0829, -0.0252],\n",
      "        [ 0.0133,  0.0184, -0.0337, -0.0362, -0.0163],\n",
      "        [ 0.0552,  0.0618, -0.0459, -0.0489,  0.0165],\n",
      "        [ 0.0041,  0.0653,  0.0196, -0.0618,  0.0414],\n",
      "        [ 0.0027, -0.1110,  0.0089,  0.0165,  0.0919]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0326, -0.0553,  0.0520, -0.0575, -0.0120],\n",
      "        [-0.0324,  0.0221, -0.0323,  0.0515,  0.0027],\n",
      "        [ 0.1625, -0.0175, -0.0872,  0.0168,  0.0182],\n",
      "        [ 0.0740,  0.0887,  0.0364, -0.0219,  0.0143],\n",
      "        [-0.0329,  0.0152, -0.0868,  0.0564,  0.0167],\n",
      "        [-0.0177, -0.0176, -0.0619,  0.0731, -0.0905],\n",
      "        [-0.0435, -0.0047, -0.0689,  0.1237,  0.0105],\n",
      "        [-0.0394,  0.0267, -0.1453,  0.1203,  0.0181]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0128, grad_fn=<MinBackward1>), tensor(0.9897, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3636015057563782\n",
      "@sample 49: tensor([[ 0.0455,  0.0158, -0.1242, -0.0522,  0.0901],\n",
      "        [ 0.0308, -0.0493,  0.0403,  0.0488, -0.0249],\n",
      "        [-0.0357,  0.0006, -0.0799,  0.0238,  0.0239],\n",
      "        [ 0.0549,  0.0415, -0.0456,  0.0034,  0.0216],\n",
      "        [ 0.0111, -0.0395,  0.0839, -0.0609, -0.0087],\n",
      "        [-0.0290, -0.1246,  0.0029,  0.0014, -0.0263],\n",
      "        [-0.0446,  0.0273,  0.0398,  0.1016,  0.1075],\n",
      "        [ 0.0524,  0.0441, -0.0102, -0.0466,  0.0393]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0224,  0.0331, -0.0687,  0.0468,  0.0761],\n",
      "        [ 0.0193, -0.0041,  0.1082, -0.0737, -0.0071],\n",
      "        [-0.1004, -0.0076, -0.0942,  0.0054,  0.0571],\n",
      "        [ 0.0646,  0.0540, -0.0052, -0.0279, -0.0350],\n",
      "        [-0.0958, -0.0739, -0.0914,  0.0468, -0.0303],\n",
      "        [ 0.0401, -0.1265, -0.0163,  0.0043,  0.0561],\n",
      "        [ 0.0693,  0.1024, -0.0950, -0.0611,  0.0157],\n",
      "        [-0.0173, -0.0075, -0.0907,  0.0883,  0.0026]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0172, grad_fn=<MinBackward1>), tensor(0.9837, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.36246392130851746\n",
      "@sample 50: tensor([[ 0.0457, -0.0968, -0.0632, -0.0920, -0.0423],\n",
      "        [-0.0446,  0.0113,  0.0098,  0.0332, -0.0708],\n",
      "        [-0.0673,  0.0970,  0.0538,  0.0038,  0.0183],\n",
      "        [-0.0212, -0.0346, -0.0659,  0.0781,  0.0122],\n",
      "        [ 0.0180, -0.0274,  0.0593,  0.0600, -0.0310],\n",
      "        [-0.0310,  0.0462, -0.0518,  0.0032,  0.0415],\n",
      "        [ 0.0250,  0.0298,  0.0276,  0.0270, -0.0427],\n",
      "        [-0.0004, -0.0349,  0.0062, -0.1119, -0.0387]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-3.4466e-02, -1.3077e-01, -1.9202e-03,  9.0876e-02,  2.5218e-02],\n",
      "        [ 3.6734e-02,  2.7473e-03, -4.6049e-02,  5.6508e-02,  1.1705e-01],\n",
      "        [ 2.3039e-02, -7.4938e-02, -1.3796e-01,  1.3979e-01,  4.8282e-02],\n",
      "        [ 1.4610e-02,  7.2323e-02, -1.9910e-02,  8.0112e-02,  1.2279e-01],\n",
      "        [-1.7230e-02,  1.2402e-01,  4.1410e-02,  3.7387e-05,  3.1773e-02],\n",
      "        [-5.1454e-02, -2.8155e-02, -6.2458e-02,  6.5142e-03, -2.6951e-02],\n",
      "        [-3.2596e-02,  2.2299e-02, -1.9902e-02, -3.1152e-02, -4.5991e-02],\n",
      "        [-7.9841e-02, -9.7193e-02, -1.9851e-02, -1.7573e-02, -7.1445e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0047, grad_fn=<MinBackward1>), tensor(0.9847, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.35867738723754883\n",
      "@sample 51: tensor([[ 3.4237e-02, -4.6278e-03, -4.5727e-02, -5.5182e-02,  2.2721e-02],\n",
      "        [ 4.4849e-02,  2.9161e-02,  9.0006e-02, -2.1976e-02, -1.3554e-02],\n",
      "        [ 2.2764e-02, -1.1112e-01, -1.8767e-02,  1.0442e-01, -1.4716e-01],\n",
      "        [ 3.8608e-02,  1.4545e-01,  1.8424e-02,  8.5617e-02,  3.7131e-02],\n",
      "        [ 7.7257e-02, -4.7822e-02,  2.4687e-02, -8.4065e-02,  1.3993e-04],\n",
      "        [-4.4012e-02,  1.3437e-02,  8.4620e-02, -1.9619e-02, -8.0384e-02],\n",
      "        [ 5.1798e-02,  4.3518e-02,  5.0013e-02,  2.8760e-02,  8.2003e-03],\n",
      "        [ 7.3115e-02, -6.0310e-02, -9.3578e-03,  9.9623e-02,  2.8136e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-4.7127e-03,  4.1239e-05, -5.0685e-02,  4.7324e-02,  3.4371e-02],\n",
      "        [ 4.6971e-02, -9.8307e-04, -2.3208e-02, -3.8640e-03, -5.7974e-02],\n",
      "        [-3.1191e-02,  7.1180e-02, -4.8898e-03, -3.9044e-02,  4.8267e-02],\n",
      "        [ 4.8257e-02,  7.6713e-02, -7.8003e-02,  5.4426e-02, -1.5070e-02],\n",
      "        [-1.1251e-01, -1.0039e-01,  6.9222e-02, -2.5368e-03, -8.4576e-02],\n",
      "        [-1.2234e-01, -1.9333e-02,  5.8242e-03,  2.5276e-03, -1.0718e-01],\n",
      "        [ 9.1131e-02,  1.3001e-02,  4.4017e-02, -4.4642e-02,  3.2987e-02],\n",
      "        [ 2.1374e-02,  8.1604e-02,  1.9410e-02, -4.1066e-02,  4.4867e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0053, grad_fn=<MinBackward1>), tensor(0.9911, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3780086934566498\n",
      "@sample 52: tensor([[-0.0286,  0.0499,  0.0423, -0.0449, -0.0282],\n",
      "        [ 0.0059,  0.0168, -0.0716,  0.0100, -0.0751],\n",
      "        [-0.0491, -0.0554, -0.0346,  0.0605, -0.0553],\n",
      "        [-0.0572,  0.0066,  0.0004,  0.0473,  0.0021],\n",
      "        [ 0.0291,  0.0843,  0.0757,  0.0039, -0.0061],\n",
      "        [-0.0015,  0.0421,  0.0685,  0.0089, -0.0888],\n",
      "        [ 0.0298,  0.0749, -0.0616, -0.0074,  0.0309],\n",
      "        [-0.0055,  0.0979, -0.0343, -0.0539, -0.0114]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0406, -0.0631, -0.0255,  0.0805, -0.0347],\n",
      "        [-0.0451, -0.0762, -0.0671,  0.0766,  0.0245],\n",
      "        [-0.0176, -0.0600,  0.1160, -0.0658, -0.1163],\n",
      "        [-0.0007,  0.0281,  0.0395, -0.0493, -0.0005],\n",
      "        [ 0.0315,  0.1216,  0.0183, -0.0536, -0.0564],\n",
      "        [-0.1033,  0.0706, -0.0670,  0.0851,  0.0217],\n",
      "        [ 0.1082,  0.0472,  0.0573, -0.0170, -0.0402],\n",
      "        [-0.0572, -0.1245, -0.1670,  0.1163,  0.0611]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0137, grad_fn=<MinBackward1>), tensor(0.9809, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.36830464005470276\n",
      "@sample 53: tensor([[-0.0143, -0.0283, -0.0153,  0.0454, -0.0635],\n",
      "        [-0.0243, -0.0410,  0.0164, -0.0196,  0.0574],\n",
      "        [-0.0444, -0.0572,  0.0370,  0.0910, -0.0492],\n",
      "        [ 0.0104,  0.0837,  0.0912, -0.0416,  0.0208],\n",
      "        [ 0.0230,  0.1025,  0.0699, -0.1150,  0.0318],\n",
      "        [-0.0476,  0.0531, -0.0290, -0.0660,  0.0402],\n",
      "        [-0.0759, -0.0487, -0.0564,  0.0133,  0.0513],\n",
      "        [ 0.0202,  0.0101,  0.0091,  0.0183, -0.0044]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0312,  0.0347, -0.0238,  0.0449, -0.0822],\n",
      "        [-0.0152,  0.0221,  0.0830,  0.0948, -0.0018],\n",
      "        [ 0.0061,  0.1398,  0.1241, -0.1056,  0.0323],\n",
      "        [ 0.0576, -0.0029,  0.0044, -0.0319,  0.0482],\n",
      "        [ 0.0035, -0.0194, -0.0050,  0.0149, -0.0574],\n",
      "        [-0.0888, -0.1080, -0.0072,  0.0782, -0.0670],\n",
      "        [-0.0222,  0.0309,  0.1375, -0.0705,  0.0147],\n",
      "        [-0.0748,  0.0180, -0.0655,  0.0097, -0.0366]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0074, grad_fn=<MinBackward1>), tensor(0.9869, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.36151352524757385\n",
      "@sample 54: tensor([[ 0.0448,  0.0130,  0.0142, -0.0230, -0.0376],\n",
      "        [-0.0259, -0.0370,  0.0070,  0.0283, -0.0463],\n",
      "        [ 0.0069,  0.0603, -0.0167,  0.0668,  0.0014],\n",
      "        [-0.0361,  0.0467,  0.0009, -0.0201, -0.0550],\n",
      "        [-0.0638,  0.0383,  0.0070,  0.0017, -0.0386],\n",
      "        [-0.0277,  0.0861, -0.0263,  0.0041,  0.0736],\n",
      "        [ 0.0226,  0.1246, -0.0558, -0.1231,  0.0302],\n",
      "        [ 0.0336,  0.0781, -0.0292,  0.0080, -0.0761]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0187, -0.0058,  0.0597, -0.0334, -0.0375],\n",
      "        [-0.0264,  0.0798, -0.0174,  0.0958, -0.0231],\n",
      "        [ 0.1055,  0.0534,  0.0943, -0.0787, -0.0264],\n",
      "        [-0.0553, -0.0441, -0.0804,  0.0396, -0.0213],\n",
      "        [ 0.0321, -0.0326, -0.1089,  0.0218,  0.0725],\n",
      "        [-0.0351,  0.0089, -0.0039, -0.0145,  0.0384],\n",
      "        [-0.0425, -0.0702, -0.0540,  0.0318,  0.0047],\n",
      "        [ 0.0473,  0.0474,  0.1135, -0.0217, -0.0175]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0109, grad_fn=<MinBackward1>), tensor(0.9874, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3430505394935608\n",
      "@sample 55: tensor([[ 0.0037, -0.0311, -0.0215, -0.0795,  0.0825],\n",
      "        [ 0.0162, -0.0111,  0.0595,  0.0728, -0.0144],\n",
      "        [-0.0339, -0.0738,  0.0002,  0.0069, -0.0992],\n",
      "        [ 0.0064,  0.0364,  0.0687, -0.0317,  0.0243],\n",
      "        [ 0.0633, -0.0420, -0.0069, -0.0038, -0.0096],\n",
      "        [-0.0319, -0.0444, -0.0126,  0.0655, -0.0257],\n",
      "        [ 0.0113, -0.1054,  0.0022,  0.0830,  0.1210],\n",
      "        [ 0.0119, -0.0434, -0.0543,  0.0915, -0.0110]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.1551, -0.0378, -0.0065,  0.0261,  0.0655],\n",
      "        [-0.0055,  0.0514, -0.0405, -0.0411, -0.0418],\n",
      "        [ 0.0350,  0.0129,  0.2131, -0.1061, -0.0859],\n",
      "        [-0.0792,  0.0221, -0.0100, -0.0311, -0.0062],\n",
      "        [ 0.0198,  0.1187,  0.0706,  0.0063, -0.0009],\n",
      "        [-0.0321,  0.0564,  0.1203, -0.1307,  0.1114],\n",
      "        [ 0.0431,  0.0134, -0.0309,  0.0427,  0.0484],\n",
      "        [ 0.1285, -0.0075,  0.0633, -0.0875,  0.0628]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0241, grad_fn=<MinBackward1>), tensor(0.9819, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3721323013305664\n",
      "@sample 56: tensor([[-0.0039, -0.0027,  0.0140,  0.0002, -0.0304],\n",
      "        [ 0.0096,  0.0897, -0.0236,  0.0170,  0.0509],\n",
      "        [-0.0096, -0.1279, -0.0351,  0.0570,  0.0363],\n",
      "        [ 0.0491, -0.1120,  0.0200,  0.0630,  0.0090],\n",
      "        [-0.0233, -0.0399,  0.0873, -0.0219,  0.0269],\n",
      "        [-0.0785, -0.0041, -0.0132, -0.0402, -0.0060],\n",
      "        [-0.0172, -0.0296,  0.1145,  0.0466, -0.0224],\n",
      "        [-0.0220,  0.0299, -0.0283,  0.0074, -0.0055]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0139, -0.1041, -0.0127,  0.0183, -0.0188],\n",
      "        [ 0.0504,  0.0088,  0.0187, -0.0520,  0.0380],\n",
      "        [ 0.0327,  0.0441,  0.1059, -0.0065, -0.0076],\n",
      "        [ 0.0163,  0.0275,  0.1411, -0.0544,  0.0042],\n",
      "        [-0.0061,  0.0920, -0.0549,  0.0056,  0.0510],\n",
      "        [ 0.0254, -0.0878,  0.0168,  0.1383,  0.0877],\n",
      "        [-0.0471, -0.0699, -0.0430,  0.0950, -0.0313],\n",
      "        [-0.0132, -0.0837,  0.0796,  0.0191, -0.0344]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0127, grad_fn=<MinBackward1>), tensor(0.9812, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3472196161746979\n",
      "@sample 57: tensor([[-0.0195, -0.0070, -0.1668, -0.0437,  0.0683],\n",
      "        [ 0.0170, -0.0681,  0.0399,  0.0328, -0.0499],\n",
      "        [ 0.0214,  0.0740,  0.0348, -0.0511,  0.0178],\n",
      "        [-0.0572,  0.0254, -0.0173,  0.0604, -0.0042],\n",
      "        [-0.0602, -0.0471, -0.0452, -0.0011,  0.0484],\n",
      "        [ 0.0070,  0.0396,  0.0603, -0.1744,  0.0392],\n",
      "        [ 0.0448,  0.0565,  0.0134, -0.0375,  0.0467],\n",
      "        [-0.0227,  0.1067,  0.0415, -0.0832,  0.0748]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.1268, -0.0300, -0.0034,  0.0838,  0.0287],\n",
      "        [ 0.0630, -0.0143,  0.0732, -0.0696,  0.0259],\n",
      "        [-0.0328, -0.0092, -0.0257,  0.0829, -0.0519],\n",
      "        [ 0.0496,  0.0303,  0.1079, -0.1026,  0.0119],\n",
      "        [ 0.0126,  0.0592,  0.0968, -0.0976,  0.0644],\n",
      "        [-0.1676, -0.1543, -0.1412,  0.0878, -0.0512],\n",
      "        [ 0.0392, -0.0645,  0.0199, -0.0434, -0.0700],\n",
      "        [-0.0126, -0.1096, -0.1386,  0.0652,  0.0392]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0290, grad_fn=<MinBackward1>), tensor(0.9907, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.4019593894481659\n",
      "@sample 58: tensor([[-0.0550,  0.0368,  0.0438, -0.0043,  0.0197],\n",
      "        [ 0.0298, -0.0639, -0.0439,  0.0457, -0.0508],\n",
      "        [-0.0534,  0.1702,  0.0406, -0.0588,  0.0206],\n",
      "        [-0.0353,  0.0270,  0.0719, -0.1330,  0.0715],\n",
      "        [-0.1152, -0.0222, -0.0293, -0.0752,  0.0667],\n",
      "        [ 0.0235,  0.0813,  0.0902, -0.0968, -0.0170],\n",
      "        [-0.1224, -0.0415,  0.0065, -0.0195,  0.0148],\n",
      "        [-0.0355, -0.1593, -0.0844,  0.0235,  0.0635]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0117, -0.0022, -0.1950,  0.1676,  0.0072],\n",
      "        [ 0.0678, -0.0154, -0.0134,  0.0291, -0.0342],\n",
      "        [-0.0518, -0.0535, -0.0464, -0.0030, -0.0659],\n",
      "        [-0.0334, -0.0364, -0.1208,  0.0385, -0.0312],\n",
      "        [-0.0167, -0.0774,  0.1055,  0.0007, -0.0860],\n",
      "        [ 0.0258, -0.0505, -0.1206,  0.1170,  0.0771],\n",
      "        [ 0.0019, -0.0573, -0.0158,  0.0600,  0.0073],\n",
      "        [-0.0936,  0.0300,  0.1295, -0.0943,  0.0018]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0133, grad_fn=<MinBackward1>), tensor(0.9868, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.40956878662109375\n",
      "@sample 59: tensor([[-4.3618e-02,  1.9679e-01,  7.4037e-02, -1.2596e-01,  8.6447e-02],\n",
      "        [ 2.0503e-02,  8.0547e-02,  9.0138e-03, -1.5720e-01,  3.7062e-02],\n",
      "        [-2.6469e-02,  1.1787e-01, -3.3622e-02, -4.2826e-02,  6.2198e-02],\n",
      "        [ 1.3344e-02, -2.1782e-02, -1.0610e-01, -6.8496e-03,  6.9387e-05],\n",
      "        [-5.2167e-02, -1.2237e-01, -2.3319e-02,  7.4124e-02, -9.9509e-02],\n",
      "        [ 5.7670e-03,  1.6098e-02,  1.1179e-01, -1.8560e-02,  4.9570e-02],\n",
      "        [ 1.7859e-02, -5.4224e-02, -2.7457e-02,  8.8455e-02, -6.6588e-02],\n",
      "        [ 1.0980e-02, -3.1436e-02, -1.1800e-01, -2.3251e-02,  5.9333e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0164, -0.1044, -0.1812,  0.0940,  0.0027],\n",
      "        [-0.0164, -0.1429, -0.0937,  0.0926,  0.0464],\n",
      "        [ 0.0008, -0.0096, -0.0343,  0.0286,  0.0785],\n",
      "        [-0.0271, -0.0798, -0.0314,  0.1157,  0.0485],\n",
      "        [ 0.0299,  0.0197,  0.1337,  0.0020,  0.0474],\n",
      "        [-0.0077, -0.0785, -0.1243,  0.0909, -0.0807],\n",
      "        [ 0.1268,  0.1650,  0.1799, -0.1341, -0.0915],\n",
      "        [-0.0046, -0.1071, -0.1180,  0.0806, -0.0930]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0153, grad_fn=<MinBackward1>), tensor(0.9764, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.45440879464149475\n",
      "@sample 60: tensor([[ 3.7297e-02,  7.1470e-03,  5.2442e-04, -6.1746e-02,  1.3851e-01],\n",
      "        [ 1.5399e-02, -5.1056e-02, -1.6076e-02, -8.4607e-03, -4.3092e-02],\n",
      "        [-2.4932e-02,  1.2671e-01, -7.9472e-02, -1.6158e-02,  3.1484e-02],\n",
      "        [-2.4681e-05,  1.0769e-01, -9.3152e-03, -6.2440e-02,  5.2734e-02],\n",
      "        [ 7.0736e-02, -3.9037e-02,  7.6483e-02, -5.0197e-02,  2.4315e-02],\n",
      "        [ 5.9227e-03,  6.2064e-02,  4.8908e-03,  2.6833e-02,  6.7212e-02],\n",
      "        [-2.7466e-02,  3.5599e-02,  6.8441e-02, -3.2212e-02,  5.9038e-02],\n",
      "        [-1.3569e-02, -1.7758e-02,  4.4550e-02, -8.0544e-03,  8.7469e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0260, -0.0481, -0.1184,  0.0316, -0.0209],\n",
      "        [-0.0010, -0.0007, -0.0319,  0.0398, -0.0769],\n",
      "        [-0.0141, -0.0503, -0.0795,  0.0200,  0.1087],\n",
      "        [ 0.0060, -0.0493, -0.0424,  0.0430, -0.0080],\n",
      "        [-0.0015, -0.1374, -0.0676,  0.0032, -0.0202],\n",
      "        [ 0.0751,  0.0244, -0.0136, -0.0655, -0.0657],\n",
      "        [-0.0576, -0.0620, -0.0750, -0.0285, -0.0424],\n",
      "        [ 0.1217,  0.0453,  0.0599, -0.0100, -0.0851]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0155, grad_fn=<MinBackward1>), tensor(0.9791, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.35732847452163696\n",
      "@sample 61: tensor([[-0.0155,  0.0437,  0.0705, -0.0526,  0.0306],\n",
      "        [-0.0024,  0.0439,  0.0667, -0.1182,  0.0120],\n",
      "        [ 0.0053, -0.0012, -0.0019,  0.0022, -0.0266],\n",
      "        [-0.0499,  0.1139,  0.0351, -0.0932,  0.0070],\n",
      "        [ 0.0220,  0.0127, -0.0221,  0.1119, -0.0132],\n",
      "        [ 0.0283, -0.0623, -0.0284,  0.0155, -0.0818],\n",
      "        [ 0.0009,  0.0780, -0.0306,  0.0482, -0.0659],\n",
      "        [ 0.0205, -0.0837,  0.0358,  0.0618, -0.1060]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0377, -0.0431, -0.0428,  0.1037, -0.0069],\n",
      "        [ 0.0493, -0.0786,  0.0143, -0.0382,  0.0598],\n",
      "        [ 0.0518, -0.0620,  0.0422,  0.0310,  0.0536],\n",
      "        [-0.0099, -0.1227, -0.0226,  0.0237,  0.0213],\n",
      "        [ 0.0212,  0.0877, -0.0172, -0.0031, -0.0342],\n",
      "        [ 0.0261, -0.0165,  0.0839,  0.0777, -0.0313],\n",
      "        [-0.0309,  0.1495, -0.0433, -0.0422, -0.0392],\n",
      "        [ 0.0047, -0.0735,  0.0919, -0.0520, -0.0284]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0110, grad_fn=<MinBackward1>), tensor(0.9825, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.36018094420433044\n",
      "@sample 62: tensor([[ 0.0843, -0.0307, -0.0382,  0.0247,  0.0078],\n",
      "        [ 0.0595,  0.0134,  0.0262, -0.0230, -0.0725],\n",
      "        [-0.0092, -0.0700,  0.0272, -0.0708,  0.1194],\n",
      "        [ 0.0040,  0.0349, -0.0009, -0.0221, -0.0422],\n",
      "        [-0.0390,  0.0497,  0.0506, -0.0088,  0.0242],\n",
      "        [ 0.0119,  0.0950, -0.0182, -0.0526,  0.0298],\n",
      "        [ 0.0577, -0.0080, -0.0545, -0.0411,  0.0789],\n",
      "        [-0.0097, -0.0252,  0.0239,  0.0147, -0.0083]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0226, -0.0030,  0.0388,  0.0077,  0.0100],\n",
      "        [-0.0609,  0.0057,  0.0363,  0.0147, -0.0708],\n",
      "        [ 0.0662, -0.0245, -0.0912,  0.0528,  0.1052],\n",
      "        [ 0.0513,  0.0011, -0.0330,  0.0135,  0.0479],\n",
      "        [ 0.1095,  0.0429,  0.1061, -0.1192, -0.0158],\n",
      "        [-0.0593, -0.0901, -0.1846,  0.1308,  0.0634],\n",
      "        [-0.0926, -0.0091,  0.0055, -0.0843, -0.0117],\n",
      "        [-0.0378, -0.0463, -0.0594, -0.0206, -0.0625]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0149, grad_fn=<MinBackward1>), tensor(0.9812, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3517773151397705\n",
      "@sample 63: tensor([[ 0.0091,  0.0755, -0.0167, -0.0642,  0.0279],\n",
      "        [-0.0086,  0.1373,  0.0515, -0.0752, -0.0254],\n",
      "        [ 0.0544,  0.0386,  0.0450, -0.0132,  0.0286],\n",
      "        [ 0.0029, -0.0542,  0.0603,  0.0588, -0.1124],\n",
      "        [ 0.0376,  0.0702, -0.0448, -0.0355, -0.0208],\n",
      "        [-0.0204,  0.0899,  0.0045, -0.0618,  0.0063],\n",
      "        [-0.0047, -0.0009, -0.0184,  0.0029, -0.0972],\n",
      "        [ 0.0043, -0.0357,  0.0006,  0.0650, -0.0478]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0342, -0.0533, -0.0372,  0.0601, -0.0689],\n",
      "        [-0.0418,  0.0184,  0.0493,  0.0081, -0.0086],\n",
      "        [ 0.0370, -0.0137,  0.0168,  0.0303,  0.0137],\n",
      "        [-0.0103, -0.0569,  0.0316, -0.0021, -0.0418],\n",
      "        [-0.0161,  0.0155, -0.0271,  0.0511, -0.0088],\n",
      "        [ 0.0090,  0.0211, -0.0910,  0.0442,  0.0019],\n",
      "        [ 0.0330, -0.0129,  0.0892, -0.0796,  0.0056],\n",
      "        [ 0.0572,  0.0237,  0.1200,  0.0063, -0.0415]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0069, grad_fn=<MinBackward1>), tensor(0.9777, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3412277400493622\n",
      "@sample 64: tensor([[-0.0371, -0.0680,  0.0204,  0.1197, -0.0678],\n",
      "        [ 0.0187, -0.0209, -0.0054,  0.0161, -0.0382],\n",
      "        [ 0.0109,  0.0089, -0.0226, -0.0417, -0.0523],\n",
      "        [-0.0315, -0.0182,  0.0390, -0.0735,  0.0926],\n",
      "        [ 0.0415,  0.0320, -0.0147, -0.0386,  0.0337],\n",
      "        [-0.0179,  0.0883,  0.0392, -0.0517,  0.0192],\n",
      "        [ 0.0420, -0.0448, -0.0115,  0.1172, -0.0664],\n",
      "        [-0.0345, -0.0011, -0.0004,  0.0641, -0.0640]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0378,  0.0897,  0.0441, -0.0966, -0.1121],\n",
      "        [ 0.0221,  0.0504, -0.0211,  0.0031,  0.0139],\n",
      "        [-0.0446,  0.0735, -0.0004,  0.0397,  0.0397],\n",
      "        [-0.0902,  0.0062, -0.1336,  0.0997,  0.1093],\n",
      "        [-0.0100, -0.0040, -0.0844,  0.0832, -0.0275],\n",
      "        [-0.0625, -0.0741, -0.0862,  0.0272, -0.0040],\n",
      "        [-0.0155, -0.0073,  0.0545, -0.0849, -0.0069],\n",
      "        [-0.0125,  0.1371, -0.0406,  0.0092, -0.0178]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0085, grad_fn=<MinBackward1>), tensor(0.9877, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.35755354166030884\n",
      "@sample 65: tensor([[-0.0199, -0.0423, -0.0639, -0.0124, -0.0027],\n",
      "        [ 0.0141,  0.0021, -0.0066, -0.0308, -0.0374],\n",
      "        [ 0.0241,  0.0356,  0.0957,  0.0065,  0.0772],\n",
      "        [-0.0301,  0.1207,  0.0580, -0.0930,  0.0684],\n",
      "        [-0.0100,  0.0476,  0.0895, -0.0261,  0.0004],\n",
      "        [ 0.0076,  0.1101,  0.0666, -0.0532,  0.0523],\n",
      "        [ 0.0665, -0.1225, -0.0372,  0.0771, -0.0412],\n",
      "        [-0.0228, -0.0051,  0.0278, -0.0628,  0.0832]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0035, -0.0414, -0.0858,  0.0914,  0.0302],\n",
      "        [-0.0046, -0.1008,  0.0768, -0.0610, -0.0546],\n",
      "        [ 0.0743,  0.0449, -0.0916,  0.0165, -0.0062],\n",
      "        [-0.0557, -0.0859, -0.1500,  0.1086,  0.0335],\n",
      "        [-0.0146, -0.0364,  0.0298,  0.0072, -0.0408],\n",
      "        [-0.0240, -0.0500, -0.0700,  0.0339, -0.0239],\n",
      "        [ 0.1219,  0.1421,  0.0523, -0.0082, -0.0049],\n",
      "        [-0.0215, -0.0635, -0.1276,  0.0085, -0.0029]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0132, grad_fn=<MinBackward1>), tensor(0.9709, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.37559109926223755\n",
      "@sample 66: tensor([[-0.0643, -0.0192,  0.0544,  0.0903, -0.0692],\n",
      "        [-0.0258,  0.0513, -0.0133, -0.0190,  0.0107],\n",
      "        [ 0.0255, -0.0284, -0.0150,  0.0245,  0.0400],\n",
      "        [-0.0588,  0.0054,  0.0157, -0.0665,  0.0544],\n",
      "        [-0.0434, -0.0235, -0.0273, -0.0164, -0.0176],\n",
      "        [ 0.0486,  0.0849, -0.0258, -0.0669, -0.0450],\n",
      "        [ 0.0142,  0.0902, -0.1399, -0.1468, -0.0038],\n",
      "        [ 0.0361, -0.0367,  0.0433, -0.0071,  0.0278]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0820,  0.0748,  0.0080, -0.0110,  0.0227],\n",
      "        [-0.0158, -0.0318, -0.0683, -0.0185,  0.0693],\n",
      "        [ 0.0929, -0.0314,  0.0368,  0.0117, -0.0148],\n",
      "        [-0.1015, -0.0099, -0.1020,  0.1102,  0.0644],\n",
      "        [ 0.0109, -0.0109,  0.1607, -0.0557,  0.0856],\n",
      "        [-0.0628, -0.0592, -0.1175,  0.0531,  0.0199],\n",
      "        [-0.0534, -0.1915, -0.0479,  0.0012, -0.0601],\n",
      "        [-0.0156, -0.0771,  0.0528, -0.0619,  0.0393]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0172, grad_fn=<MinBackward1>), tensor(0.9826, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.366229772567749\n",
      "@sample 67: tensor([[-0.0630,  0.0021,  0.0502, -0.1052,  0.0920],\n",
      "        [ 0.0517, -0.0879,  0.0151,  0.0100, -0.0907],\n",
      "        [-0.0244,  0.0475, -0.0869, -0.0905,  0.0814],\n",
      "        [-0.0144,  0.0496, -0.0069,  0.0365,  0.0122],\n",
      "        [ 0.0273, -0.0404,  0.0329,  0.0429, -0.0530],\n",
      "        [-0.0489,  0.1180, -0.0165, -0.0955, -0.0146],\n",
      "        [ 0.0028, -0.0079,  0.0643, -0.0427,  0.0038],\n",
      "        [ 0.0047, -0.0259,  0.0621,  0.0035,  0.0012]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0776,  0.0617, -0.0159,  0.0870,  0.0103],\n",
      "        [ 0.0035, -0.0646,  0.0934, -0.1000, -0.1009],\n",
      "        [ 0.1119,  0.0065,  0.0307,  0.0082, -0.0574],\n",
      "        [ 0.0676,  0.0378,  0.0037, -0.0643, -0.0321],\n",
      "        [ 0.0394,  0.0133,  0.0453,  0.0944, -0.0337],\n",
      "        [-0.1279, -0.0972, -0.1215,  0.0933, -0.0394],\n",
      "        [ 0.0395, -0.1085,  0.0127,  0.0711,  0.0551],\n",
      "        [-0.0258, -0.0458, -0.0976,  0.1154, -0.0093]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0222, grad_fn=<MinBackward1>), tensor(0.9908, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3655076324939728\n",
      "@sample 68: tensor([[ 1.2302e-02,  2.2658e-02,  5.0946e-02,  3.9499e-03, -9.6653e-02],\n",
      "        [ 1.2164e-02, -6.7304e-02, -4.1366e-03,  2.2668e-02,  9.8318e-05],\n",
      "        [-4.5943e-03,  7.8526e-02, -4.0236e-02, -1.0679e-01,  3.2515e-03],\n",
      "        [-2.1276e-02,  8.1890e-02,  5.9851e-02,  5.6860e-02, -2.8025e-02],\n",
      "        [ 3.1940e-02,  7.4381e-02,  5.0183e-02, -4.9930e-02,  3.7576e-02],\n",
      "        [ 1.1800e-02, -7.8414e-02,  8.8345e-02, -3.9048e-02,  6.0909e-02],\n",
      "        [-7.5726e-03,  6.7990e-02, -2.1021e-02, -4.4486e-02, -2.8124e-02],\n",
      "        [-3.5524e-02,  1.2969e-01,  5.3186e-03, -8.8622e-02,  5.3560e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0561,  0.0049, -0.0015, -0.0505,  0.0521],\n",
      "        [-0.0683, -0.0383, -0.2509,  0.2017,  0.0834],\n",
      "        [-0.1639, -0.1638, -0.0229,  0.0131, -0.1121],\n",
      "        [ 0.1010,  0.0539, -0.0247, -0.0500,  0.0103],\n",
      "        [-0.0308, -0.0029, -0.0463,  0.0825, -0.0090],\n",
      "        [-0.1191, -0.0126, -0.0073, -0.0069,  0.0139],\n",
      "        [-0.0366, -0.1013, -0.0220,  0.0182, -0.0097],\n",
      "        [-0.0527, -0.0849, -0.1295,  0.1093,  0.0271]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0104, grad_fn=<MinBackward1>), tensor(0.9752, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.394077330827713\n",
      "@sample 69: tensor([[ 1.7255e-02, -8.9598e-02, -4.6190e-02,  5.1231e-02, -2.7923e-03],\n",
      "        [-1.1310e-01,  7.8753e-02, -7.6057e-02, -6.9723e-02,  9.0003e-06],\n",
      "        [ 2.3775e-03,  4.0292e-03,  4.6598e-02,  5.2597e-02,  6.8266e-03],\n",
      "        [ 9.2325e-03,  4.8700e-02, -5.6081e-02,  8.3109e-02, -3.8475e-02],\n",
      "        [ 8.5053e-04,  8.0122e-02, -4.8973e-02, -1.1382e-01, -6.9561e-03],\n",
      "        [ 1.5006e-04, -3.6925e-02,  2.1468e-02,  3.8132e-02,  1.2622e-02],\n",
      "        [-3.2201e-02,  6.1050e-02, -5.2471e-02, -6.8979e-02,  4.9327e-02],\n",
      "        [-2.0174e-02,  5.0859e-02,  1.0814e-01,  7.2842e-02,  1.9338e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0926, -0.0830,  0.0686, -0.0260, -0.0030],\n",
      "        [-0.0880, -0.0778, -0.1406, -0.0084,  0.0858],\n",
      "        [ 0.0156,  0.1080,  0.0998, -0.0850, -0.0493],\n",
      "        [ 0.0182,  0.0254,  0.0497, -0.0949,  0.0057],\n",
      "        [-0.0620, -0.1106, -0.1179,  0.0406, -0.0202],\n",
      "        [ 0.0622, -0.0010, -0.0158,  0.0828,  0.0185],\n",
      "        [ 0.0072, -0.0476, -0.0208,  0.0342, -0.0327],\n",
      "        [ 0.0019,  0.1271,  0.0229, -0.0724,  0.0445]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0075, grad_fn=<MinBackward1>), tensor(0.9825, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3728884756565094\n",
      "@sample 70: tensor([[ 0.0094,  0.0384, -0.0057, -0.1158,  0.1127],\n",
      "        [-0.0515,  0.0929, -0.0806, -0.0233,  0.0163],\n",
      "        [ 0.0549,  0.0110, -0.0301,  0.0139,  0.0372],\n",
      "        [-0.0453,  0.1172,  0.0414, -0.0957,  0.0706],\n",
      "        [ 0.0028,  0.0629,  0.0364,  0.0062, -0.0596],\n",
      "        [ 0.0474, -0.0092,  0.0279,  0.0845, -0.0562],\n",
      "        [-0.0108, -0.1742, -0.0036,  0.0746, -0.0732],\n",
      "        [ 0.0232,  0.0524, -0.0526,  0.0024,  0.0407]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0659, -0.0142, -0.0261,  0.0582, -0.0551],\n",
      "        [-0.0016, -0.0181, -0.0037,  0.0453, -0.0589],\n",
      "        [ 0.0568, -0.0108,  0.0225, -0.0525, -0.0282],\n",
      "        [-0.0485, -0.1062, -0.1019,  0.1079,  0.0306],\n",
      "        [-0.0465, -0.0495,  0.0135, -0.0222, -0.1239],\n",
      "        [ 0.0452,  0.0369,  0.1473, -0.0500, -0.0885],\n",
      "        [ 0.0158, -0.0163,  0.1779, -0.2044, -0.0059],\n",
      "        [-0.0368,  0.0343, -0.0331,  0.0164, -0.0845]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0196, grad_fn=<MinBackward1>), tensor(0.9758, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3895394504070282\n",
      "@sample 71: tensor([[-0.0148, -0.0384, -0.0411, -0.0419, -0.0245],\n",
      "        [-0.0185, -0.1147,  0.0689,  0.0803,  0.0713],\n",
      "        [ 0.0080,  0.0221, -0.0474, -0.0574,  0.0094],\n",
      "        [-0.0572,  0.0446,  0.0101, -0.0246,  0.0156],\n",
      "        [ 0.1005,  0.0755,  0.0117, -0.0190, -0.0199],\n",
      "        [ 0.0039,  0.1001, -0.0833,  0.0062,  0.0415],\n",
      "        [ 0.0276, -0.0397, -0.0606, -0.0503, -0.0042],\n",
      "        [ 0.0174,  0.0159,  0.1173,  0.0071, -0.0287]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0119,  0.0394, -0.0540,  0.0234,  0.0187],\n",
      "        [-0.0449,  0.1106, -0.0391, -0.1346,  0.0131],\n",
      "        [-0.0746,  0.0278, -0.0247,  0.0428,  0.0342],\n",
      "        [ 0.0055, -0.0643, -0.1226, -0.0249, -0.1006],\n",
      "        [ 0.0017, -0.1119, -0.0721,  0.0295, -0.0171],\n",
      "        [ 0.0999, -0.0333, -0.0964, -0.0062,  0.0431],\n",
      "        [-0.0024,  0.0510,  0.1663, -0.1425, -0.1063],\n",
      "        [-0.0558,  0.1112, -0.0369,  0.0303,  0.0288]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0222, grad_fn=<MinBackward1>), tensor(0.9816, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.36483651399612427\n",
      "@sample 72: tensor([[ 0.0338, -0.1451, -0.0856,  0.1124, -0.0318],\n",
      "        [-0.0445,  0.0333,  0.0617,  0.0205,  0.1037],\n",
      "        [-0.0647, -0.0071, -0.0102,  0.0195,  0.0833],\n",
      "        [-0.0425, -0.0022, -0.0021, -0.0303,  0.0639],\n",
      "        [-0.0083,  0.0091, -0.0164, -0.0241, -0.0130],\n",
      "        [ 0.0094, -0.0702,  0.0068, -0.0339, -0.0560],\n",
      "        [-0.0267, -0.0274, -0.0553, -0.0234,  0.0432],\n",
      "        [-0.0726, -0.0029, -0.0619,  0.1000,  0.0022]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0845,  0.0312,  0.0446, -0.0345,  0.0013],\n",
      "        [ 0.0857, -0.0021, -0.0847,  0.0237, -0.0675],\n",
      "        [ 0.0864,  0.0021,  0.0342, -0.0349, -0.0205],\n",
      "        [ 0.0414, -0.0907, -0.0003, -0.0254, -0.0054],\n",
      "        [-0.0332, -0.0210, -0.0347,  0.0374, -0.0656],\n",
      "        [ 0.0149, -0.0707,  0.0214,  0.0354,  0.0531],\n",
      "        [ 0.0853, -0.1228, -0.1532,  0.0128,  0.0892],\n",
      "        [ 0.0736,  0.1668,  0.0156, -0.0730, -0.0003]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0189, grad_fn=<MinBackward1>), tensor(0.9793, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.35374873876571655\n",
      "@sample 73: tensor([[-0.0366,  0.0451, -0.0147,  0.0111,  0.0421],\n",
      "        [-0.0024, -0.0092, -0.0685, -0.0020,  0.0801],\n",
      "        [-0.0335, -0.1135, -0.0004,  0.0988,  0.0502],\n",
      "        [ 0.0657,  0.0579, -0.0499,  0.1070, -0.0178],\n",
      "        [-0.0253, -0.0151, -0.0315, -0.0616,  0.0116],\n",
      "        [-0.0091, -0.0087,  0.0392,  0.0131,  0.0540],\n",
      "        [ 0.0647,  0.0093,  0.0540, -0.1185,  0.0401],\n",
      "        [-0.1013,  0.0050,  0.0905, -0.0414,  0.0341]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0459, -0.0132, -0.0029, -0.0140, -0.0672],\n",
      "        [-0.0014, -0.0801, -0.0441,  0.0017,  0.0729],\n",
      "        [ 0.0061,  0.0401, -0.0361,  0.0422,  0.0122],\n",
      "        [ 0.0611,  0.0367, -0.0429,  0.0113, -0.0268],\n",
      "        [-0.0222, -0.0365, -0.0997,  0.0641,  0.0402],\n",
      "        [ 0.0578,  0.0818, -0.0089,  0.0113, -0.0468],\n",
      "        [-0.0646, -0.0296, -0.0819,  0.0482, -0.0379],\n",
      "        [-0.0510,  0.0660, -0.0465,  0.0159,  0.0330]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0059, grad_fn=<MinBackward1>), tensor(0.9824, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.34321191906929016\n",
      "@sample 74: tensor([[-0.0546,  0.0183,  0.0152, -0.0358, -0.0041],\n",
      "        [-0.0232,  0.0572,  0.1474,  0.0029, -0.0228],\n",
      "        [ 0.0111,  0.0181,  0.0858, -0.0049, -0.0146],\n",
      "        [-0.0323,  0.0092, -0.0600,  0.0635, -0.0737],\n",
      "        [-0.0375,  0.0865,  0.0176, -0.0573,  0.0422],\n",
      "        [ 0.0155,  0.0094, -0.0307,  0.0285, -0.0789],\n",
      "        [-0.0138,  0.0699,  0.0116,  0.0388,  0.0443],\n",
      "        [ 0.0028,  0.0705,  0.0660,  0.0022,  0.0479]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0341, -0.0728, -0.1021,  0.0894,  0.0660],\n",
      "        [-0.0267, -0.0027,  0.0003, -0.0003, -0.0577],\n",
      "        [ 0.0226, -0.0163, -0.0362,  0.0674, -0.0175],\n",
      "        [-0.0308,  0.0182, -0.0290, -0.0503,  0.1049],\n",
      "        [-0.0391, -0.0727, -0.1291,  0.1091,  0.0609],\n",
      "        [-0.0312, -0.0109, -0.0349, -0.0577,  0.1530],\n",
      "        [ 0.0267,  0.0262, -0.0140, -0.0892, -0.0679],\n",
      "        [ 0.0436, -0.0579, -0.0810,  0.0515,  0.0074]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0079, grad_fn=<MinBackward1>), tensor(0.9821, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.35333847999572754\n",
      "@sample 75: tensor([[ 0.0444,  0.0435, -0.0093, -0.0431,  0.0407],\n",
      "        [-0.0310,  0.0722,  0.0086, -0.0181,  0.0120],\n",
      "        [ 0.0244,  0.0107, -0.0184, -0.0686, -0.0198],\n",
      "        [-0.0441,  0.1004,  0.0528, -0.0915,  0.0417],\n",
      "        [-0.0693,  0.0963, -0.0368, -0.0965,  0.0836],\n",
      "        [ 0.0316, -0.0238,  0.0545, -0.0791,  0.0237],\n",
      "        [ 0.0666, -0.1846, -0.0037,  0.1522, -0.0240],\n",
      "        [ 0.0195, -0.0009, -0.0549, -0.1739,  0.1779]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0630,  0.0589,  0.0932, -0.0475, -0.0369],\n",
      "        [-0.0173, -0.0534, -0.0303, -0.0153,  0.0117],\n",
      "        [-0.0315, -0.0206, -0.0264,  0.0680,  0.0340],\n",
      "        [-0.0407, -0.0075, -0.0631,  0.0877, -0.0051],\n",
      "        [ 0.0242, -0.0144, -0.0079, -0.0005,  0.0823],\n",
      "        [-0.0836, -0.1284, -0.0275, -0.0078, -0.0494],\n",
      "        [ 0.0840,  0.0539,  0.0649, -0.0323,  0.0672],\n",
      "        [-0.0118, -0.1574, -0.0342,  0.1042, -0.0670]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0108, grad_fn=<MinBackward1>), tensor(0.9755, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.40069064497947693\n",
      "@sample 76: tensor([[-0.0159,  0.0266, -0.0019,  0.0276,  0.0466],\n",
      "        [-0.0269,  0.0752, -0.0885, -0.1344, -0.0543],\n",
      "        [-0.0320, -0.0106, -0.0366,  0.0285, -0.0697],\n",
      "        [-0.0228,  0.0819,  0.1426, -0.1069,  0.0520],\n",
      "        [-0.0494, -0.0221,  0.0253, -0.0926, -0.0095],\n",
      "        [-0.0166,  0.0260,  0.0672,  0.0035,  0.0983],\n",
      "        [-0.0635,  0.0813, -0.1408,  0.0177,  0.0029],\n",
      "        [-0.0098,  0.0794,  0.0208, -0.0971,  0.0408]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0359, -0.0135,  0.0179,  0.0134, -0.0227],\n",
      "        [-0.0502, -0.0457, -0.0237, -0.0246, -0.0203],\n",
      "        [ 0.0143, -0.0568,  0.0569,  0.0602,  0.0100],\n",
      "        [-0.0608,  0.0022, -0.1147,  0.1061, -0.0248],\n",
      "        [-0.0280, -0.0136,  0.0241,  0.0025, -0.0478],\n",
      "        [-0.0330,  0.0535,  0.0283, -0.0299,  0.0165],\n",
      "        [ 0.0280,  0.0267,  0.0824, -0.0458,  0.0329],\n",
      "        [-0.0344, -0.1141, -0.0435, -0.0005, -0.0136]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0156, grad_fn=<MinBackward1>), tensor(0.9814, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.36324232816696167\n",
      "@sample 77: tensor([[ 0.0412,  0.0215,  0.0075, -0.0256, -0.0005],\n",
      "        [ 0.0298, -0.0853, -0.0351,  0.0694, -0.0475],\n",
      "        [ 0.0338,  0.0433,  0.0078, -0.0013, -0.0275],\n",
      "        [-0.0738,  0.1039,  0.0836,  0.0269,  0.0763],\n",
      "        [-0.0269,  0.0523,  0.0624, -0.1171,  0.0563],\n",
      "        [-0.0679,  0.0919, -0.0803,  0.0343, -0.0754],\n",
      "        [ 0.0098, -0.0539,  0.0323,  0.0300, -0.0137],\n",
      "        [ 0.0376, -0.0731,  0.0375,  0.0565, -0.0388]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0388,  0.0102, -0.0002, -0.0458, -0.0153],\n",
      "        [-0.0621,  0.0933,  0.0006, -0.0523,  0.1075],\n",
      "        [ 0.0307,  0.0063, -0.1833,  0.0739,  0.0482],\n",
      "        [ 0.0639,  0.1132, -0.1303, -0.0409,  0.0128],\n",
      "        [-0.0932, -0.0398, -0.0970,  0.0850, -0.0578],\n",
      "        [ 0.0086,  0.0108, -0.0553,  0.1069, -0.0029],\n",
      "        [-0.1016,  0.0241,  0.0585,  0.0200, -0.0262],\n",
      "        [-0.0304, -0.0665,  0.0415, -0.0616,  0.0127]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0116, grad_fn=<MinBackward1>), tensor(0.9912, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3648045063018799\n",
      "@sample 78: tensor([[ 0.0049, -0.0472,  0.0065,  0.0074, -0.0528],\n",
      "        [ 0.0058,  0.0560, -0.0375, -0.0689, -0.0129],\n",
      "        [ 0.0499,  0.0229, -0.0757, -0.0432, -0.0209],\n",
      "        [-0.0055, -0.0425, -0.0516,  0.0298,  0.0475],\n",
      "        [ 0.0562,  0.0247,  0.0976, -0.0766,  0.0886],\n",
      "        [-0.0694, -0.0684,  0.0382,  0.0136, -0.0640],\n",
      "        [ 0.0217, -0.0386, -0.0513,  0.0416, -0.0710],\n",
      "        [ 0.1039,  0.0641, -0.0317,  0.0370, -0.0067]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0087, -0.0165, -0.0408,  0.0965,  0.0075],\n",
      "        [ 0.0190, -0.0738, -0.0465, -0.0087,  0.0063],\n",
      "        [-0.0273, -0.1281, -0.0527,  0.1365,  0.0452],\n",
      "        [ 0.0440, -0.0280,  0.0066, -0.0478, -0.0229],\n",
      "        [-0.0773, -0.0795, -0.0408,  0.0272, -0.0530],\n",
      "        [-0.1032, -0.0185,  0.0577, -0.0632,  0.0079],\n",
      "        [ 0.0049,  0.0270,  0.1720, -0.0164, -0.0230],\n",
      "        [ 0.0250,  0.0117,  0.1200, -0.1044, -0.0743]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0050, grad_fn=<MinBackward1>), tensor(0.9819, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3511543869972229\n",
      "@sample 79: tensor([[ 2.4614e-03,  3.1485e-02,  1.4844e-02, -1.3345e-02,  1.3680e-02],\n",
      "        [-4.0838e-02,  5.8157e-02,  7.0142e-02,  2.0314e-02,  1.1522e-02],\n",
      "        [-1.9067e-03,  2.1705e-02,  5.8738e-02, -9.1826e-02, -6.9320e-05],\n",
      "        [ 5.1350e-02, -7.0421e-02, -6.4328e-02, -2.3514e-03,  1.7789e-02],\n",
      "        [-1.2750e-02,  8.7329e-02, -2.1337e-02, -1.0729e-02,  2.8338e-02],\n",
      "        [ 4.2844e-02, -7.5780e-02, -3.0910e-02,  3.4967e-02,  4.0217e-02],\n",
      "        [ 4.2494e-02, -3.7801e-02, -9.8358e-02,  7.1311e-02, -6.2371e-02],\n",
      "        [ 3.2453e-02,  1.3705e-02,  5.0849e-02, -6.8897e-02,  9.2151e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0029, -0.0373, -0.0029, -0.0418, -0.0494],\n",
      "        [ 0.0036,  0.0151, -0.0962, -0.0823,  0.0534],\n",
      "        [-0.1156, -0.1111, -0.1377,  0.1423, -0.0003],\n",
      "        [-0.0634, -0.0977, -0.0870,  0.0576, -0.0592],\n",
      "        [-0.0159, -0.0619, -0.1659,  0.0773,  0.0229],\n",
      "        [ 0.0935,  0.0821,  0.0831, -0.0361,  0.0181],\n",
      "        [-0.0401, -0.0050, -0.0305,  0.0038, -0.0202],\n",
      "        [ 0.0269, -0.0111, -0.0303, -0.0091, -0.0243]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0135, grad_fn=<MinBackward1>), tensor(0.9813, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.35220444202423096\n",
      "@sample 80: tensor([[-0.0389,  0.0209,  0.0110,  0.0465, -0.0445],\n",
      "        [ 0.0475, -0.0200, -0.0221,  0.0426, -0.0433],\n",
      "        [-0.0400, -0.1342,  0.0101,  0.0573,  0.1538],\n",
      "        [-0.0050, -0.1087, -0.0507,  0.0727, -0.0566],\n",
      "        [-0.0213,  0.0491, -0.0623,  0.0335,  0.0486],\n",
      "        [ 0.0423, -0.0600,  0.0116, -0.0148, -0.0182],\n",
      "        [ 0.0635, -0.0486, -0.0143,  0.0919, -0.0085],\n",
      "        [ 0.0526,  0.0672,  0.0323,  0.0064,  0.0752]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0136,  0.0658,  0.0346, -0.0067,  0.0140],\n",
      "        [ 0.0554,  0.0351,  0.0326, -0.0493, -0.0509],\n",
      "        [-0.0459,  0.0018, -0.1611,  0.0215,  0.0091],\n",
      "        [ 0.0665, -0.0104,  0.1575, -0.1345, -0.0118],\n",
      "        [ 0.0011, -0.0119, -0.0198,  0.0181,  0.0706],\n",
      "        [-0.0436, -0.0539,  0.0296, -0.0517, -0.0415],\n",
      "        [-0.0091,  0.0505,  0.1003,  0.0158,  0.0418],\n",
      "        [-0.0302,  0.0761, -0.1776,  0.1353,  0.0590]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0099, grad_fn=<MinBackward1>), tensor(0.9875, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.36675795912742615\n",
      "@sample 81: tensor([[ 0.0584,  0.0452,  0.0069, -0.0249, -0.0293],\n",
      "        [-0.0481, -0.0051, -0.0116, -0.0729,  0.0389],\n",
      "        [ 0.0582, -0.0304, -0.0148,  0.0066,  0.0084],\n",
      "        [-0.0004,  0.0011,  0.0086, -0.0319,  0.0053],\n",
      "        [-0.0015, -0.0056,  0.0198, -0.0149, -0.0169],\n",
      "        [-0.0156,  0.0317, -0.0743,  0.0454, -0.0112],\n",
      "        [ 0.0138, -0.0874, -0.0461,  0.1070, -0.1186],\n",
      "        [ 0.0341, -0.0164,  0.0059,  0.0260,  0.0358]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0311,  0.0172, -0.0500,  0.0959,  0.0110],\n",
      "        [-0.0873, -0.0664,  0.0234,  0.0390, -0.0323],\n",
      "        [ 0.0099, -0.0241, -0.0787,  0.0171,  0.0166],\n",
      "        [-0.0067,  0.0053, -0.0303, -0.0130,  0.0248],\n",
      "        [ 0.0596,  0.0753, -0.0107,  0.0064,  0.0863],\n",
      "        [ 0.0818,  0.1201, -0.0076, -0.0149,  0.0217],\n",
      "        [-0.0162,  0.0384,  0.0794, -0.0169, -0.0436],\n",
      "        [-0.0089,  0.0554, -0.0445, -0.0211,  0.0057]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0063, grad_fn=<MinBackward1>), tensor(0.9924, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3149275779724121\n",
      "@sample 82: tensor([[-0.0055,  0.0692,  0.0104, -0.0648,  0.0340],\n",
      "        [ 0.0273, -0.1460,  0.0126,  0.1080,  0.0146],\n",
      "        [ 0.0065, -0.0999,  0.0051,  0.0086,  0.0172],\n",
      "        [-0.0267, -0.0698,  0.0828, -0.0297,  0.0173],\n",
      "        [-0.0021, -0.0034,  0.0096, -0.0019,  0.0439],\n",
      "        [ 0.0182,  0.0199, -0.0134,  0.0718, -0.0418],\n",
      "        [ 0.0493, -0.0885, -0.0022,  0.0771, -0.0791],\n",
      "        [-0.0024,  0.0080, -0.0137,  0.0003,  0.0084]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0166, -0.0499, -0.0817,  0.0651,  0.0990],\n",
      "        [-0.0549,  0.0644,  0.0414, -0.0344,  0.0229],\n",
      "        [-0.0260, -0.0134,  0.0801, -0.1018,  0.1138],\n",
      "        [ 0.0201, -0.0554, -0.0018,  0.0058, -0.0450],\n",
      "        [-0.0101,  0.0296, -0.1408,  0.1014, -0.0431],\n",
      "        [-0.0483,  0.0454, -0.0414,  0.0425, -0.0630],\n",
      "        [-0.0034,  0.0257,  0.0538,  0.0490, -0.0061],\n",
      "        [-0.1118, -0.0029,  0.0573, -0.0501,  0.0152]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0093, grad_fn=<MinBackward1>), tensor(0.9791, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3384244740009308\n",
      "@sample 83: tensor([[-0.0012,  0.0998,  0.0895, -0.0622,  0.0547],\n",
      "        [ 0.0350,  0.0689,  0.0606,  0.0297,  0.0276],\n",
      "        [-0.0213,  0.0549,  0.0612, -0.0514,  0.1104],\n",
      "        [ 0.0639,  0.0587, -0.0617, -0.0228, -0.0248],\n",
      "        [ 0.0722,  0.0327, -0.1322, -0.0159,  0.0240],\n",
      "        [-0.0275,  0.0328,  0.0222, -0.0433, -0.0037],\n",
      "        [ 0.0053, -0.0560,  0.0430,  0.0047, -0.0143],\n",
      "        [ 0.0137, -0.0311, -0.0332,  0.0440,  0.0588]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0284,  0.0112,  0.0108, -0.1054, -0.0698],\n",
      "        [-0.0120,  0.0365, -0.0295, -0.0459, -0.0106],\n",
      "        [ 0.0079,  0.0236, -0.1030,  0.0324,  0.0206],\n",
      "        [ 0.0452, -0.0568, -0.0026, -0.0681, -0.0395],\n",
      "        [ 0.0737, -0.0613,  0.0935, -0.0917, -0.0367],\n",
      "        [-0.1260, -0.0512, -0.0244, -0.0544, -0.0329],\n",
      "        [-0.1247, -0.0541, -0.0508,  0.0257, -0.0971],\n",
      "        [-0.0004,  0.0155, -0.0556,  0.0160,  0.0676]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0179, grad_fn=<MinBackward1>), tensor(0.9881, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.34352606534957886\n",
      "@sample 84: tensor([[ 0.0735, -0.0570, -0.0201,  0.0120, -0.0463],\n",
      "        [ 0.0150,  0.0359, -0.0500,  0.0219, -0.0122],\n",
      "        [ 0.0019, -0.0609,  0.0163, -0.0311,  0.0573],\n",
      "        [-0.0512, -0.0826,  0.0304,  0.0187, -0.0356],\n",
      "        [-0.0373, -0.0152, -0.0700,  0.0132,  0.0128],\n",
      "        [-0.0556, -0.0165,  0.0621, -0.0478,  0.1146],\n",
      "        [ 0.0470,  0.0757, -0.0955,  0.0420,  0.0717],\n",
      "        [ 0.0248, -0.0361,  0.0560,  0.0479,  0.0241]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0433, -0.0169,  0.0108, -0.0880,  0.0012],\n",
      "        [ 0.0171,  0.0352, -0.0583,  0.0952,  0.0535],\n",
      "        [-0.0532, -0.1552, -0.0691,  0.0332,  0.0387],\n",
      "        [-0.0121, -0.0212,  0.0291, -0.0175, -0.0007],\n",
      "        [ 0.0430, -0.0169,  0.0010, -0.0267, -0.0177],\n",
      "        [ 0.0357, -0.0473, -0.0123,  0.0048,  0.0257],\n",
      "        [ 0.0843,  0.0548, -0.1058, -0.0128, -0.0166],\n",
      "        [-0.0131, -0.0392, -0.0260, -0.0111, -0.0083]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0095, grad_fn=<MinBackward1>), tensor(0.9745, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.32897523045539856\n",
      "@sample 85: tensor([[ 0.0979,  0.0051, -0.0051,  0.0222, -0.0551],\n",
      "        [-0.0160, -0.0142, -0.0100, -0.1186,  0.0424],\n",
      "        [-0.0484,  0.1066, -0.0300, -0.0390, -0.0229],\n",
      "        [-0.0040,  0.0533,  0.0155,  0.0077, -0.0996],\n",
      "        [ 0.0160,  0.0448,  0.0425, -0.0055,  0.0645],\n",
      "        [-0.0472,  0.0256, -0.0130, -0.0565,  0.0512],\n",
      "        [ 0.0402, -0.1277, -0.0243, -0.0031,  0.0369],\n",
      "        [ 0.0248, -0.0758,  0.0779,  0.0623, -0.0141]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0561, -0.0004,  0.1406, -0.0961, -0.0050],\n",
      "        [-0.0207, -0.0753, -0.0542,  0.0083,  0.0683],\n",
      "        [ 0.0142,  0.0078, -0.0417, -0.0041, -0.0200],\n",
      "        [-0.0531,  0.0057, -0.0837,  0.0043, -0.0501],\n",
      "        [ 0.1325,  0.0034, -0.1016,  0.1279,  0.0917],\n",
      "        [ 0.0039, -0.0754,  0.0493,  0.0638, -0.0292],\n",
      "        [-0.0389, -0.0130,  0.0073, -0.0194,  0.0818],\n",
      "        [-0.0348, -0.0365,  0.0046, -0.0136,  0.0710]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0232, grad_fn=<MinBackward1>), tensor(0.9840, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.34583577513694763\n",
      "@sample 86: tensor([[-0.0507,  0.0220,  0.0505,  0.0660,  0.0047],\n",
      "        [-0.0360,  0.0520,  0.0111, -0.0972,  0.1038],\n",
      "        [-0.0787,  0.0054, -0.0401,  0.0161, -0.0257],\n",
      "        [-0.0093,  0.0030, -0.0865, -0.0791, -0.0501],\n",
      "        [ 0.0265,  0.0924,  0.0158,  0.0182, -0.0241],\n",
      "        [-0.0747,  0.0180,  0.0332,  0.0227,  0.0517],\n",
      "        [-0.0631,  0.0332, -0.0118, -0.0966,  0.0099],\n",
      "        [-0.0466,  0.0599,  0.0784,  0.0177, -0.0541]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0801,  0.0687,  0.1236, -0.0759,  0.0164],\n",
      "        [ 0.0446, -0.0078, -0.1062, -0.0061,  0.2075],\n",
      "        [ 0.0275, -0.0161,  0.0639, -0.0814,  0.0291],\n",
      "        [-0.0004, -0.1105,  0.0208, -0.0082, -0.0539],\n",
      "        [ 0.0006, -0.0056, -0.0771,  0.0477, -0.0492],\n",
      "        [ 0.0471,  0.0492, -0.0310,  0.0157,  0.0285],\n",
      "        [ 0.0086, -0.0904,  0.0449, -0.0253, -0.0626],\n",
      "        [-0.0862,  0.0494,  0.0188, -0.0297, -0.0602]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0078, grad_fn=<MinBackward1>), tensor(0.9934, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3476705551147461\n",
      "@sample 87: tensor([[-0.0030,  0.0504, -0.0254, -0.1055,  0.0895],\n",
      "        [ 0.0314,  0.0196,  0.0188, -0.0232, -0.0383],\n",
      "        [ 0.0457, -0.0219, -0.0181, -0.0072, -0.0780],\n",
      "        [-0.0393,  0.0295, -0.0517, -0.0518, -0.0368],\n",
      "        [-0.0107,  0.0529,  0.0830, -0.0066,  0.0291],\n",
      "        [-0.0828,  0.0853, -0.0288, -0.0387,  0.1132],\n",
      "        [-0.0404, -0.0050,  0.0643, -0.0532,  0.0303],\n",
      "        [-0.0779,  0.0565, -0.0093, -0.0358,  0.0859]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0527, -0.0168, -0.0291,  0.0521,  0.0011],\n",
      "        [-0.0381,  0.0279, -0.0042,  0.0412,  0.0006],\n",
      "        [-0.1056, -0.0285, -0.0562,  0.0453, -0.0489],\n",
      "        [-0.0998, -0.0850,  0.0184, -0.0830, -0.0355],\n",
      "        [ 0.0527,  0.0699, -0.0335,  0.1056,  0.0309],\n",
      "        [ 0.1075, -0.0111, -0.0400,  0.0138,  0.0860],\n",
      "        [ 0.0008, -0.0124, -0.0410, -0.0435,  0.0178],\n",
      "        [ 0.0660, -0.1155, -0.0113, -0.0712,  0.0157]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0173, grad_fn=<MinBackward1>), tensor(0.9839, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3415302038192749\n",
      "@sample 88: tensor([[-0.0538,  0.0076, -0.0212,  0.0129, -0.0003],\n",
      "        [ 0.0557,  0.0762, -0.0036, -0.0003, -0.1023],\n",
      "        [-0.0077,  0.0558,  0.0493, -0.0673, -0.0368],\n",
      "        [-0.0001, -0.0456, -0.0784, -0.0533,  0.0161],\n",
      "        [ 0.0073,  0.0582,  0.0267, -0.0538,  0.0020],\n",
      "        [-0.0826,  0.0957,  0.0859, -0.0122,  0.0273],\n",
      "        [-0.0154, -0.0210,  0.0622, -0.1389,  0.1023],\n",
      "        [-0.0287, -0.0245,  0.0289, -0.0494, -0.0179]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0243,  0.0326,  0.0183,  0.0520, -0.0464],\n",
      "        [-0.0775, -0.0542, -0.1164,  0.1038,  0.0303],\n",
      "        [-0.0968, -0.0830, -0.0929,  0.0526, -0.0352],\n",
      "        [ 0.0525, -0.0038,  0.1157, -0.1083, -0.0378],\n",
      "        [-0.0536, -0.0143, -0.0751,  0.0897, -0.0188],\n",
      "        [-0.0125,  0.0629, -0.1076, -0.0208, -0.0275],\n",
      "        [-0.1120, -0.0870, -0.0620,  0.0558,  0.0070],\n",
      "        [-0.0297, -0.0285,  0.0390, -0.0371, -0.0235]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0119, grad_fn=<MinBackward1>), tensor(0.9833, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.35452425479888916\n",
      "@sample 89: tensor([[-0.0065,  0.0736, -0.0335,  0.0054,  0.1292],\n",
      "        [ 0.0571, -0.0227,  0.0222, -0.1087, -0.0240],\n",
      "        [ 0.0780, -0.0310, -0.0666, -0.0588, -0.0496],\n",
      "        [ 0.0208,  0.0347,  0.0852, -0.0351,  0.0005],\n",
      "        [ 0.0357,  0.0900,  0.0086,  0.0268, -0.0648],\n",
      "        [ 0.0461,  0.0271,  0.0259,  0.0029, -0.0644],\n",
      "        [ 0.0298,  0.0389, -0.0325, -0.0394, -0.0095],\n",
      "        [-0.0267,  0.0129, -0.0022, -0.1307,  0.0487]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 3.8609e-02,  4.2838e-03, -4.2076e-02, -9.2260e-02,  3.4244e-03],\n",
      "        [-1.1790e-01, -1.5980e-01, -8.2427e-02, -7.2837e-05,  3.3368e-02],\n",
      "        [-4.7030e-02, -6.8677e-03,  6.7125e-02,  6.2074e-02, -1.6927e-03],\n",
      "        [-5.7642e-02, -2.6613e-02, -7.7190e-02,  2.3353e-02,  5.2957e-02],\n",
      "        [ 4.1631e-03,  2.2966e-02, -1.0206e-01,  4.5152e-02, -7.7371e-02],\n",
      "        [-1.4301e-02,  7.4975e-02, -2.7054e-02,  3.3358e-02, -3.5857e-02],\n",
      "        [-6.3020e-03,  2.6051e-02, -7.2108e-02,  6.3131e-02, -2.2920e-02],\n",
      "        [-5.6655e-02, -6.4732e-02, -1.2509e-01,  4.2195e-02,  5.6160e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0138, grad_fn=<MinBackward1>), tensor(0.9794, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.34868037700653076\n",
      "@sample 90: tensor([[ 0.0289, -0.0046, -0.1116,  0.0087, -0.0186],\n",
      "        [ 0.0154, -0.0610, -0.0071,  0.0192, -0.0076],\n",
      "        [ 0.0293, -0.0754, -0.0034,  0.0036,  0.0060],\n",
      "        [-0.0691,  0.0265,  0.0370, -0.0137,  0.0451],\n",
      "        [-0.0455,  0.0223,  0.0436, -0.1087,  0.0557],\n",
      "        [-0.0495,  0.0547, -0.1014,  0.0010,  0.0387],\n",
      "        [ 0.0136,  0.1410,  0.0329, -0.0223,  0.0394],\n",
      "        [-0.0304, -0.0802, -0.0542,  0.1217, -0.0988]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0516,  0.0016, -0.0262,  0.0576,  0.0746],\n",
      "        [-0.0112,  0.0239,  0.0541, -0.0261,  0.0047],\n",
      "        [-0.0725, -0.0500,  0.0299,  0.0080, -0.0810],\n",
      "        [ 0.0278, -0.0676,  0.0120, -0.0099,  0.0453],\n",
      "        [-0.0078, -0.0042, -0.0361, -0.0416,  0.0028],\n",
      "        [ 0.1040, -0.0081,  0.0624, -0.1131,  0.0092],\n",
      "        [-0.0306, -0.0219, -0.1383,  0.0792,  0.0325],\n",
      "        [ 0.0138,  0.0622,  0.1054, -0.0260, -0.0002]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0114, grad_fn=<MinBackward1>), tensor(0.9786, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3504425585269928\n",
      "@sample 91: tensor([[ 0.0052,  0.0411, -0.0048,  0.0310, -0.0248],\n",
      "        [-0.0136,  0.0116, -0.0679,  0.0191,  0.0152],\n",
      "        [ 0.0187, -0.0505, -0.0053, -0.1048, -0.0113],\n",
      "        [-0.0325,  0.0083,  0.0217, -0.1348,  0.0369],\n",
      "        [ 0.0250, -0.0955, -0.0188,  0.1654, -0.0419],\n",
      "        [-0.0282,  0.0154,  0.0169, -0.0446,  0.0338],\n",
      "        [-0.0745,  0.0414,  0.0553,  0.0743, -0.0629],\n",
      "        [ 0.0378,  0.0107, -0.0020,  0.0339, -0.0312]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0638, -0.0172, -0.1471,  0.0843,  0.0481],\n",
      "        [ 0.0311,  0.0452,  0.0243,  0.0675,  0.0433],\n",
      "        [-0.1073, -0.0883, -0.0726,  0.0300,  0.0653],\n",
      "        [-0.0752, -0.1232, -0.1224,  0.0517,  0.0310],\n",
      "        [ 0.0804,  0.0570, -0.0148,  0.0109, -0.0123],\n",
      "        [ 0.0176, -0.0684,  0.0193,  0.1188,  0.0635],\n",
      "        [-0.0118,  0.1040,  0.0382, -0.0657,  0.0735],\n",
      "        [-0.0186,  0.0413, -0.0101,  0.0123, -0.0209]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0148, grad_fn=<MinBackward1>), tensor(0.9865, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3554491698741913\n",
      "@sample 92: tensor([[ 0.0454,  0.0008,  0.0413, -0.0114,  0.0130],\n",
      "        [-0.0168, -0.0565,  0.0179,  0.0013, -0.0400],\n",
      "        [-0.0405,  0.0734,  0.0660, -0.0674,  0.0797],\n",
      "        [ 0.0841, -0.0813,  0.0849, -0.0627, -0.0611],\n",
      "        [ 0.0192, -0.0068, -0.0007, -0.0259,  0.0116],\n",
      "        [ 0.0411, -0.0057, -0.0160,  0.0289, -0.0004],\n",
      "        [ 0.0575,  0.0273,  0.0268,  0.0392, -0.0416],\n",
      "        [ 0.0049,  0.0798, -0.0203, -0.0611,  0.0075]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0207,  0.0219, -0.0271,  0.0282, -0.0784],\n",
      "        [-0.0622, -0.1000,  0.0464, -0.0358,  0.0262],\n",
      "        [-0.0673, -0.0508, -0.0878,  0.1461, -0.0384],\n",
      "        [-0.0619, -0.0018,  0.0686, -0.1161, -0.0480],\n",
      "        [-0.0078,  0.0313, -0.0954,  0.0597,  0.0748],\n",
      "        [ 0.0812,  0.0060, -0.0176,  0.0667,  0.0662],\n",
      "        [-0.0005, -0.0026, -0.1036, -0.0484, -0.0993],\n",
      "        [-0.0579, -0.0363, -0.1129,  0.0594, -0.0226]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0149, grad_fn=<MinBackward1>), tensor(0.9706, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.33984240889549255\n",
      "@sample 93: tensor([[ 0.0429, -0.0090, -0.0506,  0.0569, -0.1382],\n",
      "        [ 0.0333, -0.0993, -0.0559,  0.1692, -0.1468],\n",
      "        [ 0.0057,  0.1202,  0.0301, -0.0720,  0.0568],\n",
      "        [-0.0068,  0.0370, -0.0346,  0.0011, -0.0452],\n",
      "        [-0.0640, -0.0442,  0.0305,  0.0754,  0.0339],\n",
      "        [ 0.0427,  0.0090,  0.0057,  0.1391, -0.0818],\n",
      "        [ 0.0242, -0.0259,  0.0054, -0.0601,  0.0479],\n",
      "        [ 0.0102,  0.0356,  0.0385,  0.0257, -0.1033]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0143,  0.1016,  0.1905, -0.1175,  0.0015],\n",
      "        [ 0.0039,  0.0408,  0.1534, -0.0735, -0.0084],\n",
      "        [-0.0433, -0.0517, -0.1189,  0.0965,  0.0244],\n",
      "        [ 0.0074,  0.0389, -0.0353, -0.0048,  0.0108],\n",
      "        [-0.0546,  0.0676, -0.0828,  0.0224,  0.0049],\n",
      "        [ 0.0387,  0.0716,  0.0900, -0.0287,  0.0463],\n",
      "        [-0.0618, -0.0844, -0.0174, -0.0033, -0.0460],\n",
      "        [ 0.0061,  0.0481,  0.0025, -0.0137, -0.0538]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0127, grad_fn=<MinBackward1>), tensor(0.9699, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.39555686712265015\n",
      "@sample 94: tensor([[-0.0307,  0.0181, -0.0814, -0.0312,  0.0083],\n",
      "        [ 0.0127,  0.0162,  0.0615, -0.1645,  0.0961],\n",
      "        [-0.0493, -0.0258, -0.0861,  0.0659, -0.0560],\n",
      "        [-0.0380,  0.0869, -0.0832,  0.0487, -0.0667],\n",
      "        [-0.0317,  0.0540,  0.0888,  0.0054, -0.0320],\n",
      "        [-0.0327,  0.0020, -0.0012, -0.1100,  0.0286],\n",
      "        [-0.0058, -0.0452,  0.0044, -0.0128,  0.0493],\n",
      "        [ 0.0289, -0.0075,  0.0829,  0.1200, -0.0747]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 1.1640e-02, -1.2512e-01,  4.7259e-02, -2.8804e-03, -4.7798e-02],\n",
      "        [-7.7824e-02, -2.1341e-01, -1.7000e-01,  7.8446e-02, -9.8455e-02],\n",
      "        [-5.8508e-03,  8.0090e-02, -2.1422e-02,  2.5282e-02,  7.4051e-05],\n",
      "        [ 5.9897e-02,  9.6408e-02,  1.0624e-02, -5.1735e-02, -1.8958e-03],\n",
      "        [-5.8993e-04,  3.0823e-02, -7.3724e-02, -6.5218e-03,  2.5172e-02],\n",
      "        [ 2.3109e-02, -2.5973e-02,  7.9478e-02, -9.7136e-03,  3.7564e-02],\n",
      "        [ 2.2209e-03,  1.4389e-02,  1.5789e-03, -4.1387e-02, -1.0486e-01],\n",
      "        [ 2.2296e-02,  1.4542e-01, -1.5821e-01,  8.6170e-02,  1.3158e-01]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0145, grad_fn=<MinBackward1>), tensor(0.9684, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.38569098711013794\n",
      "@sample 95: tensor([[ 0.0136,  0.0230,  0.0115,  0.0417, -0.0144],\n",
      "        [ 0.0276,  0.0103, -0.0098, -0.0755,  0.0177],\n",
      "        [ 0.0006,  0.0950,  0.0095, -0.0137, -0.0311],\n",
      "        [-0.0090,  0.1248,  0.0105, -0.0867, -0.0972],\n",
      "        [-0.1242,  0.0636,  0.0332, -0.1058,  0.0226],\n",
      "        [-0.0155,  0.1261,  0.0797, -0.0599,  0.0653],\n",
      "        [-0.0076,  0.0089,  0.0096, -0.1284, -0.0724],\n",
      "        [-0.0078, -0.1165, -0.0079,  0.0853,  0.0028]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 6.6066e-02,  1.0882e-01,  8.0246e-02,  5.3993e-03, -3.2424e-02],\n",
      "        [-6.8895e-02, -4.0708e-02, -1.5021e-01,  7.7931e-02,  1.5920e-02],\n",
      "        [-1.5797e-02, -3.9933e-03, -5.5655e-02,  3.0347e-02, -2.6999e-02],\n",
      "        [-1.2393e-01, -1.0112e-01, -1.5486e-02, -4.6773e-02, -4.1577e-02],\n",
      "        [-8.6898e-02, -4.2409e-02, -1.7479e-01,  7.8355e-02,  1.2291e-01],\n",
      "        [-4.6097e-02, -3.4968e-02, -1.1962e-01,  6.5543e-02, -2.5772e-05],\n",
      "        [-1.1011e-01, -1.4720e-01, -1.9709e-02,  5.4638e-02, -3.7844e-02],\n",
      "        [-3.8501e-02,  7.7763e-02,  2.9259e-02, -1.3513e-02, -2.4901e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0066, grad_fn=<MinBackward1>), tensor(0.9732, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3947419822216034\n",
      "@sample 96: tensor([[ 0.0535,  0.0092,  0.0468, -0.0222,  0.0275],\n",
      "        [ 0.0121,  0.0015,  0.0257,  0.1203, -0.1099],\n",
      "        [ 0.0203,  0.0472,  0.0041,  0.0339, -0.0527],\n",
      "        [ 0.0239,  0.0055, -0.0668,  0.0673, -0.0528],\n",
      "        [-0.0561,  0.0691, -0.0057, -0.0626,  0.0661],\n",
      "        [ 0.0469, -0.0154, -0.0479, -0.0295, -0.0756],\n",
      "        [-0.0624, -0.0008, -0.0026, -0.0225,  0.0992],\n",
      "        [ 0.0045, -0.0756, -0.0806,  0.0818, -0.0974]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0687,  0.0669,  0.0189,  0.0132,  0.0027],\n",
      "        [ 0.1244,  0.0690,  0.0224,  0.0058, -0.0499],\n",
      "        [-0.0475,  0.0500,  0.0239, -0.0540, -0.0652],\n",
      "        [ 0.0663,  0.0301,  0.0281, -0.0287, -0.0284],\n",
      "        [-0.0191, -0.0537, -0.0798,  0.0429,  0.0246],\n",
      "        [-0.0509, -0.0253,  0.1469, -0.0698, -0.0627],\n",
      "        [ 0.1136,  0.0315, -0.0618,  0.0575, -0.0065],\n",
      "        [ 0.0062,  0.0619,  0.0600, -0.0045,  0.0170]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0184, grad_fn=<MinBackward1>), tensor(0.9754, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.34371787309646606\n",
      "@sample 97: tensor([[ 0.0346,  0.0779, -0.0377,  0.0138,  0.0161],\n",
      "        [-0.0505, -0.0165,  0.0363,  0.0074,  0.0164],\n",
      "        [ 0.0195,  0.0341,  0.0815, -0.0986,  0.0792],\n",
      "        [ 0.0073,  0.0945,  0.0680, -0.0264,  0.0612],\n",
      "        [ 0.0601, -0.0006, -0.0328,  0.0165, -0.0353],\n",
      "        [ 0.0221, -0.0404, -0.0238,  0.0072, -0.0068],\n",
      "        [ 0.0418, -0.0533, -0.0586,  0.1249, -0.0745],\n",
      "        [ 0.0099,  0.0499, -0.0004, -0.0527,  0.0310]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0035,  0.0168, -0.0454,  0.0582, -0.0251],\n",
      "        [ 0.0143, -0.0361, -0.0163,  0.0050,  0.0251],\n",
      "        [ 0.0414, -0.0939, -0.0482,  0.0485,  0.0092],\n",
      "        [-0.0099, -0.0005, -0.0638,  0.0278, -0.0346],\n",
      "        [ 0.0582,  0.0645,  0.0707,  0.0054,  0.0120],\n",
      "        [ 0.0605,  0.0489,  0.0604, -0.0270, -0.0207],\n",
      "        [ 0.0637,  0.0105,  0.0673, -0.0843,  0.0186],\n",
      "        [ 0.0180, -0.0643, -0.0270,  0.0422,  0.0210]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0102, grad_fn=<MinBackward1>), tensor(0.9790, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.32061946392059326\n",
      "@sample 98: tensor([[-0.0458,  0.1067, -0.0713,  0.0182,  0.0266],\n",
      "        [ 0.0162, -0.0595, -0.0431,  0.0702, -0.0066],\n",
      "        [-0.1201,  0.0786,  0.0063, -0.0820,  0.0226],\n",
      "        [-0.0789,  0.0107,  0.0424,  0.0364,  0.0281],\n",
      "        [ 0.0232, -0.0889,  0.0671,  0.0619, -0.0126],\n",
      "        [ 0.0152, -0.0576,  0.0483,  0.0686, -0.0715],\n",
      "        [ 0.0628,  0.0370, -0.0876,  0.0628, -0.0734],\n",
      "        [ 0.0918,  0.0907,  0.1213, -0.0714,  0.0183]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0344,  0.0379,  0.0141, -0.0621, -0.0371],\n",
      "        [ 0.0809,  0.0054,  0.1343, -0.0404,  0.0003],\n",
      "        [-0.0294, -0.0760, -0.1966,  0.0802, -0.0023],\n",
      "        [ 0.0265,  0.0993,  0.0217, -0.1048, -0.0937],\n",
      "        [ 0.0343, -0.0491,  0.0677, -0.0344,  0.0777],\n",
      "        [ 0.0626,  0.1054,  0.1014, -0.0404, -0.0528],\n",
      "        [ 0.0486,  0.0151,  0.1523, -0.0961,  0.0379],\n",
      "        [-0.0903, -0.0185, -0.0995,  0.0532,  0.0006]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0180, grad_fn=<MinBackward1>), tensor(0.9868, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3899157643318176\n",
      "@sample 99: tensor([[ 0.0601, -0.0384, -0.0646,  0.0495, -0.0285],\n",
      "        [ 0.0221,  0.0551,  0.0188,  0.0278,  0.0137],\n",
      "        [-0.0174,  0.0244, -0.0146, -0.0375, -0.0738],\n",
      "        [ 0.0873,  0.0733, -0.0211, -0.0617,  0.0441],\n",
      "        [ 0.0608, -0.0244,  0.0340, -0.0733,  0.0296],\n",
      "        [-0.0137, -0.0342,  0.0127, -0.0568,  0.0143],\n",
      "        [ 0.0297, -0.0353, -0.0479,  0.0615, -0.0341],\n",
      "        [ 0.0624, -0.0287,  0.0175,  0.0390, -0.0556]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0649, -0.0119,  0.0289,  0.0194, -0.0358],\n",
      "        [ 0.0125,  0.0290,  0.0064, -0.0479, -0.0638],\n",
      "        [-0.0112, -0.0363,  0.0874, -0.0329, -0.0272],\n",
      "        [-0.0257,  0.0384, -0.0100,  0.0012, -0.0590],\n",
      "        [-0.0298, -0.0956, -0.0690,  0.0542, -0.0217],\n",
      "        [ 0.0126, -0.0879, -0.0540,  0.0662, -0.0211],\n",
      "        [ 0.0058,  0.0058, -0.0048, -0.0984, -0.0764],\n",
      "        [ 0.0906,  0.1062,  0.0826, -0.0859,  0.0074]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0085, grad_fn=<MinBackward1>), tensor(0.9849, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3239319920539856\n",
      "@sample 100: tensor([[ 0.0060, -0.1304,  0.0318,  0.0924,  0.0216],\n",
      "        [ 0.0619, -0.1207, -0.0169,  0.0961, -0.0858],\n",
      "        [ 0.0260,  0.0353,  0.0228, -0.0542,  0.0323],\n",
      "        [ 0.0022, -0.0409,  0.0207,  0.0301, -0.0328],\n",
      "        [-0.0112,  0.0647,  0.0690, -0.0936,  0.0751],\n",
      "        [ 0.0022, -0.0368,  0.0143, -0.0038,  0.0144],\n",
      "        [ 0.0002, -0.0055,  0.0025, -0.0366,  0.0191],\n",
      "        [ 0.0447,  0.0574, -0.0921,  0.0120, -0.0163]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0510,  0.0624,  0.0673, -0.0971, -0.0499],\n",
      "        [-0.0233,  0.0201,  0.0928, -0.0341, -0.0410],\n",
      "        [-0.0131, -0.0246, -0.0804,  0.0353,  0.0424],\n",
      "        [ 0.0392,  0.0082,  0.0205, -0.0042,  0.0166],\n",
      "        [ 0.0205, -0.0995,  0.0136, -0.0044,  0.0057],\n",
      "        [-0.0514,  0.0392,  0.0907, -0.1851, -0.0473],\n",
      "        [ 0.0634, -0.0202,  0.0487, -0.0865, -0.0469],\n",
      "        [ 0.0083,  0.0007, -0.0607, -0.0294,  0.0497]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0155, grad_fn=<MinBackward1>), tensor(0.9819, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3479108214378357\n",
      "@sample 101: tensor([[ 0.0064,  0.0404,  0.0233, -0.0177,  0.0159],\n",
      "        [-0.0196, -0.0710, -0.0035,  0.0287,  0.0032],\n",
      "        [-0.0344,  0.0234,  0.0046, -0.0403, -0.0020],\n",
      "        [ 0.0163,  0.0578,  0.0160, -0.0719, -0.0433],\n",
      "        [-0.0092, -0.0401, -0.0291,  0.0427,  0.0209],\n",
      "        [ 0.0275, -0.1139, -0.0288,  0.0390, -0.0235],\n",
      "        [ 0.0138,  0.0790,  0.0461, -0.0376, -0.0114],\n",
      "        [ 0.0320, -0.0036,  0.0103, -0.0280,  0.0430]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0030, -0.0098, -0.0712, -0.0167,  0.0050],\n",
      "        [ 0.0143, -0.0173,  0.0905, -0.0701,  0.0248],\n",
      "        [ 0.0036, -0.0688, -0.0073, -0.0046, -0.0152],\n",
      "        [-0.0041, -0.0486, -0.0388,  0.0512,  0.0557],\n",
      "        [ 0.0388,  0.0634,  0.1148, -0.0417, -0.0550],\n",
      "        [ 0.0092, -0.0324,  0.0522, -0.0396,  0.0353],\n",
      "        [-0.0441,  0.0173, -0.0294,  0.0531, -0.0179],\n",
      "        [-0.0016,  0.0424, -0.0898,  0.0847,  0.0015]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0024, grad_fn=<MinBackward1>), tensor(0.9850, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.30838632583618164\n",
      "@sample 102: tensor([[ 0.0204,  0.0115,  0.0634, -0.0891,  0.0279],\n",
      "        [-0.0081, -0.0197,  0.0836, -0.1023,  0.0617],\n",
      "        [-0.0302, -0.1114, -0.0599,  0.0480,  0.0411],\n",
      "        [-0.0171,  0.0195,  0.0436, -0.0240, -0.0444],\n",
      "        [ 0.0246, -0.0271,  0.0045,  0.0059,  0.0067],\n",
      "        [ 0.0418, -0.0366, -0.0265,  0.1102,  0.0794],\n",
      "        [-0.0243,  0.0528,  0.0427, -0.0576, -0.0169],\n",
      "        [-0.0154, -0.0141,  0.0091, -0.0393, -0.0516]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0737, -0.0720, -0.1631,  0.1194,  0.0009],\n",
      "        [-0.0348, -0.1480, -0.1166,  0.0021, -0.0817],\n",
      "        [ 0.0259, -0.0173,  0.1260, -0.1137,  0.0600],\n",
      "        [-0.0600,  0.0454, -0.0179,  0.0205, -0.0931],\n",
      "        [ 0.0484, -0.0045,  0.0247, -0.0706,  0.0047],\n",
      "        [ 0.0524,  0.0497, -0.0545,  0.0003,  0.0137],\n",
      "        [-0.1286, -0.1096, -0.0513, -0.0065, -0.0498],\n",
      "        [-0.0572, -0.1160,  0.0306,  0.0585, -0.0712]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0221, grad_fn=<MinBackward1>), tensor(0.9834, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.35570257902145386\n",
      "@sample 103: tensor([[-0.0553, -0.0103,  0.0728, -0.0548,  0.0044],\n",
      "        [-0.0420, -0.0040,  0.0466,  0.0721, -0.0142],\n",
      "        [-0.0108,  0.0037, -0.0205, -0.0308, -0.0160],\n",
      "        [-0.0140,  0.1049,  0.0950, -0.0533,  0.0204],\n",
      "        [-0.0168, -0.0166, -0.0036, -0.0484,  0.0604],\n",
      "        [ 0.0353, -0.0044, -0.0229, -0.0730,  0.0198],\n",
      "        [-0.0436, -0.0060,  0.0245, -0.0092,  0.0701],\n",
      "        [ 0.1131,  0.0288, -0.0944, -0.0237,  0.0194]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0628,  0.0271, -0.0272, -0.0261,  0.0727],\n",
      "        [ 0.0128,  0.0310,  0.1085, -0.0696, -0.0365],\n",
      "        [ 0.0012, -0.0365, -0.0359,  0.0434, -0.0171],\n",
      "        [ 0.0698, -0.0335, -0.0489,  0.0447,  0.0129],\n",
      "        [-0.0123, -0.0327, -0.0276,  0.0982,  0.0295],\n",
      "        [ 0.0193, -0.0793, -0.0191,  0.1307,  0.0577],\n",
      "        [ 0.0975, -0.0144, -0.0225,  0.0424,  0.0208],\n",
      "        [ 0.0042,  0.0053,  0.0680, -0.0636, -0.0940]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0154, grad_fn=<MinBackward1>), tensor(0.9749, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3246036767959595\n",
      "@sample 104: tensor([[-0.0389, -0.0259,  0.0533,  0.0174,  0.0115],\n",
      "        [-0.0673, -0.0451, -0.0765,  0.0731, -0.0215],\n",
      "        [-0.0020,  0.0145,  0.0451, -0.0106,  0.0181],\n",
      "        [ 0.0699,  0.0579,  0.0283,  0.0130, -0.0219],\n",
      "        [-0.0314,  0.0027, -0.0725, -0.0447,  0.0089],\n",
      "        [ 0.0451,  0.0139, -0.0426,  0.0362,  0.0102],\n",
      "        [-0.0265, -0.0146, -0.0206, -0.0245, -0.0399],\n",
      "        [ 0.0313,  0.0816,  0.0646, -0.0946,  0.0251]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0454, -0.0037,  0.0391,  0.0053, -0.0415],\n",
      "        [ 0.0813,  0.0626,  0.1521, -0.1562, -0.0740],\n",
      "        [-0.0040, -0.0345,  0.0552, -0.0539,  0.0335],\n",
      "        [-0.0007,  0.0725,  0.0118,  0.0198, -0.0176],\n",
      "        [-0.0962, -0.0490,  0.0553, -0.0130, -0.0207],\n",
      "        [ 0.0333,  0.0471,  0.0884, -0.0776, -0.0862],\n",
      "        [-0.0232, -0.0683, -0.0352,  0.0738,  0.0575],\n",
      "        [-0.0546, -0.0904, -0.1093,  0.1349, -0.0062]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0181, grad_fn=<MinBackward1>), tensor(0.9795, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3311612010002136\n",
      "@sample 105: tensor([[-0.0015, -0.0232, -0.0772,  0.0479, -0.0176],\n",
      "        [-0.0281,  0.0710, -0.0456,  0.0266,  0.0750],\n",
      "        [-0.0475,  0.0686,  0.0047, -0.0900,  0.0535],\n",
      "        [ 0.0077,  0.0663,  0.0638, -0.0519, -0.0041],\n",
      "        [ 0.0012,  0.0215,  0.0371, -0.0138,  0.0488],\n",
      "        [ 0.0162, -0.0776,  0.0227,  0.0222, -0.0017],\n",
      "        [ 0.0232, -0.0262, -0.0082,  0.0257, -0.0043],\n",
      "        [ 0.0005, -0.0092,  0.0177, -0.0323, -0.0139]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0377,  0.0439,  0.0145,  0.0029,  0.0185],\n",
      "        [ 0.1491,  0.1082, -0.1235,  0.0545,  0.0569],\n",
      "        [-0.0730, -0.0602, -0.0110,  0.0119, -0.0109],\n",
      "        [-0.0431, -0.0295, -0.1118,  0.1133, -0.0221],\n",
      "        [-0.0130, -0.0128, -0.0464,  0.0085, -0.0283],\n",
      "        [-0.0272, -0.0317,  0.0707, -0.0117,  0.0062],\n",
      "        [-0.0364, -0.0545, -0.0389,  0.0318, -0.0911],\n",
      "        [-0.1419, -0.0809, -0.0366, -0.0425, -0.0305]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0084, grad_fn=<MinBackward1>), tensor(0.9637, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.32060104608535767\n",
      "@sample 106: tensor([[ 0.0290,  0.0906,  0.0426,  0.0058,  0.0660],\n",
      "        [-0.0246, -0.0083,  0.0131, -0.0228, -0.0362],\n",
      "        [-0.0457, -0.0174, -0.0243, -0.0418, -0.0150],\n",
      "        [ 0.0557,  0.0026, -0.0161, -0.0231,  0.0274],\n",
      "        [-0.0479,  0.0659, -0.0591, -0.0416, -0.0252],\n",
      "        [ 0.0328, -0.0578, -0.0248,  0.0884, -0.0261],\n",
      "        [-0.0035,  0.1219,  0.0095, -0.0626,  0.0121],\n",
      "        [-0.0062,  0.0101, -0.0320, -0.0428, -0.0233]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0373,  0.1306, -0.0196,  0.0141, -0.0773],\n",
      "        [-0.0579, -0.0103,  0.0843,  0.0171, -0.0568],\n",
      "        [ 0.0330, -0.0819,  0.0412, -0.0462, -0.0091],\n",
      "        [-0.0107, -0.0594, -0.0024, -0.0239,  0.0149],\n",
      "        [-0.0278, -0.0056,  0.0626, -0.0459, -0.0153],\n",
      "        [ 0.0501,  0.0711,  0.0111, -0.0363,  0.0657],\n",
      "        [-0.0617, -0.0600, -0.0952,  0.0601, -0.0721],\n",
      "        [-0.0296,  0.0200,  0.0453,  0.0203, -0.0620]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0262, grad_fn=<MinBackward1>), tensor(0.9646, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.30959585309028625\n",
      "@sample 107: tensor([[ 0.0573,  0.0062, -0.0234,  0.0785, -0.0174],\n",
      "        [ 0.0531, -0.0729,  0.0616,  0.0086, -0.0396],\n",
      "        [-0.0290, -0.0273, -0.0016, -0.0149,  0.1147],\n",
      "        [-0.0140, -0.0251, -0.0184,  0.0047,  0.0456],\n",
      "        [-0.0303, -0.0273, -0.0795, -0.0611,  0.0311],\n",
      "        [ 0.0164, -0.0319, -0.0227, -0.0244,  0.0380],\n",
      "        [-0.0671, -0.0184,  0.0985, -0.1319,  0.0399],\n",
      "        [-0.0123,  0.0018,  0.0015, -0.0357, -0.0049]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0349,  0.0799, -0.0149,  0.0853,  0.0676],\n",
      "        [ 0.0001, -0.0196,  0.0617,  0.0180, -0.0272],\n",
      "        [-0.0541, -0.0095,  0.0162, -0.0470,  0.0652],\n",
      "        [ 0.0656,  0.0251,  0.0058,  0.0245, -0.0243],\n",
      "        [-0.0416, -0.0018,  0.0527,  0.0424,  0.0259],\n",
      "        [ 0.0649,  0.0101, -0.0024,  0.0039,  0.0838],\n",
      "        [-0.0490, -0.0625,  0.0423,  0.0121, -0.0227],\n",
      "        [-0.0347,  0.0373, -0.0187, -0.0703, -0.0181]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0262, grad_fn=<MinBackward1>), tensor(0.9881, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3112225830554962\n",
      "@sample 108: tensor([[-0.0029,  0.0126,  0.0821,  0.1033,  0.0658],\n",
      "        [ 0.0368, -0.0279,  0.0307,  0.0186, -0.0185],\n",
      "        [-0.0099,  0.0515, -0.0199, -0.0468, -0.0094],\n",
      "        [ 0.0183, -0.0794,  0.0255,  0.0601, -0.0085],\n",
      "        [-0.0462,  0.0571,  0.0356, -0.1354,  0.1476],\n",
      "        [-0.0902, -0.0255,  0.0213,  0.0081,  0.0080],\n",
      "        [-0.0425, -0.0389,  0.0003,  0.0030, -0.0658],\n",
      "        [-0.0242,  0.0126, -0.1115,  0.0443, -0.0108]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0794,  0.1586, -0.0419,  0.0317,  0.0490],\n",
      "        [-0.0192,  0.0415,  0.0606, -0.0924, -0.0441],\n",
      "        [-0.0277, -0.0401, -0.1351,  0.0851,  0.0996],\n",
      "        [-0.0585,  0.0444, -0.0380, -0.0300,  0.0478],\n",
      "        [-0.0330, -0.0547, -0.1988,  0.0559, -0.0722],\n",
      "        [ 0.0578,  0.1397,  0.0447, -0.0158,  0.0265],\n",
      "        [ 0.0107, -0.0037,  0.0537, -0.0466,  0.0444],\n",
      "        [ 0.0199,  0.0328, -0.0594,  0.0665, -0.0418]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0242, grad_fn=<MinBackward1>), tensor(0.9779, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.35728001594543457\n",
      "@sample 109: tensor([[-0.0595,  0.0086,  0.0383,  0.0043,  0.0911],\n",
      "        [-0.0318,  0.0619, -0.0726,  0.0077, -0.0143],\n",
      "        [-0.0198, -0.0395,  0.0158,  0.0294,  0.0368],\n",
      "        [ 0.0015, -0.0265, -0.0535,  0.0102, -0.0794],\n",
      "        [ 0.0313, -0.0173,  0.0322,  0.0009,  0.0757],\n",
      "        [-0.0605,  0.0048,  0.0279, -0.0122, -0.0269],\n",
      "        [ 0.0309, -0.0466,  0.0257,  0.0698, -0.0914],\n",
      "        [-0.0574,  0.0069,  0.0674,  0.0195,  0.0684]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0085,  0.0132, -0.1260,  0.0695,  0.0193],\n",
      "        [-0.0325, -0.0182, -0.0589, -0.0928, -0.0077],\n",
      "        [ 0.1502, -0.0446, -0.0381, -0.0290,  0.0015],\n",
      "        [-0.0912,  0.0525, -0.1318,  0.0771,  0.0359],\n",
      "        [-0.0194, -0.0442, -0.0920,  0.0155, -0.0166],\n",
      "        [-0.1341,  0.0333, -0.0638,  0.0690, -0.0100],\n",
      "        [-0.0034, -0.0086,  0.0467, -0.0697, -0.0007],\n",
      "        [ 0.0217,  0.0195, -0.0671, -0.0023, -0.0227]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0171, grad_fn=<MinBackward1>), tensor(0.9819, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3277771472930908\n",
      "@sample 110: tensor([[-0.0130, -0.0035, -0.0787,  0.0872, -0.0076],\n",
      "        [ 0.0057, -0.0008, -0.0363,  0.0020, -0.0010],\n",
      "        [-0.1044, -0.0819,  0.0468, -0.0355,  0.0926],\n",
      "        [ 0.0404,  0.0156, -0.0003, -0.0382,  0.0707],\n",
      "        [ 0.0393,  0.0122, -0.0187,  0.0043, -0.0676],\n",
      "        [ 0.0392,  0.0096,  0.0565, -0.0389,  0.0060],\n",
      "        [-0.0207,  0.0603, -0.0164, -0.0371,  0.0881],\n",
      "        [-0.0251,  0.0317,  0.0279, -0.0700,  0.0758]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0004,  0.0482,  0.0201, -0.0623,  0.0558],\n",
      "        [-0.0028,  0.0170, -0.0103,  0.0438, -0.0453],\n",
      "        [-0.0347, -0.0358, -0.0749, -0.0347, -0.0280],\n",
      "        [-0.0585, -0.0598, -0.0550,  0.0884, -0.0829],\n",
      "        [-0.0201,  0.0394,  0.0883, -0.0770, -0.0292],\n",
      "        [ 0.0058,  0.0588,  0.0094, -0.0144, -0.0927],\n",
      "        [ 0.0163,  0.0155,  0.0565,  0.0218,  0.0457],\n",
      "        [ 0.0014,  0.0036, -0.0703,  0.0939,  0.0029]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0160, grad_fn=<MinBackward1>), tensor(0.9884, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.32154381275177\n",
      "@sample 111: tensor([[-0.0146,  0.0925, -0.0865, -0.0064,  0.0051],\n",
      "        [-0.0500, -0.0406,  0.0647, -0.0213,  0.0224],\n",
      "        [ 0.0210,  0.0934, -0.0087, -0.0418,  0.0262],\n",
      "        [ 0.0204, -0.0652, -0.0418,  0.0483,  0.0799],\n",
      "        [-0.0325,  0.1104, -0.0122, -0.0892,  0.0217],\n",
      "        [-0.0217,  0.0492,  0.0598, -0.0042,  0.0337],\n",
      "        [ 0.0458, -0.0500, -0.0443,  0.0377, -0.0496],\n",
      "        [-0.0476,  0.0626, -0.0046, -0.0476,  0.0190]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0152, -0.0183, -0.0752, -0.0034,  0.0472],\n",
      "        [-0.1491,  0.0360,  0.0244,  0.0012,  0.0387],\n",
      "        [-0.0342, -0.0478, -0.0622,  0.0007, -0.0249],\n",
      "        [ 0.0443,  0.0414,  0.0068, -0.0353,  0.0884],\n",
      "        [ 0.0326, -0.1207,  0.0227, -0.0340, -0.0815],\n",
      "        [ 0.0136, -0.0499, -0.0211,  0.0410,  0.0116],\n",
      "        [ 0.0483,  0.0062,  0.0749, -0.0685, -0.0170],\n",
      "        [-0.0190, -0.0664, -0.1414,  0.0801,  0.0205]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0169, grad_fn=<MinBackward1>), tensor(0.9711, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.33604562282562256\n",
      "@sample 112: tensor([[ 0.0546, -0.0104, -0.0091, -0.1056,  0.0457],\n",
      "        [ 0.0634, -0.0537,  0.0313,  0.0262, -0.0164],\n",
      "        [-0.1158, -0.0691,  0.0706, -0.0084, -0.0361],\n",
      "        [ 0.0446, -0.0291,  0.0105,  0.0052, -0.0409],\n",
      "        [-0.0621,  0.0159,  0.0287, -0.1064,  0.0484],\n",
      "        [ 0.0296,  0.0069,  0.0346, -0.0460,  0.0267],\n",
      "        [-0.0229,  0.0963,  0.0050, -0.0472,  0.0935],\n",
      "        [-0.0097, -0.0295, -0.0251,  0.0939, -0.0546]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0273, -0.1397, -0.0940,  0.0285,  0.0269],\n",
      "        [-0.0436,  0.0237, -0.1019,  0.1176,  0.0323],\n",
      "        [-0.1559, -0.0038, -0.0429,  0.0975,  0.0674],\n",
      "        [-0.0040, -0.0072,  0.0086,  0.0111, -0.0237],\n",
      "        [-0.0820, -0.0680, -0.0486,  0.0186,  0.0179],\n",
      "        [ 0.0302, -0.0096, -0.0363,  0.0888,  0.0126],\n",
      "        [ 0.0362, -0.0104, -0.0720,  0.0630,  0.0211],\n",
      "        [-0.0844,  0.0994,  0.0357, -0.0004, -0.0222]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0220, grad_fn=<MinBackward1>), tensor(0.9705, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.344616562128067\n",
      "@sample 113: tensor([[-0.0520,  0.0504,  0.0259, -0.0697,  0.0185],\n",
      "        [-0.0152,  0.0606,  0.0406, -0.0151, -0.0193],\n",
      "        [-0.0073,  0.1031,  0.0530, -0.0674,  0.0394],\n",
      "        [ 0.0434,  0.0006, -0.0466, -0.0263, -0.0291],\n",
      "        [-0.0269,  0.0799,  0.0155, -0.0385,  0.0558],\n",
      "        [-0.0440,  0.0434,  0.0381,  0.0201, -0.0013],\n",
      "        [-0.0075,  0.0173,  0.0597,  0.0147,  0.0359],\n",
      "        [ 0.0238,  0.0324,  0.0571, -0.0600, -0.0008]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0233, -0.0675, -0.0834,  0.0929, -0.0702],\n",
      "        [ 0.0205,  0.0845, -0.0075, -0.0404,  0.0400],\n",
      "        [-0.0458, -0.0426, -0.0668,  0.0563, -0.0267],\n",
      "        [-0.0542, -0.0660, -0.0952,  0.0350,  0.0709],\n",
      "        [-0.0112, -0.0518, -0.1375,  0.0982,  0.0440],\n",
      "        [ 0.0321,  0.0445, -0.0663,  0.0336, -0.0336],\n",
      "        [-0.0647,  0.0095, -0.0485,  0.0291,  0.0062],\n",
      "        [-0.0245,  0.0370,  0.0300, -0.0563, -0.0631]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0191, grad_fn=<MinBackward1>), tensor(0.9767, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.320770800113678\n",
      "@sample 114: tensor([[-0.0772,  0.0124, -0.0200,  0.0077,  0.0293],\n",
      "        [ 0.0022,  0.0092, -0.0026,  0.0245, -0.0500],\n",
      "        [ 0.0088, -0.0474, -0.0341,  0.0451, -0.0692],\n",
      "        [-0.0260,  0.0094,  0.0567, -0.0074,  0.0103],\n",
      "        [-0.0411,  0.0303, -0.0590,  0.0119,  0.0279],\n",
      "        [ 0.0135,  0.0257, -0.0391,  0.0752,  0.0040],\n",
      "        [ 0.0372, -0.0203,  0.0364, -0.1310,  0.0280],\n",
      "        [-0.0100, -0.0702, -0.0295, -0.0199,  0.0304]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-6.0830e-03,  7.2184e-02, -3.2330e-02, -3.4681e-02,  3.5961e-02],\n",
      "        [-1.2229e-02,  1.8535e-02,  7.3992e-02, -9.0606e-02, -4.4905e-02],\n",
      "        [ 1.2649e-02,  3.7082e-02,  1.6443e-02,  4.7638e-02,  7.4770e-02],\n",
      "        [ 5.9592e-03,  5.5358e-03, -1.2439e-02, -1.2171e-04,  3.9375e-03],\n",
      "        [ 7.0129e-03,  7.0764e-03, -8.0359e-02, -5.6566e-02,  2.5517e-02],\n",
      "        [ 5.1554e-02,  9.8333e-03, -5.9782e-02, -9.0388e-02,  4.8943e-02],\n",
      "        [-7.6579e-02, -1.6138e-01, -1.0911e-01,  5.4108e-02, -1.3529e-03],\n",
      "        [-7.8717e-02, -1.3658e-01, -9.8585e-02,  9.5032e-02,  2.7754e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0212, grad_fn=<MinBackward1>), tensor(0.9912, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3179188072681427\n",
      "@sample 115: tensor([[ 0.0045,  0.0622,  0.0026, -0.0892, -0.0193],\n",
      "        [-0.0105,  0.0415,  0.0625, -0.0092,  0.0601],\n",
      "        [-0.0050,  0.0237, -0.0061, -0.0124, -0.0298],\n",
      "        [-0.0107, -0.0150, -0.0270, -0.0285, -0.0176],\n",
      "        [-0.0481,  0.0136,  0.0468, -0.0029,  0.0582],\n",
      "        [-0.0104, -0.0303, -0.0205,  0.0781, -0.0361],\n",
      "        [-0.0324, -0.0109,  0.0273,  0.0233, -0.0151],\n",
      "        [-0.0226, -0.0268, -0.0539, -0.0044,  0.0124]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0291, -0.0378, -0.0187,  0.0828,  0.1105],\n",
      "        [-0.0707, -0.0088, -0.0574, -0.0137, -0.0182],\n",
      "        [-0.1116,  0.0200,  0.0016,  0.0240,  0.0735],\n",
      "        [-0.0170,  0.0371, -0.0414,  0.0055,  0.0058],\n",
      "        [-0.0179, -0.0174, -0.0393,  0.0372, -0.0155],\n",
      "        [ 0.0843,  0.0347,  0.0976, -0.1556,  0.0185],\n",
      "        [ 0.0418,  0.0436,  0.0015,  0.0746, -0.0630],\n",
      "        [ 0.0023, -0.0345,  0.1377, -0.0455, -0.0565]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0133, grad_fn=<MinBackward1>), tensor(0.9880, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3055877983570099\n",
      "@sample 116: tensor([[ 0.0193, -0.1234, -0.0197,  0.1256, -0.0731],\n",
      "        [ 0.0635, -0.0415, -0.0360,  0.0090, -0.0735],\n",
      "        [ 0.0216,  0.0662, -0.0547, -0.0755, -0.0345],\n",
      "        [-0.0451, -0.0218, -0.0591,  0.0132,  0.0234],\n",
      "        [ 0.0070,  0.0173, -0.0489, -0.0120,  0.0359],\n",
      "        [ 0.0037,  0.0144, -0.0060, -0.0015,  0.0048],\n",
      "        [ 0.0090, -0.0995, -0.0064,  0.1099, -0.0299],\n",
      "        [ 0.0598, -0.0456, -0.0310,  0.0775, -0.0421]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0244,  0.1301,  0.0895, -0.0086, -0.0205],\n",
      "        [-0.0384, -0.0312, -0.0046, -0.1130, -0.0413],\n",
      "        [ 0.0063, -0.0982, -0.0383,  0.0062, -0.0578],\n",
      "        [-0.0182, -0.0846, -0.0599, -0.0318,  0.0549],\n",
      "        [ 0.0234,  0.0023, -0.0965,  0.0361,  0.0578],\n",
      "        [ 0.0077, -0.0173, -0.0727, -0.0232, -0.0078],\n",
      "        [-0.0635,  0.0795,  0.0738, -0.0658,  0.0358],\n",
      "        [ 0.0035, -0.0286, -0.0277, -0.1093,  0.0465]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0230, grad_fn=<MinBackward1>), tensor(0.9930, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.33923807740211487\n",
      "@sample 117: tensor([[-0.0429,  0.0102, -0.0021, -0.0417,  0.1243],\n",
      "        [ 0.0182,  0.0208,  0.0027,  0.0249,  0.0270],\n",
      "        [ 0.0298,  0.0266, -0.0057, -0.0034,  0.0103],\n",
      "        [-0.0195, -0.0592, -0.0159,  0.1291, -0.0365],\n",
      "        [ 0.0065,  0.0509,  0.0061, -0.0494,  0.0071],\n",
      "        [ 0.0013,  0.0322, -0.0030, -0.0904,  0.0148],\n",
      "        [ 0.0175,  0.0268,  0.0576, -0.0785,  0.0637],\n",
      "        [-0.0205, -0.0195,  0.0044, -0.0861,  0.0504]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0790, -0.0268,  0.0008,  0.0245, -0.0040],\n",
      "        [ 0.0348,  0.0427, -0.0142,  0.0533, -0.0856],\n",
      "        [-0.0105,  0.0204, -0.0437,  0.0670, -0.0805],\n",
      "        [ 0.0499,  0.0488,  0.0490, -0.0362,  0.0437],\n",
      "        [-0.0770, -0.0603,  0.0287, -0.0269,  0.0453],\n",
      "        [ 0.0088, -0.0220, -0.0092,  0.0418,  0.0055],\n",
      "        [-0.0363,  0.0577, -0.0446,  0.0859,  0.0202],\n",
      "        [-0.0116, -0.0304,  0.0971, -0.0692, -0.1137]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0236, grad_fn=<MinBackward1>), tensor(0.9807, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3133450746536255\n",
      "@sample 118: tensor([[ 0.0158,  0.0765,  0.0707, -0.0382, -0.0468],\n",
      "        [ 0.0011,  0.0223, -0.0812,  0.0378, -0.0684],\n",
      "        [ 0.0436, -0.0211,  0.0208,  0.1154, -0.0857],\n",
      "        [ 0.0438, -0.0035, -0.0898, -0.0227, -0.0033],\n",
      "        [ 0.0013, -0.0782, -0.0257,  0.0344, -0.0951],\n",
      "        [-0.0530, -0.0309, -0.0266,  0.0100,  0.0089],\n",
      "        [ 0.0233, -0.0217, -0.0028, -0.0299, -0.0033],\n",
      "        [ 0.0304,  0.0257, -0.0195,  0.1156, -0.1145]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0257, -0.0400,  0.0399, -0.0332, -0.0410],\n",
      "        [-0.0050,  0.0306,  0.0220, -0.0447,  0.0456],\n",
      "        [-0.0129,  0.1075,  0.0212, -0.0382, -0.0077],\n",
      "        [ 0.0535, -0.0438,  0.0162, -0.0050, -0.0668],\n",
      "        [-0.0155,  0.0436,  0.0168,  0.0078,  0.0071],\n",
      "        [ 0.0664,  0.1085,  0.0042,  0.0105,  0.0155],\n",
      "        [-0.0159, -0.0579,  0.0119,  0.0236, -0.0166],\n",
      "        [ 0.0276,  0.0403,  0.1235, -0.0691, -0.0224]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0073, grad_fn=<MinBackward1>), tensor(0.9832, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3257112503051758\n",
      "@sample 119: tensor([[ 0.0071, -0.0207,  0.0450, -0.0034, -0.0400],\n",
      "        [ 0.0313, -0.0494,  0.0621, -0.0048,  0.0251],\n",
      "        [ 0.0231, -0.1009, -0.0387,  0.0399, -0.0453],\n",
      "        [ 0.0079, -0.0119,  0.0504,  0.0880, -0.0600],\n",
      "        [ 0.0341,  0.0950, -0.0159, -0.0654,  0.0161],\n",
      "        [ 0.0051,  0.0495,  0.0023, -0.0079, -0.0061],\n",
      "        [-0.0120, -0.0705, -0.0405, -0.0061, -0.0768],\n",
      "        [-0.0410,  0.0359,  0.0513, -0.0875, -0.0038]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.1201, -0.0328, -0.0271,  0.0158,  0.0082],\n",
      "        [ 0.0061, -0.0010, -0.1018,  0.0439,  0.0150],\n",
      "        [ 0.0254, -0.0467,  0.0384, -0.0395,  0.0095],\n",
      "        [ 0.0199,  0.1116,  0.0308, -0.0449, -0.0588],\n",
      "        [-0.0804, -0.0404,  0.0400,  0.0496,  0.0398],\n",
      "        [ 0.0076, -0.0616, -0.0748,  0.0375, -0.0092],\n",
      "        [-0.0360, -0.0588,  0.1346, -0.0662, -0.0450],\n",
      "        [-0.1515, -0.1089, -0.0886,  0.0718,  0.1328]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0282, grad_fn=<MinBackward1>), tensor(0.9884, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.33368170261383057\n",
      "@sample 120: tensor([[ 7.5206e-02, -3.2562e-02, -2.6777e-02,  7.7845e-02, -9.4977e-02],\n",
      "        [-3.0477e-02, -3.6582e-02,  5.6988e-02, -4.4474e-03, -2.6363e-02],\n",
      "        [ 7.6201e-03, -2.5400e-02,  7.1576e-02, -2.0980e-02,  2.3766e-02],\n",
      "        [-9.5844e-03,  4.5327e-02, -4.9435e-03,  2.3504e-03, -6.7719e-02],\n",
      "        [ 3.0656e-02, -4.7161e-02,  6.3815e-03,  2.5585e-05,  8.0299e-02],\n",
      "        [ 3.1191e-02,  6.0527e-02,  2.1327e-03,  7.5740e-02, -1.3663e-01],\n",
      "        [ 1.6584e-02,  1.8432e-02, -8.7452e-03, -1.8328e-02,  1.4416e-02],\n",
      "        [ 1.2380e-02,  1.4493e-02, -1.2460e-01,  1.0890e-01, -1.3510e-01]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0025, -0.0061, -0.0123,  0.0324,  0.0039],\n",
      "        [-0.0309, -0.0180, -0.0571,  0.0126, -0.0886],\n",
      "        [ 0.0106,  0.0456,  0.0104,  0.0011, -0.0379],\n",
      "        [-0.0776,  0.0361, -0.0009, -0.0644, -0.0878],\n",
      "        [-0.0572, -0.0257, -0.0005,  0.0202,  0.0690],\n",
      "        [ 0.0278,  0.1394,  0.0152, -0.0541, -0.0720],\n",
      "        [ 0.0063, -0.0083, -0.0371,  0.0541, -0.0185],\n",
      "        [-0.0108,  0.0541,  0.0968, -0.1018, -0.0841]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0142, grad_fn=<MinBackward1>), tensor(0.9748, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.33577555418014526\n",
      "@sample 121: tensor([[-0.0197, -0.0036, -0.0441,  0.0845, -0.0845],\n",
      "        [ 0.0606, -0.1140, -0.0152,  0.0458, -0.0002],\n",
      "        [-0.0220,  0.0302,  0.0050,  0.0269, -0.0063],\n",
      "        [ 0.0382, -0.1022, -0.0085,  0.0689,  0.0547],\n",
      "        [ 0.0030, -0.0099,  0.0998,  0.0139,  0.0087],\n",
      "        [ 0.0352,  0.0500,  0.0630, -0.1353,  0.0273],\n",
      "        [-0.0520,  0.0476, -0.0135, -0.0262, -0.0943],\n",
      "        [-0.0121,  0.0657, -0.0825, -0.0434, -0.0210]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0333,  0.0565, -0.0066, -0.0070,  0.0393],\n",
      "        [ 0.0829,  0.0030,  0.0478, -0.0149,  0.0445],\n",
      "        [ 0.0274, -0.0136,  0.0343, -0.0699, -0.0120],\n",
      "        [ 0.0808,  0.1177, -0.0879,  0.0956,  0.0585],\n",
      "        [-0.0220,  0.0067, -0.0183,  0.0398,  0.0163],\n",
      "        [-0.0285, -0.1295, -0.1315,  0.0706,  0.0097],\n",
      "        [-0.0591, -0.0386,  0.0398, -0.0422, -0.0143],\n",
      "        [ 0.0681, -0.0668, -0.0615,  0.0001,  0.0036]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0092, grad_fn=<MinBackward1>), tensor(0.9887, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3446112871170044\n",
      "@sample 122: tensor([[ 0.0268,  0.0075, -0.0193,  0.0368, -0.0220],\n",
      "        [-0.0204,  0.0334,  0.0115,  0.0849, -0.1533],\n",
      "        [ 0.0197,  0.0782,  0.0586, -0.0726,  0.0755],\n",
      "        [ 0.0251,  0.0050, -0.0538,  0.1350, -0.1365],\n",
      "        [-0.0387, -0.0477, -0.0509,  0.0968, -0.0807],\n",
      "        [ 0.0111,  0.0417, -0.0074,  0.0114, -0.0273],\n",
      "        [-0.0197,  0.0056,  0.0401, -0.0269,  0.0580],\n",
      "        [-0.0312,  0.0029,  0.0528, -0.0776,  0.0376]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0775,  0.0746,  0.1667, -0.1351, -0.0758],\n",
      "        [-0.0252,  0.0946,  0.0799, -0.0271, -0.0117],\n",
      "        [ 0.0533,  0.0334,  0.0049, -0.0187, -0.0399],\n",
      "        [ 0.0282,  0.0818,  0.0665, -0.0881,  0.0015],\n",
      "        [ 0.0767,  0.0570,  0.1878, -0.0521,  0.0078],\n",
      "        [ 0.0711,  0.0480,  0.0008,  0.0167,  0.0821],\n",
      "        [ 0.0653,  0.0501, -0.0217, -0.0739,  0.0691],\n",
      "        [-0.0304, -0.0272,  0.0016, -0.0322, -0.0942]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0098, grad_fn=<MinBackward1>), tensor(0.9697, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.36308401823043823\n",
      "@sample 123: tensor([[ 0.0232, -0.0517, -0.0793,  0.0195,  0.0775],\n",
      "        [ 0.0069,  0.0418,  0.0174,  0.0297, -0.0185],\n",
      "        [-0.0419,  0.0194,  0.0362, -0.0166,  0.0104],\n",
      "        [-0.0369,  0.0608,  0.0912, -0.0414,  0.0730],\n",
      "        [ 0.0160,  0.0319, -0.0157, -0.0433,  0.0223],\n",
      "        [ 0.0377,  0.0402, -0.0197, -0.0397, -0.0733],\n",
      "        [ 0.0364, -0.0069, -0.0018,  0.0205, -0.0290],\n",
      "        [ 0.0191,  0.0270,  0.0191,  0.0032, -0.0501]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0466,  0.0466, -0.1245,  0.0582,  0.0433],\n",
      "        [-0.0310,  0.0581, -0.0700,  0.0588, -0.0396],\n",
      "        [ 0.0478, -0.0239,  0.0388, -0.0004,  0.0070],\n",
      "        [-0.0823, -0.0018, -0.0377,  0.0341, -0.0405],\n",
      "        [-0.0343, -0.0254, -0.0451,  0.0481, -0.0581],\n",
      "        [-0.0477, -0.0213,  0.0182,  0.0237, -0.0544],\n",
      "        [-0.0320, -0.0068, -0.0433, -0.0062, -0.0255],\n",
      "        [ 0.0249,  0.0136,  0.0457, -0.0641, -0.0048]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0021, grad_fn=<MinBackward1>), tensor(0.9840, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.30304038524627686\n",
      "@sample 124: tensor([[ 0.0021,  0.0172,  0.0276, -0.0648,  0.0034],\n",
      "        [-0.0024, -0.0717,  0.0372, -0.0116, -0.0594],\n",
      "        [-0.0165, -0.0532,  0.0616, -0.0252,  0.1290],\n",
      "        [-0.0010, -0.0751, -0.0249, -0.0415, -0.0210],\n",
      "        [ 0.0404,  0.0234, -0.0097,  0.0076,  0.0321],\n",
      "        [ 0.0209, -0.0416, -0.0016,  0.0154, -0.0590],\n",
      "        [-0.0247,  0.0178, -0.0074, -0.0379,  0.0612],\n",
      "        [-0.0270, -0.0067,  0.0735, -0.0188, -0.0222]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-3.2666e-02, -5.0778e-02, -1.9132e-02,  5.2514e-02,  6.3755e-02],\n",
      "        [-5.4640e-02, -4.1530e-02,  1.8365e-01, -9.0414e-02, -1.8010e-02],\n",
      "        [ 5.9603e-02, -6.6914e-02, -3.2309e-02,  2.3233e-02,  7.6465e-05],\n",
      "        [-7.6751e-02, -3.2806e-02, -5.4704e-02,  7.5469e-02,  5.7993e-03],\n",
      "        [ 3.0565e-03, -1.1832e-02, -6.8756e-02, -2.2290e-03, -5.3769e-02],\n",
      "        [-3.9944e-02,  2.2466e-02,  1.4715e-01, -4.0819e-02,  1.0503e-02],\n",
      "        [ 2.3372e-02, -6.4427e-02,  2.2871e-02,  4.0007e-03,  3.1216e-02],\n",
      "        [ 3.3620e-02, -1.4626e-02,  3.9427e-02,  6.1313e-02,  1.0485e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0112, grad_fn=<MinBackward1>), tensor(0.9767, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.30860522389411926\n",
      "@sample 125: tensor([[ 0.1045, -0.0278, -0.0395,  0.0546, -0.0025],\n",
      "        [ 0.0220,  0.0207,  0.0206, -0.0622, -0.0169],\n",
      "        [ 0.0023,  0.0262,  0.0403, -0.0532,  0.0346],\n",
      "        [-0.0649, -0.0308,  0.0640, -0.0180, -0.0918],\n",
      "        [-0.0057, -0.0190, -0.0513,  0.0065,  0.0347],\n",
      "        [-0.0361,  0.0153,  0.1052, -0.0260, -0.0017],\n",
      "        [ 0.0092, -0.0841, -0.0223, -0.0273, -0.0404],\n",
      "        [-0.0103,  0.0416, -0.0251,  0.0093, -0.0047]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0177, -0.0395, -0.0089,  0.0289, -0.0436],\n",
      "        [-0.0131, -0.1434,  0.0047,  0.0448, -0.0484],\n",
      "        [-0.0388,  0.0204, -0.0638, -0.0421, -0.0539],\n",
      "        [-0.0171, -0.0418,  0.0869, -0.0662, -0.0156],\n",
      "        [ 0.1121, -0.0771, -0.0971,  0.0426, -0.0203],\n",
      "        [-0.0708, -0.0469, -0.0044, -0.0099, -0.0613],\n",
      "        [-0.0250, -0.0547,  0.1157, -0.0296, -0.0593],\n",
      "        [ 0.0348, -0.0463,  0.0237, -0.0381, -0.0490]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0162, grad_fn=<MinBackward1>), tensor(0.9643, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.311962366104126\n",
      "@sample 126: tensor([[-0.0078,  0.0919,  0.1007, -0.0077,  0.0843],\n",
      "        [-0.0276,  0.0444, -0.0117, -0.0502,  0.0436],\n",
      "        [-0.0484,  0.0394, -0.0291, -0.0299,  0.0142],\n",
      "        [-0.0330, -0.0455, -0.0996,  0.0530, -0.0012],\n",
      "        [ 0.0656, -0.0689,  0.0296,  0.1256, -0.1198],\n",
      "        [ 0.0312,  0.0230,  0.0154,  0.0088,  0.0821],\n",
      "        [-0.0242, -0.0188, -0.0081, -0.0294,  0.0588],\n",
      "        [ 0.0068,  0.0673,  0.0535,  0.0061,  0.0498]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0627,  0.0344, -0.0533,  0.0380,  0.0551],\n",
      "        [-0.0244, -0.0839, -0.1079,  0.0672,  0.0133],\n",
      "        [ 0.0337, -0.0244, -0.0894,  0.0802,  0.0271],\n",
      "        [ 0.0575,  0.0793, -0.0112, -0.0127,  0.0536],\n",
      "        [ 0.0206,  0.1364,  0.0307, -0.0401,  0.0120],\n",
      "        [ 0.0624,  0.0382, -0.0979,  0.0395, -0.0167],\n",
      "        [-0.0350, -0.0453,  0.0113, -0.0281, -0.0462],\n",
      "        [ 0.0078,  0.0213, -0.0439,  0.0231, -0.0105]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0179, grad_fn=<MinBackward1>), tensor(0.9789, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3305927813053131\n",
      "@sample 127: tensor([[ 0.0442, -0.0110, -0.0443,  0.0152,  0.0534],\n",
      "        [ 0.0158, -0.1536, -0.0163,  0.1605, -0.1063],\n",
      "        [ 0.0555,  0.1205,  0.0259, -0.0550,  0.0740],\n",
      "        [-0.0015, -0.0206, -0.0231,  0.0875, -0.0002],\n",
      "        [-0.0009, -0.0495, -0.0907,  0.1033, -0.0818],\n",
      "        [ 0.0703,  0.0088, -0.0494,  0.1107, -0.0770],\n",
      "        [-0.0241, -0.0284,  0.0475,  0.0338, -0.0222],\n",
      "        [ 0.0402,  0.0341, -0.0193,  0.0054,  0.0260]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0217, -0.0368, -0.0394,  0.0149,  0.0870],\n",
      "        [ 0.0650,  0.1004,  0.2986, -0.1689, -0.0086],\n",
      "        [ 0.0005,  0.0579, -0.0043,  0.0875, -0.0940],\n",
      "        [ 0.0895,  0.0781,  0.1148, -0.0393,  0.0073],\n",
      "        [ 0.0236,  0.0536,  0.0651, -0.0606, -0.0013],\n",
      "        [ 0.0048,  0.0194,  0.0314, -0.0272, -0.0009],\n",
      "        [-0.0162,  0.0562,  0.1292, -0.0740, -0.0195],\n",
      "        [ 0.0772,  0.0661, -0.0490,  0.0057,  0.0828]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0122, grad_fn=<MinBackward1>), tensor(0.9867, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3907666802406311\n",
      "@sample 128: tensor([[-0.0693,  0.0923,  0.0497, -0.0603,  0.0923],\n",
      "        [ 0.0148, -0.0157,  0.0661, -0.0993, -0.0287],\n",
      "        [-0.0597, -0.0224,  0.0196, -0.0286,  0.0808],\n",
      "        [-0.0164,  0.0596,  0.0002, -0.0033,  0.0400],\n",
      "        [-0.0346, -0.0078,  0.0059, -0.0168,  0.0087],\n",
      "        [-0.0571,  0.0040, -0.0416, -0.0431, -0.0889],\n",
      "        [ 0.0131, -0.0523, -0.0125,  0.0777, -0.0028],\n",
      "        [ 0.0274,  0.0237, -0.0172,  0.0373,  0.0211]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0473, -0.0309, -0.1983,  0.1560,  0.0972],\n",
      "        [-0.1276, -0.0847, -0.0577, -0.0208, -0.0042],\n",
      "        [-0.0028,  0.0444, -0.0628,  0.0322,  0.0543],\n",
      "        [ 0.0117, -0.0178, -0.1135,  0.0738,  0.0334],\n",
      "        [-0.0897, -0.0131, -0.0703, -0.0049,  0.0460],\n",
      "        [ 0.0080,  0.0021,  0.0555, -0.0047,  0.0213],\n",
      "        [ 0.0126,  0.0366,  0.0864, -0.0474,  0.0799],\n",
      "        [ 0.0015,  0.0168,  0.0451, -0.0670,  0.0268]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0300, grad_fn=<MinBackward1>), tensor(0.9878, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3316357433795929\n",
      "@sample 129: tensor([[-0.0546, -0.1873, -0.0687,  0.1366, -0.1040],\n",
      "        [-0.0217, -0.0530, -0.0615,  0.0226, -0.0176],\n",
      "        [-0.0300, -0.0620,  0.0579,  0.0088, -0.0415],\n",
      "        [-0.0162,  0.1024,  0.0064, -0.0251, -0.0045],\n",
      "        [ 0.0090,  0.0448,  0.0136, -0.0553,  0.0221],\n",
      "        [-0.0872, -0.0214, -0.0024, -0.0129, -0.0167],\n",
      "        [ 0.0326,  0.0274, -0.0019, -0.0958,  0.0317],\n",
      "        [-0.0093, -0.0457, -0.0209, -0.0343, -0.0094]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0184, -0.0417,  0.1211,  0.0060, -0.0166],\n",
      "        [ 0.0098,  0.0100,  0.0538,  0.0451,  0.0346],\n",
      "        [-0.0246, -0.0772, -0.0179, -0.0345,  0.0927],\n",
      "        [-0.0189, -0.0267, -0.0776,  0.0996,  0.0390],\n",
      "        [ 0.0100,  0.0202,  0.0062,  0.0163,  0.0160],\n",
      "        [-0.0938, -0.0369,  0.0005, -0.0841,  0.0031],\n",
      "        [-0.0384, -0.0762, -0.1041,  0.0570, -0.0470],\n",
      "        [-0.0957,  0.0289, -0.0220,  0.0058,  0.0354]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0160, grad_fn=<MinBackward1>), tensor(0.9720, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.33442455530166626\n",
      "@sample 130: tensor([[-0.0200,  0.0433,  0.0471, -0.0327,  0.0412],\n",
      "        [ 0.0180,  0.0564,  0.0280, -0.0476,  0.0241],\n",
      "        [-0.0740,  0.0364, -0.0996,  0.0132,  0.0368],\n",
      "        [ 0.0256, -0.0066,  0.0125, -0.0091, -0.0204],\n",
      "        [ 0.0615, -0.0551, -0.0151,  0.0447, -0.0386],\n",
      "        [-0.0118, -0.0172,  0.0249, -0.0458, -0.0007],\n",
      "        [ 0.0353,  0.0131, -0.0037,  0.0027, -0.0104],\n",
      "        [ 0.0430,  0.0440,  0.0277, -0.0168,  0.0261]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0399, -0.0473, -0.1107,  0.0334, -0.0105],\n",
      "        [-0.0209, -0.0067,  0.0347,  0.0686, -0.0068],\n",
      "        [ 0.0559, -0.0313, -0.0802,  0.0028, -0.0462],\n",
      "        [ 0.0090, -0.0349, -0.0258, -0.0143,  0.0370],\n",
      "        [ 0.0210,  0.0129,  0.0456, -0.0681, -0.0773],\n",
      "        [ 0.0048, -0.0447,  0.1327, -0.0533, -0.0526],\n",
      "        [-0.0762, -0.0591, -0.0372,  0.1112, -0.0737],\n",
      "        [-0.0241,  0.0149, -0.0428,  0.0500,  0.0098]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0178, grad_fn=<MinBackward1>), tensor(0.9684, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3013160824775696\n",
      "@sample 131: tensor([[-0.0046, -0.0461,  0.0604, -0.0722,  0.1788],\n",
      "        [-0.0331,  0.0678, -0.0223, -0.0221, -0.0323],\n",
      "        [-0.0013, -0.0368, -0.0058, -0.0466,  0.0461],\n",
      "        [ 0.0428, -0.0622, -0.0572,  0.0214,  0.0244],\n",
      "        [-0.0178, -0.0475,  0.0508,  0.0366, -0.0256],\n",
      "        [-0.0030, -0.0274, -0.0159,  0.0502, -0.0433],\n",
      "        [ 0.0035,  0.0077,  0.0335, -0.0063,  0.0325],\n",
      "        [ 0.0204, -0.0530,  0.0130,  0.0654,  0.0212]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0840, -0.0957, -0.1566, -0.0077,  0.0138],\n",
      "        [-0.0233, -0.0060,  0.0587, -0.0294,  0.0293],\n",
      "        [-0.0203,  0.0088, -0.1663,  0.0773, -0.0128],\n",
      "        [ 0.0370, -0.0070,  0.1437, -0.0934, -0.0943],\n",
      "        [-0.0766, -0.0121,  0.0067, -0.1220, -0.0769],\n",
      "        [-0.0009,  0.0119, -0.0313, -0.0451, -0.0233],\n",
      "        [ 0.0012,  0.0297, -0.0464,  0.0524, -0.0665],\n",
      "        [ 0.0558, -0.0132, -0.0576, -0.0071,  0.0388]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0206, grad_fn=<MinBackward1>), tensor(0.9788, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.32868626713752747\n",
      "@sample 132: tensor([[ 0.0093,  0.0822,  0.0434, -0.0432, -0.0230],\n",
      "        [-0.0658, -0.0814, -0.0457,  0.0177,  0.0125],\n",
      "        [-0.0288,  0.1196, -0.0100, -0.0152, -0.0153],\n",
      "        [-0.0132, -0.0633,  0.0094,  0.0706, -0.0901],\n",
      "        [ 0.0009,  0.0545,  0.0771, -0.0663,  0.0993],\n",
      "        [ 0.0135,  0.0748,  0.0214, -0.0427,  0.0089],\n",
      "        [-0.0013,  0.0491,  0.0513,  0.1125, -0.0196],\n",
      "        [ 0.0041,  0.0418, -0.0599, -0.0205,  0.0482]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0202, -0.0893,  0.0925,  0.0128, -0.0653],\n",
      "        [-0.0233, -0.0077,  0.0827, -0.0368, -0.0012],\n",
      "        [ 0.1014, -0.0061,  0.0869, -0.1244, -0.0778],\n",
      "        [ 0.0349, -0.0124,  0.1445, -0.0528, -0.0087],\n",
      "        [-0.0073, -0.0632, -0.0919,  0.0337, -0.0558],\n",
      "        [-0.0023,  0.0168, -0.0185, -0.0056,  0.0167],\n",
      "        [ 0.0131,  0.1238, -0.1020,  0.0036,  0.0004],\n",
      "        [-0.0307, -0.0212, -0.0701,  0.0583,  0.0158]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0148, grad_fn=<MinBackward1>), tensor(0.9849, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.33861401677131653\n",
      "@sample 133: tensor([[ 0.0253, -0.0075, -0.0013,  0.0378,  0.0219],\n",
      "        [-0.0552,  0.0201,  0.0035, -0.0193,  0.0686],\n",
      "        [-0.0281,  0.0528, -0.0337, -0.0642,  0.0400],\n",
      "        [ 0.0469,  0.1162,  0.0743,  0.0358,  0.0012],\n",
      "        [ 0.0266,  0.0594,  0.0427, -0.0410, -0.0218],\n",
      "        [ 0.0131, -0.0292,  0.0015,  0.1241, -0.0198],\n",
      "        [-0.0225,  0.1216, -0.0442, -0.0776,  0.0211],\n",
      "        [-0.0294, -0.0236,  0.0384,  0.0414, -0.0021]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0271,  0.0070,  0.0314,  0.0049,  0.0091],\n",
      "        [ 0.0636, -0.0121, -0.0730,  0.0727,  0.0725],\n",
      "        [-0.0155, -0.0396, -0.0300, -0.0001, -0.0171],\n",
      "        [ 0.0340,  0.0085, -0.0179,  0.0297,  0.0708],\n",
      "        [ 0.0115, -0.0263, -0.0231,  0.0505,  0.0323],\n",
      "        [ 0.0243,  0.1409,  0.0102, -0.0779, -0.0046],\n",
      "        [-0.0390, -0.0392, -0.0187, -0.0063, -0.1380],\n",
      "        [ 0.0342,  0.0336,  0.0630, -0.0493,  0.0185]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0134, grad_fn=<MinBackward1>), tensor(0.9662, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3155835270881653\n",
      "@sample 134: tensor([[-0.0158,  0.0164, -0.0430,  0.0147, -0.0238],\n",
      "        [-0.0012, -0.0643,  0.0082,  0.0131, -0.0106],\n",
      "        [ 0.0152, -0.0554, -0.0449,  0.0116,  0.0189],\n",
      "        [ 0.0355, -0.0305, -0.0913, -0.0113,  0.0812],\n",
      "        [ 0.0191, -0.0285, -0.0024,  0.0371, -0.0381],\n",
      "        [-0.0161,  0.0375,  0.0272, -0.0334, -0.0063],\n",
      "        [ 0.0295, -0.0050,  0.0103,  0.0547, -0.0113],\n",
      "        [-0.0258,  0.0600, -0.0691, -0.0160,  0.0597]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0472, -0.0065, -0.0382, -0.0269,  0.0296],\n",
      "        [ 0.0499, -0.0415,  0.0631,  0.0044,  0.0345],\n",
      "        [-0.0396, -0.0206,  0.0444,  0.0555,  0.0891],\n",
      "        [-0.0293,  0.0041,  0.0722, -0.1325, -0.0795],\n",
      "        [ 0.0436,  0.0505,  0.0738, -0.0240,  0.0016],\n",
      "        [ 0.0031, -0.0128,  0.0236, -0.0335, -0.0288],\n",
      "        [ 0.0222,  0.1021, -0.0116,  0.0767, -0.0089],\n",
      "        [ 0.0003, -0.0218, -0.0146, -0.0450,  0.0093]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0129, grad_fn=<MinBackward1>), tensor(0.9747, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.28848785161972046\n",
      "@sample 135: tensor([[ 0.0313, -0.0486,  0.0841,  0.0362, -0.0498],\n",
      "        [-0.0049,  0.0449,  0.0374,  0.0134,  0.0137],\n",
      "        [-0.0227,  0.1212, -0.0230, -0.0120, -0.0037],\n",
      "        [-0.0289, -0.0956, -0.0326,  0.1173, -0.0235],\n",
      "        [-0.0069,  0.0256, -0.0825,  0.0038,  0.0216],\n",
      "        [-0.0982,  0.0671, -0.0066, -0.0525,  0.1012],\n",
      "        [ 0.0444,  0.0788, -0.0055,  0.0061, -0.0322],\n",
      "        [-0.1183, -0.0195,  0.0142, -0.0097,  0.0562]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0070,  0.0580, -0.0502,  0.0406,  0.0634],\n",
      "        [-0.0312,  0.0043, -0.0640,  0.0435,  0.0135],\n",
      "        [ 0.0774,  0.0080, -0.0316,  0.0641,  0.0167],\n",
      "        [ 0.0491,  0.0555,  0.1165, -0.0587,  0.0892],\n",
      "        [ 0.0416, -0.0563,  0.0259, -0.0412, -0.0582],\n",
      "        [ 0.0142, -0.0278, -0.0776,  0.0190,  0.1041],\n",
      "        [ 0.0136, -0.0477, -0.0312,  0.0294, -0.0223],\n",
      "        [ 0.0291, -0.0011,  0.0688, -0.0727, -0.0381]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0180, grad_fn=<MinBackward1>), tensor(0.9682, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3266203999519348\n",
      "@sample 136: tensor([[ 0.0329,  0.0419,  0.0983,  0.0781, -0.0209],\n",
      "        [-0.0160,  0.0406,  0.0384, -0.0329, -0.0052],\n",
      "        [-0.0462,  0.1275, -0.0596, -0.0292,  0.0118],\n",
      "        [-0.0053, -0.0809,  0.0003, -0.0338,  0.0051],\n",
      "        [-0.0037,  0.0528, -0.0607,  0.0607,  0.0080],\n",
      "        [ 0.0518,  0.1080,  0.0079,  0.0012, -0.0697],\n",
      "        [-0.0262,  0.0228, -0.0218, -0.0351,  0.0319],\n",
      "        [ 0.0029, -0.0126, -0.0028,  0.0407, -0.0371]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0268,  0.0935,  0.0402, -0.0576, -0.0326],\n",
      "        [ 0.0140,  0.0410, -0.0037, -0.0248, -0.0422],\n",
      "        [-0.0472, -0.0060, -0.1471,  0.0981,  0.0379],\n",
      "        [-0.0360, -0.0130, -0.0189,  0.0285,  0.0076],\n",
      "        [ 0.0469,  0.0475,  0.0412, -0.0043,  0.0471],\n",
      "        [-0.0004,  0.0026,  0.0097,  0.0646,  0.0032],\n",
      "        [ 0.0039, -0.0636, -0.1035,  0.0529,  0.0025],\n",
      "        [ 0.0116,  0.0590,  0.1099, -0.0267,  0.0168]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0082, grad_fn=<MinBackward1>), tensor(0.9869, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.31425541639328003\n",
      "@sample 137: tensor([[-0.0116,  0.0352,  0.0560, -0.0830,  0.0018],\n",
      "        [-0.0048,  0.0310, -0.0220, -0.0088,  0.0168],\n",
      "        [ 0.0599, -0.0115, -0.0265,  0.0176, -0.0892],\n",
      "        [-0.0303,  0.0291, -0.0217,  0.0376,  0.0365],\n",
      "        [-0.0190,  0.0473, -0.0069, -0.0325, -0.0222],\n",
      "        [-0.0064,  0.0193, -0.0119,  0.0631,  0.0420],\n",
      "        [-0.0292,  0.0319,  0.0294, -0.0837, -0.0484],\n",
      "        [ 0.0364, -0.0492, -0.0407,  0.0077,  0.0593]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0878, -0.0892, -0.0074,  0.0354, -0.0108],\n",
      "        [ 0.0051, -0.0314, -0.1306,  0.0783,  0.0422],\n",
      "        [-0.0467, -0.0297, -0.0875, -0.0120,  0.0626],\n",
      "        [-0.0293,  0.0426, -0.1654,  0.1002,  0.0385],\n",
      "        [ 0.0061, -0.0186,  0.0454,  0.0372, -0.0026],\n",
      "        [ 0.0438,  0.0203, -0.0453, -0.0661, -0.0430],\n",
      "        [-0.0457, -0.0342, -0.1020, -0.0692, -0.0285],\n",
      "        [ 0.0349, -0.0495, -0.0495, -0.0338,  0.0714]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0184, grad_fn=<MinBackward1>), tensor(0.9862, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3086116909980774\n",
      "@sample 138: tensor([[-0.0856,  0.0346,  0.0030, -0.0218,  0.0328],\n",
      "        [ 0.0111,  0.0101,  0.0217,  0.0659, -0.0396],\n",
      "        [-0.0028, -0.0296, -0.0218, -0.0596, -0.0378],\n",
      "        [-0.0294,  0.0103,  0.0270, -0.0103,  0.0545],\n",
      "        [-0.0180,  0.0860,  0.0314, -0.0817,  0.0943],\n",
      "        [-0.0067, -0.0411,  0.0068,  0.0539, -0.0358],\n",
      "        [-0.0090,  0.0656,  0.0335, -0.0545,  0.0452],\n",
      "        [-0.0265, -0.0039,  0.0371,  0.0008, -0.0371]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0191,  0.0024, -0.0663, -0.0011,  0.0332],\n",
      "        [ 0.0443,  0.0830,  0.0154,  0.0202,  0.0564],\n",
      "        [ 0.0480, -0.0570,  0.0060, -0.0476, -0.0310],\n",
      "        [-0.0489, -0.0492, -0.1060, -0.0119, -0.0384],\n",
      "        [-0.0019, -0.0347, -0.0732,  0.0142, -0.0368],\n",
      "        [-0.0175, -0.0347,  0.0504, -0.0555, -0.0557],\n",
      "        [-0.0368, -0.0554, -0.0566,  0.0308,  0.0140],\n",
      "        [ 0.0144,  0.0375,  0.0330, -0.1525, -0.0480]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0143, grad_fn=<MinBackward1>), tensor(0.9736, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3003985285758972\n",
      "@sample 139: tensor([[-0.0135,  0.0295,  0.0526, -0.0062,  0.0575],\n",
      "        [ 0.0008, -0.0457,  0.0573, -0.0724, -0.0020],\n",
      "        [-0.0061,  0.0147,  0.0206, -0.0420,  0.0174],\n",
      "        [-0.0344, -0.0933,  0.0260,  0.0145, -0.0299],\n",
      "        [-0.0155, -0.0266,  0.0584, -0.0348, -0.0131],\n",
      "        [ 0.0281,  0.1210,  0.0520, -0.0817,  0.0424],\n",
      "        [ 0.0574, -0.1023,  0.0057,  0.1263, -0.0367],\n",
      "        [ 0.0033, -0.0779,  0.0060,  0.0145,  0.0484]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0100,  0.0032,  0.0321, -0.0157, -0.0007],\n",
      "        [-0.0473, -0.1121,  0.1346,  0.0120, -0.0288],\n",
      "        [-0.0811, -0.0701, -0.1725,  0.1418,  0.0370],\n",
      "        [-0.0258, -0.0056,  0.0460, -0.0109, -0.0283],\n",
      "        [-0.0458,  0.0362,  0.0706, -0.0382,  0.0405],\n",
      "        [-0.0130, -0.0442, -0.0073,  0.0998,  0.0097],\n",
      "        [ 0.0085,  0.0796, -0.0299,  0.0170, -0.0359],\n",
      "        [ 0.0219,  0.0281,  0.0197, -0.0563,  0.0050]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0156, grad_fn=<MinBackward1>), tensor(0.9842, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.32944217324256897\n",
      "@sample 140: tensor([[ 0.0018,  0.0092,  0.0814, -0.1301,  0.1294],\n",
      "        [-0.0545,  0.0223, -0.0535, -0.0429, -0.0160],\n",
      "        [-0.0350,  0.0173,  0.0413, -0.0846,  0.0655],\n",
      "        [-0.0187, -0.0591, -0.0289,  0.0382,  0.0549],\n",
      "        [-0.0457,  0.0590,  0.0843, -0.0497,  0.0454],\n",
      "        [-0.0279,  0.0886,  0.0457, -0.1206,  0.0851],\n",
      "        [-0.0022,  0.0318, -0.0307,  0.0024,  0.0067],\n",
      "        [-0.0004, -0.0421,  0.0556,  0.0332, -0.0140]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0312, -0.1102, -0.1050,  0.0133, -0.0924],\n",
      "        [ 0.0293, -0.0400,  0.0402, -0.0564,  0.0004],\n",
      "        [ 0.0264, -0.0901, -0.0805,  0.0483,  0.0208],\n",
      "        [ 0.1169,  0.0826,  0.0183,  0.0535,  0.0266],\n",
      "        [ 0.0500, -0.0537,  0.0835,  0.0083, -0.0641],\n",
      "        [-0.0197, -0.0503, -0.0787,  0.1177, -0.0357],\n",
      "        [ 0.0322,  0.0270,  0.0245, -0.1201, -0.0012],\n",
      "        [-0.0276, -0.0062,  0.0530,  0.0342, -0.0362]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0210, grad_fn=<MinBackward1>), tensor(0.9887, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.34418535232543945\n",
      "@sample 141: tensor([[-0.0502, -0.0067,  0.0543, -0.0332,  0.0286],\n",
      "        [ 0.0003, -0.0534,  0.0044, -0.0373, -0.0134],\n",
      "        [-0.1081,  0.0610,  0.0842, -0.1268,  0.0013],\n",
      "        [-0.0094, -0.0642,  0.0316, -0.1238,  0.0243],\n",
      "        [-0.0520, -0.0602,  0.0314, -0.0281,  0.0107],\n",
      "        [-0.0493,  0.0686,  0.0360, -0.0725, -0.0455],\n",
      "        [-0.0442,  0.0761,  0.0483, -0.0769,  0.0230],\n",
      "        [ 0.0273, -0.0076, -0.0353, -0.0072,  0.0332]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0294, -0.0351, -0.0813,  0.0742,  0.1116],\n",
      "        [-0.0117, -0.0717, -0.0126, -0.0257,  0.0215],\n",
      "        [-0.0917, -0.0622, -0.0726, -0.0672, -0.0616],\n",
      "        [ 0.0155, -0.0465,  0.0968, -0.0210, -0.0710],\n",
      "        [ 0.0002,  0.0006,  0.1126, -0.0451, -0.0124],\n",
      "        [ 0.0345,  0.0034, -0.0333,  0.0301,  0.0411],\n",
      "        [-0.0587, -0.0954, -0.0017,  0.0929,  0.0950],\n",
      "        [ 0.0233, -0.0221, -0.0169,  0.0078,  0.0483]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0139, grad_fn=<MinBackward1>), tensor(0.9854, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3343612849712372\n",
      "@sample 142: tensor([[-0.0280, -0.0163, -0.0047,  0.0182, -0.0087],\n",
      "        [-0.0179, -0.1339,  0.0142,  0.1308, -0.0154],\n",
      "        [ 0.0028, -0.0100,  0.0526, -0.0149, -0.0207],\n",
      "        [-0.0033,  0.0669,  0.0413,  0.0280,  0.0218],\n",
      "        [-0.0074, -0.0461,  0.0129,  0.0722, -0.0348],\n",
      "        [ 0.0011, -0.0742,  0.0191,  0.0304,  0.0572],\n",
      "        [-0.0489, -0.0166, -0.0598, -0.0912,  0.0286],\n",
      "        [-0.0855,  0.1144, -0.0183, -0.0528,  0.0336]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-1.0081e-02,  5.2883e-02,  3.5882e-02,  8.8432e-03,  6.7792e-02],\n",
      "        [-6.0691e-03,  7.3164e-02,  6.3226e-02, -2.7988e-02, -7.8475e-02],\n",
      "        [ 2.8165e-02, -2.1877e-02, -1.9571e-02, -2.0532e-02, -1.0079e-02],\n",
      "        [ 3.4187e-02,  1.1863e-02, -6.3614e-02,  3.9169e-02, -2.8736e-03],\n",
      "        [ 6.7250e-02,  1.7901e-01,  6.7978e-02, -5.6309e-02,  6.1028e-05],\n",
      "        [ 1.9573e-02,  6.0764e-02,  1.1484e-01, -8.5546e-02, -3.4567e-02],\n",
      "        [-1.0158e-01, -7.4351e-02, -8.1673e-02,  4.0868e-02, -1.3663e-02],\n",
      "        [-8.8697e-02, -1.6962e-02, -2.0623e-01,  8.2770e-02,  6.9768e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0064, grad_fn=<MinBackward1>), tensor(0.9779, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.34187987446784973\n",
      "@sample 143: tensor([[-0.0303,  0.0274,  0.0028, -0.0391,  0.0370],\n",
      "        [-0.0115,  0.0311, -0.0094, -0.0501,  0.0229],\n",
      "        [-0.0153, -0.0356,  0.0026,  0.0510, -0.0461],\n",
      "        [ 0.0077, -0.0258,  0.0319,  0.0019,  0.0644],\n",
      "        [ 0.0065,  0.0456, -0.0339, -0.0671, -0.0086],\n",
      "        [-0.0684,  0.0381, -0.0026, -0.0591, -0.0283],\n",
      "        [ 0.0371,  0.0815, -0.0299, -0.0285,  0.0006],\n",
      "        [-0.0217,  0.0547,  0.0220, -0.0489,  0.0241]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0626, -0.0257, -0.0437,  0.0230,  0.0078],\n",
      "        [-0.0673, -0.0196, -0.0824, -0.0173,  0.0258],\n",
      "        [-0.0040,  0.0372,  0.0454, -0.0664,  0.0814],\n",
      "        [ 0.0153,  0.0784,  0.0179,  0.1014,  0.0114],\n",
      "        [-0.0835, -0.0716, -0.0547, -0.0047,  0.0018],\n",
      "        [-0.0015,  0.0257,  0.0135, -0.0041, -0.0288],\n",
      "        [ 0.0314, -0.0357,  0.0094,  0.0244, -0.0109],\n",
      "        [-0.0061, -0.0790, -0.0095, -0.0248, -0.0039]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0154, grad_fn=<MinBackward1>), tensor(0.9909, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.28076785802841187\n",
      "@sample 144: tensor([[ 0.0352,  0.0211, -0.0310, -0.0324, -0.0591],\n",
      "        [ 0.0370,  0.0354,  0.0402, -0.0513,  0.0677],\n",
      "        [-0.0305, -0.0195, -0.0325, -0.0563,  0.0065],\n",
      "        [-0.0065,  0.0407,  0.0569, -0.0390,  0.0674],\n",
      "        [ 0.0055, -0.0361,  0.0021, -0.0154, -0.0057],\n",
      "        [ 0.0087, -0.0655,  0.0171,  0.0198, -0.0361],\n",
      "        [ 0.0140, -0.0269,  0.0141, -0.0250, -0.0140],\n",
      "        [ 0.0065,  0.0773,  0.0109, -0.1012,  0.0498]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0580, -0.0026, -0.0436,  0.0179,  0.1275],\n",
      "        [ 0.0138, -0.0021, -0.1228,  0.0996,  0.1382],\n",
      "        [-0.0858, -0.0381,  0.0537, -0.0157,  0.0814],\n",
      "        [-0.0079, -0.0345, -0.0746,  0.0431, -0.0304],\n",
      "        [-0.0356, -0.0287, -0.0093,  0.0210, -0.0716],\n",
      "        [-0.0227, -0.0215,  0.1091,  0.0102,  0.0783],\n",
      "        [-0.0495, -0.0110,  0.0268,  0.0368, -0.0260],\n",
      "        [-0.0791, -0.0755, -0.0490,  0.0725, -0.0337]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0256, grad_fn=<MinBackward1>), tensor(0.9840, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3082132935523987\n",
      "@sample 145: tensor([[ 9.9983e-03, -1.3236e-02, -4.9734e-02, -1.2646e-02, -1.1674e-02],\n",
      "        [-2.6738e-02,  8.8788e-02,  4.3608e-03, -6.7550e-02,  6.3547e-02],\n",
      "        [ 3.3212e-02,  3.8359e-02,  6.0875e-02, -1.5596e-02, -1.1001e-03],\n",
      "        [ 2.1870e-02,  4.5927e-02,  4.7221e-02, -1.2688e-02,  8.2680e-03],\n",
      "        [ 5.0991e-03, -1.0138e-02,  3.6269e-03, -3.2209e-02,  1.7850e-02],\n",
      "        [ 4.2658e-02,  2.1687e-02,  1.9155e-02,  9.0236e-03, -1.7341e-02],\n",
      "        [ 2.3907e-02,  5.5639e-02,  7.7337e-05, -1.4700e-02, -2.0576e-02],\n",
      "        [ 2.5348e-02,  5.7881e-03,  2.0645e-02, -6.3865e-02, -5.1475e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0263, -0.0409, -0.1154,  0.0425,  0.0776],\n",
      "        [ 0.0542, -0.0515,  0.0228, -0.0195, -0.0424],\n",
      "        [ 0.0569,  0.0339, -0.0018, -0.0652,  0.0140],\n",
      "        [ 0.0287, -0.0292, -0.0507,  0.0682,  0.0729],\n",
      "        [-0.0141,  0.0104, -0.0803,  0.0592,  0.0602],\n",
      "        [ 0.0027,  0.0858, -0.0024,  0.0113,  0.0129],\n",
      "        [ 0.0035, -0.0106,  0.0107,  0.0448, -0.0394],\n",
      "        [-0.0340, -0.0585, -0.0682,  0.0255, -0.0109]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0129, grad_fn=<MinBackward1>), tensor(0.9670, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.28934353590011597\n",
      "@sample 146: tensor([[ 0.0257, -0.1347, -0.0444,  0.0594, -0.0710],\n",
      "        [-0.0439, -0.0509, -0.0249, -0.0257, -0.0246],\n",
      "        [-0.0262, -0.0341,  0.0421,  0.0601,  0.0531],\n",
      "        [ 0.0024,  0.0009, -0.0908, -0.0230, -0.0126],\n",
      "        [ 0.0329, -0.0622, -0.0610,  0.0306,  0.0160],\n",
      "        [ 0.0065,  0.0251, -0.0197,  0.0146, -0.0073],\n",
      "        [ 0.0351,  0.0481,  0.0226,  0.0142, -0.0144],\n",
      "        [ 0.0241, -0.0847,  0.0122,  0.0964, -0.0072]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0357,  0.0160,  0.1403, -0.0073,  0.0737],\n",
      "        [-0.0867, -0.0135,  0.0282, -0.0545, -0.0510],\n",
      "        [ 0.0862,  0.0608, -0.0396,  0.0267,  0.1175],\n",
      "        [-0.0130, -0.0501,  0.1076, -0.0535, -0.0571],\n",
      "        [ 0.0179,  0.0495, -0.1168, -0.0383,  0.0088],\n",
      "        [ 0.0056,  0.0011,  0.0016, -0.0304,  0.0643],\n",
      "        [-0.0200, -0.0066, -0.0349,  0.0452, -0.0055],\n",
      "        [ 0.0286,  0.0160,  0.0167, -0.0062, -0.0505]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0196, grad_fn=<MinBackward1>), tensor(0.9727, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.30961981415748596\n",
      "@sample 147: tensor([[-0.0085,  0.0418,  0.0611, -0.0235,  0.0452],\n",
      "        [-0.0417, -0.0326, -0.0401, -0.0368, -0.0375],\n",
      "        [ 0.0242,  0.0146, -0.0041, -0.0087,  0.0121],\n",
      "        [-0.0226, -0.0331,  0.0131, -0.0165, -0.0282],\n",
      "        [ 0.0156,  0.0102, -0.0345, -0.0484,  0.0043],\n",
      "        [-0.0214,  0.0256, -0.0309, -0.0020,  0.0033],\n",
      "        [ 0.0309,  0.0522,  0.0220, -0.0590, -0.0345],\n",
      "        [ 0.0043,  0.0425, -0.0279, -0.0317, -0.0014]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0159,  0.0240, -0.0267,  0.0461,  0.0693],\n",
      "        [-0.0003,  0.0118,  0.0691, -0.0005, -0.0537],\n",
      "        [-0.0507, -0.0322, -0.0032,  0.0898,  0.0031],\n",
      "        [-0.0454,  0.0720, -0.0404, -0.0290,  0.0054],\n",
      "        [-0.0281, -0.0393, -0.0318,  0.0782, -0.0493],\n",
      "        [-0.0137, -0.0249,  0.0282, -0.0759, -0.0344],\n",
      "        [-0.0053, -0.0434, -0.1119,  0.0990,  0.0158],\n",
      "        [-0.0377, -0.0223, -0.1211,  0.0957,  0.0265]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0163, grad_fn=<MinBackward1>), tensor(0.9745, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2879054546356201\n",
      "@sample 148: tensor([[ 5.1400e-02, -3.4408e-02, -4.9278e-02,  3.4801e-02, -1.1485e-02],\n",
      "        [-8.6793e-02,  5.6759e-02,  8.5992e-02, -7.5758e-02,  7.7906e-02],\n",
      "        [-1.4729e-02, -8.5367e-02, -8.2277e-02,  4.2085e-02, -2.0185e-02],\n",
      "        [-2.0613e-02,  2.9726e-02, -3.5609e-02, -1.6160e-02,  2.3961e-02],\n",
      "        [-5.7463e-02,  7.0308e-02,  5.0897e-02, -6.1172e-02,  9.8744e-02],\n",
      "        [ 8.8189e-03,  1.3599e-02, -2.9321e-02,  9.9882e-02,  9.7170e-05],\n",
      "        [-1.0932e-02, -5.0296e-02, -4.6627e-02, -8.4229e-03, -6.1930e-02],\n",
      "        [ 7.6921e-02, -3.0301e-02, -4.8532e-03,  3.5256e-02, -4.6447e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0871,  0.0601, -0.0449,  0.0280,  0.0694],\n",
      "        [ 0.0405, -0.0444, -0.1721,  0.0247,  0.0624],\n",
      "        [ 0.0339,  0.0170,  0.1089, -0.0739, -0.0285],\n",
      "        [-0.0551, -0.0367, -0.0310,  0.0682,  0.0132],\n",
      "        [-0.0292, -0.0671, -0.0577,  0.0375,  0.0242],\n",
      "        [ 0.1014,  0.1663,  0.1789, -0.0739, -0.0071],\n",
      "        [-0.0338,  0.0330,  0.0093,  0.0133, -0.0069],\n",
      "        [ 0.0016,  0.0338,  0.0656,  0.0101, -0.0261]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0123, grad_fn=<MinBackward1>), tensor(0.9825, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3406166434288025\n",
      "@sample 149: tensor([[-0.0117,  0.0045, -0.0212,  0.0307, -0.0439],\n",
      "        [-0.0358, -0.0003,  0.0267,  0.0185,  0.0255],\n",
      "        [ 0.0389, -0.0019, -0.0294,  0.0534,  0.0103],\n",
      "        [-0.0148,  0.0401,  0.0375, -0.0605,  0.0441],\n",
      "        [ 0.0295, -0.0486, -0.0345,  0.0512,  0.0323],\n",
      "        [-0.0072,  0.0308, -0.0479, -0.0266,  0.0137],\n",
      "        [ 0.0146,  0.0327,  0.0096,  0.0297, -0.0484],\n",
      "        [ 0.0181, -0.0822, -0.0902,  0.0946,  0.0092]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0031, -0.0019, -0.0315, -0.0081,  0.0266],\n",
      "        [ 0.0385, -0.0420, -0.0172,  0.0412,  0.0135],\n",
      "        [ 0.0293,  0.0351, -0.0377,  0.0338,  0.0006],\n",
      "        [-0.0187, -0.0291, -0.1363,  0.0895, -0.0278],\n",
      "        [ 0.0259, -0.0225, -0.0655, -0.0286, -0.0231],\n",
      "        [ 0.0091, -0.0519, -0.0115,  0.0060, -0.0071],\n",
      "        [-0.0388,  0.0482, -0.1052,  0.0894,  0.0326],\n",
      "        [ 0.0796,  0.0775,  0.1228, -0.0507,  0.0050]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0115, grad_fn=<MinBackward1>), tensor(0.9807, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.29552823305130005\n",
      "@sample 150: tensor([[-0.0868,  0.1056,  0.0769, -0.1190,  0.0530],\n",
      "        [ 0.0092,  0.0184,  0.0653, -0.0510,  0.1720],\n",
      "        [ 0.0424, -0.1222, -0.0433,  0.1025, -0.1153],\n",
      "        [-0.0014, -0.0838,  0.0200,  0.0469, -0.0224],\n",
      "        [ 0.0155, -0.0170,  0.0229, -0.0117,  0.0598],\n",
      "        [ 0.0055, -0.0140,  0.0236,  0.0152, -0.0294],\n",
      "        [ 0.0330,  0.0019, -0.0152, -0.0468, -0.0344],\n",
      "        [ 0.0196, -0.0118, -0.0371, -0.0696,  0.0822]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 7.3640e-03, -4.2829e-02, -1.0162e-01,  7.0650e-02,  7.3437e-02],\n",
      "        [-1.5184e-03, -6.0260e-02, -1.1749e-01,  5.4992e-02, -1.3984e-02],\n",
      "        [ 2.8233e-02,  5.3829e-02,  2.4360e-01, -1.1047e-01, -4.5851e-05],\n",
      "        [-2.9692e-02, -4.8363e-02, -4.0669e-02,  2.0358e-02, -7.7010e-02],\n",
      "        [-2.9395e-02, -3.1505e-02, -7.0017e-02,  1.5618e-02, -9.4816e-03],\n",
      "        [ 2.5475e-02,  3.6170e-02, -1.0271e-04,  1.8010e-02, -2.0476e-02],\n",
      "        [ 2.6867e-03, -1.0142e-01, -7.0414e-03,  2.7701e-02, -6.2161e-02],\n",
      "        [ 8.3664e-03, -3.0127e-02,  1.7735e-02, -8.0390e-02,  1.8434e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0200, grad_fn=<MinBackward1>), tensor(0.9828, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3599892854690552\n",
      "@sample 151: tensor([[-0.0413, -0.0939,  0.0539,  0.0719,  0.0170],\n",
      "        [ 0.0144, -0.0009,  0.0244, -0.0127,  0.0145],\n",
      "        [-0.0547,  0.0535,  0.0489, -0.1400,  0.0307],\n",
      "        [-0.0149, -0.0676,  0.0197,  0.0324,  0.0444],\n",
      "        [-0.0312,  0.1074,  0.0672, -0.0642,  0.0447],\n",
      "        [ 0.0390, -0.0227,  0.0214,  0.0225,  0.0287],\n",
      "        [-0.0589,  0.0891,  0.0293, -0.0433,  0.0068],\n",
      "        [ 0.0277,  0.0835,  0.0424, -0.0183,  0.0306]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0411,  0.0568,  0.1201,  0.0004,  0.0026],\n",
      "        [ 0.0496,  0.0293, -0.0221,  0.0269, -0.0080],\n",
      "        [-0.0184, -0.0843, -0.0420,  0.0526,  0.0185],\n",
      "        [ 0.0532, -0.0340,  0.0370, -0.0228, -0.0516],\n",
      "        [-0.0090,  0.0055, -0.0218,  0.1164,  0.0093],\n",
      "        [ 0.0229, -0.0005, -0.1435,  0.0836,  0.0143],\n",
      "        [-0.0660,  0.0520, -0.0630, -0.0434, -0.0056],\n",
      "        [-0.0187, -0.0149, -0.0645,  0.0503, -0.0159]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0202, grad_fn=<MinBackward1>), tensor(0.9821, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3252209424972534\n",
      "@sample 152: tensor([[-6.8209e-02,  2.7915e-02,  4.5160e-02, -1.6457e-01,  6.8178e-02],\n",
      "        [ 2.7253e-02, -2.6103e-02, -2.2735e-02,  7.2828e-02, -3.6853e-02],\n",
      "        [ 6.1308e-02, -4.3065e-02,  4.4205e-02,  6.2183e-02, -4.2602e-02],\n",
      "        [ 1.1670e-02, -3.5300e-02, -8.6469e-02,  4.7325e-02, -3.8574e-02],\n",
      "        [ 3.4291e-02, -5.4687e-02, -6.9053e-02,  2.4955e-02, -7.0648e-02],\n",
      "        [ 4.9351e-02, -1.6598e-01,  8.7618e-03,  7.6981e-02, -1.5251e-02],\n",
      "        [ 5.8107e-02,  8.1880e-02, -4.1569e-03, -3.9207e-02,  5.1477e-02],\n",
      "        [-1.0822e-04, -3.8324e-02,  2.7329e-02,  1.3063e-01, -6.3502e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0811, -0.1128, -0.1403,  0.0710,  0.0429],\n",
      "        [-0.0307,  0.0087,  0.1014,  0.0210, -0.0367],\n",
      "        [ 0.0165,  0.0528, -0.0084,  0.0312,  0.0326],\n",
      "        [-0.0297,  0.0206, -0.0865,  0.0177,  0.0363],\n",
      "        [-0.0141, -0.0320,  0.0706, -0.0535,  0.0351],\n",
      "        [-0.0271, -0.0309, -0.0580, -0.0447, -0.0650],\n",
      "        [-0.0253, -0.0179, -0.1020,  0.0126, -0.0275],\n",
      "        [-0.0042,  0.1486,  0.0830, -0.0195, -0.0518]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0214, grad_fn=<MinBackward1>), tensor(0.9884, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.36022859811782837\n",
      "@sample 153: tensor([[ 0.0406, -0.0024, -0.0432,  0.0164, -0.0464],\n",
      "        [ 0.0634, -0.0118, -0.0047,  0.0042, -0.0668],\n",
      "        [-0.0344, -0.0432,  0.0222,  0.0464, -0.0269],\n",
      "        [ 0.0065,  0.0327,  0.0171, -0.0854,  0.0585],\n",
      "        [ 0.0428,  0.0225, -0.1165,  0.0631, -0.0520],\n",
      "        [ 0.0286,  0.0336, -0.0033, -0.0572, -0.0049],\n",
      "        [ 0.0426,  0.0414, -0.0575,  0.0294, -0.0624],\n",
      "        [ 0.0182,  0.0224, -0.0106, -0.0131,  0.0245]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0288, -0.0011,  0.0653, -0.0751, -0.0516],\n",
      "        [-0.0576, -0.0045,  0.0358, -0.0345, -0.0761],\n",
      "        [-0.0128,  0.0304,  0.0832,  0.0639,  0.0355],\n",
      "        [-0.0223, -0.0513, -0.1428,  0.0584, -0.0100],\n",
      "        [-0.0076,  0.0278, -0.0362,  0.0271, -0.0187],\n",
      "        [-0.0524, -0.0331,  0.0083,  0.0417,  0.0279],\n",
      "        [ 0.0088,  0.0063,  0.0444, -0.0651, -0.0469],\n",
      "        [ 0.0014,  0.0169, -0.0670,  0.0370, -0.0341]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0154, grad_fn=<MinBackward1>), tensor(0.9674, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3024856448173523\n",
      "@sample 154: tensor([[ 0.0172, -0.0160, -0.0297,  0.0734, -0.0050],\n",
      "        [-0.0082,  0.0881, -0.0078, -0.1218,  0.0031],\n",
      "        [ 0.0271,  0.0330, -0.0128,  0.0192, -0.0191],\n",
      "        [-0.0117, -0.0473,  0.0441,  0.0842, -0.0400],\n",
      "        [-0.0019,  0.0558,  0.0440, -0.0174,  0.0551],\n",
      "        [-0.0135,  0.0714, -0.0023,  0.0047, -0.0041],\n",
      "        [ 0.0057,  0.0613,  0.0476, -0.0338,  0.0069],\n",
      "        [ 0.0138, -0.0482,  0.0930,  0.0466,  0.0008]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0101,  0.0344,  0.0498, -0.0563, -0.0519],\n",
      "        [-0.0869, -0.1095, -0.0601,  0.0987, -0.1165],\n",
      "        [-0.0840,  0.0003,  0.0850, -0.0428, -0.0763],\n",
      "        [ 0.0710,  0.0886,  0.1122,  0.0172, -0.0435],\n",
      "        [ 0.0734, -0.0256, -0.0981,  0.0585,  0.0024],\n",
      "        [ 0.0017,  0.0281,  0.0458, -0.0508, -0.0601],\n",
      "        [ 0.0102,  0.0206, -0.0286,  0.0193, -0.0454],\n",
      "        [-0.0742,  0.0372, -0.0610, -0.0342, -0.0153]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0138, grad_fn=<MinBackward1>), tensor(0.9554, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3158796429634094\n",
      "@sample 155: tensor([[-5.1393e-03, -5.4526e-03,  2.0161e-02,  2.0315e-02,  1.2192e-04],\n",
      "        [ 1.8916e-02, -7.3955e-03,  5.3259e-02, -2.6248e-02,  1.5372e-02],\n",
      "        [-2.5524e-02,  1.0032e-01,  2.8000e-02, -1.1238e-01,  4.7442e-02],\n",
      "        [ 1.4288e-02, -2.8831e-02, -1.5627e-02,  5.0004e-02,  6.3576e-03],\n",
      "        [-8.4023e-03,  3.3531e-02,  3.9018e-02,  4.4154e-02, -1.9819e-02],\n",
      "        [ 2.0914e-02, -1.3183e-01, -4.4150e-02,  1.1309e-01, -4.1188e-02],\n",
      "        [ 5.0567e-02,  3.0810e-03, -2.8649e-03,  3.7276e-02, -3.7824e-02],\n",
      "        [ 3.5078e-02, -1.3767e-02,  1.4304e-02,  7.3711e-02, -5.6393e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.1051, -0.0420, -0.0426,  0.0149, -0.0195],\n",
      "        [-0.0299, -0.0896, -0.1070,  0.0120,  0.0802],\n",
      "        [-0.0316, -0.0864, -0.0182,  0.0772,  0.0069],\n",
      "        [ 0.0063,  0.0592,  0.0344, -0.0351, -0.0118],\n",
      "        [-0.0140,  0.0278, -0.0365, -0.0471, -0.0180],\n",
      "        [ 0.0372,  0.0575, -0.0386,  0.0532, -0.0258],\n",
      "        [-0.0263,  0.0486,  0.1066, -0.1020, -0.0409],\n",
      "        [ 0.0353,  0.0675,  0.1032, -0.0223,  0.0156]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0067, grad_fn=<MinBackward1>), tensor(0.9875, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3066273331642151\n",
      "@sample 156: tensor([[-0.0315,  0.0951, -0.0067, -0.0453,  0.0564],\n",
      "        [ 0.0101, -0.0760,  0.0267,  0.0949,  0.0123],\n",
      "        [-0.0122,  0.0901,  0.0562, -0.0829,  0.0602],\n",
      "        [-0.0071, -0.0114,  0.0120, -0.0025, -0.0377],\n",
      "        [ 0.0098, -0.0609, -0.0099,  0.0591, -0.0321],\n",
      "        [-0.0255, -0.0307,  0.0970,  0.0383, -0.0288],\n",
      "        [ 0.0558,  0.0114, -0.0380, -0.0019, -0.0725],\n",
      "        [-0.0115,  0.0158,  0.0047,  0.0220, -0.0358]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0209, -0.0164, -0.1173,  0.0947,  0.0206],\n",
      "        [-0.0804,  0.0593, -0.0826,  0.0220,  0.0862],\n",
      "        [-0.0775, -0.0065, -0.1478,  0.0779, -0.0006],\n",
      "        [-0.0218, -0.0096, -0.0861, -0.0350, -0.0107],\n",
      "        [ 0.0031, -0.0022,  0.0213, -0.0132, -0.0432],\n",
      "        [-0.1095,  0.0052, -0.0539,  0.0741, -0.0355],\n",
      "        [-0.0085, -0.0283, -0.0306, -0.0083,  0.0388],\n",
      "        [-0.0306,  0.0605,  0.0772, -0.0574,  0.0161]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0227, grad_fn=<MinBackward1>), tensor(0.9734, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3112297058105469\n",
      "@sample 157: tensor([[ 0.0122,  0.0782, -0.0085, -0.0155,  0.0406],\n",
      "        [-0.0136,  0.0538,  0.0491, -0.0110, -0.0005],\n",
      "        [ 0.0093, -0.0958, -0.0345,  0.0682, -0.0296],\n",
      "        [ 0.0113, -0.0316,  0.0155,  0.0347, -0.0386],\n",
      "        [-0.0737,  0.0556,  0.0108, -0.0202, -0.0007],\n",
      "        [ 0.0072,  0.0156,  0.0324,  0.0420, -0.0768],\n",
      "        [ 0.0329, -0.1288, -0.0200,  0.0904, -0.1208],\n",
      "        [ 0.0492,  0.0043,  0.0457, -0.1760,  0.0112]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0137, -0.0266, -0.0829,  0.0532,  0.0232],\n",
      "        [-0.0486,  0.0031,  0.0058, -0.0432, -0.0545],\n",
      "        [ 0.0655,  0.0180,  0.0673, -0.0593, -0.0207],\n",
      "        [ 0.0161,  0.0109,  0.0202, -0.0521, -0.1088],\n",
      "        [-0.0927,  0.0237, -0.1489,  0.0825,  0.1516],\n",
      "        [ 0.0085,  0.0788,  0.0674, -0.0905, -0.0343],\n",
      "        [-0.0061,  0.0105,  0.1145, -0.1322, -0.0520],\n",
      "        [-0.1030, -0.1677, -0.0478,  0.0332, -0.0574]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0203, grad_fn=<MinBackward1>), tensor(0.9683, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3568190932273865\n",
      "@sample 158: tensor([[-0.0109,  0.0082, -0.0364,  0.0450,  0.0601],\n",
      "        [-0.0213, -0.0319,  0.0099,  0.0048,  0.0191],\n",
      "        [-0.0141,  0.0562, -0.0293, -0.0031, -0.0400],\n",
      "        [-0.0474,  0.0448,  0.0760, -0.0602,  0.0467],\n",
      "        [-0.0126, -0.0025, -0.0102,  0.0002, -0.0095],\n",
      "        [ 0.0360, -0.0088,  0.0101,  0.0002,  0.0192],\n",
      "        [ 0.0033, -0.1668, -0.0414,  0.1044, -0.0186],\n",
      "        [ 0.0337,  0.0669, -0.0327,  0.0019,  0.0338]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0485, -0.0181,  0.0020, -0.0520, -0.0055],\n",
      "        [ 0.0147,  0.0029,  0.0360,  0.0350, -0.0278],\n",
      "        [ 0.0113,  0.0071, -0.0438,  0.0215,  0.0083],\n",
      "        [-0.0173, -0.0333, -0.0891,  0.0324, -0.0147],\n",
      "        [-0.0368,  0.0472, -0.0046,  0.0049, -0.0386],\n",
      "        [ 0.0640,  0.0601,  0.1596, -0.0696,  0.0131],\n",
      "        [ 0.0862,  0.0367,  0.1213, -0.0670,  0.0024],\n",
      "        [ 0.0233,  0.0299, -0.0563, -0.0067, -0.0316]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0192, grad_fn=<MinBackward1>), tensor(0.9727, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.30085277557373047\n",
      "@sample 159: tensor([[ 0.0335,  0.0200, -0.0190,  0.0284, -0.0097],\n",
      "        [ 0.0140, -0.0229,  0.0242,  0.0374, -0.0169],\n",
      "        [ 0.0422,  0.0251,  0.0144, -0.0021,  0.0084],\n",
      "        [ 0.0055,  0.0656,  0.0300, -0.0682,  0.0459],\n",
      "        [-0.0068,  0.0690, -0.0579, -0.0142, -0.0223],\n",
      "        [-0.0169,  0.0131, -0.0462,  0.0388,  0.0073],\n",
      "        [ 0.0359,  0.0489,  0.0517, -0.0536,  0.0276],\n",
      "        [ 0.0084,  0.0848,  0.0869, -0.0595,  0.0678]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0363, -0.0028,  0.1046, -0.1000, -0.0212],\n",
      "        [-0.0443, -0.0338,  0.0417, -0.0674, -0.0135],\n",
      "        [-0.0125,  0.0097, -0.0089,  0.0255, -0.0208],\n",
      "        [-0.0309, -0.0249, -0.1152,  0.0161, -0.0123],\n",
      "        [-0.0375, -0.0182, -0.0310,  0.0338, -0.0217],\n",
      "        [ 0.0460,  0.0164, -0.0564,  0.0463,  0.0160],\n",
      "        [-0.0078,  0.0058, -0.1036,  0.0365, -0.0231],\n",
      "        [ 0.0856,  0.0185, -0.0065, -0.0083,  0.0140]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0151, grad_fn=<MinBackward1>), tensor(0.9793, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.28358256816864014\n",
      "@sample 160: tensor([[ 0.0218,  0.0412,  0.0165, -0.0005, -0.0164],\n",
      "        [ 0.0146, -0.0193,  0.0491,  0.0554,  0.0232],\n",
      "        [-0.0310,  0.0086,  0.0501, -0.0196,  0.0684],\n",
      "        [ 0.0131,  0.0175, -0.0642, -0.0268, -0.0002],\n",
      "        [ 0.0026, -0.0063, -0.0232,  0.0198, -0.0009],\n",
      "        [-0.0309,  0.0225,  0.0114, -0.0443,  0.0494],\n",
      "        [ 0.0333,  0.0410, -0.0294, -0.0561,  0.0398],\n",
      "        [-0.0241,  0.0420,  0.0531, -0.0237,  0.0612]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0558, -0.0121, -0.0654,  0.0559,  0.0194],\n",
      "        [-0.0531,  0.0196, -0.0901,  0.0091, -0.0883],\n",
      "        [-0.0266, -0.0186,  0.0289, -0.0085, -0.0184],\n",
      "        [-0.0089, -0.0378,  0.0580,  0.0117,  0.0022],\n",
      "        [-0.0066,  0.0198, -0.0069,  0.0289, -0.0236],\n",
      "        [ 0.0544, -0.0262,  0.0257,  0.0029, -0.0234],\n",
      "        [-0.0497, -0.0874, -0.1650,  0.1118,  0.0153],\n",
      "        [ 0.0225, -0.0332, -0.0101,  0.0233,  0.0520]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0099, grad_fn=<MinBackward1>), tensor(0.9747, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2815275192260742\n",
      "@sample 161: tensor([[-0.0205,  0.0246, -0.0227, -0.0271, -0.0072],\n",
      "        [ 0.0572, -0.0482, -0.0006,  0.0787, -0.0027],\n",
      "        [-0.0123,  0.0202, -0.0148,  0.0084,  0.0226],\n",
      "        [-0.0635,  0.0303, -0.0116, -0.0352,  0.0332],\n",
      "        [-0.0443,  0.0771, -0.0182, -0.0620,  0.0506],\n",
      "        [-0.0101, -0.0356, -0.0690, -0.0291,  0.0434],\n",
      "        [-0.0115, -0.0135,  0.0091, -0.0197, -0.0136],\n",
      "        [ 0.0413, -0.0045, -0.0523, -0.0216,  0.0326]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0010, -0.0385, -0.0532,  0.1041, -0.0400],\n",
      "        [ 0.0473,  0.0110,  0.0603,  0.0685, -0.0628],\n",
      "        [ 0.0328,  0.0239,  0.0139, -0.0095, -0.0173],\n",
      "        [-0.0082, -0.0171, -0.0074,  0.0309,  0.0047],\n",
      "        [-0.0119, -0.0771, -0.0689,  0.0497, -0.0184],\n",
      "        [ 0.0249,  0.0338,  0.0194,  0.0044,  0.0294],\n",
      "        [ 0.0128, -0.0241, -0.0287,  0.0242,  0.0401],\n",
      "        [ 0.0397,  0.0710,  0.0091, -0.0455, -0.0092]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0133, grad_fn=<MinBackward1>), tensor(0.9836, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2771337628364563\n",
      "@sample 162: tensor([[ 0.0214,  0.0731, -0.0119, -0.0846,  0.0987],\n",
      "        [-0.0007, -0.0258, -0.0361, -0.0204,  0.0603],\n",
      "        [-0.0425, -0.0229,  0.0365, -0.0392, -0.0167],\n",
      "        [-0.0394,  0.0305,  0.0010, -0.0160, -0.0046],\n",
      "        [ 0.0075, -0.0578, -0.0397,  0.0692,  0.0313],\n",
      "        [-0.0394, -0.0414,  0.0263, -0.0032,  0.0346],\n",
      "        [-0.0349, -0.0194, -0.0116,  0.0258,  0.0465],\n",
      "        [-0.0039,  0.0350, -0.0669, -0.0049, -0.0032]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 5.5359e-02, -8.1600e-02, -6.5731e-02,  6.2259e-02,  4.0127e-02],\n",
      "        [ 3.0621e-02, -5.4625e-02,  2.3300e-02, -4.0990e-02, -2.8180e-02],\n",
      "        [ 2.5330e-03, -7.6048e-05,  3.3125e-02, -4.4566e-02, -3.2596e-02],\n",
      "        [ 2.0695e-03, -8.7942e-02, -2.8292e-03,  5.0974e-04,  8.0516e-03],\n",
      "        [ 2.9592e-02,  1.3956e-02, -1.1243e-01,  5.9478e-02,  1.8899e-02],\n",
      "        [-1.4765e-02, -2.4099e-02,  5.2753e-03, -3.4335e-02, -4.4576e-02],\n",
      "        [-5.1513e-02, -1.7187e-03, -8.5918e-02,  8.3511e-02, -2.7317e-03],\n",
      "        [ 2.0774e-02, -9.9604e-03,  6.7375e-02, -3.0498e-02, -1.7719e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0072, grad_fn=<MinBackward1>), tensor(0.9718, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2890191078186035\n",
      "@sample 163: tensor([[-0.0305,  0.0620,  0.0527, -0.0035,  0.0291],\n",
      "        [ 0.0288, -0.0818, -0.0640,  0.1490, -0.0441],\n",
      "        [-0.0415,  0.0183,  0.0253, -0.0659, -0.0185],\n",
      "        [-0.0098,  0.0275,  0.0384, -0.0501,  0.0387],\n",
      "        [-0.0572, -0.0145,  0.0371, -0.0610, -0.0182],\n",
      "        [-0.0135, -0.0464, -0.0319,  0.0540, -0.0300],\n",
      "        [-0.0403, -0.0181, -0.0222,  0.0431, -0.0431],\n",
      "        [-0.0829, -0.0049, -0.0080, -0.0265,  0.0368]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0507, -0.0635, -0.0713,  0.0240,  0.0318],\n",
      "        [ 0.1422,  0.0676,  0.1230, -0.1074, -0.0378],\n",
      "        [-0.0942,  0.0186, -0.0319,  0.0322, -0.0017],\n",
      "        [-0.0309, -0.0569, -0.0310,  0.0189, -0.0032],\n",
      "        [ 0.0193, -0.0826,  0.0618, -0.0187, -0.0261],\n",
      "        [ 0.0562,  0.0446,  0.0718, -0.1190, -0.0509],\n",
      "        [-0.0953,  0.0285,  0.0423, -0.0106,  0.0564],\n",
      "        [ 0.0134, -0.0353, -0.0947, -0.0193, -0.0358]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0121, grad_fn=<MinBackward1>), tensor(0.9672, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.31973403692245483\n",
      "@sample 164: tensor([[-0.0068, -0.0461, -0.0091, -0.0189, -0.0079],\n",
      "        [ 0.0069,  0.0019, -0.0016, -0.0268,  0.0043],\n",
      "        [ 0.0163,  0.0782, -0.0265,  0.0241, -0.0309],\n",
      "        [-0.0141,  0.0047, -0.0243,  0.0008,  0.0064],\n",
      "        [ 0.0155, -0.0159,  0.0389, -0.0138,  0.0132],\n",
      "        [-0.0286, -0.0054,  0.0162, -0.0270,  0.0338],\n",
      "        [ 0.0248,  0.0357,  0.0020, -0.0769,  0.0149],\n",
      "        [-0.0075, -0.0499, -0.0084,  0.0279, -0.0692]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0236,  0.0025, -0.0582,  0.0296,  0.0462],\n",
      "        [ 0.0044,  0.0048, -0.0521,  0.0535,  0.0086],\n",
      "        [-0.0602, -0.0155,  0.0227,  0.0170, -0.0055],\n",
      "        [ 0.0505, -0.0030,  0.1102, -0.1158, -0.0773],\n",
      "        [ 0.0470, -0.0517,  0.0452, -0.0052, -0.0204],\n",
      "        [-0.0507,  0.0058, -0.0441,  0.0517,  0.0746],\n",
      "        [-0.0772, -0.0596, -0.0563,  0.0779, -0.0412],\n",
      "        [-0.0139, -0.0096,  0.0726, -0.0721, -0.0070]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0121, grad_fn=<MinBackward1>), tensor(0.9668, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2744685411453247\n",
      "@sample 165: tensor([[-0.0488, -0.0738,  0.0593,  0.0037, -0.0026],\n",
      "        [ 0.0051, -0.0521, -0.0482, -0.0497,  0.0717],\n",
      "        [-0.0215, -0.0091, -0.0176,  0.0017,  0.0006],\n",
      "        [-0.0078, -0.0499,  0.0145, -0.0630,  0.0829],\n",
      "        [ 0.0084, -0.0004, -0.0225, -0.0347,  0.0342],\n",
      "        [ 0.0453, -0.0085,  0.0423, -0.0574,  0.1152],\n",
      "        [-0.0488, -0.0043, -0.0043,  0.0326,  0.0183],\n",
      "        [-0.0323,  0.0179, -0.0265, -0.0581,  0.0738]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0555,  0.0362,  0.1132, -0.0319, -0.0471],\n",
      "        [ 0.0260, -0.0077,  0.1361, -0.1332,  0.0058],\n",
      "        [-0.0306, -0.0019,  0.0024,  0.0054,  0.0122],\n",
      "        [-0.0225,  0.0063, -0.0436,  0.0903,  0.0642],\n",
      "        [ 0.0005, -0.0318, -0.0265,  0.0310, -0.0567],\n",
      "        [-0.0124, -0.0083, -0.1339,  0.0607,  0.0597],\n",
      "        [ 0.0470,  0.0211,  0.0020, -0.0681, -0.1268],\n",
      "        [-0.0950, -0.0776, -0.0557,  0.0222, -0.0103]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0163, grad_fn=<MinBackward1>), tensor(0.9686, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3108636438846588\n",
      "@sample 166: tensor([[-0.0007, -0.0570,  0.0394, -0.0144,  0.0385],\n",
      "        [-0.0058,  0.0280, -0.0220,  0.0472,  0.0041],\n",
      "        [ 0.0537,  0.0304, -0.0098,  0.0141,  0.0459],\n",
      "        [ 0.0042, -0.0843, -0.0243, -0.0049, -0.0390],\n",
      "        [ 0.0597, -0.0021, -0.0256,  0.0496, -0.0881],\n",
      "        [-0.0357,  0.0478, -0.0081, -0.0849,  0.0479],\n",
      "        [-0.0162, -0.0067, -0.0212, -0.0199, -0.0150],\n",
      "        [-0.0128, -0.0111, -0.0604,  0.0228,  0.0437]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0227,  0.0398,  0.0462, -0.0778, -0.0704],\n",
      "        [ 0.0530,  0.0314,  0.0130, -0.0438,  0.0033],\n",
      "        [ 0.0615, -0.0093, -0.0019, -0.0236, -0.0064],\n",
      "        [-0.0198, -0.0018,  0.0358, -0.0108, -0.0503],\n",
      "        [-0.0110,  0.0129,  0.1162, -0.0524,  0.0365],\n",
      "        [-0.1105, -0.0531, -0.0670, -0.0824, -0.0423],\n",
      "        [ 0.0336, -0.0587,  0.0114, -0.0476, -0.0639],\n",
      "        [ 0.0495,  0.0208, -0.0895,  0.0211, -0.0200]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0199, grad_fn=<MinBackward1>), tensor(0.9779, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.28931349515914917\n",
      "@sample 167: tensor([[ 0.0786, -0.0513, -0.0671,  0.0105, -0.0382],\n",
      "        [ 0.0086, -0.0139, -0.0420,  0.0288, -0.0620],\n",
      "        [ 0.0053, -0.0239, -0.0096,  0.0083, -0.0053],\n",
      "        [-0.0477,  0.0393, -0.0602, -0.0099,  0.0567],\n",
      "        [ 0.0295,  0.0300, -0.0116,  0.0211, -0.0616],\n",
      "        [-0.0246,  0.0680, -0.0157, -0.0307,  0.0018],\n",
      "        [-0.0008,  0.0259,  0.0320, -0.0282, -0.0403],\n",
      "        [ 0.0082, -0.0250, -0.0489,  0.0021, -0.0010]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0301,  0.0013,  0.1153, -0.0915, -0.0059],\n",
      "        [-0.0083,  0.0396,  0.0224, -0.0530, -0.0101],\n",
      "        [ 0.0307, -0.0319, -0.0175, -0.0470, -0.0789],\n",
      "        [-0.0084, -0.0038, -0.0292, -0.0168, -0.0404],\n",
      "        [ 0.0190,  0.0476,  0.0250, -0.0092, -0.0266],\n",
      "        [-0.0087, -0.0074, -0.0196,  0.0594,  0.0502],\n",
      "        [-0.0450,  0.0093,  0.0505, -0.0033, -0.0113],\n",
      "        [-0.0030,  0.0121, -0.0299, -0.0045, -0.0182]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0145, grad_fn=<MinBackward1>), tensor(0.9769, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2707887291908264\n",
      "@sample 168: tensor([[ 0.0257,  0.1018, -0.0023, -0.0039,  0.0666],\n",
      "        [ 0.0027,  0.0278,  0.0077,  0.0120,  0.0403],\n",
      "        [-0.0410, -0.0561,  0.0563, -0.0281,  0.0062],\n",
      "        [ 0.0018,  0.0550,  0.0071,  0.0486, -0.0031],\n",
      "        [-0.0087, -0.0142,  0.0728,  0.0719, -0.0028],\n",
      "        [-0.0402, -0.0220, -0.0707, -0.0213,  0.0362],\n",
      "        [-0.0065,  0.0762,  0.0084, -0.0136,  0.0120],\n",
      "        [ 0.0137,  0.0345,  0.0361, -0.0757,  0.0275]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0426,  0.0009, -0.1475,  0.0722,  0.0106],\n",
      "        [ 0.0385,  0.0393, -0.0217,  0.0026, -0.0424],\n",
      "        [-0.0937, -0.0908, -0.0255,  0.0575,  0.0479],\n",
      "        [ 0.0441,  0.1029, -0.0339, -0.0098,  0.0317],\n",
      "        [ 0.0781,  0.1525,  0.0523, -0.0765, -0.0059],\n",
      "        [ 0.0157, -0.0356,  0.0488, -0.0300,  0.0075],\n",
      "        [ 0.0258,  0.0037, -0.0865,  0.0722, -0.0004],\n",
      "        [-0.0181, -0.0701, -0.0377,  0.0363, -0.0284]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0181, grad_fn=<MinBackward1>), tensor(0.9742, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.29757973551750183\n",
      "@sample 169: tensor([[-4.4679e-02,  3.9855e-02, -1.9388e-02,  8.2185e-02,  1.1296e-02],\n",
      "        [-2.8249e-02, -3.8433e-02, -3.7161e-04,  3.5461e-02, -1.7001e-02],\n",
      "        [ 1.5163e-02,  6.3022e-03,  4.2535e-02, -2.0242e-02,  2.6967e-02],\n",
      "        [ 2.7640e-02,  2.4078e-02, -6.4201e-03,  1.5747e-02, -3.2898e-03],\n",
      "        [-1.7612e-02, -2.8945e-02,  5.5410e-02,  4.1947e-05, -5.2584e-02],\n",
      "        [-3.3684e-02, -3.4213e-02, -1.7186e-02,  4.6429e-02, -6.9546e-02],\n",
      "        [ 1.1421e-02, -1.6307e-02,  1.3234e-02,  8.9078e-04,  2.4132e-02],\n",
      "        [-9.1377e-03, -1.6568e-02, -2.6043e-03, -8.8210e-02,  1.2973e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0308,  0.0149, -0.0603, -0.0405, -0.0296],\n",
      "        [-0.0152, -0.0079,  0.0957, -0.0432,  0.0654],\n",
      "        [-0.0625, -0.0138, -0.0725,  0.0312,  0.0184],\n",
      "        [-0.0017,  0.0006, -0.0045, -0.0716,  0.0159],\n",
      "        [ 0.0287,  0.0178, -0.0402,  0.0409,  0.0643],\n",
      "        [-0.0359,  0.0398,  0.0951, -0.0078,  0.0768],\n",
      "        [ 0.0123, -0.0275, -0.0481,  0.0083,  0.0670],\n",
      "        [ 0.0366, -0.0339,  0.0396, -0.0253, -0.0640]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0237, grad_fn=<MinBackward1>), tensor(0.9824, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2760690152645111\n",
      "@sample 170: tensor([[-0.1177,  0.0327, -0.0518, -0.0310,  0.0092],\n",
      "        [-0.0424, -0.0746,  0.0098, -0.0213,  0.0557],\n",
      "        [-0.0324,  0.0115, -0.0112, -0.0443,  0.0046],\n",
      "        [ 0.0289,  0.0138,  0.0439,  0.0615, -0.0132],\n",
      "        [-0.0256,  0.0048, -0.0391, -0.0062, -0.0396],\n",
      "        [-0.0106, -0.0606, -0.0184, -0.0012,  0.0194],\n",
      "        [ 0.0538, -0.1204, -0.0254,  0.1147, -0.0983],\n",
      "        [ 0.0288,  0.0288,  0.0766, -0.0304,  0.0236]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0549, -0.0559, -0.0017, -0.0421,  0.0362],\n",
      "        [ 0.0199,  0.0046, -0.0672, -0.0390,  0.0845],\n",
      "        [-0.0118, -0.0111, -0.0916,  0.0489,  0.0339],\n",
      "        [-0.0576,  0.0350, -0.0023,  0.0159,  0.0089],\n",
      "        [-0.0409,  0.0079, -0.0363, -0.0629,  0.0105],\n",
      "        [ 0.0914, -0.0778, -0.0104, -0.0100,  0.0150],\n",
      "        [ 0.0135,  0.0619,  0.0895, -0.1086, -0.0132],\n",
      "        [ 0.0110,  0.0729, -0.0304,  0.0476,  0.0664]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0188, grad_fn=<MinBackward1>), tensor(0.9780, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3087230324745178\n",
      "@sample 171: tensor([[-0.0417,  0.0455,  0.0081, -0.0849,  0.0299],\n",
      "        [-0.0016,  0.0157,  0.0104,  0.0070,  0.0960],\n",
      "        [-0.0205,  0.0354,  0.0024, -0.0007, -0.0096],\n",
      "        [-0.0266, -0.0175,  0.0269, -0.0103,  0.0560],\n",
      "        [-0.0225,  0.0093, -0.0571,  0.0112,  0.0034],\n",
      "        [ 0.0129,  0.0097, -0.0023, -0.0383,  0.0018],\n",
      "        [-0.0172,  0.0273, -0.0485, -0.0583,  0.0510],\n",
      "        [ 0.0224, -0.0344, -0.0323, -0.0047, -0.0643]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0255, -0.0826, -0.0697,  0.0690,  0.0818],\n",
      "        [ 0.0600,  0.0225, -0.1272,  0.0742,  0.1264],\n",
      "        [-0.0073, -0.0195, -0.0266, -0.0018, -0.0004],\n",
      "        [ 0.0368,  0.0284,  0.0421, -0.0362,  0.0562],\n",
      "        [ 0.0312,  0.0124,  0.0452, -0.0387,  0.0419],\n",
      "        [ 0.0224, -0.0235, -0.0233, -0.0042,  0.0084],\n",
      "        [-0.0239, -0.1152, -0.0751, -0.0527,  0.0320],\n",
      "        [-0.0394, -0.0467,  0.0502, -0.0352, -0.0132]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0115, grad_fn=<MinBackward1>), tensor(0.9638, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2922382950782776\n",
      "@sample 172: tensor([[ 0.0316,  0.0314, -0.0236, -0.0111,  0.0430],\n",
      "        [ 0.0169,  0.0008,  0.0888,  0.0280, -0.0337],\n",
      "        [-0.0623,  0.0143,  0.0331, -0.0888, -0.0132],\n",
      "        [-0.0298, -0.0417,  0.0517, -0.0998, -0.0438],\n",
      "        [-0.0032,  0.0517,  0.0241, -0.0891, -0.0102],\n",
      "        [-0.0283,  0.0082,  0.0192, -0.0642,  0.0303],\n",
      "        [-0.0575, -0.0179,  0.0346, -0.0292,  0.0754],\n",
      "        [ 0.0041,  0.0926, -0.0108, -0.0482,  0.0356]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0071, -0.0580, -0.0774,  0.0515,  0.1007],\n",
      "        [ 0.0207,  0.0767,  0.0008,  0.0265,  0.0851],\n",
      "        [-0.1063, -0.1463,  0.0047,  0.0394,  0.0189],\n",
      "        [-0.0409, -0.0219,  0.1211, -0.0519, -0.0222],\n",
      "        [-0.0615, -0.0338,  0.0022, -0.0274, -0.0219],\n",
      "        [-0.0331, -0.0520, -0.1154,  0.0544,  0.0613],\n",
      "        [-0.0979, -0.0757, -0.0053, -0.0405,  0.0314],\n",
      "        [-0.0166, -0.0513, -0.1033,  0.0957,  0.0586]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0055, grad_fn=<MinBackward1>), tensor(0.9702, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3205963373184204\n",
      "@sample 173: tensor([[ 0.0007,  0.0860, -0.0706, -0.1229,  0.0711],\n",
      "        [ 0.0042,  0.0183,  0.0176,  0.0230, -0.0132],\n",
      "        [-0.0194,  0.0035, -0.0408,  0.0604, -0.0279],\n",
      "        [-0.0387, -0.0175,  0.0400, -0.0202, -0.0207],\n",
      "        [ 0.0094, -0.0558, -0.0738,  0.0620, -0.0438],\n",
      "        [-0.0660,  0.0274, -0.0010, -0.0621,  0.0245],\n",
      "        [-0.0036,  0.0392,  0.0429, -0.0428,  0.0144],\n",
      "        [ 0.0393,  0.0344,  0.0357, -0.0642, -0.0311]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0748, -0.1050, -0.0606, -0.0187, -0.0620],\n",
      "        [ 0.0400,  0.0834,  0.0173, -0.0408,  0.0131],\n",
      "        [ 0.0586,  0.1153, -0.0070,  0.0001,  0.0471],\n",
      "        [-0.0281,  0.0163, -0.0586,  0.0402,  0.0424],\n",
      "        [ 0.0061, -0.0248,  0.0407, -0.0450, -0.0293],\n",
      "        [ 0.0249, -0.0637, -0.0264, -0.0132, -0.0456],\n",
      "        [ 0.0082, -0.0550, -0.0172,  0.0557,  0.0080],\n",
      "        [-0.1305, -0.0659, -0.1067,  0.0900,  0.0297]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0152, grad_fn=<MinBackward1>), tensor(0.9758, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3068062663078308\n",
      "@sample 174: tensor([[ 0.0078, -0.0461, -0.0203,  0.0257, -0.0426],\n",
      "        [ 0.0350, -0.0366, -0.0419,  0.0369, -0.0430],\n",
      "        [ 0.0346,  0.1078, -0.0104, -0.0315, -0.0374],\n",
      "        [-0.0066,  0.0339,  0.0413,  0.0104, -0.0054],\n",
      "        [-0.0285, -0.0179,  0.0476,  0.0046, -0.0679],\n",
      "        [-0.0055,  0.0544,  0.0060, -0.0011, -0.0069],\n",
      "        [ 0.0001,  0.0758,  0.0522, -0.0410,  0.0587],\n",
      "        [ 0.0007, -0.0233, -0.0544,  0.0071, -0.0192]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0230, -0.0025, -0.0002, -0.0283, -0.0369],\n",
      "        [ 0.0061, -0.0140,  0.0303, -0.0318,  0.0200],\n",
      "        [ 0.0201, -0.0221, -0.0663,  0.0407,  0.0368],\n",
      "        [ 0.0289, -0.0063, -0.0572,  0.0265,  0.0194],\n",
      "        [-0.0412,  0.0015,  0.0132, -0.0143, -0.0738],\n",
      "        [-0.0144,  0.0031, -0.0624,  0.0379,  0.0313],\n",
      "        [-0.0286, -0.0125, -0.0954,  0.0897,  0.0479],\n",
      "        [-0.0139, -0.0293,  0.0226, -0.0335,  0.0368]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0072, grad_fn=<MinBackward1>), tensor(0.9893, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2674378454685211\n",
      "@sample 175: tensor([[-0.0202,  0.0545,  0.0174, -0.1290,  0.0349],\n",
      "        [-0.0096,  0.0520,  0.0081,  0.0089,  0.0467],\n",
      "        [ 0.0228,  0.0649,  0.0252, -0.0572,  0.0193],\n",
      "        [ 0.0024, -0.0236, -0.0431,  0.1066, -0.0581],\n",
      "        [-0.0449, -0.0029, -0.0127, -0.1147,  0.0426],\n",
      "        [-0.0665, -0.0691,  0.0612, -0.0028,  0.0425],\n",
      "        [-0.0308,  0.0710,  0.0174, -0.0641,  0.0352],\n",
      "        [-0.0500,  0.0052,  0.0057, -0.0948,  0.0079]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0984, -0.0813, -0.1604,  0.0485,  0.0336],\n",
      "        [-0.0414,  0.0378, -0.0143, -0.0625, -0.0157],\n",
      "        [-0.1026, -0.0208, -0.1099,  0.0457, -0.0612],\n",
      "        [ 0.0308,  0.0322,  0.0462, -0.0708, -0.0327],\n",
      "        [-0.0633, -0.1335, -0.0171,  0.0523,  0.0065],\n",
      "        [-0.1052, -0.0281, -0.0384,  0.0152,  0.0296],\n",
      "        [-0.0613, -0.0522, -0.0877,  0.0917, -0.0234],\n",
      "        [-0.0771, -0.0553,  0.0445,  0.0168, -0.0631]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0168, grad_fn=<MinBackward1>), tensor(0.9707, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3227476477622986\n",
      "@sample 176: tensor([[ 0.0075, -0.0095,  0.0105, -0.0427, -0.0120],\n",
      "        [ 0.0056,  0.0456,  0.0277, -0.0070,  0.0418],\n",
      "        [-0.0103, -0.0526,  0.0627,  0.0714, -0.0316],\n",
      "        [ 0.0486,  0.0305,  0.0355,  0.0786, -0.0573],\n",
      "        [-0.0172,  0.0041, -0.0398, -0.1168,  0.0249],\n",
      "        [-0.0128,  0.0692,  0.0235, -0.0096,  0.0496],\n",
      "        [-0.0022, -0.0349, -0.0533,  0.0053,  0.0095],\n",
      "        [ 0.0659, -0.0157, -0.0087,  0.0431, -0.0046]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0286, -0.0115, -0.0276,  0.0424,  0.0037],\n",
      "        [-0.0079,  0.0195, -0.0458,  0.0370,  0.0281],\n",
      "        [-0.0326,  0.0151,  0.0652, -0.0478, -0.0326],\n",
      "        [ 0.0239,  0.0296, -0.0945,  0.0113,  0.0430],\n",
      "        [ 0.0167, -0.0762, -0.0439,  0.0065,  0.0612],\n",
      "        [-0.0079,  0.0297, -0.1293, -0.0410,  0.0018],\n",
      "        [ 0.0729, -0.0327, -0.0096, -0.0396,  0.0120],\n",
      "        [-0.0025, -0.0053, -0.0560,  0.0035,  0.0070]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0132, grad_fn=<MinBackward1>), tensor(0.9740, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2819335460662842\n",
      "@sample 177: tensor([[ 0.0594,  0.0300,  0.0757,  0.0504,  0.0725],\n",
      "        [ 0.0581,  0.0179, -0.0073, -0.0387, -0.0026],\n",
      "        [-0.0242, -0.1087, -0.0222,  0.0985, -0.0371],\n",
      "        [ 0.0315, -0.0117,  0.0184,  0.0237,  0.0182],\n",
      "        [ 0.0175,  0.0504,  0.0021, -0.0327,  0.0225],\n",
      "        [ 0.0166,  0.0010, -0.0290,  0.0290,  0.0499],\n",
      "        [-0.0017,  0.0555,  0.0505, -0.0179, -0.0455],\n",
      "        [-0.0133,  0.0698, -0.0155, -0.1080,  0.0224]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0147,  0.0576, -0.1383,  0.0263, -0.0230],\n",
      "        [-0.0142,  0.0368, -0.0628,  0.0445,  0.0315],\n",
      "        [ 0.0326,  0.0633,  0.0625, -0.0584,  0.0043],\n",
      "        [-0.0187,  0.0558, -0.0609,  0.0170, -0.0073],\n",
      "        [ 0.0124, -0.0741, -0.0503,  0.0514,  0.0570],\n",
      "        [ 0.0844, -0.0004, -0.0992,  0.0589, -0.0093],\n",
      "        [-0.0092,  0.0022,  0.0103,  0.0500,  0.0019],\n",
      "        [-0.0819, -0.0859, -0.1577,  0.1300,  0.0375]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0121, grad_fn=<MinBackward1>), tensor(0.9787, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3052651286125183\n",
      "@sample 178: tensor([[-0.0028, -0.0112,  0.0567, -0.0113,  0.0084],\n",
      "        [-0.0268,  0.0564, -0.0159, -0.0266,  0.0133],\n",
      "        [-0.0256,  0.0443,  0.0217, -0.0077,  0.0459],\n",
      "        [ 0.0163,  0.0542, -0.0047, -0.1054, -0.0026],\n",
      "        [-0.0799,  0.0347,  0.0660, -0.0189,  0.0501],\n",
      "        [ 0.0299, -0.0283, -0.0014, -0.0409,  0.0150],\n",
      "        [ 0.0016, -0.0580, -0.0307,  0.0208,  0.0197],\n",
      "        [ 0.0020, -0.0058, -0.0414,  0.0597, -0.0974]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0399,  0.0620, -0.0094,  0.0845,  0.0686],\n",
      "        [ 0.0111,  0.0181, -0.0765, -0.0509, -0.0221],\n",
      "        [ 0.0321,  0.0135, -0.0846,  0.0063, -0.0535],\n",
      "        [-0.0223, -0.0977, -0.0938,  0.0575,  0.0188],\n",
      "        [ 0.0642, -0.0061,  0.0911, -0.0224, -0.0486],\n",
      "        [-0.0507, -0.0707, -0.1077,  0.0179, -0.0675],\n",
      "        [-0.0242, -0.0022, -0.0574,  0.0502,  0.0292],\n",
      "        [-0.0039,  0.0447,  0.0828, -0.0642,  0.0312]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0133, grad_fn=<MinBackward1>), tensor(0.9858, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2843947112560272\n",
      "@sample 179: tensor([[ 0.0621,  0.0627, -0.0432, -0.0503, -0.0226],\n",
      "        [ 0.0907, -0.0673, -0.0590,  0.1613, -0.0848],\n",
      "        [ 0.0238,  0.0464, -0.0380, -0.0224, -0.0112],\n",
      "        [ 0.0453, -0.0954,  0.0028,  0.0955, -0.0327],\n",
      "        [-0.0070,  0.0952,  0.0615, -0.0401,  0.0451],\n",
      "        [-0.0145,  0.0042,  0.0289, -0.0694,  0.0163],\n",
      "        [ 0.0084, -0.0408, -0.0405, -0.0210,  0.0066],\n",
      "        [ 0.0415, -0.0061, -0.0383, -0.0200, -0.0437]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0413, -0.0204, -0.0395,  0.0339, -0.0066],\n",
      "        [ 0.0145,  0.1467, -0.0785, -0.0030, -0.0086],\n",
      "        [-0.0154, -0.0614,  0.0518,  0.0317, -0.0427],\n",
      "        [ 0.0718,  0.1094,  0.1187, -0.0775,  0.0079],\n",
      "        [-0.0346, -0.0065, -0.0804,  0.0795,  0.0253],\n",
      "        [-0.1050, -0.0138,  0.0040,  0.0411,  0.0573],\n",
      "        [-0.0275, -0.0186, -0.0628, -0.0344, -0.0431],\n",
      "        [ 0.0218,  0.0005, -0.0019,  0.0913,  0.0632]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0118, grad_fn=<MinBackward1>), tensor(0.9755, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.32094427943229675\n",
      "@sample 180: tensor([[ 0.0477, -0.0390,  0.0288,  0.0544,  0.0556],\n",
      "        [-0.0024, -0.1196, -0.0447,  0.1275, -0.0759],\n",
      "        [-0.0616,  0.0125, -0.0242, -0.1151,  0.1118],\n",
      "        [ 0.0176,  0.0295, -0.0116, -0.0719,  0.0192],\n",
      "        [ 0.0414, -0.1052, -0.0199,  0.0480, -0.0888],\n",
      "        [ 0.0483,  0.0494, -0.0409,  0.0346,  0.0316],\n",
      "        [-0.0266,  0.0949,  0.0654, -0.1050,  0.0122],\n",
      "        [ 0.0484, -0.0500,  0.0468,  0.0211,  0.0252]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0223,  0.0309, -0.1181, -0.0200, -0.0073],\n",
      "        [ 0.0443,  0.0173,  0.1141, -0.0983, -0.0449],\n",
      "        [-0.0451, -0.1472, -0.2053,  0.1780,  0.0453],\n",
      "        [-0.0027, -0.0976,  0.0619, -0.0521, -0.0030],\n",
      "        [ 0.0044,  0.0632,  0.2269, -0.1144, -0.0123],\n",
      "        [ 0.0814,  0.0577, -0.0770, -0.0216, -0.0541],\n",
      "        [-0.0504, -0.0438, -0.0083,  0.0582, -0.0317],\n",
      "        [ 0.0410,  0.0214, -0.0117,  0.0317,  0.0033]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0173, grad_fn=<MinBackward1>), tensor(0.9711, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.37175673246383667\n",
      "@sample 181: tensor([[-0.0347,  0.1045,  0.0400, -0.0654,  0.0501],\n",
      "        [ 0.0294, -0.0813,  0.0570,  0.0093, -0.0871],\n",
      "        [-0.0517,  0.0033, -0.0080, -0.0523,  0.0501],\n",
      "        [-0.0221,  0.0195,  0.0546, -0.0419, -0.0057],\n",
      "        [ 0.0181,  0.0128,  0.0329, -0.0347, -0.0319],\n",
      "        [-0.0111, -0.0418,  0.0287, -0.0024,  0.0464],\n",
      "        [-0.0157, -0.0202, -0.0607,  0.0739,  0.0021],\n",
      "        [-0.0108,  0.0122,  0.0410, -0.0639,  0.0076]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-3.9753e-02, -6.3785e-02,  3.0103e-03, -1.4565e-02, -4.4564e-02],\n",
      "        [-3.6613e-02, -2.5970e-02,  1.2379e-01, -8.5062e-02,  1.0004e-02],\n",
      "        [ 2.1519e-02, -5.3097e-02,  2.4691e-02,  2.2333e-02,  6.6471e-03],\n",
      "        [-1.0193e-02,  1.6531e-02, -2.1594e-02,  4.4166e-02,  4.3256e-04],\n",
      "        [-1.3302e-03,  8.7871e-02,  1.0626e-01, -7.1902e-02, -8.3187e-02],\n",
      "        [-2.2553e-02, -6.4513e-03, -4.0000e-02,  1.1750e-02,  2.6278e-05],\n",
      "        [ 1.0139e-01,  2.6712e-02,  3.0665e-02, -1.1000e-01,  3.2379e-03],\n",
      "        [-4.6465e-02, -1.0497e-02, -4.8307e-02,  3.5096e-02,  2.6459e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0179, grad_fn=<MinBackward1>), tensor(0.9892, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.29197272658348083\n",
      "@sample 182: tensor([[ 0.0042,  0.0437,  0.0169,  0.0052,  0.0426],\n",
      "        [ 0.0491, -0.0378,  0.0981, -0.0331,  0.0421],\n",
      "        [-0.0195,  0.0533, -0.0071, -0.0399, -0.0043],\n",
      "        [-0.0311, -0.1055, -0.0182,  0.0370, -0.0120],\n",
      "        [-0.0396,  0.0405,  0.0360, -0.0047,  0.0289],\n",
      "        [-0.0089,  0.0193,  0.0281,  0.0043, -0.0093],\n",
      "        [ 0.0430,  0.0606, -0.0801, -0.1047,  0.0394],\n",
      "        [-0.0310,  0.0114,  0.0367,  0.0022,  0.0631]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0764,  0.0073,  0.0040, -0.0303, -0.0250],\n",
      "        [-0.0143,  0.0170, -0.1036,  0.0019,  0.0105],\n",
      "        [ 0.0139, -0.0098, -0.0200, -0.0131, -0.0475],\n",
      "        [ 0.0333,  0.0656,  0.0488, -0.0133, -0.0321],\n",
      "        [ 0.0582, -0.0242, -0.0418, -0.0176, -0.0303],\n",
      "        [ 0.0089, -0.0397, -0.0460,  0.0646,  0.0167],\n",
      "        [-0.0241, -0.1437, -0.0960,  0.0065, -0.0303],\n",
      "        [-0.0020,  0.0245, -0.0127, -0.0491, -0.0266]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0123, grad_fn=<MinBackward1>), tensor(0.9812, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2874518632888794\n",
      "@sample 183: tensor([[-0.0138, -0.0358, -0.0639,  0.0656,  0.0023],\n",
      "        [-0.0138, -0.0214,  0.0425, -0.0733, -0.0077],\n",
      "        [ 0.0198, -0.0288, -0.0263,  0.0295, -0.0014],\n",
      "        [ 0.0056,  0.0090, -0.0221, -0.0252,  0.0513],\n",
      "        [-0.0137, -0.0808, -0.0286,  0.0437, -0.0397],\n",
      "        [ 0.0357,  0.0258,  0.0279,  0.0252, -0.0225],\n",
      "        [ 0.0117,  0.0578,  0.0008, -0.0397,  0.0961],\n",
      "        [ 0.0145,  0.0047, -0.0745,  0.0485,  0.0085]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0284,  0.0641, -0.0141, -0.0130, -0.0020],\n",
      "        [-0.1033, -0.0642,  0.0360, -0.0459, -0.0112],\n",
      "        [-0.0062, -0.0833, -0.0224,  0.0241,  0.0118],\n",
      "        [-0.0464, -0.0821, -0.0841,  0.0716, -0.0297],\n",
      "        [-0.0142, -0.0102,  0.0375, -0.0356,  0.0021],\n",
      "        [-0.0078,  0.0855,  0.0490,  0.0049, -0.0131],\n",
      "        [ 0.0073,  0.0181, -0.0968,  0.1174,  0.0400],\n",
      "        [ 0.0607,  0.0013, -0.0620, -0.0191, -0.0350]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0086, grad_fn=<MinBackward1>), tensor(0.9698, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.28559598326683044\n",
      "@sample 184: tensor([[-0.0190,  0.0236,  0.0294, -0.0757,  0.0153],\n",
      "        [-0.0500,  0.0699,  0.0160, -0.1405,  0.0995],\n",
      "        [ 0.0224,  0.0008,  0.0158, -0.0233,  0.0148],\n",
      "        [-0.0396, -0.1013,  0.1228, -0.0099,  0.0769],\n",
      "        [-0.0106,  0.0290,  0.0293, -0.0855,  0.0434],\n",
      "        [ 0.0023,  0.0354, -0.0124,  0.0162,  0.0004],\n",
      "        [ 0.0170, -0.0384, -0.0160,  0.0674, -0.0619],\n",
      "        [ 0.0329, -0.0381,  0.0066,  0.0064, -0.0041]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0542, -0.0334, -0.0987,  0.0705,  0.0248],\n",
      "        [ 0.0092, -0.1655, -0.1005,  0.0572,  0.0036],\n",
      "        [-0.0083,  0.0168, -0.0293,  0.0210, -0.0347],\n",
      "        [-0.0319, -0.0534, -0.0342,  0.0390, -0.0397],\n",
      "        [-0.0730, -0.0205, -0.0090, -0.0070, -0.0683],\n",
      "        [ 0.0516,  0.0524,  0.0226, -0.0406, -0.0188],\n",
      "        [ 0.0282,  0.0840,  0.0810, -0.0827, -0.0213],\n",
      "        [ 0.0121,  0.0091,  0.0267,  0.0071,  0.0148]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0110, grad_fn=<MinBackward1>), tensor(0.9784, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.30802828073501587\n",
      "@sample 185: tensor([[ 0.0009,  0.0685,  0.0265, -0.0476,  0.0555],\n",
      "        [-0.0351,  0.0093,  0.0255, -0.0639,  0.0382],\n",
      "        [ 0.0102, -0.0086,  0.0127, -0.0055,  0.0043],\n",
      "        [ 0.0270,  0.0635,  0.0406, -0.0608,  0.0175],\n",
      "        [-0.0057, -0.0016,  0.0145, -0.0135,  0.0212],\n",
      "        [-0.0111, -0.0755,  0.0066,  0.1030, -0.0461],\n",
      "        [ 0.0334,  0.0548,  0.0519, -0.1266,  0.0586],\n",
      "        [ 0.0048, -0.0709,  0.0016,  0.0381, -0.0146]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0238, -0.0529, -0.0800,  0.0489,  0.0043],\n",
      "        [-0.0074, -0.0433, -0.1068,  0.0511,  0.0369],\n",
      "        [ 0.0184,  0.0637,  0.0962, -0.0533, -0.0639],\n",
      "        [ 0.0015, -0.0147, -0.0225,  0.0338, -0.0366],\n",
      "        [-0.0400, -0.0192, -0.0405,  0.0086, -0.0575],\n",
      "        [ 0.0664,  0.0258,  0.1308, -0.0783,  0.0024],\n",
      "        [-0.1054, -0.1280, -0.1267,  0.0474, -0.0814],\n",
      "        [ 0.0381, -0.0568,  0.0469, -0.0092, -0.0420]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0146, grad_fn=<MinBackward1>), tensor(0.9820, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.30976009368896484\n",
      "@sample 186: tensor([[ 0.0018, -0.0014, -0.0363,  0.0093,  0.0009],\n",
      "        [ 0.0257, -0.0450,  0.0170,  0.0147, -0.0478],\n",
      "        [ 0.0284, -0.0896, -0.0558,  0.0121, -0.0287],\n",
      "        [ 0.0406, -0.0323, -0.0306,  0.0063,  0.0081],\n",
      "        [-0.0240,  0.1096,  0.0021, -0.0148, -0.0252],\n",
      "        [-0.0201,  0.0846,  0.0233, -0.1175,  0.0390],\n",
      "        [ 0.0076,  0.0520, -0.0440, -0.0053, -0.0196],\n",
      "        [-0.0070,  0.0808,  0.0368, -0.0520,  0.0172]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0396,  0.0028, -0.0342, -0.0608, -0.0163],\n",
      "        [-0.0034,  0.0515,  0.0477, -0.0191, -0.0371],\n",
      "        [-0.0625,  0.0041, -0.0340,  0.0255,  0.0019],\n",
      "        [ 0.0115, -0.0220, -0.0315, -0.0006,  0.0259],\n",
      "        [-0.0501,  0.0506, -0.0319, -0.0008, -0.0088],\n",
      "        [-0.1740, -0.0495, -0.0322,  0.0579, -0.0223],\n",
      "        [ 0.0573, -0.0204, -0.0243,  0.0574,  0.0604],\n",
      "        [-0.0554, -0.0215, -0.0387,  0.0644, -0.0233]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0203, grad_fn=<MinBackward1>), tensor(0.9751, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2883005142211914\n",
      "@sample 187: tensor([[-0.0544,  0.0894,  0.0002, -0.0966,  0.0559],\n",
      "        [ 0.0197,  0.0224, -0.0546,  0.0566,  0.0123],\n",
      "        [ 0.0153,  0.0749,  0.0252, -0.0253, -0.0234],\n",
      "        [ 0.0435,  0.0979,  0.0385, -0.0310, -0.0111],\n",
      "        [ 0.0174,  0.0167, -0.0418,  0.0657, -0.0347],\n",
      "        [-0.0188,  0.0699, -0.0128, -0.0340,  0.0128],\n",
      "        [ 0.0621,  0.0184,  0.0193,  0.0019, -0.0432],\n",
      "        [ 0.0558,  0.0198, -0.0028,  0.1111, -0.0812]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-3.6131e-02, -9.7708e-02, -1.5178e-01,  5.0567e-02, -7.6377e-02],\n",
      "        [ 4.8456e-02,  3.4610e-03,  1.3059e-03, -6.0451e-02, -1.6702e-02],\n",
      "        [-5.1091e-02, -7.7417e-02, -3.6920e-02,  5.9672e-03, -4.9871e-02],\n",
      "        [-2.6935e-02, -5.4939e-03,  5.3741e-02,  1.1301e-01, -3.9702e-02],\n",
      "        [-4.4891e-02,  3.8069e-02, -1.0701e-02, -3.6869e-02,  8.8215e-03],\n",
      "        [ 5.4858e-02,  1.2211e-02, -8.4506e-04,  2.9319e-02,  6.9813e-03],\n",
      "        [-6.5274e-02, -1.6539e-02,  2.1530e-02,  3.1606e-02, -1.5080e-03],\n",
      "        [-2.3133e-02,  6.0133e-02, -8.8736e-05, -8.3084e-02,  5.0095e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0126, grad_fn=<MinBackward1>), tensor(0.9695, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3090476393699646\n",
      "@sample 188: tensor([[ 0.0088, -0.0311, -0.0252,  0.1016, -0.0357],\n",
      "        [ 0.0129, -0.0913, -0.0194,  0.0753, -0.0225],\n",
      "        [-0.0181, -0.0161,  0.0355, -0.0166, -0.0135],\n",
      "        [-0.0277,  0.0884,  0.0309, -0.0702,  0.0072],\n",
      "        [-0.0176, -0.0290,  0.0296, -0.0650, -0.0167],\n",
      "        [-0.0331,  0.0614,  0.0077, -0.0364, -0.0345],\n",
      "        [-0.0112,  0.0039, -0.0294, -0.0050, -0.0732],\n",
      "        [-0.0253, -0.0058, -0.0125,  0.0145,  0.0761]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0111,  0.0430,  0.0091, -0.0732,  0.0425],\n",
      "        [ 0.0545,  0.0743,  0.1208, -0.0543, -0.0228],\n",
      "        [-0.0057,  0.0078,  0.1182, -0.0254,  0.0272],\n",
      "        [ 0.0541, -0.0039, -0.0016,  0.0131,  0.0183],\n",
      "        [-0.1340, -0.0580,  0.0169, -0.0057,  0.0044],\n",
      "        [-0.1155, -0.0450, -0.0249,  0.0562, -0.0617],\n",
      "        [-0.0483, -0.0036,  0.0882, -0.0519, -0.0332],\n",
      "        [ 0.0530,  0.0385, -0.0447,  0.0511,  0.0467]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0205, grad_fn=<MinBackward1>), tensor(0.9763, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2900163531303406\n",
      "@sample 189: tensor([[ 3.8462e-03,  2.8577e-02, -2.1189e-02, -1.1886e-02,  1.0468e-02],\n",
      "        [-4.2520e-03, -5.0146e-02, -2.5853e-02, -1.2019e-03, -9.2605e-03],\n",
      "        [-2.5711e-02,  4.2804e-02, -7.4874e-03,  1.7708e-02, -1.3479e-03],\n",
      "        [-3.2506e-02,  6.6056e-03, -6.1140e-02,  4.5094e-03,  4.8060e-03],\n",
      "        [ 1.0718e-02,  2.0675e-03, -3.4453e-02,  6.8326e-02, -1.4782e-05],\n",
      "        [-2.1087e-03, -1.7808e-02, -1.6725e-02, -1.4352e-02, -3.5117e-02],\n",
      "        [-1.3520e-02,  2.2847e-02,  6.0984e-02, -1.5874e-02, -3.4277e-02],\n",
      "        [ 2.6984e-03, -6.6208e-03, -1.7255e-02,  2.1591e-02, -7.2467e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0003, -0.0272, -0.0251, -0.0153, -0.0270],\n",
      "        [ 0.0116, -0.0157,  0.0413,  0.0249,  0.0428],\n",
      "        [-0.0041,  0.0023, -0.0147,  0.0005, -0.0264],\n",
      "        [-0.0365, -0.0013, -0.0410, -0.0259,  0.0748],\n",
      "        [ 0.0456,  0.0314, -0.0085, -0.0223, -0.0164],\n",
      "        [-0.0118,  0.0202,  0.0164,  0.0062, -0.0221],\n",
      "        [ 0.0543, -0.0286, -0.0360, -0.0171,  0.0401],\n",
      "        [-0.0433,  0.0323,  0.1673, -0.0192,  0.0034]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0129, grad_fn=<MinBackward1>), tensor(0.9721, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2559809684753418\n",
      "@sample 190: tensor([[ 0.0015, -0.1383, -0.0158,  0.1268, -0.0399],\n",
      "        [-0.0177, -0.0251, -0.0237,  0.0133, -0.0137],\n",
      "        [-0.0228,  0.0320,  0.0042, -0.0404, -0.0288],\n",
      "        [-0.0144,  0.0658,  0.0609, -0.0831,  0.0089],\n",
      "        [-0.0325,  0.0124,  0.0385, -0.1106,  0.0820],\n",
      "        [-0.1067,  0.0944,  0.0304, -0.0496,  0.0290],\n",
      "        [-0.0465,  0.0187,  0.0200, -0.0243,  0.0537],\n",
      "        [ 0.0171,  0.0244,  0.0203, -0.0352,  0.0220]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0100,  0.0132,  0.1004, -0.0328, -0.0300],\n",
      "        [ 0.0345, -0.0435,  0.0024,  0.0115,  0.0400],\n",
      "        [-0.0277,  0.0505, -0.0060, -0.0060, -0.0129],\n",
      "        [-0.0593, -0.0781, -0.0328, -0.0504, -0.0459],\n",
      "        [ 0.0266, -0.0691,  0.0056, -0.0249,  0.0552],\n",
      "        [-0.0772,  0.0419, -0.0623,  0.0365,  0.0872],\n",
      "        [-0.0749, -0.0040,  0.0346, -0.0170,  0.0831],\n",
      "        [-0.0140, -0.0300, -0.0438,  0.0400,  0.0002]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0117, grad_fn=<MinBackward1>), tensor(0.9622, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3057207465171814\n",
      "@sample 191: tensor([[ 0.0471, -0.0392,  0.0445, -0.0002, -0.0459],\n",
      "        [-0.0169,  0.0514, -0.0155, -0.0283,  0.0715],\n",
      "        [ 0.0345, -0.0775, -0.0167,  0.0874, -0.0275],\n",
      "        [ 0.0040,  0.0076,  0.0119, -0.0535,  0.0075],\n",
      "        [-0.0007, -0.0462, -0.0369, -0.0213, -0.0820],\n",
      "        [-0.0644,  0.0359,  0.0134,  0.0567, -0.0165],\n",
      "        [ 0.0497,  0.0160, -0.0557,  0.0225, -0.0215],\n",
      "        [-0.0121, -0.0161, -0.0405, -0.0121,  0.0753]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0156,  0.0212, -0.0076, -0.0035,  0.0031],\n",
      "        [-0.0161, -0.0806, -0.1117,  0.0878,  0.0212],\n",
      "        [ 0.0579,  0.0352,  0.0571, -0.0292,  0.0390],\n",
      "        [-0.0524, -0.0010, -0.0661,  0.0150,  0.0455],\n",
      "        [-0.0565,  0.0387,  0.0397, -0.0459, -0.0461],\n",
      "        [ 0.0218,  0.1356,  0.1128, -0.0256, -0.0045],\n",
      "        [ 0.0059, -0.0120, -0.1394,  0.0601, -0.0248],\n",
      "        [ 0.0172, -0.0036, -0.0270,  0.0137, -0.0270]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0181, grad_fn=<MinBackward1>), tensor(0.9749, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.29180359840393066\n",
      "@sample 192: tensor([[ 0.0045,  0.0215,  0.0218, -0.0554,  0.0070],\n",
      "        [-0.0230, -0.0398, -0.0006,  0.0353, -0.0172],\n",
      "        [ 0.0027,  0.0105,  0.0326, -0.0325,  0.0061],\n",
      "        [-0.0072, -0.0151,  0.0362,  0.0227,  0.0341],\n",
      "        [-0.0299, -0.0230,  0.0028,  0.0241, -0.0185],\n",
      "        [ 0.0051,  0.0785,  0.0194,  0.0142,  0.0287],\n",
      "        [ 0.0267, -0.0634, -0.0331,  0.0062, -0.0075],\n",
      "        [ 0.0217, -0.0010, -0.0067,  0.0544, -0.0088]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0293,  0.0165, -0.0571,  0.0251,  0.0204],\n",
      "        [ 0.0348,  0.0112,  0.0217, -0.0721,  0.0297],\n",
      "        [-0.0156,  0.0126, -0.0048,  0.0125, -0.0567],\n",
      "        [ 0.0301,  0.0419,  0.0026, -0.0169, -0.0191],\n",
      "        [ 0.0751,  0.0726,  0.0074,  0.0404,  0.0778],\n",
      "        [-0.0273,  0.0667, -0.0299,  0.0590,  0.0614],\n",
      "        [-0.0112, -0.0254,  0.0535, -0.0492, -0.0285],\n",
      "        [ 0.0261,  0.0439,  0.0536, -0.1166, -0.0161]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0181, grad_fn=<MinBackward1>), tensor(0.9584, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2642584443092346\n",
      "@sample 193: tensor([[ 0.0015,  0.0461,  0.0160, -0.0324,  0.0130],\n",
      "        [-0.0155, -0.0294,  0.0224,  0.0086,  0.0110],\n",
      "        [-0.0444,  0.0218,  0.0245,  0.0041,  0.0424],\n",
      "        [-0.0530, -0.0363,  0.0016, -0.0038, -0.0181],\n",
      "        [-0.0722,  0.0699, -0.0086, -0.0790,  0.0681],\n",
      "        [ 0.0022,  0.0423,  0.0299, -0.0742,  0.0471],\n",
      "        [ 0.0042,  0.0391,  0.0267, -0.0158, -0.0181],\n",
      "        [-0.0639,  0.0330,  0.0184, -0.0259,  0.0003]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0161, -0.0039, -0.0320,  0.0601, -0.0665],\n",
      "        [ 0.0179,  0.0255,  0.0875,  0.0306, -0.0053],\n",
      "        [ 0.0117, -0.0264, -0.0513,  0.0133, -0.0157],\n",
      "        [-0.0922, -0.0366, -0.0508, -0.0011,  0.0200],\n",
      "        [-0.0393, -0.0352, -0.0236,  0.0111,  0.0803],\n",
      "        [ 0.0056, -0.0403, -0.0680,  0.0480, -0.0072],\n",
      "        [-0.0619,  0.0165, -0.0715,  0.0771,  0.0097],\n",
      "        [-0.0850,  0.0301,  0.0127,  0.0100,  0.0025]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0101, grad_fn=<MinBackward1>), tensor(0.9783, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.274791955947876\n",
      "@sample 194: tensor([[-0.0190,  0.0073, -0.0148, -0.0004, -0.0093],\n",
      "        [-0.0311, -0.0969,  0.0054,  0.0668, -0.0168],\n",
      "        [ 0.0106,  0.0083, -0.0950,  0.0165, -0.0328],\n",
      "        [-0.0184,  0.0245, -0.0094,  0.0174,  0.0254],\n",
      "        [-0.0666,  0.0167, -0.0187,  0.0068,  0.0034],\n",
      "        [-0.0062,  0.0053,  0.0049,  0.0723, -0.0444],\n",
      "        [-0.0713,  0.0409,  0.0176, -0.0592,  0.0392],\n",
      "        [ 0.0023,  0.0472,  0.0146, -0.0013,  0.0176]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 8.6169e-03, -5.5979e-02,  1.4937e-02, -5.7115e-02, -1.0819e-02],\n",
      "        [-3.4819e-02,  5.6658e-02,  6.8361e-02, -5.5567e-02, -1.1777e-02],\n",
      "        [ 7.2908e-03, -2.1388e-02, -6.1046e-03, -1.0036e-02, -1.3713e-02],\n",
      "        [ 2.8920e-02,  1.6408e-02,  4.5991e-02, -6.9144e-03,  4.5289e-02],\n",
      "        [-3.9582e-02,  5.4098e-02,  1.1976e-01, -8.7275e-02,  4.4540e-02],\n",
      "        [ 7.0981e-02,  3.3094e-02,  7.8459e-02, -2.3542e-02,  5.3251e-02],\n",
      "        [-3.7099e-02, -2.5693e-02, -2.4544e-02,  6.1736e-05, -2.6866e-02],\n",
      "        [ 1.4874e-02,  1.4237e-02, -6.3263e-02,  5.1555e-02,  4.5174e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0161, grad_fn=<MinBackward1>), tensor(0.9754, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.27155566215515137\n",
      "@sample 195: tensor([[-0.0250, -0.0476,  0.0145,  0.0206,  0.0045],\n",
      "        [ 0.0082,  0.0377, -0.0238,  0.0194, -0.0076],\n",
      "        [-0.0012,  0.0284,  0.0235, -0.0592,  0.0443],\n",
      "        [ 0.0307, -0.0579,  0.0312, -0.0202, -0.0170],\n",
      "        [-0.0490, -0.0772,  0.0150, -0.0226,  0.0373],\n",
      "        [ 0.0075,  0.0083, -0.0311,  0.0195,  0.0267],\n",
      "        [ 0.0124,  0.0728,  0.0974, -0.0542,  0.0076],\n",
      "        [-0.0829, -0.0304, -0.0123, -0.0369,  0.0082]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0546, -0.0045,  0.0678, -0.0278,  0.0498],\n",
      "        [ 0.0296,  0.0927, -0.0622, -0.0269,  0.0886],\n",
      "        [-0.0245, -0.0238, -0.0469, -0.0225, -0.0203],\n",
      "        [ 0.0037, -0.0103,  0.0352,  0.0106,  0.0007],\n",
      "        [-0.0222, -0.0326, -0.0574,  0.0250, -0.0622],\n",
      "        [ 0.0030,  0.0267, -0.0506,  0.0211,  0.0424],\n",
      "        [-0.0287,  0.0147, -0.0083,  0.0453, -0.0123],\n",
      "        [ 0.0376, -0.0213,  0.0847, -0.0598,  0.0360]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0185, grad_fn=<MinBackward1>), tensor(0.9732, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.27085068821907043\n",
      "@sample 196: tensor([[-0.0197, -0.0524, -0.0046,  0.0843, -0.0229],\n",
      "        [ 0.0175,  0.0468,  0.0283, -0.0336,  0.0584],\n",
      "        [-0.0333, -0.0178, -0.0529,  0.0017,  0.0036],\n",
      "        [ 0.0214,  0.0276, -0.0400, -0.0160,  0.0029],\n",
      "        [ 0.0186,  0.0686, -0.0361, -0.0187,  0.0256],\n",
      "        [ 0.0243, -0.0028, -0.0432,  0.0550, -0.0683],\n",
      "        [ 0.0184,  0.0560,  0.0564, -0.0693,  0.0067],\n",
      "        [-0.0192,  0.0079, -0.0097,  0.0336, -0.0547]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0884,  0.0672,  0.1092, -0.0320,  0.0469],\n",
      "        [ 0.0384,  0.0081, -0.1467,  0.0729,  0.0639],\n",
      "        [ 0.0369,  0.0162, -0.0015, -0.0432,  0.0244],\n",
      "        [-0.0027, -0.0156, -0.0327,  0.0168, -0.0042],\n",
      "        [-0.0774, -0.0179, -0.0653,  0.0736, -0.0538],\n",
      "        [ 0.0325,  0.0332,  0.0258,  0.0034,  0.0668],\n",
      "        [-0.0173, -0.0613, -0.0246,  0.0054, -0.0200],\n",
      "        [ 0.0433,  0.0580,  0.1172, -0.0552, -0.0206]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0192, grad_fn=<MinBackward1>), tensor(0.9607, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2821451425552368\n",
      "@sample 197: tensor([[-0.0052,  0.0057, -0.0055, -0.0339, -0.0073],\n",
      "        [-0.0194,  0.0961,  0.0124, -0.0462,  0.0217],\n",
      "        [ 0.0176,  0.0643, -0.0353, -0.0833,  0.0131],\n",
      "        [ 0.0255,  0.0168, -0.0040,  0.0604, -0.0195],\n",
      "        [-0.0109,  0.0003, -0.0328, -0.0071,  0.0182],\n",
      "        [-0.0168,  0.0475, -0.0303, -0.0181,  0.0069],\n",
      "        [ 0.0358,  0.0491,  0.0230,  0.0217, -0.1137],\n",
      "        [-0.0499,  0.0554, -0.0027, -0.0730, -0.0311]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0017,  0.0274,  0.0719, -0.1076, -0.0641],\n",
      "        [-0.1183, -0.0110, -0.1315,  0.0245, -0.0030],\n",
      "        [ 0.0113, -0.0751, -0.0310,  0.0667, -0.0014],\n",
      "        [ 0.0655,  0.0885, -0.0532,  0.0323,  0.0673],\n",
      "        [ 0.0160, -0.0437, -0.0487,  0.0997,  0.0578],\n",
      "        [ 0.0082, -0.0099,  0.0113, -0.0149,  0.0345],\n",
      "        [-0.0205,  0.0611, -0.0269,  0.0309,  0.0103],\n",
      "        [-0.0718, -0.0907, -0.0808,  0.0561, -0.0716]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0337, grad_fn=<MinBackward1>), tensor(0.9825, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.28506556153297424\n",
      "@sample 198: tensor([[ 0.0165,  0.0411, -0.0169, -0.0407,  0.0210],\n",
      "        [ 0.0139, -0.0160, -0.0139,  0.0537, -0.0060],\n",
      "        [ 0.0206,  0.0529, -0.0356, -0.0097, -0.0200],\n",
      "        [-0.0400,  0.0232,  0.0856, -0.0659,  0.0359],\n",
      "        [-0.0108,  0.0069,  0.0128, -0.0180,  0.0310],\n",
      "        [-0.0038, -0.0095, -0.0334, -0.0297, -0.0238],\n",
      "        [-0.0474, -0.0132, -0.0555, -0.0326, -0.0797],\n",
      "        [-0.0058,  0.0387, -0.0451,  0.0727,  0.0426]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-2.0945e-02,  4.2464e-03, -8.9157e-02,  5.0017e-02, -3.1158e-03],\n",
      "        [ 9.2660e-02,  4.0213e-02, -1.4406e-02, -2.3772e-02,  7.0502e-02],\n",
      "        [ 1.3337e-02,  9.8653e-05, -7.7015e-02,  7.9061e-02, -2.8495e-03],\n",
      "        [-1.7330e-02, -3.1185e-02, -3.4885e-02,  5.6308e-02,  5.8590e-02],\n",
      "        [-2.1926e-02, -1.7412e-02, -5.7794e-02,  3.3247e-02,  3.1833e-02],\n",
      "        [-2.0387e-02,  2.2575e-02,  1.4269e-01, -7.0822e-02, -6.4500e-02],\n",
      "        [-2.5050e-02, -3.0708e-02,  1.1785e-01, -5.8524e-02, -5.1581e-02],\n",
      "        [ 6.6930e-02,  5.7201e-02,  6.2065e-03, -2.3058e-02, -2.9734e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0259, grad_fn=<MinBackward1>), tensor(0.9781, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.28431832790374756\n",
      "@sample 199: tensor([[-0.0053, -0.0111, -0.0738, -0.0068,  0.0126],\n",
      "        [-0.0015,  0.0036, -0.0042,  0.0085,  0.0228],\n",
      "        [-0.0322,  0.0934,  0.0548, -0.1300,  0.0521],\n",
      "        [ 0.0109, -0.0137, -0.0031, -0.0074,  0.0007],\n",
      "        [ 0.0259,  0.0306, -0.0660,  0.0289, -0.0304],\n",
      "        [-0.0106,  0.0093,  0.0426,  0.0170, -0.0325],\n",
      "        [ 0.0424,  0.0169, -0.0189, -0.0067, -0.0320],\n",
      "        [ 0.0531, -0.0230, -0.0269,  0.0206, -0.0256]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0494, -0.0262,  0.0403, -0.0035, -0.0134],\n",
      "        [ 0.0508,  0.0522,  0.0180,  0.0085, -0.0569],\n",
      "        [-0.0312, -0.0826, -0.0973,  0.0540,  0.0096],\n",
      "        [-0.0291, -0.0365,  0.0058,  0.0527,  0.0967],\n",
      "        [-0.0184, -0.0170, -0.0749,  0.0650,  0.0337],\n",
      "        [-0.0875,  0.0281, -0.0021, -0.0157, -0.0029],\n",
      "        [ 0.0233, -0.0273, -0.0266,  0.0332,  0.0357],\n",
      "        [-0.0167, -0.0052,  0.0591,  0.0350,  0.0321]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0178, grad_fn=<MinBackward1>), tensor(0.9597, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.27125290036201477\n",
      "@sample 200: tensor([[-0.0384,  0.0215,  0.1388, -0.0044,  0.0690],\n",
      "        [ 0.0337, -0.0053, -0.0431,  0.0141,  0.0328],\n",
      "        [-0.0289,  0.0617,  0.0405, -0.0444,  0.0389],\n",
      "        [ 0.0270, -0.0505,  0.0252, -0.0302,  0.0396],\n",
      "        [-0.0277, -0.0496, -0.0605,  0.0384, -0.0331],\n",
      "        [ 0.0055, -0.0320, -0.0035, -0.0070,  0.0052],\n",
      "        [-0.0388,  0.0351, -0.0290, -0.0067,  0.0399],\n",
      "        [-0.0140,  0.0681,  0.0195, -0.1342,  0.0848]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0058,  0.0627, -0.0650, -0.0410, -0.0308],\n",
      "        [ 0.0709, -0.0285,  0.0621, -0.0870, -0.0678],\n",
      "        [-0.0079,  0.0559, -0.0440, -0.0008, -0.0328],\n",
      "        [ 0.0683, -0.0311,  0.0742, -0.0554, -0.0629],\n",
      "        [ 0.0777,  0.0714,  0.1063, -0.0116,  0.0840],\n",
      "        [-0.0263,  0.0216,  0.0542,  0.0021, -0.0311],\n",
      "        [ 0.0491,  0.0286, -0.0363,  0.0234,  0.0986],\n",
      "        [-0.0195, -0.1452, -0.1919,  0.0616, -0.0938]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0238, grad_fn=<MinBackward1>), tensor(0.9848, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3160876929759979\n",
      "@sample 201: tensor([[ 0.0097,  0.0063,  0.0390, -0.0504, -0.0394],\n",
      "        [ 0.0111, -0.0005,  0.0006, -0.0128,  0.0131],\n",
      "        [ 0.0061, -0.0036,  0.1167,  0.0129,  0.0375],\n",
      "        [ 0.0197, -0.0103,  0.0357,  0.0064,  0.0399],\n",
      "        [-0.0566,  0.0039,  0.0230, -0.0458, -0.0043],\n",
      "        [-0.0167, -0.0672, -0.0107,  0.0210, -0.0148],\n",
      "        [ 0.0017,  0.0163, -0.0111, -0.0062, -0.0027],\n",
      "        [ 0.0337, -0.0200,  0.0135, -0.0314, -0.0274]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0454,  0.0299,  0.0572, -0.0265, -0.0191],\n",
      "        [ 0.0275, -0.0375,  0.0392, -0.0104, -0.0317],\n",
      "        [ 0.0206,  0.0234, -0.0199,  0.0226, -0.0046],\n",
      "        [-0.0328,  0.0338, -0.0316,  0.0915, -0.0116],\n",
      "        [-0.0447, -0.0917,  0.0406,  0.0041,  0.0276],\n",
      "        [ 0.0069, -0.0063,  0.0598,  0.0361,  0.0488],\n",
      "        [-0.0029, -0.0109, -0.0389, -0.0029,  0.0224],\n",
      "        [-0.0028,  0.0281,  0.1029, -0.0620, -0.0336]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0152, grad_fn=<MinBackward1>), tensor(0.9689, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.25482308864593506\n",
      "@sample 202: tensor([[-0.0040,  0.0178, -0.0051, -0.0278, -0.0018],\n",
      "        [-0.0074, -0.0039,  0.0537, -0.0367,  0.0055],\n",
      "        [ 0.0233, -0.0820,  0.0025, -0.0708, -0.0218],\n",
      "        [-0.0164,  0.0387, -0.0250, -0.1281,  0.0545],\n",
      "        [ 0.0057,  0.0618,  0.0143, -0.0791,  0.0730],\n",
      "        [-0.1052,  0.0567, -0.0348, -0.1070,  0.0563],\n",
      "        [-0.0276, -0.0346,  0.0289,  0.0548, -0.0238],\n",
      "        [-0.0353,  0.0121,  0.0583,  0.0068, -0.0203]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0553, -0.0013,  0.0409, -0.0301, -0.0259],\n",
      "        [-0.0814,  0.0145, -0.0241,  0.0797,  0.0169],\n",
      "        [-0.0918, -0.0424, -0.0213,  0.0728,  0.0185],\n",
      "        [-0.0448, -0.1024, -0.0663,  0.1107, -0.0028],\n",
      "        [ 0.0080, -0.0991, -0.0716,  0.0580,  0.0053],\n",
      "        [ 0.0177, -0.0769,  0.0181,  0.0362, -0.0023],\n",
      "        [-0.0099,  0.0274, -0.0420,  0.0471, -0.0665],\n",
      "        [-0.0115, -0.0111,  0.0095, -0.0412,  0.0072]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0198, grad_fn=<MinBackward1>), tensor(0.9784, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.29965847730636597\n",
      "@sample 203: tensor([[-3.8622e-02, -2.4333e-02, -4.1969e-04, -1.8089e-02, -1.6135e-03],\n",
      "        [ 1.9377e-02,  7.5018e-02,  5.3356e-03, -3.8177e-02,  3.7749e-02],\n",
      "        [ 2.1285e-02, -1.2222e-02,  7.7267e-03,  2.3581e-02,  1.7946e-02],\n",
      "        [ 3.1579e-03, -6.9548e-02,  3.0161e-02,  4.1815e-02, -6.8689e-02],\n",
      "        [ 2.8302e-02, -5.8806e-02,  4.6378e-02,  7.4159e-02,  1.5611e-02],\n",
      "        [ 4.1992e-02, -3.9861e-02, -9.1990e-03,  6.0258e-02,  7.0385e-03],\n",
      "        [ 1.1144e-04, -5.5506e-02, -4.1151e-02, -1.2879e-01,  8.6584e-03],\n",
      "        [ 1.1977e-02, -7.6290e-02, -1.3395e-02,  1.1481e-01,  1.0198e-04]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0025, -0.0371,  0.0328, -0.0529, -0.0662],\n",
      "        [-0.0391, -0.0353, -0.0861,  0.0841,  0.0201],\n",
      "        [-0.0368,  0.0195, -0.0116,  0.0060, -0.0309],\n",
      "        [ 0.0006,  0.0620,  0.0357, -0.0278, -0.0718],\n",
      "        [-0.0632,  0.0004, -0.0937,  0.1381, -0.0664],\n",
      "        [ 0.0834, -0.0109,  0.1080, -0.0296, -0.0084],\n",
      "        [-0.0554, -0.0937, -0.0131,  0.0029, -0.0091],\n",
      "        [ 0.0400,  0.0570,  0.0334, -0.0319, -0.0120]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0203, grad_fn=<MinBackward1>), tensor(0.9699, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.29472607374191284\n",
      "@sample 204: tensor([[-0.0186,  0.0968, -0.0171, -0.0626, -0.0124],\n",
      "        [-0.0051,  0.0403, -0.0250, -0.0366,  0.0830],\n",
      "        [-0.0434,  0.0167,  0.0387,  0.0151,  0.0308],\n",
      "        [ 0.0120, -0.0754,  0.0483,  0.0923,  0.0037],\n",
      "        [ 0.0061,  0.0131, -0.0415,  0.0063,  0.0668],\n",
      "        [ 0.0002,  0.0669, -0.0232, -0.0156,  0.0152],\n",
      "        [-0.0216, -0.0777, -0.0507, -0.0108,  0.0253],\n",
      "        [ 0.0584, -0.0018,  0.0226,  0.0017, -0.0272]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0444, -0.0591, -0.0847,  0.0407,  0.0042],\n",
      "        [-0.0007, -0.0038, -0.1621,  0.0842,  0.0024],\n",
      "        [-0.0083,  0.0416, -0.0617,  0.0438,  0.0363],\n",
      "        [-0.0084,  0.0620,  0.1071, -0.0749,  0.0422],\n",
      "        [-0.0535,  0.0193,  0.0285, -0.0471, -0.0264],\n",
      "        [-0.0145, -0.0432, -0.1047,  0.0820,  0.0484],\n",
      "        [ 0.0168, -0.0526, -0.0545,  0.0516,  0.0744],\n",
      "        [-0.0691,  0.0330, -0.0579,  0.0218, -0.0078]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0111, grad_fn=<MinBackward1>), tensor(0.9755, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.28190940618515015\n",
      "@sample 205: tensor([[ 0.0096,  0.0049, -0.0595,  0.0454, -0.0397],\n",
      "        [-0.0204, -0.0120,  0.0196,  0.0312, -0.0055],\n",
      "        [ 0.0090, -0.0891,  0.0074,  0.0577,  0.0166],\n",
      "        [-0.0396, -0.0472, -0.0108, -0.0073,  0.0226],\n",
      "        [-0.0434, -0.0721,  0.0428,  0.0154,  0.0357],\n",
      "        [-0.0074,  0.0032,  0.0603,  0.0181,  0.0326],\n",
      "        [-0.0449,  0.0429,  0.0103, -0.0646,  0.0364],\n",
      "        [-0.0134,  0.0417,  0.0272, -0.0240,  0.0773]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0750, -0.0088,  0.0159, -0.0471, -0.0462],\n",
      "        [-0.0026, -0.0226, -0.0400, -0.0135, -0.0292],\n",
      "        [ 0.0315,  0.0398,  0.1255, -0.0067, -0.0631],\n",
      "        [-0.0138, -0.0260, -0.0557,  0.0349,  0.0108],\n",
      "        [ 0.0373, -0.0287,  0.0313, -0.0561, -0.0386],\n",
      "        [ 0.0507,  0.0604, -0.0417,  0.0469,  0.0373],\n",
      "        [-0.0081, -0.0463, -0.1472,  0.0365, -0.0042],\n",
      "        [ 0.0574,  0.0493, -0.0923,  0.0366, -0.0170]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0098, grad_fn=<MinBackward1>), tensor(0.9786, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.27172747254371643\n",
      "@sample 206: tensor([[-0.0341, -0.0617, -0.0381,  0.0386,  0.0020],\n",
      "        [ 0.0081,  0.0032,  0.0103, -0.0704,  0.0820],\n",
      "        [-0.0263, -0.0032,  0.0250,  0.0089,  0.0668],\n",
      "        [ 0.0097,  0.0393, -0.0081,  0.0294,  0.0168],\n",
      "        [-0.0022,  0.0565,  0.0123, -0.0409,  0.0248],\n",
      "        [ 0.0268, -0.0200, -0.0411,  0.0419,  0.0239],\n",
      "        [-0.0208, -0.0133,  0.0414,  0.0497, -0.0174],\n",
      "        [-0.0280,  0.0184, -0.0517,  0.0067, -0.0059]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0227,  0.0574,  0.0539, -0.0351,  0.0627],\n",
      "        [ 0.0358, -0.0166, -0.0986, -0.0086, -0.0389],\n",
      "        [-0.0463,  0.0505, -0.0410, -0.0431, -0.0359],\n",
      "        [-0.0227,  0.0026, -0.1045,  0.0385, -0.0268],\n",
      "        [-0.0681, -0.0879, -0.0905,  0.0072, -0.0257],\n",
      "        [ 0.0442, -0.0041,  0.0079,  0.0103,  0.0119],\n",
      "        [ 0.0191,  0.0127,  0.0880, -0.0374, -0.0185],\n",
      "        [ 0.0222, -0.0036,  0.0377,  0.0282,  0.0139]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0230, grad_fn=<MinBackward1>), tensor(0.9772, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.26870203018188477\n",
      "@sample 207: tensor([[-0.0410, -0.0041,  0.0010,  0.0522,  0.0110],\n",
      "        [-0.0357,  0.1457, -0.0064, -0.1085,  0.0294],\n",
      "        [ 0.0417, -0.0303,  0.0327,  0.0426,  0.0385],\n",
      "        [ 0.0240, -0.0411,  0.0290, -0.0034,  0.0438],\n",
      "        [-0.0035,  0.0625,  0.0472, -0.0556,  0.0667],\n",
      "        [-0.0193,  0.0338, -0.0216,  0.0085, -0.0201],\n",
      "        [-0.0105, -0.0266,  0.0202,  0.0097, -0.0212],\n",
      "        [ 0.0119, -0.0336,  0.0472, -0.0048,  0.0428]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0077,  0.0501,  0.0226, -0.0525, -0.0302],\n",
      "        [-0.0298, -0.0627, -0.1232,  0.0481, -0.0402],\n",
      "        [-0.0264,  0.0398, -0.0454, -0.0114, -0.0139],\n",
      "        [-0.0483, -0.0390, -0.0655,  0.0744, -0.0009],\n",
      "        [-0.0345, -0.0452, -0.0869,  0.0801,  0.0176],\n",
      "        [-0.0171, -0.0234, -0.0572,  0.0082,  0.0033],\n",
      "        [ 0.0574, -0.0236, -0.0105, -0.0224,  0.0659],\n",
      "        [ 0.0461, -0.0287, -0.0051,  0.0273,  0.0540]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0113, grad_fn=<MinBackward1>), tensor(0.9559, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2810589671134949\n",
      "@sample 208: tensor([[ 0.0420, -0.0394, -0.0467,  0.0011,  0.0464],\n",
      "        [ 0.0228, -0.0131,  0.0444,  0.0437, -0.0069],\n",
      "        [-0.0129,  0.0190,  0.0086,  0.0144,  0.0130],\n",
      "        [ 0.0171,  0.0418, -0.0388, -0.0201, -0.0068],\n",
      "        [-0.0097, -0.0022,  0.0016,  0.0062,  0.0426],\n",
      "        [ 0.0055, -0.0274,  0.0119,  0.0466, -0.0018],\n",
      "        [ 0.0120, -0.0315,  0.0349, -0.0064, -0.0327],\n",
      "        [ 0.0004,  0.0615,  0.0923, -0.0608,  0.0091]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0108,  0.0619,  0.0502, -0.1018, -0.0643],\n",
      "        [ 0.0095,  0.0746, -0.0059, -0.0262, -0.0796],\n",
      "        [-0.0310,  0.0058, -0.0390, -0.0026, -0.0721],\n",
      "        [ 0.0142, -0.0247, -0.0023, -0.0321,  0.0157],\n",
      "        [-0.0190, -0.0355, -0.0661,  0.0334, -0.0195],\n",
      "        [ 0.0354, -0.0054, -0.0750, -0.0062, -0.0049],\n",
      "        [ 0.0072, -0.0485,  0.0588, -0.0567, -0.0027],\n",
      "        [-0.0203, -0.0485, -0.0888,  0.0717, -0.0149]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0130, grad_fn=<MinBackward1>), tensor(0.9657, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.25537753105163574\n",
      "@sample 209: tensor([[-0.0198,  0.0468,  0.0206,  0.0019,  0.0493],\n",
      "        [-0.0276,  0.0007, -0.0370, -0.0134,  0.0236],\n",
      "        [ 0.0007,  0.0128,  0.0240, -0.0904,  0.0887],\n",
      "        [ 0.0297,  0.0421,  0.0165, -0.0453,  0.0453],\n",
      "        [ 0.0012,  0.0080,  0.0250,  0.0104, -0.0027],\n",
      "        [ 0.0462,  0.0128, -0.0125, -0.0635, -0.0057],\n",
      "        [ 0.0629, -0.0586,  0.1008,  0.0680,  0.0095],\n",
      "        [ 0.0713, -0.0035,  0.0345,  0.1128,  0.0068]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0100, -0.0400, -0.0511,  0.0458, -0.0166],\n",
      "        [-0.0745, -0.0493, -0.0359, -0.0138, -0.0559],\n",
      "        [-0.0284, -0.0941, -0.1279,  0.0405, -0.0154],\n",
      "        [-0.0109, -0.0560, -0.0267,  0.0522,  0.0098],\n",
      "        [-0.0393,  0.0009, -0.0020, -0.0392,  0.0726],\n",
      "        [ 0.0045, -0.0327, -0.0018, -0.0059,  0.0549],\n",
      "        [ 0.0373,  0.0674, -0.0422,  0.0346, -0.0341],\n",
      "        [-0.0019,  0.0773, -0.0672, -0.0199, -0.0185]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0291, grad_fn=<MinBackward1>), tensor(0.9774, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.28475093841552734\n",
      "@sample 210: tensor([[-0.0013,  0.0888,  0.0536, -0.0342,  0.0574],\n",
      "        [-0.0293, -0.0170, -0.0030, -0.0431, -0.0103],\n",
      "        [-0.0043,  0.0027, -0.0014,  0.0057,  0.0201],\n",
      "        [ 0.0076,  0.0021, -0.0189, -0.1134,  0.0423],\n",
      "        [ 0.0545, -0.0091,  0.0012,  0.0598, -0.0238],\n",
      "        [-0.0460,  0.0349,  0.0537, -0.0534, -0.0141],\n",
      "        [ 0.0209, -0.0138, -0.0129,  0.0823, -0.0959],\n",
      "        [ 0.0116,  0.0460, -0.0241, -0.0224,  0.0208]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-3.0701e-02, -3.6547e-02, -9.5829e-02,  7.3953e-02, -6.2361e-04],\n",
      "        [-2.5876e-02, -9.6795e-02,  1.4629e-02, -1.0863e-02, -5.6122e-02],\n",
      "        [-1.0728e-02, -6.7873e-02, -5.9740e-02, -3.9845e-03, -2.6767e-02],\n",
      "        [-5.5386e-02, -7.9804e-02, -1.2238e-02,  1.9266e-02,  9.4151e-02],\n",
      "        [ 2.0558e-02, -9.5445e-03, -4.8342e-02,  3.2289e-02, -1.3451e-02],\n",
      "        [-6.4462e-05, -4.9145e-02, -1.2665e-02,  5.4258e-02,  2.0954e-03],\n",
      "        [ 4.9312e-02,  1.6717e-02,  1.0322e-01,  2.4888e-02,  2.4718e-02],\n",
      "        [ 1.9703e-02, -7.3021e-02, -1.0163e-01,  8.6784e-02,  3.1937e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0137, grad_fn=<MinBackward1>), tensor(0.9767, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2765392065048218\n",
      "@sample 211: tensor([[-2.3812e-02, -1.4013e-02,  1.8997e-02, -5.2476e-02,  7.5065e-02],\n",
      "        [-3.6067e-02,  3.6506e-02,  6.1344e-02, -3.1201e-02,  2.6568e-02],\n",
      "        [ 2.6112e-02, -7.8985e-02, -1.2275e-02,  1.5941e-01, -9.5427e-02],\n",
      "        [ 3.9029e-02,  2.6883e-02,  7.6328e-03, -8.6912e-02, -5.3466e-04],\n",
      "        [-3.8184e-02,  6.3331e-02,  8.3289e-02, -2.8948e-02, -1.3540e-02],\n",
      "        [-4.1603e-02, -8.6167e-02,  1.1001e-04,  4.9710e-02,  2.7933e-02],\n",
      "        [-1.5258e-02,  1.0948e-01,  4.0782e-02, -4.9894e-02, -5.9540e-03],\n",
      "        [ 3.1563e-02, -1.1046e-02, -4.6157e-02,  3.6040e-02, -1.9289e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0261, -0.0534,  0.0290,  0.0471,  0.0197],\n",
      "        [-0.0269,  0.0049, -0.0423,  0.0067, -0.0519],\n",
      "        [ 0.0893,  0.1384,  0.0923, -0.0717, -0.0579],\n",
      "        [-0.0418, -0.0300, -0.0161,  0.0377, -0.0471],\n",
      "        [ 0.0096, -0.0477, -0.0511, -0.0159,  0.0216],\n",
      "        [ 0.0463, -0.0456,  0.0035,  0.0055, -0.0015],\n",
      "        [-0.0433,  0.0088, -0.0285,  0.1162,  0.0387],\n",
      "        [-0.0147,  0.0044, -0.0723,  0.0083, -0.0246]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0200, grad_fn=<MinBackward1>), tensor(0.9536, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3097606599330902\n",
      "@sample 212: tensor([[ 0.0281,  0.0388,  0.0026,  0.0030, -0.0049],\n",
      "        [-0.0181,  0.0710, -0.0178,  0.0439,  0.0096],\n",
      "        [ 0.0402, -0.0709,  0.0336,  0.0695, -0.0888],\n",
      "        [ 0.0263, -0.0215, -0.0479,  0.0282, -0.0828],\n",
      "        [-0.0156, -0.0481, -0.0397,  0.0034, -0.0545],\n",
      "        [ 0.0229, -0.0561,  0.0324, -0.0183,  0.0171],\n",
      "        [ 0.0175, -0.0629, -0.0241,  0.0590, -0.0850],\n",
      "        [-0.0266,  0.0477,  0.0899, -0.0547,  0.0409]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0037,  0.0123, -0.0255,  0.0516, -0.0452],\n",
      "        [ 0.0388,  0.0558,  0.0989, -0.0720, -0.0212],\n",
      "        [-0.0176,  0.0698,  0.0428,  0.0358,  0.0370],\n",
      "        [ 0.0191, -0.0393,  0.0590, -0.0583, -0.0189],\n",
      "        [-0.0027,  0.0182, -0.0322, -0.0096,  0.0121],\n",
      "        [ 0.0429,  0.0115,  0.0259, -0.0345, -0.0388],\n",
      "        [-0.0093,  0.0081,  0.0937, -0.0475, -0.0241],\n",
      "        [ 0.0398, -0.0331, -0.0751,  0.0133,  0.0322]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0099, grad_fn=<MinBackward1>), tensor(0.9805, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2781594395637512\n",
      "@sample 213: tensor([[ 0.0068,  0.0625, -0.0190,  0.0250, -0.0011],\n",
      "        [ 0.0464, -0.0456, -0.0237,  0.0486, -0.1056],\n",
      "        [-0.0055,  0.0494,  0.0412, -0.0489,  0.0236],\n",
      "        [ 0.0293, -0.0237, -0.0525,  0.0580, -0.0287],\n",
      "        [ 0.0206, -0.0027, -0.0342, -0.0522, -0.0121],\n",
      "        [-0.0139, -0.0331, -0.0259, -0.0050,  0.0016],\n",
      "        [ 0.0053,  0.0472,  0.0159,  0.0054, -0.0022],\n",
      "        [ 0.0212, -0.0112, -0.0430,  0.0696, -0.0471]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.1014,  0.0003, -0.0993,  0.0241,  0.0327],\n",
      "        [-0.0036, -0.0092,  0.0976, -0.0706,  0.0501],\n",
      "        [-0.0590, -0.0309, -0.0974,  0.0999,  0.0437],\n",
      "        [-0.0053,  0.0447,  0.0537, -0.0271, -0.0992],\n",
      "        [-0.0289, -0.1647, -0.0793,  0.0090, -0.0216],\n",
      "        [ 0.0678, -0.0319,  0.0422, -0.0231, -0.0015],\n",
      "        [ 0.0109, -0.0127,  0.0287, -0.0507, -0.0257],\n",
      "        [ 0.0322,  0.0380,  0.1016, -0.1013, -0.0341]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0188, grad_fn=<MinBackward1>), tensor(0.9855, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.27846580743789673\n",
      "@sample 214: tensor([[ 0.0184,  0.0282,  0.0262, -0.0485,  0.0177],\n",
      "        [ 0.0164,  0.0462, -0.0329, -0.0773,  0.0368],\n",
      "        [-0.0022,  0.0131, -0.0752, -0.0683,  0.0648],\n",
      "        [ 0.0408,  0.0799, -0.0354, -0.0546, -0.0448],\n",
      "        [-0.0412,  0.0405, -0.0425, -0.0515,  0.0551],\n",
      "        [-0.0210, -0.0321,  0.0351,  0.0065, -0.0634],\n",
      "        [-0.0099, -0.0115, -0.0153,  0.1055, -0.0151],\n",
      "        [-0.0339,  0.0171, -0.0075, -0.0513,  0.0539]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0067,  0.0072, -0.0162,  0.0060, -0.0388],\n",
      "        [-0.0193, -0.1215, -0.0445,  0.1114, -0.0188],\n",
      "        [ 0.0106, -0.1353, -0.0723,  0.0471,  0.0111],\n",
      "        [-0.0533, -0.0790, -0.0221,  0.0757, -0.0277],\n",
      "        [-0.0002, -0.0667, -0.0357,  0.0348,  0.0387],\n",
      "        [ 0.0352, -0.0457,  0.1448, -0.0957, -0.0529],\n",
      "        [ 0.0243,  0.0620,  0.0719, -0.0532,  0.0300],\n",
      "        [-0.0204, -0.0762, -0.0114,  0.0379, -0.0049]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0122, grad_fn=<MinBackward1>), tensor(0.9835, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2911297380924225\n",
      "@sample 215: tensor([[ 0.0129, -0.0106, -0.0306,  0.0194, -0.0332],\n",
      "        [ 0.0237, -0.0050,  0.0114, -0.0634, -0.0383],\n",
      "        [-0.0183,  0.0197, -0.0411, -0.0079, -0.0069],\n",
      "        [-0.0439, -0.0421,  0.0091, -0.0669, -0.0088],\n",
      "        [ 0.0251, -0.0044, -0.0725,  0.0854, -0.0747],\n",
      "        [-0.0221,  0.0410, -0.0035,  0.0136,  0.0155],\n",
      "        [ 0.0212,  0.0015, -0.0705,  0.1001, -0.0716],\n",
      "        [-0.0018, -0.0383, -0.1174,  0.0829, -0.0649]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0115,  0.0068, -0.0754,  0.0293,  0.0597],\n",
      "        [-0.0847, -0.0806, -0.0830,  0.0342, -0.0094],\n",
      "        [ 0.0278, -0.0142, -0.0097, -0.0293, -0.0235],\n",
      "        [-0.0787, -0.0638, -0.0300,  0.0418,  0.0782],\n",
      "        [ 0.0599,  0.0973, -0.0113, -0.0640,  0.0075],\n",
      "        [ 0.0580, -0.0456,  0.0072, -0.0086,  0.0078],\n",
      "        [ 0.0968,  0.1366,  0.1007, -0.1363,  0.0007],\n",
      "        [ 0.0430,  0.0260,  0.1954, -0.0839, -0.0057]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0155, grad_fn=<MinBackward1>), tensor(0.9763, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.3093101978302002\n",
      "@sample 216: tensor([[-0.0229,  0.0032, -0.0256, -0.0431, -0.0075],\n",
      "        [-0.0225, -0.0173,  0.0202,  0.0440, -0.0255],\n",
      "        [ 0.0027, -0.0031,  0.0193,  0.0600, -0.0606],\n",
      "        [-0.0187, -0.0170,  0.0421,  0.0087,  0.0337],\n",
      "        [-0.0121, -0.0010,  0.0256,  0.0134, -0.0003],\n",
      "        [-0.0434,  0.0543,  0.0173, -0.0636,  0.0183],\n",
      "        [ 0.0471, -0.0371, -0.0199, -0.0024, -0.0619],\n",
      "        [-0.0472,  0.0681,  0.0338, -0.0426,  0.0215]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0446, -0.0795,  0.0642,  0.0053, -0.0495],\n",
      "        [ 0.0636,  0.0398,  0.0817, -0.0612, -0.0018],\n",
      "        [ 0.0056, -0.0111,  0.0618, -0.0857, -0.0298],\n",
      "        [ 0.0259, -0.0202, -0.0502,  0.0247,  0.0330],\n",
      "        [-0.0363, -0.0040, -0.0280,  0.0419,  0.0580],\n",
      "        [ 0.0476, -0.0359, -0.0549,  0.0080,  0.0127],\n",
      "        [-0.0283,  0.0030, -0.0288,  0.0098, -0.0200],\n",
      "        [ 0.0333,  0.0110, -0.0416,  0.0578,  0.0033]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0220, grad_fn=<MinBackward1>), tensor(0.9674, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.25845766067504883\n",
      "@sample 217: tensor([[ 0.0263, -0.0659,  0.0198,  0.0204,  0.0276],\n",
      "        [ 0.0632, -0.0619,  0.0092,  0.0671, -0.0393],\n",
      "        [ 0.0025,  0.0333,  0.0154, -0.0138,  0.0184],\n",
      "        [ 0.0190, -0.0150,  0.0382,  0.0236,  0.0414],\n",
      "        [-0.0080, -0.0065, -0.0465,  0.0478, -0.0438],\n",
      "        [-0.0225,  0.0828,  0.0177, -0.0507, -0.0051],\n",
      "        [-0.0369,  0.0385,  0.0819, -0.0296,  0.1090],\n",
      "        [-0.0150,  0.0806, -0.0226, -0.0538, -0.0342]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0172, -0.0086,  0.0483, -0.0451,  0.0204],\n",
      "        [ 0.0176,  0.0237,  0.1043, -0.0379, -0.0226],\n",
      "        [-0.0267, -0.0285, -0.0348, -0.0090, -0.0018],\n",
      "        [ 0.0054,  0.0196,  0.0670, -0.0325,  0.0591],\n",
      "        [ 0.0049, -0.0094, -0.0259,  0.0541,  0.0886],\n",
      "        [-0.0102, -0.0409, -0.0272,  0.0233, -0.0283],\n",
      "        [-0.0402, -0.0698, -0.1000,  0.0842,  0.0525],\n",
      "        [-0.0885, -0.0575,  0.0204,  0.0049, -0.0444]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0217, grad_fn=<MinBackward1>), tensor(0.9555, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.28463128209114075\n",
      "@sample 218: tensor([[ 0.0146,  0.0413,  0.0172, -0.0580, -0.0279],\n",
      "        [-0.0098, -0.0183, -0.0502,  0.0435, -0.0559],\n",
      "        [-0.0309, -0.0174, -0.1116,  0.0540, -0.0232],\n",
      "        [ 0.0153, -0.0904, -0.0262,  0.0874, -0.0507],\n",
      "        [ 0.0643, -0.0035, -0.0549,  0.0021, -0.0408],\n",
      "        [ 0.0160,  0.1277, -0.0168, -0.0076,  0.0200],\n",
      "        [ 0.0097,  0.0492, -0.0098, -0.0908,  0.0338],\n",
      "        [ 0.0111, -0.0067, -0.0292,  0.0391, -0.0385]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0427, -0.0812, -0.0381,  0.0009, -0.0093],\n",
      "        [ 0.0568,  0.0373,  0.0603, -0.0512,  0.0214],\n",
      "        [ 0.0554, -0.0092,  0.0808, -0.0504, -0.0296],\n",
      "        [-0.0110, -0.0039,  0.0736, -0.0351, -0.0249],\n",
      "        [-0.0232, -0.0019,  0.0398, -0.0575, -0.0059],\n",
      "        [ 0.0358, -0.0375, -0.1068,  0.0716, -0.0057],\n",
      "        [-0.0744, -0.0756, -0.1107,  0.0925, -0.0305],\n",
      "        [ 0.0241,  0.0123,  0.0363, -0.0207, -0.0237]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0228, grad_fn=<MinBackward1>), tensor(0.9552, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2912542223930359\n",
      "@sample 219: tensor([[ 0.0173,  0.0496,  0.0377, -0.0123,  0.0430],\n",
      "        [ 0.0179,  0.0344,  0.0314, -0.0300, -0.0120],\n",
      "        [ 0.0110, -0.0070, -0.0243,  0.0087, -0.0420],\n",
      "        [ 0.0052, -0.0453, -0.0075,  0.0085, -0.0080],\n",
      "        [-0.0128,  0.0408,  0.0049, -0.0708,  0.0207],\n",
      "        [-0.0414, -0.0225,  0.0045,  0.0622, -0.0755],\n",
      "        [ 0.0151, -0.0714, -0.0245, -0.0070, -0.0240],\n",
      "        [-0.0066, -0.0868, -0.0059, -0.0527,  0.0601]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0073, -0.0312, -0.0543,  0.0499, -0.0061],\n",
      "        [-0.0410, -0.0179, -0.0917,  0.1007, -0.0125],\n",
      "        [ 0.0070,  0.0388,  0.0124, -0.0404,  0.0700],\n",
      "        [ 0.0516,  0.0020,  0.0829, -0.0098,  0.0081],\n",
      "        [-0.0278, -0.0016, -0.0240,  0.0092, -0.0256],\n",
      "        [ 0.0303,  0.0340,  0.0803, -0.1009, -0.0053],\n",
      "        [ 0.0525,  0.0408,  0.0449,  0.0034, -0.0045],\n",
      "        [-0.0243, -0.0373,  0.0151,  0.0077, -0.0239]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0240, grad_fn=<MinBackward1>), tensor(0.9707, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.26214438676834106\n",
      "@sample 220: tensor([[ 0.0116,  0.0455,  0.0022, -0.0280,  0.0056],\n",
      "        [-0.0255,  0.0397,  0.0113, -0.0729,  0.0206],\n",
      "        [-0.0233, -0.0351, -0.0364,  0.0692,  0.0099],\n",
      "        [-0.0201,  0.1328, -0.0472, -0.0573,  0.0047],\n",
      "        [-0.0416,  0.0583,  0.0147, -0.0749, -0.0357],\n",
      "        [-0.0225, -0.0288, -0.0586,  0.0472, -0.0134],\n",
      "        [ 0.0185,  0.0056, -0.0009,  0.0264, -0.0258],\n",
      "        [ 0.0280, -0.0625, -0.0314,  0.0952, -0.0751]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0297, -0.0115, -0.0426,  0.0039,  0.0187],\n",
      "        [-0.0250, -0.0287, -0.0476,  0.0300,  0.0254],\n",
      "        [ 0.0403,  0.0186, -0.0009, -0.0466,  0.0841],\n",
      "        [-0.0080, -0.0027, -0.1121,  0.0134, -0.0105],\n",
      "        [-0.1093, -0.0592, -0.0538, -0.0337, -0.0967],\n",
      "        [ 0.0202,  0.0192,  0.0200, -0.0373,  0.0121],\n",
      "        [ 0.0302,  0.0168,  0.1029, -0.0345, -0.0242],\n",
      "        [ 0.0149,  0.0741,  0.0775, -0.0535,  0.0295]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0279, grad_fn=<MinBackward1>), tensor(0.9638, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2797251343727112\n",
      "@sample 221: tensor([[-0.0208,  0.0002,  0.0055,  0.0428, -0.0225],\n",
      "        [ 0.0123, -0.0346, -0.0412, -0.0028, -0.0378],\n",
      "        [-0.0194, -0.0128, -0.0417,  0.0181,  0.0006],\n",
      "        [ 0.0228,  0.0053,  0.0213, -0.1162, -0.0359],\n",
      "        [ 0.0236,  0.0232, -0.0079,  0.0378,  0.0287],\n",
      "        [-0.0236,  0.0474, -0.0308, -0.0097,  0.0185],\n",
      "        [-0.0623,  0.0463,  0.0154, -0.0868,  0.0108],\n",
      "        [ 0.0347, -0.0253, -0.0529,  0.0258, -0.0161]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0297,  0.0239,  0.0017, -0.0642, -0.0116],\n",
      "        [-0.0243, -0.0070,  0.0491, -0.0569,  0.0720],\n",
      "        [ 0.0434,  0.0581,  0.0276, -0.0410,  0.0247],\n",
      "        [-0.0447, -0.1053, -0.0056, -0.0272, -0.0334],\n",
      "        [ 0.0062,  0.0080,  0.0262,  0.0443, -0.0339],\n",
      "        [-0.0091, -0.0263, -0.0601,  0.0493,  0.0324],\n",
      "        [ 0.0108, -0.0801,  0.0479, -0.0398, -0.0030],\n",
      "        [-0.0421, -0.0025, -0.0285, -0.0019,  0.0710]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0224, grad_fn=<MinBackward1>), tensor(0.9696, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.26525425910949707\n",
      "@sample 222: tensor([[ 0.0594,  0.0128, -0.0284,  0.0128, -0.0645],\n",
      "        [ 0.0044, -0.0351, -0.0359,  0.0084, -0.0316],\n",
      "        [ 0.0054,  0.0541, -0.0281, -0.0976,  0.0286],\n",
      "        [-0.0401,  0.0059,  0.0178, -0.0584,  0.0154],\n",
      "        [-0.0069, -0.0210,  0.0174,  0.0262, -0.0127],\n",
      "        [-0.0298,  0.0863,  0.0568, -0.0043, -0.0006],\n",
      "        [ 0.0366,  0.0058,  0.0291, -0.0758, -0.0052],\n",
      "        [-0.0142,  0.0705,  0.0893, -0.1077,  0.0747]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0390, -0.0420,  0.0592, -0.0745, -0.0453],\n",
      "        [ 0.0515, -0.0096,  0.0987, -0.0819, -0.0540],\n",
      "        [ 0.0056, -0.0593, -0.0602,  0.0337, -0.0037],\n",
      "        [-0.0382, -0.0725, -0.0513,  0.0014,  0.0081],\n",
      "        [-0.0367, -0.0077,  0.0124, -0.0305, -0.0491],\n",
      "        [-0.0007, -0.0215, -0.0480, -0.0082, -0.0232],\n",
      "        [-0.0882, -0.0459, -0.0867,  0.0377, -0.0205],\n",
      "        [-0.0734, -0.0276, -0.1099,  0.0365,  0.0028]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0204, grad_fn=<MinBackward1>), tensor(0.9576, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.27854782342910767\n",
      "@sample 223: tensor([[-0.0389,  0.0142,  0.0370, -0.0623,  0.0266],\n",
      "        [ 0.0312, -0.0365, -0.0524,  0.0588, -0.0153],\n",
      "        [ 0.0020, -0.0012,  0.0198, -0.0185,  0.0631],\n",
      "        [ 0.0599, -0.0384, -0.0359,  0.0196, -0.0166],\n",
      "        [-0.0498,  0.0995,  0.0718, -0.0642,  0.0145],\n",
      "        [ 0.0108, -0.0412,  0.0232,  0.0206,  0.0058],\n",
      "        [ 0.0126,  0.0413, -0.0040,  0.0275, -0.0020],\n",
      "        [-0.0084,  0.0163, -0.0193, -0.0653, -0.0072]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0719, -0.0044, -0.0050,  0.0191,  0.0628],\n",
      "        [-0.0740,  0.0091,  0.0426, -0.0542, -0.0967],\n",
      "        [ 0.0372, -0.0848,  0.0100, -0.0181, -0.0619],\n",
      "        [-0.0057,  0.0471,  0.0962, -0.1009, -0.0658],\n",
      "        [-0.0539, -0.0175, -0.1043,  0.0532,  0.0501],\n",
      "        [ 0.0291,  0.0574,  0.0191,  0.0399,  0.0651],\n",
      "        [-0.0326,  0.0046, -0.1161,  0.0702, -0.0916],\n",
      "        [-0.0544, -0.0652, -0.0456,  0.0558, -0.0791]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0186, grad_fn=<MinBackward1>), tensor(0.9859, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2761434316635132\n",
      "@sample 224: tensor([[-0.0105,  0.0765,  0.0453, -0.1017,  0.0619],\n",
      "        [ 0.0296,  0.0010,  0.1014,  0.0121,  0.0401],\n",
      "        [ 0.0007,  0.0303,  0.0249, -0.0181,  0.0154],\n",
      "        [ 0.0149,  0.0332, -0.0069, -0.0372,  0.0394],\n",
      "        [-0.0189, -0.0345,  0.0109, -0.0222, -0.0061],\n",
      "        [-0.0381, -0.0575,  0.0377,  0.0221,  0.0326],\n",
      "        [-0.0101,  0.0677,  0.0936, -0.1079,  0.0424],\n",
      "        [-0.0017, -0.0678, -0.0003,  0.0006, -0.0023]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0346, -0.0523, -0.0588,  0.0847, -0.0176],\n",
      "        [ 0.0332, -0.0054, -0.0496,  0.0334,  0.0073],\n",
      "        [-0.0222, -0.0011, -0.0351,  0.0285, -0.0473],\n",
      "        [-0.0265, -0.0103, -0.0874,  0.1081, -0.0468],\n",
      "        [-0.0613, -0.0086, -0.0269, -0.0121, -0.0397],\n",
      "        [-0.0043, -0.0319,  0.0288, -0.0681, -0.0799],\n",
      "        [-0.0428, -0.0935, -0.0333,  0.1119, -0.0207],\n",
      "        [ 0.0128,  0.0398, -0.0040, -0.0368,  0.0183]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0173, grad_fn=<MinBackward1>), tensor(0.9780, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2730395197868347\n",
      "@sample 225: tensor([[ 0.0049, -0.0890, -0.1263,  0.0975, -0.0466],\n",
      "        [-0.0216,  0.0330,  0.0350, -0.0102,  0.0242],\n",
      "        [-0.0436, -0.0042,  0.0256, -0.0171,  0.0487],\n",
      "        [ 0.0024,  0.0027, -0.0098, -0.0233,  0.0616],\n",
      "        [-0.0279,  0.0399,  0.0241, -0.0270,  0.0204],\n",
      "        [-0.0072,  0.0029, -0.0930,  0.0653,  0.0193],\n",
      "        [-0.0112, -0.0589, -0.0022,  0.0124,  0.0506],\n",
      "        [-0.0622, -0.0015,  0.0310,  0.0483, -0.0238]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0590,  0.0224,  0.1545, -0.0516, -0.0068],\n",
      "        [ 0.0889, -0.0008,  0.0186, -0.0107,  0.0315],\n",
      "        [-0.0213, -0.0589, -0.0917,  0.0442,  0.0340],\n",
      "        [ 0.0303, -0.0528, -0.0872,  0.0073,  0.0122],\n",
      "        [ 0.0272, -0.0241,  0.0292, -0.0652,  0.0454],\n",
      "        [ 0.1228,  0.1516,  0.1241, -0.0935, -0.0304],\n",
      "        [-0.0271,  0.0437,  0.0229, -0.0154, -0.0107],\n",
      "        [ 0.0297,  0.0519, -0.0143, -0.0034,  0.0635]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0246, grad_fn=<MinBackward1>), tensor(0.9702, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2950783371925354\n",
      "@sample 226: tensor([[-0.0383,  0.0354,  0.0336, -0.0055,  0.0270],\n",
      "        [-0.0048,  0.0473, -0.0398,  0.0073, -0.0129],\n",
      "        [-0.0122, -0.0221, -0.0190,  0.0258, -0.0407],\n",
      "        [ 0.0122,  0.0371,  0.0235, -0.0623, -0.0009],\n",
      "        [ 0.0162,  0.0413, -0.0431,  0.0037,  0.0288],\n",
      "        [-0.0368,  0.0268, -0.0009, -0.0196,  0.0639],\n",
      "        [-0.0587, -0.0269, -0.0225, -0.0692,  0.0135],\n",
      "        [ 0.0252, -0.0601,  0.0118,  0.0482,  0.0112]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0196,  0.0354,  0.0132,  0.0002,  0.0186],\n",
      "        [-0.0439,  0.0003, -0.0771,  0.0171,  0.0097],\n",
      "        [ 0.0665,  0.0293,  0.0172, -0.0460,  0.0001],\n",
      "        [-0.0540, -0.0256, -0.0373,  0.0521, -0.0429],\n",
      "        [ 0.0206,  0.0549,  0.0100, -0.0129,  0.0023],\n",
      "        [-0.0241, -0.0383, -0.0860,  0.0540,  0.1116],\n",
      "        [-0.0492, -0.0910, -0.0305,  0.0336,  0.0468],\n",
      "        [ 0.0172,  0.0187, -0.0228,  0.0580,  0.0119]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0142, grad_fn=<MinBackward1>), tensor(0.9556, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2577199935913086\n",
      "@sample 227: tensor([[ 0.0250, -0.0105,  0.0131, -0.0054, -0.0021],\n",
      "        [-0.0265,  0.0326,  0.0360, -0.0186,  0.0492],\n",
      "        [ 0.0136,  0.0380, -0.0401, -0.0535, -0.0530],\n",
      "        [ 0.0307,  0.0703,  0.0248,  0.0180, -0.0090],\n",
      "        [-0.0240,  0.0075,  0.0076,  0.0013,  0.0489],\n",
      "        [-0.0351,  0.0294,  0.0302, -0.0332,  0.0194],\n",
      "        [-0.0265,  0.0142, -0.0011,  0.0078,  0.0308],\n",
      "        [-0.0270, -0.0618, -0.0022,  0.0441, -0.0475]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0326, -0.0010,  0.0047, -0.0026, -0.0017],\n",
      "        [ 0.0334, -0.0390, -0.0680,  0.0719,  0.0192],\n",
      "        [-0.0973, -0.1146, -0.0947,  0.0236, -0.0744],\n",
      "        [-0.0531, -0.0354, -0.0115,  0.0059,  0.0123],\n",
      "        [ 0.0536,  0.0769,  0.0307,  0.0018,  0.0442],\n",
      "        [-0.0297, -0.0232, -0.1083,  0.0449, -0.0204],\n",
      "        [-0.0809, -0.0179, -0.1227,  0.0997,  0.0937],\n",
      "        [-0.0172,  0.0435,  0.0445, -0.0330, -0.0360]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0232, grad_fn=<MinBackward1>), tensor(0.9763, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.26504799723625183\n",
      "@sample 228: tensor([[-0.0177,  0.0061,  0.0029,  0.0149, -0.0623],\n",
      "        [-0.0514, -0.0162,  0.0430, -0.0248,  0.0455],\n",
      "        [ 0.0268,  0.0618,  0.0360,  0.0021,  0.0235],\n",
      "        [-0.0269,  0.0726,  0.0248, -0.1358, -0.0068],\n",
      "        [-0.0347,  0.0183,  0.0475, -0.0245,  0.0462],\n",
      "        [-0.0863, -0.0222, -0.0029,  0.0082,  0.0476],\n",
      "        [ 0.0551,  0.0158, -0.0675,  0.0320, -0.0453],\n",
      "        [-0.0186, -0.0004, -0.0156, -0.0454,  0.0889]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0112,  0.0440,  0.0381, -0.0195,  0.0056],\n",
      "        [-0.0642, -0.0273, -0.0380, -0.0170,  0.1052],\n",
      "        [-0.0111,  0.1110, -0.0103, -0.0244, -0.0115],\n",
      "        [-0.0504, -0.0428,  0.0286,  0.0541, -0.0132],\n",
      "        [-0.0442, -0.0262, -0.0417,  0.0346, -0.0337],\n",
      "        [ 0.0026,  0.0413, -0.0214, -0.0147,  0.0114],\n",
      "        [ 0.0310,  0.0431,  0.0014, -0.0519, -0.0569],\n",
      "        [ 0.0120, -0.0890, -0.0286,  0.0469, -0.0354]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0247, grad_fn=<MinBackward1>), tensor(0.9690, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2767642140388489\n",
      "@sample 229: tensor([[ 0.0111,  0.0649,  0.0243, -0.0467, -0.0006],\n",
      "        [-0.0464,  0.0504,  0.0165, -0.0487, -0.0177],\n",
      "        [-0.0249, -0.0402,  0.0592, -0.0093,  0.0295],\n",
      "        [-0.0003,  0.0077,  0.0552, -0.0389,  0.0805],\n",
      "        [ 0.0283, -0.0450, -0.0355, -0.0698, -0.0033],\n",
      "        [-0.0110,  0.0776,  0.0941, -0.0395, -0.0052],\n",
      "        [ 0.0035, -0.0173,  0.0088, -0.0461, -0.0189],\n",
      "        [-0.0434,  0.0625, -0.0353, -0.0893,  0.0940]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0022, -0.0082,  0.0320, -0.0436, -0.0167],\n",
      "        [ 0.0363, -0.0373,  0.0028, -0.0300,  0.0205],\n",
      "        [-0.0564, -0.0015, -0.0200,  0.0098, -0.0207],\n",
      "        [-0.0371, -0.0070, -0.0990,  0.0898,  0.0257],\n",
      "        [ 0.0002, -0.0462,  0.0107, -0.0209,  0.0292],\n",
      "        [-0.0294, -0.0086, -0.0505,  0.0254, -0.0377],\n",
      "        [-0.0376,  0.0295,  0.0409,  0.0012,  0.0127],\n",
      "        [-0.0568, -0.0894, -0.0820,  0.0462, -0.0104]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0150, grad_fn=<MinBackward1>), tensor(0.9758, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.27600812911987305\n",
      "@sample 230: tensor([[-1.5696e-02,  6.6768e-02, -8.0976e-02, -6.0877e-02, -4.2540e-03],\n",
      "        [ 9.8779e-02, -2.6268e-02,  7.5840e-02, -5.3064e-02,  1.8158e-02],\n",
      "        [ 1.1127e-02, -2.1872e-02,  4.2564e-02,  1.4105e-02, -2.9160e-02],\n",
      "        [ 1.1122e-03,  2.8627e-02,  2.3987e-02, -3.1501e-02,  1.6242e-02],\n",
      "        [ 7.2286e-03, -6.4253e-03, -4.7801e-03,  2.6532e-02, -7.6256e-03],\n",
      "        [-1.4425e-02, -1.2684e-03, -3.6619e-02,  3.8174e-02,  2.0862e-07],\n",
      "        [-4.1404e-02, -1.2719e-03,  1.4855e-02, -2.4930e-02,  4.3821e-02],\n",
      "        [ 9.4557e-03,  2.7041e-02, -2.8243e-02, -2.2886e-02,  7.2769e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0194, -0.0281, -0.0052,  0.0033,  0.0075],\n",
      "        [-0.0967, -0.0471, -0.0900,  0.0911, -0.0523],\n",
      "        [-0.0144,  0.0021, -0.0254,  0.0077,  0.0093],\n",
      "        [-0.0257,  0.0638,  0.0575, -0.0261, -0.0077],\n",
      "        [ 0.0417,  0.0094,  0.0910, -0.0438, -0.0713],\n",
      "        [ 0.0203,  0.0219,  0.0270, -0.0299, -0.0269],\n",
      "        [-0.0781, -0.0311, -0.0824,  0.0494, -0.0225],\n",
      "        [-0.0230, -0.0622, -0.0739,  0.0010,  0.0134]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0182, grad_fn=<MinBackward1>), tensor(0.9531, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.25672245025634766\n",
      "@sample 231: tensor([[ 0.0247, -0.0276, -0.0117,  0.0146, -0.0430],\n",
      "        [-0.0130, -0.0336,  0.0014, -0.0026, -0.0029],\n",
      "        [ 0.0014,  0.0083, -0.0032, -0.0235, -0.0182],\n",
      "        [-0.0005,  0.0263,  0.0271, -0.0354,  0.0172],\n",
      "        [-0.0175, -0.0245,  0.0082,  0.0040,  0.0088],\n",
      "        [-0.0167, -0.0027,  0.0322,  0.0169, -0.0049],\n",
      "        [-0.0051,  0.0254,  0.0654, -0.0119,  0.0462],\n",
      "        [-0.0286,  0.0435,  0.0124, -0.0121, -0.0278]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0142,  0.0390,  0.0961, -0.0182, -0.0107],\n",
      "        [-0.0104,  0.0407,  0.1085, -0.0548, -0.0047],\n",
      "        [-0.0531, -0.0674, -0.0567, -0.0288,  0.1006],\n",
      "        [ 0.0240, -0.0053,  0.0443, -0.0562, -0.0398],\n",
      "        [-0.0151,  0.0478,  0.0875, -0.0290,  0.0129],\n",
      "        [-0.0335,  0.0876, -0.0032, -0.0215, -0.0014],\n",
      "        [ 0.0665, -0.0080, -0.1025, -0.0320,  0.0217],\n",
      "        [ 0.0359,  0.0485,  0.0343, -0.1016, -0.0138]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0272, grad_fn=<MinBackward1>), tensor(0.9644, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.253763347864151\n",
      "@sample 232: tensor([[-0.0111,  0.0394, -0.0353, -0.0541, -0.0019],\n",
      "        [-0.0050,  0.0019,  0.0205,  0.0817,  0.0242],\n",
      "        [ 0.0483,  0.0204, -0.0249, -0.0241,  0.0001],\n",
      "        [-0.0045,  0.0072, -0.0062, -0.0150, -0.0053],\n",
      "        [-0.0533,  0.0293,  0.0132,  0.0273,  0.0476],\n",
      "        [ 0.0360, -0.0581, -0.0246,  0.0929,  0.0014],\n",
      "        [ 0.0114,  0.0379, -0.0152, -0.0331,  0.0090],\n",
      "        [ 0.0091, -0.0412,  0.0041,  0.0900, -0.0566]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0122, -0.0053, -0.0997,  0.0942,  0.0165],\n",
      "        [ 0.0366,  0.0598, -0.0078,  0.0203, -0.0043],\n",
      "        [ 0.0043,  0.0356, -0.0237,  0.0032,  0.0394],\n",
      "        [-0.0065,  0.0075, -0.0223,  0.0465, -0.0072],\n",
      "        [ 0.0784,  0.0304, -0.0411, -0.0453,  0.0038],\n",
      "        [ 0.0302,  0.0072,  0.0025,  0.0392,  0.1011],\n",
      "        [-0.0408, -0.0251, -0.0435,  0.0662, -0.0314],\n",
      "        [ 0.0655, -0.0121,  0.0487,  0.0351,  0.0191]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0158, grad_fn=<MinBackward1>), tensor(0.9761, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2552953362464905\n",
      "@sample 233: tensor([[ 0.0375, -0.0533, -0.0086,  0.0671, -0.0111],\n",
      "        [ 0.0359, -0.0170,  0.0248,  0.0109,  0.0182],\n",
      "        [ 0.0117, -0.0350, -0.0159,  0.0788,  0.0270],\n",
      "        [ 0.0340, -0.0034,  0.0482,  0.0042, -0.0298],\n",
      "        [ 0.0003,  0.0026,  0.0010, -0.0550,  0.0501],\n",
      "        [ 0.0024, -0.1026,  0.0127,  0.1155, -0.0190],\n",
      "        [ 0.0198, -0.0220,  0.0047, -0.0563,  0.0599],\n",
      "        [ 0.0130, -0.0252,  0.0047,  0.0447,  0.0102]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0694,  0.1140, -0.0016, -0.0520,  0.0337],\n",
      "        [-0.0164,  0.0025,  0.0709, -0.0345, -0.0368],\n",
      "        [ 0.0301,  0.0769, -0.0947,  0.0209, -0.0130],\n",
      "        [-0.0478,  0.0383,  0.0295, -0.0245, -0.0302],\n",
      "        [ 0.0157, -0.0825, -0.1398,  0.0565, -0.0156],\n",
      "        [ 0.0352,  0.0646,  0.0696, -0.0554, -0.0050],\n",
      "        [-0.0241, -0.0880, -0.0537, -0.0491, -0.0315],\n",
      "        [-0.0141,  0.0918, -0.1052,  0.0628,  0.0284]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0180, grad_fn=<MinBackward1>), tensor(0.9637, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2856816053390503\n",
      "@sample 234: tensor([[-0.0169,  0.0828,  0.0198, -0.0237,  0.0348],\n",
      "        [ 0.0211,  0.0207, -0.0333,  0.0735, -0.0808],\n",
      "        [ 0.0026,  0.0009, -0.0453,  0.0257, -0.0214],\n",
      "        [ 0.0287,  0.0046, -0.0284,  0.0232, -0.0119],\n",
      "        [-0.0001,  0.0268,  0.0209, -0.0520,  0.0231],\n",
      "        [ 0.0245,  0.0076, -0.0134,  0.0017, -0.0070],\n",
      "        [-0.0067,  0.0137, -0.0053,  0.0348,  0.0153],\n",
      "        [-0.0018,  0.0436,  0.0159, -0.0144, -0.0148]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0144, -0.0335, -0.1023,  0.0576,  0.0028],\n",
      "        [ 0.0103,  0.0313,  0.0551, -0.0447, -0.0103],\n",
      "        [-0.0102, -0.0015,  0.0392, -0.0145, -0.0012],\n",
      "        [-0.0718,  0.0156,  0.0192, -0.1361, -0.0259],\n",
      "        [-0.0934, -0.0192, -0.1653,  0.0827,  0.0009],\n",
      "        [-0.0035, -0.0062, -0.0557,  0.0382, -0.0133],\n",
      "        [-0.0010, -0.0099, -0.0072, -0.0044, -0.0357],\n",
      "        [-0.0472,  0.0887, -0.0738,  0.0714, -0.0035]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0180, grad_fn=<MinBackward1>), tensor(0.9612, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.26395082473754883\n",
      "@sample 235: tensor([[-0.0202,  0.0423,  0.0435, -0.0114,  0.0489],\n",
      "        [-0.0070, -0.0257,  0.0568, -0.0420,  0.0135],\n",
      "        [ 0.0289, -0.0472, -0.0120,  0.0178,  0.0361],\n",
      "        [-0.0107, -0.0689, -0.0512,  0.0744, -0.0024],\n",
      "        [ 0.0143,  0.0481, -0.0577, -0.0509, -0.0083],\n",
      "        [ 0.0454,  0.0925, -0.0310, -0.0394,  0.0327],\n",
      "        [ 0.0086,  0.0190,  0.0120,  0.0156, -0.0068],\n",
      "        [ 0.0414, -0.0566, -0.0296, -0.0186,  0.0114]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0274,  0.0061, -0.1188,  0.0649,  0.0223],\n",
      "        [-0.0887, -0.0241, -0.0383, -0.0326,  0.0065],\n",
      "        [ 0.0175,  0.0067, -0.0335,  0.0369,  0.0570],\n",
      "        [ 0.0185,  0.0447,  0.0344, -0.0364,  0.0155],\n",
      "        [-0.0629, -0.0759, -0.1110,  0.0916, -0.0208],\n",
      "        [-0.0068, -0.0266,  0.0084,  0.0335, -0.0417],\n",
      "        [-0.0173,  0.0406,  0.0098,  0.0295,  0.0166],\n",
      "        [-0.0456, -0.0135, -0.0308,  0.0747,  0.0367]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0138, grad_fn=<MinBackward1>), tensor(0.9765, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.26676857471466064\n",
      "@sample 236: tensor([[ 0.0184, -0.0302, -0.0068,  0.0589, -0.0375],\n",
      "        [-0.0076, -0.0184, -0.0225, -0.0043,  0.0189],\n",
      "        [-0.0382,  0.0139, -0.0027, -0.0162,  0.0254],\n",
      "        [ 0.0228, -0.0184, -0.0342,  0.0464, -0.0279],\n",
      "        [ 0.0023,  0.0390,  0.0324,  0.0028, -0.0564],\n",
      "        [ 0.0093,  0.0831, -0.0032, -0.0581,  0.0527],\n",
      "        [ 0.0311, -0.0824, -0.0074,  0.0852, -0.0162],\n",
      "        [-0.0362,  0.0126,  0.0158,  0.0214,  0.0258]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0291,  0.0181,  0.0473,  0.0013, -0.0196],\n",
      "        [ 0.0208,  0.0094,  0.0060, -0.0487, -0.0375],\n",
      "        [ 0.0218,  0.0172, -0.0454, -0.0256,  0.0358],\n",
      "        [ 0.0203,  0.0957,  0.0593, -0.0044,  0.0075],\n",
      "        [-0.0536,  0.0150, -0.0119, -0.0140,  0.0148],\n",
      "        [-0.0244, -0.0391, -0.0040, -0.0310,  0.0332],\n",
      "        [ 0.0158,  0.0235,  0.0678, -0.0627, -0.0008],\n",
      "        [ 0.0379,  0.0619, -0.0577,  0.0113, -0.0712]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0180, grad_fn=<MinBackward1>), tensor(0.9721, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.25163915753364563\n",
      "@sample 237: tensor([[ 0.0120, -0.0617, -0.0072,  0.0416, -0.0108],\n",
      "        [ 0.0243, -0.0863, -0.0029,  0.0442, -0.0297],\n",
      "        [ 0.0214, -0.0889, -0.0499, -0.0176, -0.0114],\n",
      "        [-0.0086, -0.0256,  0.0095,  0.0833,  0.0090],\n",
      "        [-0.0049,  0.0400,  0.0048, -0.0106,  0.0111],\n",
      "        [-0.0086,  0.1265,  0.0343, -0.0364,  0.0423],\n",
      "        [-0.0211, -0.0371, -0.0127,  0.0655, -0.0513],\n",
      "        [ 0.0011, -0.0304, -0.0105,  0.0313, -0.0423]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0249,  0.0356,  0.0490,  0.0160, -0.0094],\n",
      "        [ 0.0002,  0.0225,  0.0354, -0.0614,  0.0146],\n",
      "        [ 0.0443, -0.0081, -0.0379, -0.0151, -0.0401],\n",
      "        [ 0.0238,  0.0488, -0.0441,  0.0025, -0.0235],\n",
      "        [-0.0887, -0.0334, -0.1340,  0.0336,  0.0081],\n",
      "        [-0.0210, -0.0234, -0.1022,  0.0852,  0.0015],\n",
      "        [ 0.0165,  0.0719,  0.0165, -0.0176,  0.0420],\n",
      "        [ 0.0234,  0.0776,  0.0046,  0.0318,  0.0015]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0232, grad_fn=<MinBackward1>), tensor(0.9829, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2730628252029419\n",
      "@sample 238: tensor([[-0.0112,  0.0572,  0.0361, -0.0005,  0.0181],\n",
      "        [-0.0049, -0.0032, -0.0145, -0.0584,  0.0425],\n",
      "        [-0.0150,  0.0259,  0.0232, -0.0756,  0.0417],\n",
      "        [ 0.0329, -0.0756, -0.0393,  0.0728, -0.0242],\n",
      "        [-0.0308,  0.0559, -0.0249, -0.0720,  0.0622],\n",
      "        [ 0.0167, -0.0009,  0.0462, -0.0240,  0.0061],\n",
      "        [ 0.0015, -0.0254, -0.0548, -0.0121,  0.0129],\n",
      "        [ 0.0063, -0.0585, -0.0991,  0.0778, -0.0411]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0173, -0.0173, -0.0437,  0.0272, -0.0063],\n",
      "        [-0.0230, -0.0244, -0.0448,  0.0371, -0.0183],\n",
      "        [ 0.0152, -0.0467, -0.0416,  0.1041, -0.1190],\n",
      "        [ 0.0461,  0.0803,  0.0379, -0.0138,  0.0142],\n",
      "        [-0.0527, -0.0768, -0.0321,  0.0022, -0.0107],\n",
      "        [-0.0014, -0.0018, -0.0714,  0.0343, -0.0044],\n",
      "        [ 0.0386,  0.0074,  0.0459, -0.0434, -0.0105],\n",
      "        [ 0.0293,  0.0416,  0.1718, -0.0581,  0.0149]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0221, grad_fn=<MinBackward1>), tensor(0.9815, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2861310839653015\n",
      "@sample 239: tensor([[-4.6861e-02,  9.5416e-03, -2.4410e-02, -2.8377e-02,  1.0570e-01],\n",
      "        [-3.2843e-02, -5.0194e-03,  2.7854e-03,  2.8614e-03,  4.0665e-03],\n",
      "        [-3.8156e-03,  6.7370e-02, -2.1083e-02, -2.5758e-02,  5.9100e-02],\n",
      "        [-1.6499e-02,  9.7861e-03, -9.6240e-03,  3.0671e-02,  2.2843e-02],\n",
      "        [-3.5375e-02,  1.1220e-02,  5.4587e-02,  2.7511e-02,  3.8524e-02],\n",
      "        [ 4.5069e-03,  1.5069e-02,  1.4656e-02, -7.1733e-02,  1.6198e-02],\n",
      "        [-4.3603e-02,  5.6770e-02,  2.3938e-02, -7.1600e-02,  1.9692e-05],\n",
      "        [ 3.4503e-02, -7.3677e-02, -6.7118e-02,  5.4898e-02, -6.2660e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0468, -0.0745, -0.0214, -0.0076, -0.0345],\n",
      "        [ 0.0566,  0.0248,  0.0083,  0.0005, -0.0178],\n",
      "        [ 0.0273, -0.0501, -0.0768,  0.0566,  0.0215],\n",
      "        [ 0.0525, -0.0053, -0.0163, -0.0177, -0.0597],\n",
      "        [ 0.0832,  0.0596, -0.0039, -0.0208,  0.0538],\n",
      "        [-0.0439, -0.0802, -0.0098,  0.0303, -0.0035],\n",
      "        [ 0.0468, -0.0406,  0.0613,  0.0294,  0.0192],\n",
      "        [-0.0121,  0.0176, -0.0128, -0.0119, -0.0235]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0121, grad_fn=<MinBackward1>), tensor(0.9497, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2539616823196411\n",
      "@sample 240: tensor([[ 0.0443,  0.0239, -0.0227, -0.0153,  0.0346],\n",
      "        [-0.0540, -0.0166, -0.0015, -0.0045,  0.0014],\n",
      "        [ 0.0166, -0.0387,  0.0337,  0.0052, -0.0167],\n",
      "        [-0.0054,  0.0036,  0.0044, -0.0152, -0.0069],\n",
      "        [ 0.0078, -0.0198,  0.0029,  0.0131,  0.0209],\n",
      "        [ 0.0075,  0.0638,  0.0011, -0.0241,  0.0039],\n",
      "        [-0.0559, -0.0441, -0.0241,  0.0458, -0.0264],\n",
      "        [-0.0140, -0.0784, -0.0277, -0.0501,  0.0428]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0051,  0.0209, -0.0093, -0.0822, -0.0674],\n",
      "        [-0.0629,  0.0205, -0.0030, -0.0178,  0.0078],\n",
      "        [ 0.0171, -0.0135,  0.0380, -0.0408,  0.0086],\n",
      "        [-0.0190,  0.0159, -0.0082,  0.0241, -0.0114],\n",
      "        [-0.0134,  0.0035, -0.0127,  0.0081, -0.0255],\n",
      "        [-0.0454,  0.0192, -0.0867,  0.1079,  0.0218],\n",
      "        [-0.0263,  0.0029,  0.0925, -0.0459,  0.0111],\n",
      "        [ 0.0206, -0.0824, -0.0120, -0.0419, -0.0087]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0053, grad_fn=<MinBackward1>), tensor(0.9618, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.24232220649719238\n",
      "@sample 241: tensor([[-0.0433, -0.0280, -0.0323, -0.0555,  0.0465],\n",
      "        [ 0.0147, -0.0181,  0.0131,  0.0706, -0.0232],\n",
      "        [-0.0128,  0.0060, -0.0587,  0.0015,  0.0007],\n",
      "        [ 0.0332,  0.0045, -0.0177, -0.0018,  0.0226],\n",
      "        [-0.0012,  0.0509,  0.0221, -0.0314, -0.0469],\n",
      "        [ 0.0034,  0.0732, -0.0030, -0.0621,  0.0688],\n",
      "        [ 0.0390, -0.0470, -0.0209, -0.0099,  0.0322],\n",
      "        [ 0.0172,  0.0213, -0.0519,  0.0039,  0.0416]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0079, -0.0550, -0.0143, -0.0295,  0.0503],\n",
      "        [-0.0463, -0.0240, -0.0120, -0.0061,  0.0102],\n",
      "        [ 0.0693, -0.0100,  0.0422, -0.0722, -0.0781],\n",
      "        [ 0.0103, -0.0299, -0.0093,  0.0506,  0.0346],\n",
      "        [-0.0456, -0.0266, -0.1100,  0.0588, -0.0138],\n",
      "        [-0.0668, -0.0536, -0.0661,  0.0261, -0.0390],\n",
      "        [-0.0090, -0.0104,  0.0038, -0.0012, -0.0107],\n",
      "        [-0.0086,  0.0803, -0.0325, -0.0015,  0.1007]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0181, grad_fn=<MinBackward1>), tensor(0.9590, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2569800019264221\n",
      "@sample 242: tensor([[ 0.0039, -0.0155, -0.0481, -0.0096,  0.0159],\n",
      "        [ 0.0223,  0.0022,  0.0145,  0.0255, -0.0138],\n",
      "        [-0.0094, -0.0306,  0.0294,  0.0545, -0.0483],\n",
      "        [-0.0186, -0.0003,  0.0316,  0.0052,  0.0050],\n",
      "        [ 0.0025, -0.0530,  0.0891, -0.0644,  0.0670],\n",
      "        [ 0.0083, -0.0202, -0.0197,  0.0097, -0.0230],\n",
      "        [-0.0141, -0.0191,  0.0102,  0.1019,  0.0334],\n",
      "        [-0.0320, -0.0500,  0.0402, -0.0634,  0.0915]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0344, -0.0214,  0.0079, -0.0161,  0.0285],\n",
      "        [ 0.0045, -0.0037,  0.0014, -0.0038, -0.0047],\n",
      "        [ 0.0597,  0.0715,  0.0919, -0.0773, -0.0046],\n",
      "        [-0.0035,  0.0433,  0.0029,  0.0003, -0.0004],\n",
      "        [-0.0631, -0.0774, -0.0802,  0.0217, -0.0259],\n",
      "        [ 0.0209,  0.0377,  0.0389,  0.0848,  0.0594],\n",
      "        [ 0.0161,  0.0860,  0.0208, -0.0414,  0.0047],\n",
      "        [ 0.0366, -0.0724, -0.0411,  0.0180,  0.0294]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0207, grad_fn=<MinBackward1>), tensor(0.9730, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2698363959789276\n",
      "@sample 243: tensor([[ 0.0070,  0.0110,  0.0121,  0.0211, -0.0558],\n",
      "        [-0.0128, -0.0083,  0.0226,  0.0088, -0.0157],\n",
      "        [-0.0228,  0.0127, -0.0290, -0.0571,  0.0025],\n",
      "        [-0.0226, -0.0111,  0.0073,  0.0343, -0.0018],\n",
      "        [-0.0270,  0.0232,  0.0612, -0.0208, -0.0159],\n",
      "        [-0.0163, -0.0553, -0.0047,  0.0373, -0.0052],\n",
      "        [ 0.0084,  0.0577,  0.0422, -0.0869,  0.0273],\n",
      "        [-0.0137, -0.0140, -0.0388,  0.0475,  0.0049]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0163,  0.0883,  0.0500, -0.0156,  0.0329],\n",
      "        [-0.0028, -0.0020,  0.0856, -0.0716, -0.0492],\n",
      "        [-0.1091, -0.0984, -0.0840,  0.0447, -0.0158],\n",
      "        [-0.0206, -0.0314,  0.0006, -0.0138,  0.0073],\n",
      "        [-0.0094,  0.0005, -0.0356,  0.0030, -0.0425],\n",
      "        [ 0.0043, -0.0792,  0.0409, -0.0466, -0.0077],\n",
      "        [-0.0650, -0.0084,  0.0004,  0.1093, -0.0638],\n",
      "        [-0.0239,  0.0379,  0.1061, -0.0940, -0.0525]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0145, grad_fn=<MinBackward1>), tensor(0.9719, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.25990745425224304\n",
      "@sample 244: tensor([[ 0.0161,  0.0158, -0.0024,  0.0157,  0.0075],\n",
      "        [-0.0643,  0.0342,  0.0384, -0.0258,  0.0288],\n",
      "        [-0.0182,  0.0707,  0.0202, -0.0128,  0.0234],\n",
      "        [-0.0101,  0.1011,  0.0608, -0.0056,  0.0260],\n",
      "        [-0.0356,  0.0408,  0.0229, -0.0304,  0.0176],\n",
      "        [-0.0474, -0.0585,  0.0187, -0.0579,  0.0303],\n",
      "        [ 0.0461, -0.0379, -0.0353,  0.0248, -0.0681],\n",
      "        [-0.0077, -0.0215, -0.0357, -0.0048, -0.0674]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0436, -0.0093,  0.0209, -0.0080, -0.0461],\n",
      "        [ 0.0779,  0.0069, -0.0517, -0.0319, -0.0587],\n",
      "        [-0.0306, -0.0315, -0.0383,  0.0019, -0.0378],\n",
      "        [-0.0813,  0.0317, -0.0536,  0.0012, -0.0225],\n",
      "        [-0.0173, -0.0305, -0.0503,  0.0476,  0.0067],\n",
      "        [-0.0280, -0.0757, -0.0677, -0.0221,  0.0413],\n",
      "        [ 0.0543, -0.0145,  0.0308, -0.0708,  0.0117],\n",
      "        [ 0.0157,  0.0020,  0.0774, -0.0640,  0.0183]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0137, grad_fn=<MinBackward1>), tensor(0.9786, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.25278741121292114\n",
      "@sample 245: tensor([[ 0.0099, -0.0109,  0.0524,  0.0194, -0.0099],\n",
      "        [ 0.0248,  0.0583,  0.0031, -0.0258,  0.0698],\n",
      "        [-0.0369,  0.0618,  0.0072,  0.0156,  0.0001],\n",
      "        [-0.0051, -0.0069, -0.0049, -0.0280,  0.0520],\n",
      "        [-0.0161, -0.0511, -0.0267, -0.0183, -0.0072],\n",
      "        [-0.0145,  0.0075,  0.0360, -0.0072, -0.0303],\n",
      "        [-0.0722,  0.0433,  0.0584, -0.0187, -0.0429],\n",
      "        [-0.0112, -0.0208,  0.0482, -0.0158,  0.0098]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0317, -0.0274, -0.0738,  0.0654, -0.0231],\n",
      "        [-0.0217, -0.0666, -0.0631,  0.0583,  0.0377],\n",
      "        [ 0.0160, -0.0031,  0.0148, -0.0548, -0.0048],\n",
      "        [-0.0267, -0.0733, -0.0611, -0.0139, -0.0383],\n",
      "        [-0.0679, -0.0274, -0.0324,  0.0668,  0.1091],\n",
      "        [-0.0145,  0.0161, -0.0027,  0.0190, -0.0507],\n",
      "        [ 0.0109,  0.0250,  0.0420,  0.0396, -0.0724],\n",
      "        [ 0.0057, -0.0134,  0.0322,  0.0076, -0.0397]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0216, grad_fn=<MinBackward1>), tensor(0.9546, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.25449904799461365\n",
      "@sample 246: tensor([[-0.0459, -0.0305,  0.0239, -0.0531,  0.0143],\n",
      "        [-0.0169,  0.0207,  0.0231,  0.0048,  0.0084],\n",
      "        [-0.0597, -0.0200, -0.0151, -0.0193,  0.0011],\n",
      "        [-0.0502,  0.1086, -0.0079, -0.0924,  0.0010],\n",
      "        [-0.0184, -0.0187,  0.0595,  0.0301,  0.0090],\n",
      "        [-0.0204,  0.0876,  0.0287, -0.0622,  0.0603],\n",
      "        [ 0.0247,  0.0403,  0.0412, -0.0091,  0.0122],\n",
      "        [-0.0388,  0.0419, -0.0235, -0.0312, -0.0347]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0796, -0.0061, -0.0643, -0.0355,  0.0161],\n",
      "        [-0.0121, -0.0153, -0.0233,  0.0290,  0.0019],\n",
      "        [ 0.0525, -0.0039,  0.0256,  0.0195, -0.0178],\n",
      "        [-0.0207,  0.0307,  0.0672, -0.0824,  0.0034],\n",
      "        [ 0.0620,  0.0399,  0.0652, -0.0656, -0.0263],\n",
      "        [ 0.0104, -0.0448, -0.0670,  0.0283,  0.0002],\n",
      "        [ 0.0156, -0.0016, -0.0481,  0.1011, -0.0473],\n",
      "        [-0.1004, -0.0298, -0.0397,  0.0153, -0.0133]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0195, grad_fn=<MinBackward1>), tensor(0.9597, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2629278302192688\n",
      "@sample 247: tensor([[ 0.0071,  0.0706,  0.0295,  0.0087,  0.0266],\n",
      "        [-0.0043, -0.0062,  0.0037,  0.0243, -0.0270],\n",
      "        [-0.0254,  0.0224, -0.0126, -0.0928,  0.0135],\n",
      "        [-0.0020,  0.0423,  0.0350, -0.0643,  0.0222],\n",
      "        [ 0.1058, -0.0101,  0.0003, -0.0071, -0.0084],\n",
      "        [-0.0342, -0.0529,  0.0261,  0.0294, -0.0549],\n",
      "        [ 0.0082,  0.0216,  0.0202, -0.0223, -0.0069],\n",
      "        [-0.0070, -0.0176, -0.0014,  0.0763, -0.0210]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0179,  0.0306, -0.0538, -0.0104, -0.0071],\n",
      "        [-0.0041,  0.0053,  0.0253, -0.0271,  0.0451],\n",
      "        [-0.0460, -0.0964,  0.0224,  0.0662, -0.0369],\n",
      "        [-0.0422, -0.0486, -0.0752,  0.0180, -0.0101],\n",
      "        [-0.0257,  0.0110,  0.0718, -0.0083,  0.0017],\n",
      "        [ 0.0079,  0.0018,  0.0942, -0.0456, -0.0325],\n",
      "        [-0.0285, -0.0301, -0.0614,  0.0420,  0.0228],\n",
      "        [ 0.0448,  0.0406,  0.0252, -0.0366,  0.0258]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0179, grad_fn=<MinBackward1>), tensor(0.9849, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.25092026591300964\n",
      "@sample 248: tensor([[ 0.0183,  0.0085, -0.0225, -0.0236, -0.0020],\n",
      "        [-0.0003,  0.1351,  0.0702, -0.0983,  0.0363],\n",
      "        [ 0.0102, -0.0346,  0.0541, -0.0502, -0.0221],\n",
      "        [-0.0118,  0.0071, -0.0210,  0.0285,  0.0168],\n",
      "        [ 0.0192,  0.0412, -0.0433,  0.0259, -0.0168],\n",
      "        [-0.0171,  0.0356,  0.1346, -0.1286,  0.0384],\n",
      "        [ 0.0014, -0.0052,  0.0558, -0.0040, -0.0218],\n",
      "        [ 0.0371,  0.0329,  0.0040, -0.0373,  0.0081]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0363, -0.0316,  0.0065,  0.0235,  0.0015],\n",
      "        [-0.0076, -0.0031, -0.1580,  0.1404,  0.0347],\n",
      "        [-0.0263, -0.1301, -0.0317, -0.0031, -0.0986],\n",
      "        [ 0.0243,  0.0441,  0.0571,  0.0396,  0.0436],\n",
      "        [ 0.0150,  0.0254,  0.0044,  0.0243, -0.0021],\n",
      "        [-0.0348, -0.0430, -0.0796,  0.0155, -0.0547],\n",
      "        [ 0.0408,  0.0734, -0.0296, -0.0423, -0.0237],\n",
      "        [-0.0273, -0.0025, -0.0576,  0.0011, -0.0378]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0211, grad_fn=<MinBackward1>), tensor(0.9847, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2814980745315552\n",
      "@sample 249: tensor([[-0.0096,  0.0730, -0.0232, -0.0508,  0.0136],\n",
      "        [-0.0113, -0.0119, -0.0589,  0.0396,  0.0604],\n",
      "        [ 0.0037, -0.0357, -0.0205,  0.0702, -0.0759],\n",
      "        [-0.0392,  0.0360, -0.0249, -0.0370, -0.0159],\n",
      "        [-0.0735,  0.0582,  0.0422, -0.0271,  0.0086],\n",
      "        [-0.0340,  0.0309, -0.0110,  0.0148,  0.0206],\n",
      "        [-0.0070, -0.0288,  0.0193, -0.0120,  0.0099],\n",
      "        [-0.0140, -0.0206, -0.0074,  0.0415, -0.0096]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0346, -0.0142, -0.0848,  0.0380, -0.0671],\n",
      "        [ 0.0507, -0.0167, -0.0657,  0.0142, -0.0202],\n",
      "        [ 0.0123,  0.0287,  0.0343, -0.0345,  0.0002],\n",
      "        [ 0.0224, -0.0292,  0.0011,  0.0186,  0.0414],\n",
      "        [-0.1142, -0.0609, -0.1113,  0.0477,  0.0448],\n",
      "        [-0.0013, -0.0409, -0.0827,  0.0357,  0.0914],\n",
      "        [-0.0680, -0.0101,  0.0961, -0.0475,  0.0270],\n",
      "        [ 0.0418, -0.0089,  0.0355, -0.0839, -0.0449]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0180, grad_fn=<MinBackward1>), tensor(0.9495, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2677551507949829\n",
      "@sample 250: tensor([[ 0.0542, -0.0384,  0.0218, -0.0589,  0.0646],\n",
      "        [ 0.0384, -0.0264,  0.0592,  0.0048, -0.0709],\n",
      "        [ 0.0189,  0.0403,  0.0333, -0.0646,  0.0502],\n",
      "        [-0.0078,  0.0405, -0.0127,  0.0273, -0.0205],\n",
      "        [-0.0218, -0.0193,  0.0035,  0.0433,  0.0182],\n",
      "        [-0.0131,  0.0314, -0.0031, -0.0386, -0.0115],\n",
      "        [-0.0232,  0.0311, -0.0264, -0.0391,  0.0016],\n",
      "        [-0.0205,  0.0209, -0.0235,  0.0038,  0.0093]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0129, -0.0555, -0.0555,  0.0036, -0.0142],\n",
      "        [-0.0348, -0.0168,  0.0480,  0.0156,  0.0089],\n",
      "        [ 0.0441,  0.0146, -0.0050, -0.0136, -0.0104],\n",
      "        [ 0.0640,  0.0649, -0.0087,  0.0493,  0.0235],\n",
      "        [ 0.0368,  0.0023,  0.0267, -0.0246, -0.0114],\n",
      "        [-0.0402, -0.0107, -0.0725, -0.0112,  0.0403],\n",
      "        [-0.0069, -0.0243, -0.0304,  0.0033,  0.0009],\n",
      "        [-0.0076, -0.0221,  0.0069, -0.0061,  0.0235]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0218, grad_fn=<MinBackward1>), tensor(0.9761, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.23959201574325562\n",
      "@sample 251: tensor([[-0.0084, -0.0085, -0.0149, -0.0009,  0.0493],\n",
      "        [-0.0307, -0.0108,  0.0259, -0.0111, -0.0497],\n",
      "        [ 0.0027,  0.0397, -0.0431, -0.0174,  0.0419],\n",
      "        [ 0.0436, -0.0328, -0.0141,  0.0201, -0.0124],\n",
      "        [ 0.0511, -0.0209,  0.0166, -0.0222, -0.0356],\n",
      "        [-0.0361,  0.0851,  0.0159, -0.0249, -0.0106],\n",
      "        [-0.0339,  0.0337, -0.0146, -0.0263, -0.0185],\n",
      "        [ 0.0204,  0.0189, -0.0323, -0.0109,  0.0305]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 8.9566e-03,  1.5240e-02,  8.0540e-03, -4.1980e-03, -1.7066e-03],\n",
      "        [-9.0767e-02,  1.5445e-05,  1.3085e-02,  1.8485e-02, -3.7912e-03],\n",
      "        [ 8.3229e-03, -7.3079e-03,  2.5554e-03, -2.8587e-02, -4.3616e-02],\n",
      "        [ 4.3168e-03,  4.9891e-02, -1.0978e-02,  1.2368e-02,  1.2878e-02],\n",
      "        [-7.5777e-03, -2.6962e-02,  1.7164e-02,  1.0377e-02, -1.8321e-02],\n",
      "        [-6.0499e-02, -1.8510e-02, -1.0658e-01,  8.4375e-02,  5.4550e-02],\n",
      "        [ 3.8624e-02, -2.7658e-02,  3.9782e-02,  3.0407e-02,  4.1191e-02],\n",
      "        [ 1.3250e-02, -2.7191e-02, -7.5197e-02,  4.1527e-02,  3.5058e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0144, grad_fn=<MinBackward1>), tensor(0.9492, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.23554998636245728\n",
      "@sample 252: tensor([[-0.0142,  0.0183, -0.0321, -0.1097,  0.1115],\n",
      "        [ 0.0294, -0.0158, -0.0048,  0.0533, -0.0489],\n",
      "        [-0.0107, -0.0379, -0.0402, -0.0278,  0.0211],\n",
      "        [-0.0306,  0.0493, -0.0065, -0.0597,  0.0110],\n",
      "        [-0.0471,  0.0398, -0.0273, -0.0461,  0.0035],\n",
      "        [ 0.0267,  0.0674,  0.0345, -0.0609,  0.0119],\n",
      "        [ 0.0126,  0.0189, -0.0019, -0.0426, -0.0131],\n",
      "        [ 0.0048,  0.0705,  0.0063,  0.0043,  0.0073]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0054, -0.0529,  0.0322,  0.0548, -0.0271],\n",
      "        [ 0.0189, -0.0271,  0.0430, -0.1048, -0.0071],\n",
      "        [-0.0338, -0.0005,  0.0418,  0.0114, -0.0380],\n",
      "        [ 0.0336, -0.0560, -0.0455,  0.0742, -0.0134],\n",
      "        [ 0.0295, -0.0085, -0.0289,  0.0267,  0.0091],\n",
      "        [-0.1287, -0.0460, -0.0783,  0.1127,  0.0402],\n",
      "        [ 0.0102, -0.0131, -0.0065, -0.0258, -0.0346],\n",
      "        [-0.0330, -0.0400, -0.0123, -0.0262, -0.0437]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0188, grad_fn=<MinBackward1>), tensor(0.9622, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2685732841491699\n",
      "@sample 253: tensor([[-0.0044, -0.0397,  0.0351,  0.0426,  0.0003],\n",
      "        [-0.0525, -0.0021,  0.0527,  0.0164, -0.0411],\n",
      "        [-0.0594, -0.0117, -0.0332, -0.0440, -0.0052],\n",
      "        [-0.0001,  0.0024,  0.0154, -0.0279,  0.0048],\n",
      "        [ 0.0132, -0.0093, -0.0281,  0.0573,  0.0302],\n",
      "        [ 0.0229, -0.1071, -0.0415,  0.0467, -0.0262],\n",
      "        [-0.0243, -0.0051, -0.0544,  0.0514, -0.0337],\n",
      "        [ 0.0242, -0.0704, -0.0306,  0.0483, -0.0292]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0244, -0.0221, -0.0574, -0.0024,  0.0146],\n",
      "        [-0.0332, -0.0400,  0.0106,  0.0017, -0.0882],\n",
      "        [-0.0697, -0.0426, -0.0374, -0.0068, -0.0184],\n",
      "        [ 0.0093, -0.0489, -0.0600,  0.0664, -0.0272],\n",
      "        [ 0.0554,  0.0640,  0.0887, -0.0844,  0.0041],\n",
      "        [ 0.0394, -0.0146,  0.0591, -0.1162, -0.0366],\n",
      "        [-0.0200,  0.0136, -0.0162, -0.0757, -0.0131],\n",
      "        [-0.0020, -0.0005,  0.0708, -0.0269, -0.0005]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0148, grad_fn=<MinBackward1>), tensor(0.9720, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2598305344581604\n",
      "@sample 254: tensor([[-0.0081, -0.0442, -0.0348,  0.1008, -0.0936],\n",
      "        [-0.0160, -0.0166,  0.0172,  0.0016, -0.0075],\n",
      "        [ 0.0278,  0.0349, -0.0012,  0.0309,  0.0200],\n",
      "        [-0.0056,  0.0152, -0.0182,  0.0477, -0.0322],\n",
      "        [ 0.0418,  0.0080, -0.0012,  0.0536, -0.0292],\n",
      "        [ 0.0074,  0.0493,  0.0703, -0.0155, -0.0248],\n",
      "        [ 0.0514,  0.0331,  0.0293, -0.0380,  0.0086],\n",
      "        [ 0.0547, -0.0700, -0.0275,  0.1550, -0.1031]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0732,  0.0743,  0.1118, -0.0368,  0.0188],\n",
      "        [-0.0648, -0.0286, -0.0561, -0.0055,  0.0522],\n",
      "        [ 0.0217, -0.0467, -0.0068,  0.0062,  0.0190],\n",
      "        [ 0.0722,  0.0536,  0.0400,  0.0166,  0.0838],\n",
      "        [ 0.0397,  0.0125, -0.0091, -0.0026,  0.0369],\n",
      "        [ 0.0023,  0.0020, -0.0354,  0.0186, -0.0282],\n",
      "        [-0.0596, -0.0132, -0.1000,  0.0966,  0.0849],\n",
      "        [ 0.0732,  0.1393,  0.0821,  0.0290,  0.0216]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0095, grad_fn=<MinBackward1>), tensor(0.9740, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.28567928075790405\n",
      "@sample 255: tensor([[-0.0016,  0.0320, -0.0396, -0.0279,  0.0081],\n",
      "        [-0.0259,  0.0050,  0.0073,  0.0079,  0.0157],\n",
      "        [ 0.0265, -0.0700, -0.0497,  0.0288, -0.0391],\n",
      "        [ 0.0206,  0.0147, -0.0284,  0.0125, -0.0369],\n",
      "        [-0.0053,  0.0851,  0.0131, -0.0376,  0.0431],\n",
      "        [ 0.0326, -0.0149, -0.0321, -0.0002, -0.0010],\n",
      "        [ 0.0065,  0.0109, -0.0261,  0.0339, -0.0146],\n",
      "        [ 0.0074,  0.0270,  0.0002,  0.0261, -0.0096]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0285, -0.0424,  0.0086,  0.0963, -0.0228],\n",
      "        [ 0.0326,  0.0618,  0.1083, -0.1466, -0.0507],\n",
      "        [ 0.0148, -0.0364, -0.0065, -0.0397, -0.0490],\n",
      "        [ 0.0280, -0.0071,  0.0782, -0.0542, -0.0304],\n",
      "        [-0.0205, -0.0576, -0.1241,  0.0453, -0.0057],\n",
      "        [ 0.0427,  0.0189,  0.0037, -0.0326, -0.0184],\n",
      "        [ 0.0202,  0.0036, -0.0434,  0.0174,  0.0053],\n",
      "        [ 0.0060,  0.0328, -0.0250, -0.0330,  0.0291]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0161, grad_fn=<MinBackward1>), tensor(0.9640, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.25556159019470215\n",
      "@sample 256: tensor([[ 0.0308, -0.0134,  0.0196, -0.0869,  0.0259],\n",
      "        [-0.0129, -0.0721, -0.0629,  0.0594, -0.0368],\n",
      "        [ 0.0132, -0.0043,  0.0267, -0.0042, -0.0050],\n",
      "        [ 0.0404,  0.0143,  0.0475, -0.0217,  0.0194],\n",
      "        [ 0.0066,  0.0206,  0.0570, -0.0162,  0.0472],\n",
      "        [-0.0077,  0.0530,  0.0114, -0.0535,  0.0096],\n",
      "        [ 0.0541,  0.0033,  0.0461,  0.0193,  0.0157],\n",
      "        [ 0.0292, -0.0232, -0.0504,  0.0222, -0.0359]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0401, -0.1275, -0.0983, -0.0297, -0.0569],\n",
      "        [ 0.0633,  0.0519,  0.1257, -0.0118, -0.0224],\n",
      "        [-0.0417,  0.0103, -0.0269,  0.0230, -0.0105],\n",
      "        [-0.0109, -0.0421, -0.1120,  0.0348, -0.0324],\n",
      "        [-0.0118,  0.0816,  0.0284, -0.0111,  0.0263],\n",
      "        [-0.0029, -0.0507, -0.0017,  0.0182,  0.0289],\n",
      "        [ 0.0352,  0.0127,  0.0812, -0.0134,  0.0098],\n",
      "        [ 0.0011,  0.0456,  0.0669,  0.0246,  0.0050]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0131, grad_fn=<MinBackward1>), tensor(0.9696, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2533063292503357\n",
      "@sample 257: tensor([[-0.0231,  0.0046, -0.0106,  0.0279, -0.0315],\n",
      "        [ 0.0003,  0.0369, -0.0288, -0.0097,  0.0049],\n",
      "        [ 0.0396,  0.0135, -0.0190, -0.0060,  0.0224],\n",
      "        [ 0.0067, -0.0562, -0.0006,  0.0092,  0.0076],\n",
      "        [ 0.0525, -0.0727, -0.0882,  0.0479,  0.0009],\n",
      "        [-0.0235, -0.0569, -0.0284, -0.0319, -0.0256],\n",
      "        [ 0.0037,  0.0530,  0.0331, -0.0103, -0.0270],\n",
      "        [ 0.0376, -0.0373,  0.0071, -0.0198, -0.0037]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0312,  0.0417,  0.0813, -0.0148,  0.0121],\n",
      "        [ 0.0390,  0.0094,  0.0766,  0.0070,  0.0183],\n",
      "        [-0.0166, -0.0263, -0.0904,  0.0756, -0.0141],\n",
      "        [ 0.0147, -0.0022,  0.0031,  0.0397,  0.0111],\n",
      "        [ 0.0344,  0.0374,  0.0424, -0.0284,  0.0401],\n",
      "        [-0.0511, -0.0476,  0.0681,  0.0564,  0.0252],\n",
      "        [ 0.0165,  0.0596,  0.0506,  0.0318, -0.0179],\n",
      "        [-0.0267, -0.0014, -0.0028,  0.0163,  0.0075]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0188, grad_fn=<MinBackward1>), tensor(0.9686, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.24066318571567535\n",
      "@sample 258: tensor([[ 0.0191, -0.1325,  0.0246,  0.0977, -0.0475],\n",
      "        [ 0.0044, -0.0374, -0.0631,  0.0305,  0.0204],\n",
      "        [ 0.0384,  0.0396, -0.0419, -0.0352,  0.0362],\n",
      "        [-0.0260, -0.0029,  0.0041,  0.0113, -0.0320],\n",
      "        [-0.0543, -0.0214,  0.0325,  0.0484,  0.0387],\n",
      "        [ 0.0085,  0.0546,  0.0035, -0.0479,  0.0414],\n",
      "        [ 0.0156, -0.0485,  0.0244, -0.0601,  0.0415],\n",
      "        [ 0.0028, -0.0204, -0.0113,  0.0140, -0.0114]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0202, -0.0272,  0.1100,  0.0071, -0.0344],\n",
      "        [ 0.0586,  0.0559,  0.0517, -0.0416, -0.0111],\n",
      "        [ 0.0121, -0.0209, -0.0876,  0.0692,  0.0572],\n",
      "        [-0.0089,  0.0052, -0.0086, -0.0651, -0.0007],\n",
      "        [-0.0142,  0.0523, -0.0807,  0.0208, -0.0142],\n",
      "        [-0.0153, -0.0328, -0.0599,  0.0812,  0.0219],\n",
      "        [-0.0991, -0.0481, -0.0911,  0.0160, -0.0357],\n",
      "        [ 0.0286,  0.0017,  0.0435,  0.0177,  0.0413]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0189, grad_fn=<MinBackward1>), tensor(0.9556, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.25627613067626953\n",
      "@sample 259: tensor([[ 0.0104, -0.0434, -0.0283,  0.0864, -0.0441],\n",
      "        [ 0.0103,  0.0564, -0.0016, -0.1018,  0.0216],\n",
      "        [ 0.0420,  0.0283,  0.0142, -0.0775, -0.0259],\n",
      "        [ 0.0277, -0.0462,  0.0504,  0.0217, -0.0381],\n",
      "        [ 0.0117,  0.0491, -0.0111, -0.0078, -0.0146],\n",
      "        [ 0.0107, -0.0249,  0.0437,  0.0260, -0.0369],\n",
      "        [-0.0186,  0.0245, -0.0403,  0.0162, -0.0067],\n",
      "        [ 0.0011,  0.0149, -0.0420, -0.0175, -0.0149]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 2.5209e-02,  7.1988e-02,  4.6931e-02, -5.7841e-02, -2.0197e-02],\n",
      "        [-2.4228e-02, -2.0358e-02, -8.6092e-02,  6.7382e-02,  6.5369e-02],\n",
      "        [-6.1586e-02, -3.6605e-03,  3.6653e-02, -9.2971e-02,  1.2703e-02],\n",
      "        [-1.4981e-02,  9.3566e-03,  5.7666e-02, -1.7397e-02, -3.2756e-02],\n",
      "        [-1.7343e-02, -1.3150e-05, -9.1857e-03,  3.3739e-02,  5.5062e-03],\n",
      "        [-4.0762e-02,  4.7862e-02, -1.9133e-02,  2.2072e-02, -2.0144e-02],\n",
      "        [ 1.9309e-02,  1.5728e-03, -6.3969e-03,  9.9380e-03, -2.3341e-02],\n",
      "        [-4.5022e-02, -5.1211e-02,  5.0434e-04,  3.6241e-02,  2.2386e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0102, grad_fn=<MinBackward1>), tensor(0.9728, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2504885792732239\n",
      "@sample 260: tensor([[ 0.0475, -0.0379, -0.0183,  0.0555, -0.0592],\n",
      "        [-0.0215, -0.0092,  0.0638,  0.0131, -0.0120],\n",
      "        [-0.0302,  0.0335,  0.0030,  0.0879,  0.0302],\n",
      "        [ 0.0389, -0.0433, -0.0214,  0.0799, -0.0639],\n",
      "        [ 0.0303,  0.0888, -0.0096,  0.0021,  0.0033],\n",
      "        [-0.0050, -0.0170, -0.0018,  0.0856, -0.0485],\n",
      "        [-0.0233,  0.0395, -0.0478,  0.0250, -0.0117],\n",
      "        [ 0.0217,  0.0061,  0.0250, -0.0306,  0.0382]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0335,  0.0021,  0.0538, -0.0752,  0.0690],\n",
      "        [ 0.0198, -0.0390, -0.0692,  0.0553,  0.0199],\n",
      "        [ 0.1119,  0.0841, -0.0720, -0.0013, -0.0184],\n",
      "        [ 0.0113,  0.0968,  0.0376, -0.0385, -0.0187],\n",
      "        [-0.0019, -0.0270, -0.0791,  0.0306,  0.0067],\n",
      "        [ 0.0173,  0.0538,  0.0565, -0.0110,  0.0657],\n",
      "        [ 0.0114, -0.0720, -0.1213,  0.0780,  0.0371],\n",
      "        [ 0.0255,  0.0029, -0.0210,  0.0238,  0.0252]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0084, grad_fn=<MinBackward1>), tensor(0.9680, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.273994117975235\n",
      "@sample 261: tensor([[ 0.0058,  0.0109, -0.0169, -0.0492,  0.0161],\n",
      "        [-0.0040, -0.0010, -0.0433,  0.0430, -0.0184],\n",
      "        [-0.0412,  0.0211, -0.0546, -0.0386,  0.0220],\n",
      "        [ 0.0296,  0.0268,  0.0039, -0.0309,  0.0206],\n",
      "        [ 0.0110, -0.0068, -0.0243,  0.0352,  0.0196],\n",
      "        [-0.0252,  0.0050, -0.0020,  0.0011, -0.0072],\n",
      "        [-0.0427,  0.0391,  0.0170, -0.0348,  0.0451],\n",
      "        [ 0.0175,  0.0067, -0.0718,  0.0086, -0.0724]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0153,  0.0032, -0.0624,  0.0791,  0.0147],\n",
      "        [ 0.0477, -0.0013,  0.0015, -0.0472,  0.0017],\n",
      "        [-0.0237, -0.0080, -0.0357,  0.0002,  0.0056],\n",
      "        [-0.0585,  0.0097, -0.1130,  0.0531, -0.0332],\n",
      "        [ 0.0241,  0.0160, -0.0084,  0.0118,  0.0539],\n",
      "        [ 0.0583, -0.0069,  0.0522, -0.0845, -0.0521],\n",
      "        [ 0.0043, -0.0047, -0.1267,  0.0665,  0.0609],\n",
      "        [ 0.0298, -0.0093,  0.0890, -0.0784,  0.0008]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0176, grad_fn=<MinBackward1>), tensor(0.9663, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2386149913072586\n",
      "@sample 262: tensor([[-0.0041, -0.0243, -0.0015, -0.0006, -0.0441],\n",
      "        [-0.0035, -0.0531,  0.0485, -0.0299,  0.0095],\n",
      "        [-0.0239,  0.0305,  0.0474, -0.0040,  0.0137],\n",
      "        [-0.0502,  0.0578,  0.0474, -0.0774,  0.0364],\n",
      "        [ 0.0037,  0.0359, -0.0213, -0.0061,  0.0421],\n",
      "        [ 0.0143,  0.0454,  0.0053, -0.0027, -0.0096],\n",
      "        [-0.0268,  0.0576,  0.0512, -0.0263, -0.0215],\n",
      "        [ 0.0055,  0.0061,  0.0139, -0.0045,  0.0232]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0173, -0.0679,  0.0553, -0.0372,  0.0033],\n",
      "        [-0.0750, -0.0163,  0.0235, -0.0420, -0.0080],\n",
      "        [-0.0493,  0.0228,  0.0234, -0.0519, -0.0595],\n",
      "        [-0.0229, -0.0730, -0.1436,  0.0667, -0.0457],\n",
      "        [-0.0067, -0.0106, -0.0779,  0.0319, -0.0358],\n",
      "        [-0.0565, -0.0230, -0.0804,  0.0212, -0.0228],\n",
      "        [-0.0229,  0.0005, -0.0388,  0.0137, -0.0250],\n",
      "        [-0.0052,  0.0108, -0.0229,  0.0446, -0.0137]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0206, grad_fn=<MinBackward1>), tensor(0.9871, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2461962103843689\n",
      "@sample 263: tensor([[-0.0370,  0.0167, -0.0205, -0.0989,  0.0219],\n",
      "        [-0.0186, -0.0188, -0.0255, -0.0052, -0.0086],\n",
      "        [-0.0382, -0.0205,  0.0051, -0.0276, -0.0651],\n",
      "        [ 0.0091,  0.0144,  0.0079, -0.0137,  0.0247],\n",
      "        [ 0.0033, -0.0444,  0.0622, -0.0458, -0.0084],\n",
      "        [-0.0187,  0.0411,  0.0184, -0.0384, -0.0147],\n",
      "        [-0.0133,  0.0251, -0.0044,  0.0020,  0.0530],\n",
      "        [ 0.0273, -0.0030,  0.0228,  0.0161,  0.0727]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-6.3311e-02, -4.7021e-02, -9.3630e-02,  7.7501e-02, -1.1174e-02],\n",
      "        [ 4.2780e-03, -5.8689e-03,  4.1815e-02, -2.1691e-02, -7.6413e-03],\n",
      "        [-6.3926e-02, -5.5121e-02, -2.1681e-02, -5.3540e-02,  2.3981e-02],\n",
      "        [-8.6434e-03, -1.6866e-02, -3.7662e-02,  4.9816e-02, -1.9393e-02],\n",
      "        [-9.4821e-03, -1.9069e-02,  7.0730e-02, -6.3911e-05, -6.1516e-02],\n",
      "        [ 1.4043e-03, -2.8420e-02, -4.1711e-02,  4.7728e-02, -2.9104e-02],\n",
      "        [ 4.0889e-02,  2.9921e-03,  2.0905e-02, -2.0647e-02, -1.5349e-02],\n",
      "        [ 6.1678e-02,  9.2989e-02, -1.7833e-02, -5.6494e-03, -1.7176e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0127, grad_fn=<MinBackward1>), tensor(0.9559, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.24163267016410828\n",
      "@sample 264: tensor([[ 0.0440, -0.0211,  0.0205,  0.0106, -0.0198],\n",
      "        [-0.0544,  0.0950,  0.0983, -0.0564, -0.0097],\n",
      "        [-0.0440,  0.0794, -0.0099,  0.0132, -0.0138],\n",
      "        [-0.0026,  0.0860,  0.0020, -0.0384,  0.0362],\n",
      "        [-0.0293, -0.0511, -0.0162,  0.0106,  0.0341],\n",
      "        [-0.0063,  0.1014, -0.0055, -0.0484,  0.0343],\n",
      "        [-0.0017, -0.0132,  0.0282, -0.0195,  0.0014],\n",
      "        [ 0.0256,  0.0493,  0.0382, -0.0146, -0.0072]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0041,  0.0380, -0.0506, -0.0251,  0.0903],\n",
      "        [-0.0515, -0.0240, -0.0440,  0.0314, -0.0054],\n",
      "        [ 0.0381, -0.0113,  0.0433, -0.0675, -0.0874],\n",
      "        [-0.0771, -0.0423, -0.0282, -0.0099, -0.0275],\n",
      "        [ 0.0187,  0.0007,  0.0304, -0.0226, -0.0294],\n",
      "        [-0.0320, -0.0732, -0.1460,  0.0841,  0.0104],\n",
      "        [-0.0216,  0.0038, -0.0582,  0.0376,  0.0344],\n",
      "        [-0.0240, -0.0706, -0.0534,  0.0754,  0.0120]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0149, grad_fn=<MinBackward1>), tensor(0.9630, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.27630698680877686\n",
      "@sample 265: tensor([[ 0.0191, -0.0609,  0.0009,  0.0776, -0.0176],\n",
      "        [ 0.0015, -0.0150,  0.0177, -0.0599,  0.0180],\n",
      "        [-0.0198, -0.0459, -0.0135,  0.0099,  0.0344],\n",
      "        [-0.0288,  0.0063,  0.0126,  0.0290,  0.0228],\n",
      "        [-0.0074,  0.0072,  0.0319, -0.0064,  0.0447],\n",
      "        [ 0.0107,  0.0942,  0.0759, -0.0624, -0.0061],\n",
      "        [ 0.0256,  0.0136, -0.0193, -0.0241,  0.0312],\n",
      "        [ 0.0245, -0.0226,  0.0346,  0.0384, -0.0144]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0054,  0.1104,  0.0580, -0.0656,  0.0111],\n",
      "        [-0.0585, -0.0822,  0.0841, -0.0656, -0.0125],\n",
      "        [ 0.0003, -0.0513, -0.0547, -0.0219, -0.0189],\n",
      "        [ 0.0602,  0.0410,  0.0236, -0.0187,  0.0436],\n",
      "        [-0.0056,  0.0373,  0.0549,  0.0220,  0.0441],\n",
      "        [-0.0597, -0.0284, -0.0913,  0.0929,  0.0123],\n",
      "        [-0.0188, -0.0191, -0.0915,  0.0229, -0.0270],\n",
      "        [ 0.0045, -0.0151, -0.0287,  0.0356,  0.1139]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0128, grad_fn=<MinBackward1>), tensor(0.9659, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.25903916358947754\n",
      "@sample 266: tensor([[ 0.0121, -0.0305,  0.0143,  0.0279, -0.0306],\n",
      "        [ 0.0193, -0.0729, -0.0299,  0.0944, -0.0399],\n",
      "        [ 0.0382, -0.0964, -0.0155,  0.0963,  0.0044],\n",
      "        [-0.0299, -0.0423,  0.0493,  0.0048, -0.0072],\n",
      "        [ 0.0275, -0.0294,  0.0127,  0.0557, -0.0072],\n",
      "        [ 0.0057, -0.0627,  0.0121,  0.0223, -0.0142],\n",
      "        [-0.0037, -0.0049,  0.0692, -0.0838,  0.0568],\n",
      "        [-0.0139, -0.0378,  0.0181, -0.0288, -0.0321]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0281,  0.0501,  0.0665, -0.0262,  0.0411],\n",
      "        [ 0.0609, -0.0033,  0.0428,  0.0208,  0.0592],\n",
      "        [ 0.0509,  0.0408,  0.0553, -0.0727,  0.0108],\n",
      "        [-0.0197,  0.0045, -0.0055, -0.0003, -0.0013],\n",
      "        [-0.0330,  0.0451, -0.0181,  0.0121, -0.0113],\n",
      "        [-0.0710,  0.0127, -0.0151,  0.0415, -0.0090],\n",
      "        [ 0.0108, -0.0474,  0.0010,  0.0205, -0.0531],\n",
      "        [-0.0575, -0.0570,  0.1404, -0.1155, -0.0644]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0131, grad_fn=<MinBackward1>), tensor(0.9840, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.27362391352653503\n",
      "@sample 267: tensor([[-0.0104,  0.0970, -0.0216, -0.0760,  0.0179],\n",
      "        [-0.0407, -0.0190,  0.0192,  0.0199,  0.0134],\n",
      "        [-0.0161,  0.0168,  0.0298,  0.0073,  0.0575],\n",
      "        [-0.0019,  0.0132,  0.0070, -0.0320,  0.0032],\n",
      "        [-0.0136,  0.0719,  0.0398, -0.1191,  0.0444],\n",
      "        [-0.0147, -0.0276, -0.0137, -0.0131,  0.0014],\n",
      "        [-0.0023, -0.0376,  0.0021, -0.0255,  0.0657],\n",
      "        [-0.0042,  0.1185,  0.0676, -0.0842,  0.0500]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0228, -0.0236, -0.0453, -0.0011,  0.0390],\n",
      "        [ 0.0864,  0.0171,  0.0371,  0.0165,  0.0315],\n",
      "        [ 0.0068,  0.0317,  0.0124,  0.0072, -0.0402],\n",
      "        [ 0.0015, -0.0009, -0.0492,  0.0020,  0.0310],\n",
      "        [ 0.0049, -0.1133, -0.1887,  0.1088,  0.0450],\n",
      "        [ 0.0010,  0.0266, -0.0593,  0.0108,  0.0068],\n",
      "        [ 0.0071, -0.0249, -0.0181, -0.0018,  0.0414],\n",
      "        [-0.0385, -0.0160, -0.0677,  0.0703, -0.0311]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0137, grad_fn=<MinBackward1>), tensor(0.9722, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.26727092266082764\n",
      "@sample 268: tensor([[ 0.0134,  0.0762,  0.0342, -0.1795,  0.0938],\n",
      "        [ 0.0106, -0.0487,  0.0014,  0.0263,  0.0428],\n",
      "        [-0.0037, -0.0357, -0.0642,  0.0599, -0.0460],\n",
      "        [-0.0424, -0.0078, -0.0130,  0.0512, -0.0222],\n",
      "        [ 0.0050, -0.0555, -0.0025,  0.0320, -0.0067],\n",
      "        [-0.0163, -0.0142,  0.0390,  0.0185,  0.0201],\n",
      "        [-0.0029, -0.0126,  0.0250, -0.0205,  0.0261],\n",
      "        [-0.0468, -0.0031,  0.0749, -0.0232,  0.0276]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0628, -0.0804, -0.0922,  0.1149,  0.0507],\n",
      "        [ 0.0726,  0.0421, -0.0177,  0.0036, -0.0280],\n",
      "        [ 0.1092,  0.0431,  0.1075, -0.0607, -0.0404],\n",
      "        [ 0.0532,  0.0816,  0.0500, -0.1058,  0.0198],\n",
      "        [ 0.0073,  0.0359,  0.0292, -0.0171,  0.0284],\n",
      "        [ 0.0130,  0.0563,  0.0108, -0.0459, -0.0290],\n",
      "        [-0.0166,  0.0605,  0.0028,  0.0498,  0.0267],\n",
      "        [-0.0195,  0.0226, -0.0905,  0.0204, -0.0326]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0104, grad_fn=<MinBackward1>), tensor(0.9743, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2859041094779968\n",
      "@sample 269: tensor([[ 0.0322, -0.0484, -0.0149,  0.0848, -0.0656],\n",
      "        [-0.0088,  0.0196,  0.0233, -0.0143,  0.0235],\n",
      "        [ 0.0061, -0.0266,  0.0094,  0.0006, -0.0186],\n",
      "        [-0.0021,  0.0175,  0.0093,  0.0235, -0.0061],\n",
      "        [-0.0198, -0.0404, -0.0020,  0.0797, -0.0311],\n",
      "        [ 0.0146,  0.0721,  0.0638, -0.0092, -0.0129],\n",
      "        [-0.0036,  0.0132,  0.0347,  0.0076,  0.0138],\n",
      "        [-0.0114, -0.0225, -0.0579, -0.0104,  0.0211]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0081,  0.0868,  0.0503,  0.0109,  0.0435],\n",
      "        [-0.0190, -0.0220, -0.0093, -0.0188,  0.0039],\n",
      "        [-0.0171,  0.0142, -0.0730,  0.0148, -0.0271],\n",
      "        [-0.0223, -0.0020, -0.0507, -0.0403, -0.0153],\n",
      "        [ 0.0728,  0.0350,  0.1187, -0.0573,  0.0181],\n",
      "        [ 0.0202,  0.0766, -0.0252,  0.0030, -0.0464],\n",
      "        [ 0.0217,  0.0171,  0.0114, -0.0044, -0.0066],\n",
      "        [-0.0281,  0.0141, -0.0121, -0.0157,  0.0078]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0127, grad_fn=<MinBackward1>), tensor(0.9740, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2474282830953598\n",
      "@sample 270: tensor([[ 0.0039,  0.0561,  0.0285,  0.0033,  0.0350],\n",
      "        [-0.0263, -0.0573, -0.0063,  0.0193,  0.0171],\n",
      "        [-0.0036, -0.0026,  0.0409,  0.0407,  0.0127],\n",
      "        [-0.0175, -0.0493,  0.0077,  0.0359,  0.0180],\n",
      "        [ 0.0100, -0.0238, -0.0014,  0.0178,  0.0017],\n",
      "        [ 0.0204,  0.0124,  0.0104,  0.0118, -0.0046],\n",
      "        [ 0.0287,  0.0652,  0.0113, -0.0234, -0.0083],\n",
      "        [-0.0143,  0.0049,  0.0426, -0.0189,  0.0009]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0106, -0.0215, -0.0572,  0.0153,  0.0095],\n",
      "        [ 0.0301, -0.0183,  0.0073,  0.0194,  0.0842],\n",
      "        [ 0.0582,  0.0221, -0.0643,  0.0232, -0.0230],\n",
      "        [ 0.0300,  0.0340,  0.0791, -0.0463,  0.0348],\n",
      "        [ 0.0431, -0.0284, -0.0004, -0.0148,  0.0414],\n",
      "        [-0.0427, -0.0151, -0.0537,  0.0572,  0.0255],\n",
      "        [-0.0540,  0.0060, -0.1278,  0.0624,  0.0388],\n",
      "        [ 0.0172,  0.0540, -0.0105, -0.0622, -0.0396]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0162, grad_fn=<MinBackward1>), tensor(0.9511, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.23023423552513123\n",
      "@sample 271: tensor([[ 0.0223,  0.0362,  0.0340, -0.0094, -0.0121],\n",
      "        [-0.0065, -0.0452, -0.0305,  0.0436,  0.0079],\n",
      "        [-0.0368, -0.0237,  0.0299, -0.0416,  0.0393],\n",
      "        [ 0.0280, -0.0138,  0.0202, -0.0079, -0.0202],\n",
      "        [-0.0147,  0.1063,  0.0407, -0.1279,  0.0695],\n",
      "        [-0.0117,  0.0225, -0.0013, -0.0867,  0.0230],\n",
      "        [ 0.0385, -0.0591, -0.0343,  0.0849, -0.0654],\n",
      "        [-0.0462,  0.0718,  0.0729, -0.0184, -0.0255]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0203, -0.0180,  0.0166,  0.0463,  0.0145],\n",
      "        [ 0.0466,  0.0332,  0.0652, -0.0671,  0.0029],\n",
      "        [-0.0465, -0.0346, -0.0092,  0.0199, -0.0130],\n",
      "        [-0.0527, -0.0175,  0.0532,  0.0240, -0.0043],\n",
      "        [-0.0790, -0.0915, -0.0903,  0.0832, -0.0146],\n",
      "        [-0.0131, -0.0520, -0.0781,  0.0764,  0.0618],\n",
      "        [ 0.0169, -0.0094,  0.1006, -0.1075, -0.0083],\n",
      "        [-0.0429, -0.0293, -0.0642, -0.0065, -0.0344]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0295, grad_fn=<MinBackward1>), tensor(0.9664, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.27085745334625244\n",
      "@sample 272: tensor([[-0.0149, -0.0086, -0.0547, -0.0465,  0.0200],\n",
      "        [-0.0042,  0.0386,  0.0224, -0.0130,  0.0540],\n",
      "        [ 0.0375,  0.0470, -0.0009, -0.0385,  0.0101],\n",
      "        [-0.0003, -0.0507, -0.0098,  0.0080, -0.0224],\n",
      "        [-0.0021,  0.0499,  0.0441, -0.0343,  0.0755],\n",
      "        [-0.0147,  0.0464,  0.0539, -0.0322,  0.0485],\n",
      "        [ 0.0117, -0.0557,  0.0056,  0.0814,  0.0029],\n",
      "        [-0.0408,  0.0025,  0.0083,  0.0246,  0.0085]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0255, -0.0539,  0.0172,  0.0119,  0.0137],\n",
      "        [-0.0292,  0.0543, -0.0511, -0.0871, -0.0299],\n",
      "        [ 0.0397,  0.0109,  0.0456,  0.0562,  0.0159],\n",
      "        [-0.0173,  0.0067,  0.0625, -0.1015, -0.0373],\n",
      "        [-0.0067, -0.0342, -0.0840,  0.0704,  0.0176],\n",
      "        [ 0.0190,  0.0380, -0.0254,  0.0999,  0.0142],\n",
      "        [ 0.0203,  0.0130,  0.0470, -0.0084, -0.0108],\n",
      "        [-0.0222,  0.0465, -0.0212,  0.0049, -0.0712]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0174, grad_fn=<MinBackward1>), tensor(0.9891, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.24470019340515137\n",
      "@sample 273: tensor([[ 2.2874e-03,  9.2985e-02,  1.7839e-02, -2.5950e-02, -3.5014e-02],\n",
      "        [ 1.1119e-02,  4.5049e-02, -1.1297e-02, -2.1329e-02,  1.4767e-02],\n",
      "        [ 2.3673e-02, -4.2927e-02,  3.0285e-04,  5.6889e-02, -2.9938e-02],\n",
      "        [-6.8431e-03,  5.2302e-03,  1.8476e-02,  6.7561e-03,  2.2717e-02],\n",
      "        [ 3.9843e-03,  6.6504e-03,  2.9055e-03,  5.6767e-02, -6.1908e-04],\n",
      "        [ 3.4542e-02,  9.2804e-05, -3.6789e-02, -7.4820e-03,  1.6083e-02],\n",
      "        [ 3.4093e-03,  1.1721e-02,  4.8580e-02, -4.4657e-02,  3.7428e-02],\n",
      "        [-1.3892e-02,  6.9458e-02,  1.3706e-02, -6.4263e-02,  4.6610e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0730, -0.0469, -0.0867,  0.0178, -0.0258],\n",
      "        [-0.0139, -0.0332, -0.0386,  0.0449,  0.0373],\n",
      "        [ 0.0290,  0.0290,  0.0277, -0.0333, -0.0279],\n",
      "        [-0.0323,  0.0177, -0.0725,  0.0120,  0.0076],\n",
      "        [-0.0446,  0.0641, -0.0736,  0.0152, -0.0261],\n",
      "        [-0.0751, -0.0472, -0.0802,  0.0432, -0.0620],\n",
      "        [ 0.0513, -0.0255, -0.0665,  0.0119, -0.0074],\n",
      "        [-0.0287, -0.0289, -0.1457,  0.1054,  0.0156]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0193, grad_fn=<MinBackward1>), tensor(0.9810, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.24447566270828247\n",
      "@sample 274: tensor([[ 0.0268,  0.0252, -0.0211, -0.0587,  0.0501],\n",
      "        [-0.0075,  0.0078, -0.0225, -0.0565,  0.0280],\n",
      "        [-0.0351,  0.0528, -0.0083, -0.0391,  0.0471],\n",
      "        [-0.0068,  0.0381, -0.0042, -0.0034,  0.0244],\n",
      "        [ 0.0448,  0.0207,  0.0378,  0.0090, -0.0115],\n",
      "        [-0.0159,  0.0021,  0.0076, -0.0205, -0.0316],\n",
      "        [ 0.0178,  0.0360,  0.0372, -0.0679,  0.0339],\n",
      "        [ 0.0098,  0.0780, -0.0440, -0.0917,  0.0107]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0041, -0.1074, -0.1406,  0.1034, -0.0197],\n",
      "        [ 0.0321, -0.0830,  0.0526, -0.0563, -0.0030],\n",
      "        [-0.0399, -0.0387, -0.1264, -0.0022,  0.0011],\n",
      "        [-0.0110, -0.0144, -0.0419,  0.0442,  0.0179],\n",
      "        [-0.0228,  0.0589,  0.0070, -0.0218,  0.0071],\n",
      "        [-0.0637, -0.0165, -0.0281, -0.0433, -0.0103],\n",
      "        [-0.0884, -0.0094, -0.0188, -0.0178, -0.0369],\n",
      "        [-0.0758, -0.0346, -0.1170,  0.1009, -0.0311]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0100, grad_fn=<MinBackward1>), tensor(0.9696, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.25237128138542175\n",
      "@sample 275: tensor([[ 0.0154,  0.0289, -0.0161,  0.0086, -0.0582],\n",
      "        [ 0.0134, -0.0089, -0.0259,  0.0269, -0.0088],\n",
      "        [-0.0381, -0.0439, -0.0043,  0.0120, -0.0148],\n",
      "        [ 0.0235, -0.0039, -0.0199,  0.0121, -0.0165],\n",
      "        [ 0.0215,  0.0068, -0.0401, -0.0381, -0.0278],\n",
      "        [-0.0290,  0.0495,  0.0354, -0.0870,  0.0274],\n",
      "        [-0.0014, -0.0150, -0.0519,  0.0360, -0.0221],\n",
      "        [-0.0165, -0.0143,  0.0329, -0.0196, -0.0057]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0373,  0.0383, -0.0029, -0.0416,  0.0170],\n",
      "        [ 0.0154,  0.0167, -0.0535,  0.0517,  0.0694],\n",
      "        [-0.0062, -0.0183,  0.0997, -0.0466, -0.0262],\n",
      "        [-0.0346, -0.0234, -0.0489,  0.0300, -0.0067],\n",
      "        [-0.0220, -0.0154,  0.0369,  0.0090,  0.0429],\n",
      "        [-0.0538, -0.0260, -0.0524,  0.0478,  0.0532],\n",
      "        [-0.0030,  0.0847,  0.0242, -0.0497, -0.0844],\n",
      "        [-0.0182,  0.0025, -0.0297,  0.0610,  0.0297]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0271, grad_fn=<MinBackward1>), tensor(0.9705, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.23498834669589996\n",
      "@sample 276: tensor([[ 0.0427, -0.1198, -0.0625,  0.1164, -0.0649],\n",
      "        [ 0.0327,  0.0562, -0.0121, -0.0587,  0.0659],\n",
      "        [ 0.0072,  0.0132, -0.0453,  0.0084, -0.0289],\n",
      "        [-0.0038, -0.0030,  0.0155, -0.0334,  0.0018],\n",
      "        [ 0.0143, -0.0446, -0.0092,  0.0222, -0.0334],\n",
      "        [-0.0201,  0.0037, -0.0053, -0.0321, -0.0060],\n",
      "        [ 0.0141, -0.0034,  0.0357, -0.0005,  0.0533],\n",
      "        [ 0.0006,  0.0235, -0.0143, -0.0109,  0.0101]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0543,  0.0439,  0.0743, -0.0454,  0.0418],\n",
      "        [ 0.0037, -0.0258, -0.1164,  0.0277, -0.0293],\n",
      "        [-0.0018, -0.0158,  0.0592,  0.0180,  0.0084],\n",
      "        [-0.0067,  0.0017,  0.0278, -0.0407,  0.0377],\n",
      "        [-0.0460, -0.0367, -0.0038, -0.0800,  0.0285],\n",
      "        [-0.0742, -0.0053, -0.0496,  0.0565, -0.0336],\n",
      "        [-0.0071,  0.0315, -0.0410,  0.0021, -0.0110],\n",
      "        [-0.0022, -0.0101, -0.0253,  0.0255, -0.0651]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0229, grad_fn=<MinBackward1>), tensor(0.9588, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.25290602445602417\n",
      "@sample 277: tensor([[-3.1999e-02, -3.0940e-02,  1.0688e-02,  1.4941e-03, -4.1495e-02],\n",
      "        [ 2.0868e-02, -2.8087e-02, -3.1658e-02,  1.5788e-02, -1.4991e-03],\n",
      "        [-1.2860e-03,  3.8170e-02,  2.1328e-02, -4.0098e-02,  1.1803e-02],\n",
      "        [ 1.3227e-02, -1.7213e-02, -2.5951e-02, -1.7060e-02, -6.0469e-05],\n",
      "        [ 5.1633e-02,  6.2114e-03,  4.9818e-02,  5.6895e-02, -2.0614e-02],\n",
      "        [-2.3248e-02, -5.4348e-02, -1.6166e-02,  5.1032e-02, -8.1319e-02],\n",
      "        [-4.5670e-02,  1.0614e-02,  9.7785e-03, -3.3605e-02, -1.4117e-03],\n",
      "        [ 1.4131e-02, -9.2517e-03, -6.9698e-02,  2.8532e-02, -5.6556e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.1004, -0.0305, -0.0445,  0.0784,  0.0114],\n",
      "        [-0.0141, -0.0072,  0.0013,  0.0274,  0.0414],\n",
      "        [-0.0756, -0.0090, -0.0765,  0.1141, -0.0082],\n",
      "        [-0.0304, -0.0425, -0.0636, -0.0049, -0.0527],\n",
      "        [-0.0014, -0.0151,  0.0102,  0.0177,  0.0265],\n",
      "        [-0.0322,  0.0415,  0.1699, -0.1237,  0.0223],\n",
      "        [ 0.0123, -0.0179,  0.0421, -0.0412, -0.0190],\n",
      "        [ 0.0571, -0.0573, -0.0008, -0.0267,  0.0256]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0224, grad_fn=<MinBackward1>), tensor(0.9472, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.243308424949646\n",
      "@sample 278: tensor([[-0.0425, -0.0774, -0.0032,  0.0272, -0.0130],\n",
      "        [ 0.0119, -0.0208, -0.1324,  0.0505, -0.0352],\n",
      "        [-0.0368,  0.0927, -0.0108,  0.0316, -0.0056],\n",
      "        [-0.0164, -0.0076, -0.0064, -0.0204,  0.0165],\n",
      "        [-0.0130, -0.0865,  0.0353,  0.0532, -0.0416],\n",
      "        [-0.0024,  0.0320, -0.0592, -0.0096,  0.0162],\n",
      "        [-0.0080,  0.0114,  0.0214, -0.0064,  0.0160],\n",
      "        [ 0.0044,  0.0133,  0.0037, -0.0357,  0.0253]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0200,  0.0039,  0.1270, -0.0588,  0.0645],\n",
      "        [ 0.0414, -0.0148,  0.1420, -0.1223,  0.0190],\n",
      "        [ 0.0483,  0.0210, -0.0697,  0.0804,  0.0686],\n",
      "        [ 0.0268,  0.0068, -0.0863,  0.0079, -0.0014],\n",
      "        [ 0.0261,  0.0457,  0.0591, -0.0403, -0.0736],\n",
      "        [-0.0122, -0.0082,  0.0557,  0.0282,  0.0069],\n",
      "        [-0.0130,  0.0178, -0.0472,  0.0343,  0.0225],\n",
      "        [-0.0082, -0.0745, -0.0146, -0.0114, -0.0446]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0176, grad_fn=<MinBackward1>), tensor(0.9404, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2609161138534546\n",
      "@sample 279: tensor([[ 0.0084,  0.0349, -0.0857,  0.0051,  0.0026],\n",
      "        [-0.0064, -0.1173, -0.1055,  0.0955, -0.0457],\n",
      "        [-0.0274, -0.0535, -0.0110,  0.0521, -0.0368],\n",
      "        [ 0.0102,  0.0452, -0.0463, -0.0186,  0.0195],\n",
      "        [-0.0178, -0.0030, -0.0877, -0.0535,  0.0268],\n",
      "        [ 0.0199,  0.0356, -0.0135, -0.0285,  0.0026],\n",
      "        [-0.0217,  0.0730, -0.0033, -0.1138,  0.0306],\n",
      "        [-0.0076,  0.0147, -0.0028,  0.0036,  0.0121]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0520, -0.0636, -0.0351,  0.0448, -0.0104],\n",
      "        [ 0.0330, -0.0317,  0.1852, -0.1357, -0.0029],\n",
      "        [-0.0185,  0.0800,  0.1036, -0.1180,  0.0145],\n",
      "        [-0.0343, -0.0855,  0.0093, -0.0195, -0.0746],\n",
      "        [-0.0020, -0.0678, -0.0895,  0.0363, -0.0155],\n",
      "        [-0.0445, -0.0629, -0.0462, -0.0272,  0.0042],\n",
      "        [-0.0131, -0.1125, -0.0448,  0.0083,  0.0290],\n",
      "        [-0.0327, -0.0181, -0.0171,  0.0113, -0.0022]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0227, grad_fn=<MinBackward1>), tensor(0.9539, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.28570711612701416\n",
      "@sample 280: tensor([[-0.0150, -0.0036,  0.0026,  0.0026, -0.0085],\n",
      "        [-0.0133, -0.0057, -0.0427, -0.0040,  0.0119],\n",
      "        [-0.0774,  0.0104, -0.0474, -0.0074, -0.0076],\n",
      "        [-0.0060,  0.0477, -0.0432, -0.0512, -0.0050],\n",
      "        [ 0.0064,  0.0078, -0.0103, -0.0247,  0.0694],\n",
      "        [-0.0290,  0.0289, -0.0049, -0.0551, -0.0507],\n",
      "        [-0.0141,  0.0440,  0.0124, -0.0075, -0.0093],\n",
      "        [ 0.0194,  0.0221, -0.0215, -0.0626,  0.0152]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0196,  0.0389, -0.0384,  0.0126, -0.0246],\n",
      "        [ 0.0316, -0.0015, -0.0237, -0.0018,  0.0599],\n",
      "        [-0.0440, -0.0064,  0.0004, -0.0332,  0.0260],\n",
      "        [-0.0490, -0.0359, -0.1394,  0.0907,  0.0287],\n",
      "        [-0.0094, -0.0228, -0.0581,  0.0329,  0.0208],\n",
      "        [ 0.0200, -0.0690, -0.0173, -0.0089,  0.0126],\n",
      "        [-0.0064,  0.0405,  0.0004,  0.0558,  0.0699],\n",
      "        [-0.0550, -0.0156, -0.0221,  0.0330,  0.0201]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0111, grad_fn=<MinBackward1>), tensor(0.9768, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.23311759531497955\n",
      "@sample 281: tensor([[-0.0035,  0.0376,  0.0351, -0.0530,  0.0490],\n",
      "        [-0.0084,  0.0129,  0.0021,  0.0011, -0.0025],\n",
      "        [ 0.0073, -0.0207, -0.0217, -0.0353, -0.0006],\n",
      "        [-0.0083, -0.0321,  0.0287,  0.0581, -0.0070],\n",
      "        [-0.0108,  0.0168,  0.0107, -0.0048,  0.0427],\n",
      "        [-0.0065,  0.0030, -0.0685, -0.0421, -0.0079],\n",
      "        [ 0.0062,  0.0316,  0.0377,  0.0231,  0.0098],\n",
      "        [ 0.0095, -0.0498, -0.0020,  0.0404,  0.0029]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0375, -0.0397, -0.0454,  0.0535, -0.0131],\n",
      "        [ 0.0171,  0.0212,  0.0368, -0.0132, -0.0585],\n",
      "        [-0.0165, -0.0362,  0.0547, -0.0104, -0.0248],\n",
      "        [ 0.0183,  0.0425,  0.0139, -0.0166,  0.0020],\n",
      "        [ 0.0352, -0.0161, -0.0297,  0.0154,  0.0127],\n",
      "        [-0.0004, -0.0496, -0.0565, -0.0506, -0.0697],\n",
      "        [ 0.0561,  0.0410, -0.0533,  0.0400,  0.0321],\n",
      "        [ 0.0345,  0.0698,  0.0151,  0.0059,  0.0042]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0202, grad_fn=<MinBackward1>), tensor(0.9733, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.22333359718322754\n",
      "@sample 282: tensor([[-0.0054,  0.0267, -0.0313,  0.0484, -0.0068],\n",
      "        [-0.0168,  0.0128,  0.0129, -0.0444,  0.0252],\n",
      "        [-0.0206,  0.0148,  0.0468,  0.0153,  0.0238],\n",
      "        [ 0.0188, -0.0280,  0.0391, -0.0313,  0.0095],\n",
      "        [-0.0194, -0.0069, -0.0086,  0.0217,  0.0110],\n",
      "        [-0.0207,  0.0152, -0.0162, -0.0131,  0.0198],\n",
      "        [ 0.0060,  0.0786,  0.0339, -0.0929,  0.0109],\n",
      "        [-0.0402,  0.0380, -0.0078, -0.0643,  0.1095]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0044,  0.0290,  0.0317, -0.0769,  0.0013],\n",
      "        [-0.0665, -0.0007, -0.0674,  0.0276, -0.0569],\n",
      "        [ 0.0003,  0.0491,  0.0033, -0.0014,  0.0005],\n",
      "        [ 0.0066,  0.0094, -0.0031, -0.0039, -0.0135],\n",
      "        [ 0.0169,  0.0200, -0.0011, -0.0087, -0.0011],\n",
      "        [ 0.0376,  0.0175,  0.0064, -0.0077, -0.0571],\n",
      "        [-0.0420, -0.0428, -0.1181,  0.0971,  0.0197],\n",
      "        [ 0.1071, -0.0663, -0.0722,  0.0798, -0.0540]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0166, grad_fn=<MinBackward1>), tensor(0.9600, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.24678412079811096\n",
      "@sample 283: tensor([[-0.0250,  0.0050,  0.0327, -0.0503,  0.0177],\n",
      "        [-0.0054,  0.0027, -0.0330,  0.0143,  0.0232],\n",
      "        [-0.0176,  0.0102, -0.0119, -0.0676, -0.0009],\n",
      "        [-0.0235,  0.0087, -0.0250, -0.0598,  0.0687],\n",
      "        [-0.0303,  0.0006, -0.0052,  0.0727, -0.0495],\n",
      "        [ 0.0198,  0.0441,  0.0466, -0.0530,  0.0203],\n",
      "        [-0.0164,  0.0337,  0.0432, -0.0148,  0.0204],\n",
      "        [ 0.0203,  0.0198,  0.0262,  0.0540, -0.0561]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0093, -0.0353, -0.0144,  0.0216,  0.0062],\n",
      "        [ 0.0383, -0.0572,  0.0336,  0.0390, -0.0228],\n",
      "        [-0.0045, -0.0196,  0.0082,  0.0336,  0.0187],\n",
      "        [-0.0232, -0.0530, -0.0596,  0.0471, -0.0067],\n",
      "        [ 0.0775,  0.0584,  0.0814, -0.0618, -0.0020],\n",
      "        [-0.0184, -0.0096, -0.0222,  0.0424, -0.0133],\n",
      "        [ 0.0420,  0.0178, -0.0093, -0.0454,  0.0045],\n",
      "        [ 0.0148,  0.0877,  0.0239, -0.0236,  0.0141]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0173, grad_fn=<MinBackward1>), tensor(0.9826, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.23484505712985992\n",
      "@sample 284: tensor([[-0.0035,  0.0074,  0.0440, -0.0485, -0.0069],\n",
      "        [-0.0455,  0.0765,  0.0450, -0.0874,  0.0142],\n",
      "        [-0.0507, -0.0528,  0.0017,  0.0245, -0.0173],\n",
      "        [ 0.0050, -0.0038,  0.0071, -0.0274, -0.0003],\n",
      "        [-0.0241,  0.0235,  0.0212, -0.0403,  0.0070],\n",
      "        [ 0.0157,  0.0010, -0.0418, -0.0461,  0.0441],\n",
      "        [-0.0183,  0.0256,  0.0544, -0.0176,  0.0336],\n",
      "        [-0.0225,  0.0312, -0.0016, -0.0197,  0.0229]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0151, -0.0455, -0.0290,  0.0284,  0.0167],\n",
      "        [-0.0890, -0.0829, -0.1532,  0.1355, -0.0518],\n",
      "        [-0.0039, -0.0380, -0.0069,  0.0159, -0.0324],\n",
      "        [-0.0487, -0.0707, -0.1053,  0.0398,  0.0224],\n",
      "        [-0.0204, -0.0352, -0.0651,  0.0166, -0.0004],\n",
      "        [-0.0093, -0.0291, -0.0190,  0.0292,  0.0431],\n",
      "        [-0.0002,  0.0472, -0.0576,  0.0319,  0.0257],\n",
      "        [-0.0534,  0.0262, -0.0769, -0.0354, -0.0176]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0219, grad_fn=<MinBackward1>), tensor(0.9698, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.25479888916015625\n",
      "@sample 285: tensor([[-0.0235, -0.0170,  0.0092, -0.0135,  0.0136],\n",
      "        [ 0.0386,  0.0109, -0.0471, -0.0400, -0.0252],\n",
      "        [-0.0059,  0.0084,  0.0171,  0.0187, -0.0366],\n",
      "        [ 0.0235,  0.0397,  0.0438, -0.0146,  0.0350],\n",
      "        [-0.0226,  0.0501,  0.0593, -0.0414,  0.0573],\n",
      "        [-0.0035,  0.0241, -0.0211,  0.0029,  0.0025],\n",
      "        [ 0.0015, -0.0328,  0.0065,  0.0247, -0.0008],\n",
      "        [ 0.0129,  0.0435,  0.0605, -0.0442, -0.0403]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0454, -0.0240, -0.0658,  0.0100, -0.0343],\n",
      "        [ 0.0304, -0.0665, -0.0058, -0.0657, -0.0643],\n",
      "        [ 0.0311, -0.0175, -0.0591,  0.0123, -0.0077],\n",
      "        [ 0.0267, -0.0015, -0.0390,  0.0426,  0.0016],\n",
      "        [-0.0399, -0.0392, -0.0505,  0.0425, -0.0016],\n",
      "        [ 0.0134,  0.0020,  0.0040, -0.0085,  0.0007],\n",
      "        [-0.0006,  0.0114,  0.0292, -0.0446, -0.0620],\n",
      "        [-0.0623, -0.0096, -0.0452,  0.0105, -0.0232]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0195, grad_fn=<MinBackward1>), tensor(0.9682, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.235516756772995\n",
      "@sample 286: tensor([[-0.0044,  0.0637,  0.0127, -0.0622, -0.0127],\n",
      "        [-0.0754, -0.0528,  0.0205,  0.0305, -0.0323],\n",
      "        [ 0.0294,  0.0056,  0.0353, -0.0398,  0.0390],\n",
      "        [ 0.0176, -0.0461, -0.0021, -0.0639, -0.0016],\n",
      "        [ 0.0111, -0.1082, -0.0065,  0.0825, -0.0395],\n",
      "        [-0.0211,  0.0116,  0.0123,  0.0631,  0.0050],\n",
      "        [-0.0287, -0.0046,  0.0135, -0.0185,  0.0061],\n",
      "        [-0.0124, -0.0393, -0.0104,  0.0156, -0.0172]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0612, -0.0451,  0.0251, -0.0060, -0.0364],\n",
      "        [ 0.0090,  0.0020,  0.0978, -0.0405, -0.0650],\n",
      "        [-0.0113, -0.0452, -0.0145,  0.0241,  0.0054],\n",
      "        [-0.0022, -0.0738, -0.0107, -0.0229, -0.0135],\n",
      "        [ 0.0036,  0.0473,  0.0397, -0.0482, -0.0053],\n",
      "        [-0.0120,  0.0378,  0.0024, -0.0220, -0.0690],\n",
      "        [ 0.0056, -0.0013, -0.1125,  0.0415,  0.0027],\n",
      "        [ 0.0210,  0.0454,  0.0524, -0.0171, -0.0099]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0166, grad_fn=<MinBackward1>), tensor(0.9620, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.24043667316436768\n",
      "@sample 287: tensor([[ 0.0277,  0.0535,  0.0236, -0.0010, -0.0168],\n",
      "        [-0.0579,  0.0167, -0.0326,  0.0122, -0.0185],\n",
      "        [-0.0285,  0.0005, -0.0224, -0.0129,  0.0153],\n",
      "        [-0.0179,  0.0135,  0.0299, -0.0073,  0.0241],\n",
      "        [-0.0154, -0.0427, -0.0511, -0.0201,  0.0414],\n",
      "        [-0.0144, -0.0479, -0.0386,  0.0473, -0.0333],\n",
      "        [ 0.0205,  0.0428, -0.0047, -0.0482, -0.0315],\n",
      "        [ 0.0174, -0.0655, -0.0213, -0.0264,  0.0074]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0009,  0.0260, -0.0501, -0.0074, -0.0288],\n",
      "        [ 0.0501,  0.0098,  0.1549, -0.0993,  0.0210],\n",
      "        [ 0.0269, -0.0016,  0.0214, -0.0094, -0.0329],\n",
      "        [-0.0339, -0.0060,  0.0010, -0.0288,  0.0063],\n",
      "        [-0.0539, -0.0410, -0.0333, -0.0018,  0.0366],\n",
      "        [ 0.0202,  0.0177,  0.0951, -0.0596, -0.0220],\n",
      "        [-0.0857, -0.0959, -0.0157,  0.0892,  0.0191],\n",
      "        [ 0.0002, -0.0316,  0.0104, -0.0385, -0.0039]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0165, grad_fn=<MinBackward1>), tensor(0.9733, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2401757389307022\n",
      "@sample 288: tensor([[-0.0289, -0.0252, -0.0082,  0.0546, -0.0196],\n",
      "        [ 0.0071,  0.0980,  0.0333, -0.0168,  0.0449],\n",
      "        [-0.0221,  0.0295,  0.0320, -0.0359,  0.0321],\n",
      "        [ 0.0119, -0.0174,  0.0117, -0.0389,  0.0318],\n",
      "        [-0.0249,  0.0148, -0.0304, -0.0204, -0.0430],\n",
      "        [-0.0142, -0.0261, -0.0293,  0.0309,  0.0055],\n",
      "        [-0.0288,  0.0467,  0.0194, -0.0529,  0.0389],\n",
      "        [ 0.0151, -0.0021, -0.0198, -0.0403,  0.0249]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0420,  0.0222,  0.0436, -0.0602, -0.0234],\n",
      "        [ 0.0046, -0.0211, -0.0808,  0.0479, -0.0136],\n",
      "        [-0.0324, -0.0448,  0.0120, -0.0490, -0.0843],\n",
      "        [-0.0152, -0.0091, -0.0267,  0.0082,  0.0379],\n",
      "        [-0.0364,  0.0101,  0.0649, -0.0523, -0.0104],\n",
      "        [ 0.0706,  0.0436,  0.0122, -0.0243, -0.0095],\n",
      "        [-0.0305, -0.0427, -0.0283,  0.0383, -0.0073],\n",
      "        [-0.0229, -0.0208, -0.0012,  0.0025,  0.0290]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0157, grad_fn=<MinBackward1>), tensor(0.9623, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2305956929922104\n",
      "@sample 289: tensor([[ 0.0212, -0.0219,  0.0104, -0.0158,  0.0056],\n",
      "        [-0.0104,  0.0537, -0.0382, -0.0777,  0.0467],\n",
      "        [ 0.0084, -0.0022,  0.0399,  0.0330, -0.0445],\n",
      "        [ 0.0269, -0.0165,  0.0232, -0.0050, -0.0027],\n",
      "        [ 0.0294,  0.0131,  0.0463,  0.0075,  0.0352],\n",
      "        [-0.0092,  0.0625,  0.0215, -0.0333,  0.0093],\n",
      "        [-0.0023, -0.0233, -0.0222,  0.0472, -0.0385],\n",
      "        [-0.0212,  0.0086, -0.0003, -0.0367,  0.0005]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0745, -0.0099, -0.0113,  0.0173, -0.0301],\n",
      "        [-0.0544, -0.0735, -0.1060,  0.0696, -0.0331],\n",
      "        [ 0.0224,  0.0098,  0.0146, -0.0572, -0.0459],\n",
      "        [ 0.0123,  0.0037, -0.0055, -0.1022, -0.0099],\n",
      "        [ 0.0034,  0.0270,  0.0440, -0.0830, -0.0570],\n",
      "        [-0.0450, -0.0173, -0.0440,  0.0572, -0.0210],\n",
      "        [-0.0487,  0.0135,  0.0700,  0.0021, -0.0364],\n",
      "        [ 0.0675,  0.0351,  0.0504, -0.0324,  0.0088]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0179, grad_fn=<MinBackward1>), tensor(0.9704, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.23402409255504608\n",
      "@sample 290: tensor([[-0.0366,  0.0033,  0.0030,  0.0132, -0.0077],\n",
      "        [-0.0292,  0.0916,  0.0521, -0.0871,  0.0565],\n",
      "        [ 0.0173, -0.0437,  0.0259,  0.0439,  0.0079],\n",
      "        [ 0.0302,  0.0139,  0.0038,  0.0718, -0.0302],\n",
      "        [ 0.0037, -0.0391, -0.0055,  0.0978, -0.0093],\n",
      "        [ 0.0077, -0.0059,  0.0081,  0.0088,  0.0191],\n",
      "        [ 0.0007,  0.0345, -0.0147, -0.0154,  0.0014],\n",
      "        [-0.0165,  0.0109, -0.0027, -0.0655,  0.0537]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0167, -0.0121, -0.0167,  0.0084,  0.0097],\n",
      "        [-0.0025, -0.0374, -0.0511,  0.0702,  0.0102],\n",
      "        [ 0.0310, -0.0504,  0.0371, -0.0260, -0.0323],\n",
      "        [ 0.0588,  0.0971,  0.0187,  0.0426,  0.0297],\n",
      "        [ 0.0113,  0.0292,  0.0159, -0.0093, -0.0036],\n",
      "        [-0.0017,  0.0220, -0.0673,  0.0829,  0.0260],\n",
      "        [-0.0443, -0.0127, -0.0284, -0.0010, -0.0291],\n",
      "        [-0.0111, -0.0150, -0.0742,  0.0821,  0.0199]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0186, grad_fn=<MinBackward1>), tensor(0.9519, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.24233098328113556\n",
      "@sample 291: tensor([[ 0.0686, -0.1130, -0.0841,  0.1269, -0.0941],\n",
      "        [-0.0247,  0.0677,  0.0423, -0.0300,  0.0376],\n",
      "        [-0.0378,  0.0490,  0.0264, -0.0839,  0.0575],\n",
      "        [ 0.0090,  0.0169, -0.0128,  0.0053, -0.0193],\n",
      "        [ 0.0030,  0.0048,  0.0171, -0.0120,  0.0472],\n",
      "        [ 0.0059,  0.0453, -0.0032, -0.0049,  0.0094],\n",
      "        [ 0.0045,  0.0124, -0.0126,  0.0162,  0.0267],\n",
      "        [ 0.0168,  0.0057,  0.0253,  0.0028,  0.0027]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0129,  0.0634,  0.0151, -0.0041,  0.0511],\n",
      "        [-0.0293, -0.0309, -0.0780,  0.0431, -0.0097],\n",
      "        [ 0.0378, -0.0396, -0.0608,  0.0299, -0.0347],\n",
      "        [-0.0230, -0.0089,  0.0217, -0.0226, -0.0126],\n",
      "        [ 0.0054,  0.0086,  0.0172,  0.0093,  0.0302],\n",
      "        [-0.0169, -0.0354, -0.1130,  0.1119,  0.0507],\n",
      "        [-0.1059, -0.0206, -0.0416,  0.0385,  0.0196],\n",
      "        [-0.0613, -0.0181,  0.0300,  0.0320, -0.0207]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0211, grad_fn=<MinBackward1>), tensor(0.9610, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.25976741313934326\n",
      "@sample 292: tensor([[ 1.6236e-02,  1.4052e-02,  3.4400e-02, -2.7357e-03,  1.2436e-04],\n",
      "        [-6.8941e-03,  6.5193e-05,  1.2468e-02, -3.1664e-02,  3.6687e-02],\n",
      "        [-3.1305e-02,  5.7319e-02,  2.3177e-02, -1.4150e-01,  8.0422e-02],\n",
      "        [-4.8765e-02,  9.6318e-02, -1.4242e-02, -6.6131e-02,  4.0064e-03],\n",
      "        [-1.9249e-02, -3.6930e-03,  9.5562e-03, -5.3515e-03,  1.0908e-02],\n",
      "        [ 3.1847e-02,  2.4123e-02,  5.8614e-02,  8.6802e-04, -1.3387e-02],\n",
      "        [-1.2603e-02, -4.9676e-02,  1.5427e-02,  9.4602e-03,  2.6795e-02],\n",
      "        [-2.9952e-02,  6.5483e-02, -8.5258e-04, -8.4878e-02,  3.3292e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0038,  0.0282, -0.0082,  0.0259, -0.0334],\n",
      "        [-0.0311, -0.0508, -0.0218,  0.0657, -0.0259],\n",
      "        [-0.0199, -0.1065, -0.0639,  0.0424,  0.0385],\n",
      "        [-0.0533, -0.0989, -0.0691,  0.0710,  0.0213],\n",
      "        [ 0.0298,  0.0043,  0.0275, -0.0015, -0.0499],\n",
      "        [-0.0925,  0.0492, -0.0476,  0.0322, -0.0203],\n",
      "        [-0.0111, -0.0204, -0.0870,  0.0238,  0.0076],\n",
      "        [-0.0741, -0.0515, -0.0575,  0.0073, -0.0217]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0214, grad_fn=<MinBackward1>), tensor(0.9863, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.24672716856002808\n",
      "@sample 293: tensor([[-0.0557, -0.0299,  0.0143,  0.0514,  0.0213],\n",
      "        [-0.0034,  0.0256, -0.0308, -0.0087, -0.0286],\n",
      "        [ 0.0006,  0.0273,  0.0197, -0.0515, -0.0028],\n",
      "        [ 0.0065, -0.0008,  0.0326, -0.0105, -0.0255],\n",
      "        [ 0.0326,  0.0094,  0.0026,  0.0059,  0.0344],\n",
      "        [ 0.0101,  0.0443, -0.0172, -0.0307,  0.0051],\n",
      "        [ 0.0183,  0.0398, -0.0129, -0.0085, -0.0019],\n",
      "        [ 0.0077, -0.0578, -0.0394,  0.0514, -0.0467]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0642,  0.0205,  0.0224, -0.0522, -0.0154],\n",
      "        [ 0.0865, -0.0214, -0.0736,  0.0928, -0.0024],\n",
      "        [-0.0310, -0.0071, -0.0623,  0.0358,  0.0437],\n",
      "        [-0.0498,  0.0215, -0.0016,  0.0010, -0.0193],\n",
      "        [-0.0148, -0.0199, -0.0415,  0.0335, -0.0390],\n",
      "        [-0.0173, -0.0284, -0.0847,  0.0667, -0.0093],\n",
      "        [-0.0348, -0.0091, -0.0838,  0.0369,  0.0441],\n",
      "        [-0.0053,  0.0196, -0.0070, -0.0550,  0.0316]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0186, grad_fn=<MinBackward1>), tensor(0.9844, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.23519432544708252\n",
      "@sample 294: tensor([[-0.0507,  0.0576,  0.0013, -0.0014,  0.0179],\n",
      "        [-0.0099,  0.0031, -0.0499,  0.1041,  0.0099],\n",
      "        [-0.0112,  0.0051,  0.0435, -0.0580, -0.0055],\n",
      "        [-0.0071,  0.0057, -0.0123, -0.0163,  0.0192],\n",
      "        [-0.0169,  0.0010, -0.0243,  0.0954,  0.0141],\n",
      "        [-0.0078, -0.0054, -0.0286,  0.0097,  0.0069],\n",
      "        [-0.0184,  0.0423,  0.0112, -0.0645,  0.0429],\n",
      "        [-0.0163,  0.0015,  0.0134, -0.0325, -0.0294]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0528, -0.0114, -0.0427, -0.0223,  0.0267],\n",
      "        [ 0.0601,  0.1192, -0.0341,  0.0174,  0.0701],\n",
      "        [-0.0497,  0.0125,  0.0134, -0.0262, -0.0144],\n",
      "        [-0.0093,  0.0063,  0.0342, -0.0416, -0.0089],\n",
      "        [ 0.0270,  0.0999, -0.0612,  0.0071,  0.0344],\n",
      "        [-0.0007, -0.0091,  0.0186, -0.0366, -0.0644],\n",
      "        [-0.0216, -0.0407, -0.1307,  0.0320,  0.0044],\n",
      "        [-0.0424, -0.0812, -0.0638,  0.0427,  0.0060]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0244, grad_fn=<MinBackward1>), tensor(0.9654, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2401716262102127\n",
      "@sample 295: tensor([[-0.0266, -0.0423, -0.0042,  0.0179, -0.0290],\n",
      "        [ 0.0253,  0.0037, -0.0050, -0.0157,  0.0460],\n",
      "        [-0.0319,  0.0868, -0.0078, -0.0664,  0.0598],\n",
      "        [ 0.0092, -0.0551,  0.0247,  0.0423, -0.1008],\n",
      "        [-0.0209, -0.0347,  0.0166, -0.0357,  0.0266],\n",
      "        [-0.0056,  0.0119,  0.0388, -0.0061,  0.0172],\n",
      "        [ 0.0664,  0.0206,  0.0327,  0.0332, -0.0410],\n",
      "        [ 0.0194,  0.0311,  0.0040,  0.0092,  0.0061]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0090, -0.0136,  0.0150,  0.0257,  0.0128],\n",
      "        [-0.0140,  0.0328, -0.0703,  0.0676,  0.0246],\n",
      "        [-0.0377, -0.0087, -0.0290,  0.0291, -0.0575],\n",
      "        [-0.0286, -0.0442,  0.0793, -0.0432, -0.0064],\n",
      "        [-0.0607, -0.0376, -0.0692,  0.0570, -0.0084],\n",
      "        [ 0.0350, -0.0110, -0.0098, -0.0085,  0.1017],\n",
      "        [-0.0263,  0.0075, -0.0091, -0.0431, -0.0388],\n",
      "        [ 0.0022,  0.0069, -0.0321,  0.0182,  0.0053]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0215, grad_fn=<MinBackward1>), tensor(0.9349, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.245571568608284\n",
      "@sample 296: tensor([[-0.0297,  0.0113, -0.0066, -0.0637,  0.0849],\n",
      "        [-0.0344, -0.0049,  0.0207,  0.0039, -0.0219],\n",
      "        [ 0.0141,  0.0420,  0.0497, -0.0901,  0.0031],\n",
      "        [-0.0151, -0.0331, -0.0424,  0.0360,  0.0016],\n",
      "        [-0.0192, -0.0096,  0.0153,  0.0396,  0.0492],\n",
      "        [-0.0025, -0.0394, -0.0061,  0.0457, -0.0307],\n",
      "        [ 0.0209, -0.0080, -0.0562,  0.0291,  0.0157],\n",
      "        [-0.0028,  0.0419,  0.0480, -0.0589,  0.0883]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 5.8132e-02, -9.3034e-02, -1.2914e-01,  1.1223e-01,  3.1987e-02],\n",
      "        [-2.6481e-02, -2.4535e-02,  1.1901e-02,  9.8363e-03, -4.6768e-03],\n",
      "        [-2.7691e-02, -1.8749e-02, -5.4967e-02,  9.8392e-02,  3.2658e-02],\n",
      "        [ 5.3269e-02,  1.1774e-02,  3.9077e-02, -4.3266e-02,  3.7311e-02],\n",
      "        [ 7.4112e-03,  1.3059e-03, -3.0113e-03,  3.9637e-02,  4.6552e-02],\n",
      "        [ 4.1115e-02,  5.0504e-02,  4.8423e-02, -6.2260e-02,  7.5669e-03],\n",
      "        [ 1.1402e-02, -5.6334e-05,  4.6347e-02,  1.8928e-02,  3.1193e-02],\n",
      "        [ 9.3182e-03, -4.7143e-02, -1.0373e-01,  3.2729e-03,  2.3189e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0150, grad_fn=<MinBackward1>), tensor(0.9798, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.24224287271499634\n",
      "@sample 297: tensor([[ 0.0065,  0.0539, -0.0305,  0.0434,  0.0089],\n",
      "        [-0.0115,  0.0339,  0.0090, -0.0372,  0.0176],\n",
      "        [-0.0096, -0.0645, -0.0125,  0.0829, -0.0093],\n",
      "        [-0.0458,  0.0077, -0.0428, -0.0124,  0.0032],\n",
      "        [ 0.0138, -0.0850,  0.0057,  0.0659,  0.0033],\n",
      "        [-0.0568,  0.0247, -0.0221,  0.0554,  0.0011],\n",
      "        [-0.0009,  0.0059,  0.0249, -0.0637,  0.0132],\n",
      "        [ 0.0097, -0.0235,  0.0643,  0.0228,  0.0070]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0506,  0.0143, -0.0013,  0.0427,  0.0021],\n",
      "        [-0.0131, -0.0259, -0.0527,  0.0363,  0.0460],\n",
      "        [ 0.0736,  0.0911,  0.1342, -0.0572,  0.0339],\n",
      "        [ 0.0408, -0.0081,  0.0134, -0.0386,  0.0054],\n",
      "        [ 0.0190, -0.0008,  0.0206,  0.0146,  0.0566],\n",
      "        [ 0.0081,  0.0238,  0.0421, -0.0321,  0.0523],\n",
      "        [-0.0126, -0.0357, -0.0522, -0.0223,  0.0162],\n",
      "        [-0.0271, -0.0413, -0.1099,  0.0743,  0.0390]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0113, grad_fn=<MinBackward1>), tensor(0.9557, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2456013709306717\n",
      "@sample 298: tensor([[-0.0275,  0.0271,  0.0179, -0.0521,  0.0225],\n",
      "        [ 0.0310, -0.0129,  0.0118,  0.0312, -0.0279],\n",
      "        [-0.0059, -0.0090,  0.0227,  0.0152,  0.0196],\n",
      "        [-0.0051,  0.0556,  0.0022, -0.0342,  0.0117],\n",
      "        [ 0.0075, -0.0468, -0.0574,  0.1269, -0.0820],\n",
      "        [-0.0012,  0.0415,  0.0116, -0.0371,  0.0150],\n",
      "        [ 0.0128, -0.0382, -0.0328,  0.1223, -0.0152],\n",
      "        [ 0.0067, -0.0540, -0.0165,  0.0460, -0.0114]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0161,  0.0370, -0.1216,  0.0140,  0.0043],\n",
      "        [-0.0477,  0.0054, -0.0397, -0.0092, -0.0227],\n",
      "        [ 0.0030,  0.0045,  0.0060, -0.0083,  0.0444],\n",
      "        [-0.0416, -0.0616, -0.0857,  0.0328, -0.0170],\n",
      "        [ 0.0764,  0.0747,  0.1695, -0.1204,  0.0286],\n",
      "        [-0.0125, -0.0261, -0.0159, -0.0186,  0.0166],\n",
      "        [ 0.0969,  0.0533,  0.0447, -0.0314,  0.0126],\n",
      "        [ 0.0576,  0.0300,  0.1334, -0.0162,  0.0298]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0204, grad_fn=<MinBackward1>), tensor(0.9793, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2683635950088501\n",
      "@sample 299: tensor([[ 0.0325, -0.0194,  0.0035,  0.0069,  0.0179],\n",
      "        [-0.0034,  0.0496,  0.0101, -0.0508,  0.0218],\n",
      "        [ 0.0118,  0.0279,  0.0247, -0.0058,  0.0230],\n",
      "        [-0.0223, -0.0307, -0.0180, -0.0070,  0.0143],\n",
      "        [ 0.0024,  0.0171,  0.0414, -0.0804, -0.0015],\n",
      "        [-0.0276, -0.0045,  0.0058, -0.0046,  0.0099],\n",
      "        [-0.0405, -0.0488, -0.0486,  0.0251, -0.0291],\n",
      "        [ 0.0073,  0.0698, -0.0267, -0.0072,  0.0366]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0072,  0.0462, -0.0260,  0.0642,  0.0976],\n",
      "        [-0.0016, -0.0222,  0.0478,  0.0036,  0.1080],\n",
      "        [-0.0098, -0.0004, -0.0273, -0.0020, -0.0032],\n",
      "        [ 0.0010, -0.0015,  0.0848, -0.1074,  0.0184],\n",
      "        [ 0.0769, -0.0414,  0.0231,  0.0257,  0.0487],\n",
      "        [ 0.0084,  0.0044,  0.0021,  0.0028,  0.0344],\n",
      "        [-0.0078, -0.0205,  0.0648, -0.0723,  0.0205],\n",
      "        [-0.0345,  0.0321, -0.1065,  0.0292,  0.0274]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0123, grad_fn=<MinBackward1>), tensor(0.9659, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.23348122835159302\n",
      "@sample 300: tensor([[-0.0366,  0.0571,  0.0700, -0.0404,  0.0250],\n",
      "        [ 0.0364, -0.0655, -0.0105,  0.0293,  0.0084],\n",
      "        [-0.0324, -0.0223,  0.0442, -0.0108,  0.0077],\n",
      "        [-0.0161, -0.0686, -0.0197,  0.0419, -0.0270],\n",
      "        [ 0.0011,  0.0575, -0.0449,  0.0319,  0.0098],\n",
      "        [-0.0072, -0.0338, -0.0438,  0.0652, -0.0044],\n",
      "        [-0.0242,  0.0328, -0.0503, -0.0008,  0.0189],\n",
      "        [ 0.0276, -0.0644, -0.0519,  0.0671,  0.0058]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0027, -0.0385, -0.0211,  0.0441, -0.0314],\n",
      "        [ 0.0088,  0.0355,  0.0179,  0.0374,  0.0593],\n",
      "        [ 0.0169, -0.0145,  0.0780,  0.0443,  0.0326],\n",
      "        [ 0.0326,  0.0259,  0.0549, -0.0679, -0.0311],\n",
      "        [ 0.0154, -0.0014, -0.0599,  0.0032,  0.0027],\n",
      "        [ 0.0238,  0.0499,  0.0164, -0.0733,  0.0155],\n",
      "        [ 0.0644, -0.0038,  0.0285, -0.0052,  0.0066],\n",
      "        [ 0.0506,  0.0620,  0.0215, -0.0365,  0.0168]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0072, grad_fn=<MinBackward1>), tensor(0.9442, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.24073578417301178\n",
      "@sample 301: tensor([[ 2.1644e-02,  7.5394e-03, -1.4276e-02,  1.1337e-02, -2.6388e-02],\n",
      "        [ 3.4898e-02,  2.4256e-03, -2.0018e-02, -2.1272e-02, -6.7833e-03],\n",
      "        [ 5.1091e-03,  1.9005e-02,  1.7266e-02,  5.7108e-03,  3.0197e-03],\n",
      "        [ 3.0871e-02,  8.0601e-02, -1.5484e-02,  1.8214e-03, -1.6427e-03],\n",
      "        [-9.1062e-03, -1.2178e-02, -9.6089e-03,  2.3194e-02,  2.7943e-02],\n",
      "        [-7.4785e-03, -2.7958e-02,  1.7932e-02, -3.2444e-02,  1.0014e-02],\n",
      "        [ 1.4344e-02,  6.2975e-02,  6.3728e-02, -6.6162e-02,  4.7563e-02],\n",
      "        [-7.9209e-06,  1.2206e-03, -4.6174e-03,  4.1175e-02, -8.4228e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0375, -0.0020,  0.0109, -0.0676, -0.0539],\n",
      "        [-0.0294, -0.0844, -0.0059,  0.0222, -0.0715],\n",
      "        [-0.0158,  0.0193, -0.0416,  0.0208, -0.0316],\n",
      "        [ 0.0096, -0.0184, -0.0455,  0.0025,  0.0196],\n",
      "        [-0.0395,  0.0284,  0.0211, -0.0110, -0.0194],\n",
      "        [-0.0498,  0.0096, -0.0168, -0.0106,  0.0385],\n",
      "        [-0.0227, -0.0340, -0.0948,  0.0462, -0.0399],\n",
      "        [-0.0111,  0.0179,  0.0100, -0.0213,  0.0040]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0207, grad_fn=<MinBackward1>), tensor(0.9649, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.22065506875514984\n",
      "@sample 302: tensor([[-0.0956,  0.1003,  0.0694, -0.0861, -0.0066],\n",
      "        [-0.0281, -0.0211, -0.0176,  0.0552,  0.0185],\n",
      "        [-0.0398, -0.0042,  0.0369,  0.0145,  0.0328],\n",
      "        [ 0.0162, -0.0381, -0.0151,  0.0567,  0.0345],\n",
      "        [-0.0220,  0.0582,  0.0250, -0.0132,  0.0128],\n",
      "        [-0.0032,  0.0050,  0.0009,  0.0538, -0.0189],\n",
      "        [-0.0799, -0.0390,  0.0092,  0.0191,  0.0361],\n",
      "        [ 0.0352, -0.0620, -0.0333,  0.0720, -0.0193]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0618, -0.0484, -0.1153,  0.0045,  0.0066],\n",
      "        [ 0.0670,  0.0145,  0.0956, -0.0426, -0.0204],\n",
      "        [ 0.0559,  0.0295, -0.0947,  0.0437,  0.0748],\n",
      "        [ 0.0641,  0.0150, -0.0445,  0.0251,  0.0624],\n",
      "        [-0.0301, -0.0043, -0.0360,  0.0539,  0.0135],\n",
      "        [ 0.0334,  0.0242,  0.0264,  0.0090, -0.0053],\n",
      "        [ 0.0082,  0.0002, -0.0135,  0.0243,  0.0364],\n",
      "        [ 0.0591,  0.0674,  0.0680, -0.0810, -0.0037]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0142, grad_fn=<MinBackward1>), tensor(0.9491, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2645499110221863\n",
      "@sample 303: tensor([[ 3.6086e-02, -2.3279e-02, -6.9349e-02,  1.6556e-02,  7.5213e-03],\n",
      "        [-1.1542e-04, -9.2834e-05,  2.5453e-02, -5.0875e-02, -3.6112e-03],\n",
      "        [-5.1619e-03, -5.6685e-03,  1.1901e-02, -6.1794e-02, -2.9594e-02],\n",
      "        [ 1.1143e-02, -1.1242e-02, -2.6898e-03,  4.3647e-02, -2.8183e-02],\n",
      "        [ 3.5184e-02,  1.6965e-02, -4.8951e-02,  3.4977e-02,  4.2426e-02],\n",
      "        [-6.3285e-03, -8.0707e-02,  3.1480e-02,  2.2811e-02, -1.4563e-02],\n",
      "        [ 2.5906e-02,  1.5498e-02,  2.8462e-02, -5.8479e-03, -2.0518e-02],\n",
      "        [-2.8886e-02,  6.9589e-02,  1.6662e-02, -7.0845e-02,  1.1200e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0179,  0.0248,  0.0316, -0.0105,  0.0569],\n",
      "        [-0.0274, -0.0021, -0.0274,  0.0152,  0.0039],\n",
      "        [-0.0278, -0.0504, -0.1014,  0.1153,  0.0384],\n",
      "        [-0.0054,  0.0518,  0.1122, -0.0133,  0.0750],\n",
      "        [ 0.0105,  0.0309,  0.0054, -0.0364, -0.0186],\n",
      "        [ 0.0066, -0.0253,  0.1097, -0.0449,  0.0007],\n",
      "        [-0.0403,  0.0301, -0.0497,  0.0462,  0.0034],\n",
      "        [-0.0510, -0.0433, -0.0485,  0.0179,  0.0570]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0205, grad_fn=<MinBackward1>), tensor(0.9643, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2462390661239624\n",
      "@sample 304: tensor([[ 0.0017,  0.0415,  0.0019, -0.0279,  0.0047],\n",
      "        [ 0.0818,  0.0037,  0.0299, -0.0241, -0.0229],\n",
      "        [ 0.0115, -0.0172,  0.0014,  0.0560, -0.0322],\n",
      "        [ 0.0203, -0.0011,  0.0056, -0.0103,  0.0052],\n",
      "        [ 0.0296, -0.0404,  0.0029,  0.0828, -0.0477],\n",
      "        [-0.0108, -0.0891, -0.0130, -0.0196,  0.0139],\n",
      "        [-0.0067,  0.0308,  0.0706,  0.0114,  0.0374],\n",
      "        [-0.0233, -0.0185, -0.0319, -0.0006, -0.0277]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0022, -0.0054,  0.0663, -0.0021, -0.0041],\n",
      "        [-0.0638, -0.0038, -0.0201, -0.0280, -0.0680],\n",
      "        [ 0.0175,  0.0582,  0.0714, -0.0289,  0.0135],\n",
      "        [ 0.0314, -0.0324, -0.0065, -0.0311, -0.0516],\n",
      "        [ 0.0299,  0.0167, -0.0031, -0.0600, -0.0534],\n",
      "        [-0.0707, -0.0276,  0.0945, -0.0865, -0.0162],\n",
      "        [-0.0376,  0.0139, -0.0829, -0.0301,  0.0759],\n",
      "        [-0.0113, -0.0185,  0.0098, -0.0111,  0.0236]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0241, grad_fn=<MinBackward1>), tensor(0.9773, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.23169636726379395\n",
      "@sample 305: tensor([[-0.0297, -0.0290,  0.0228,  0.0033, -0.0203],\n",
      "        [ 0.0093, -0.0112, -0.0098,  0.0134,  0.0141],\n",
      "        [ 0.0111, -0.0218,  0.0076, -0.0058,  0.0120],\n",
      "        [ 0.0527, -0.0317, -0.0011,  0.0174, -0.0236],\n",
      "        [ 0.0058,  0.0148,  0.0439,  0.0283, -0.0036],\n",
      "        [ 0.0240,  0.0020,  0.0019,  0.0005,  0.0016],\n",
      "        [-0.0154, -0.0037,  0.0829,  0.0305,  0.0057],\n",
      "        [-0.0245, -0.0144, -0.0129,  0.0558, -0.0186]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0199, -0.0138,  0.0770, -0.0665, -0.0323],\n",
      "        [ 0.0709, -0.0073,  0.0570, -0.0237,  0.0046],\n",
      "        [ 0.0074,  0.0229,  0.0188, -0.0219, -0.0454],\n",
      "        [ 0.0603,  0.0335,  0.0031, -0.0207, -0.0947],\n",
      "        [ 0.0281,  0.0797,  0.0185,  0.0454,  0.0449],\n",
      "        [-0.0081, -0.0221,  0.0100, -0.0150,  0.0100],\n",
      "        [-0.0529,  0.0515,  0.0242, -0.0616,  0.0178],\n",
      "        [ 0.0202,  0.0440,  0.0688, -0.0623,  0.0127]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0205, grad_fn=<MinBackward1>), tensor(0.9368, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.21993696689605713\n",
      "@sample 306: tensor([[-0.0046, -0.0319,  0.0303, -0.0109,  0.0499],\n",
      "        [-0.0178,  0.0317,  0.0118, -0.0377,  0.0315],\n",
      "        [-0.0349,  0.0034,  0.0090,  0.0539, -0.0247],\n",
      "        [-0.0067,  0.0142,  0.0197, -0.0722,  0.0847],\n",
      "        [ 0.0104, -0.0334, -0.0017, -0.0326, -0.0036],\n",
      "        [-0.0122,  0.0356,  0.0119, -0.0821, -0.0013],\n",
      "        [ 0.0266,  0.0535,  0.0292, -0.0596,  0.0085],\n",
      "        [-0.0009, -0.0846, -0.0384,  0.0333, -0.0326]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0127, -0.0123,  0.0581,  0.0235, -0.0025],\n",
      "        [-0.0408, -0.0277, -0.0476,  0.0476,  0.0037],\n",
      "        [ 0.0023, -0.0098, -0.0357, -0.0524, -0.0397],\n",
      "        [-0.0241, -0.0068, -0.0589,  0.0118, -0.0619],\n",
      "        [-0.0747, -0.0766,  0.0891, -0.0424, -0.1221],\n",
      "        [-0.0515, -0.0367, -0.0122, -0.0339, -0.0931],\n",
      "        [-0.0970, -0.0415, -0.0905,  0.0850,  0.0286],\n",
      "        [-0.0167,  0.0125,  0.1303, -0.0862,  0.0589]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0199, grad_fn=<MinBackward1>), tensor(0.9787, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2555343508720398\n",
      "@sample 307: tensor([[-0.0080,  0.0289,  0.0333, -0.0377,  0.0045],\n",
      "        [-0.0272, -0.0073, -0.0441,  0.0143, -0.0499],\n",
      "        [ 0.0325, -0.0094, -0.0206, -0.0070, -0.0189],\n",
      "        [-0.0097, -0.0029,  0.0182, -0.0132,  0.0059],\n",
      "        [ 0.0218, -0.0456,  0.0207,  0.0527, -0.0361],\n",
      "        [-0.0066,  0.1200,  0.0005, -0.0729,  0.0391],\n",
      "        [-0.0044,  0.0152, -0.0146, -0.0279,  0.0096],\n",
      "        [-0.0108,  0.0110, -0.0165, -0.0036,  0.0379]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0254, -0.0097, -0.0191,  0.0622,  0.0631],\n",
      "        [-0.0457,  0.0141,  0.0401, -0.0247, -0.0040],\n",
      "        [-0.0259,  0.0249,  0.0572, -0.0399, -0.0215],\n",
      "        [ 0.0142, -0.0338,  0.0041, -0.0111, -0.0181],\n",
      "        [ 0.0331,  0.0922,  0.0981,  0.0102, -0.0011],\n",
      "        [-0.0337, -0.0797, -0.1447,  0.1150,  0.0046],\n",
      "        [-0.0182, -0.0218, -0.0298,  0.0082, -0.0596],\n",
      "        [ 0.0167,  0.0664, -0.0289,  0.0145,  0.0434]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0231, grad_fn=<MinBackward1>), tensor(0.9586, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2351049929857254\n",
      "@sample 308: tensor([[-0.0183,  0.0567,  0.0617,  0.0254, -0.0385],\n",
      "        [-0.0129,  0.0204, -0.0343, -0.0094, -0.0049],\n",
      "        [-0.0027,  0.0224,  0.0976, -0.0194, -0.0099],\n",
      "        [-0.0750, -0.0279,  0.0179, -0.0576,  0.0182],\n",
      "        [-0.0346,  0.0151,  0.0560, -0.0512,  0.0068],\n",
      "        [ 0.0132, -0.0317, -0.0363,  0.0302, -0.0407],\n",
      "        [-0.0569,  0.0018, -0.0304,  0.0259, -0.0383],\n",
      "        [ 0.0084, -0.0185,  0.0192,  0.0267, -0.1001]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0583,  0.0779, -0.0264, -0.0060, -0.0016],\n",
      "        [-0.0102,  0.0003, -0.0194,  0.0093, -0.0290],\n",
      "        [-0.0782, -0.0133, -0.0799,  0.1186,  0.0321],\n",
      "        [ 0.0295, -0.0935,  0.0391, -0.0479,  0.0038],\n",
      "        [-0.0544, -0.0310, -0.0181,  0.0069, -0.0376],\n",
      "        [-0.0126, -0.0070,  0.0164,  0.0099, -0.0186],\n",
      "        [-0.0140, -0.0138,  0.0786, -0.0719, -0.0567],\n",
      "        [ 0.0073, -0.0164,  0.0691, -0.0377, -0.0377]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0211, grad_fn=<MinBackward1>), tensor(0.9613, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2467186003923416\n",
      "@sample 309: tensor([[ 0.0216, -0.0032, -0.0027, -0.0366, -0.0594],\n",
      "        [-0.0052, -0.0109,  0.0194,  0.0264, -0.0013],\n",
      "        [ 0.0103,  0.0197, -0.0277, -0.1571,  0.0305],\n",
      "        [ 0.0151,  0.0314, -0.0265, -0.0038,  0.0309],\n",
      "        [-0.0193,  0.0167, -0.0034,  0.0087,  0.0087],\n",
      "        [-0.0384, -0.0771, -0.0445,  0.0448, -0.0087],\n",
      "        [ 0.0364, -0.0050, -0.0041,  0.0293, -0.0360],\n",
      "        [ 0.0402,  0.0530,  0.0856, -0.0584,  0.0407]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0436, -0.0599,  0.0540, -0.0191, -0.0437],\n",
      "        [ 0.0272, -0.0024,  0.0762,  0.0229,  0.0382],\n",
      "        [-0.0976, -0.0996, -0.0646,  0.0654,  0.0422],\n",
      "        [-0.0112, -0.0219, -0.0748,  0.0314,  0.0424],\n",
      "        [ 0.0523,  0.0414,  0.0515, -0.0223, -0.0591],\n",
      "        [ 0.0112,  0.0029,  0.0625, -0.0843, -0.0045],\n",
      "        [-0.0191,  0.0311,  0.0738, -0.0160, -0.0128],\n",
      "        [-0.0064,  0.0019, -0.0383,  0.0070,  0.0261]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0223, grad_fn=<MinBackward1>), tensor(0.9443, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.24917806684970856\n",
      "@sample 310: tensor([[-0.0158,  0.0131, -0.0144, -0.0338, -0.0089],\n",
      "        [-0.0004,  0.0004,  0.0161, -0.0136,  0.0405],\n",
      "        [-0.0278,  0.0049,  0.0807, -0.0684,  0.0400],\n",
      "        [-0.0286,  0.0409,  0.0168, -0.0416,  0.0567],\n",
      "        [-0.0012,  0.0529,  0.0461, -0.0365,  0.0404],\n",
      "        [-0.0126, -0.0027,  0.0722,  0.0192,  0.0406],\n",
      "        [ 0.0090, -0.0024, -0.0343, -0.1247,  0.0410],\n",
      "        [ 0.0099, -0.0341, -0.0482,  0.0243, -0.0280]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0190, -0.0788,  0.0270, -0.0484, -0.0855],\n",
      "        [ 0.0125, -0.0341, -0.0366,  0.0285,  0.0335],\n",
      "        [-0.0236, -0.0722, -0.0862,  0.0686,  0.0003],\n",
      "        [ 0.0217, -0.0052, -0.0958,  0.0427, -0.0012],\n",
      "        [-0.0283, -0.0152, -0.0360,  0.0396,  0.0048],\n",
      "        [ 0.0270,  0.0055,  0.0133,  0.0212,  0.0124],\n",
      "        [-0.0783, -0.0519, -0.0738,  0.0688, -0.0044],\n",
      "        [-0.0029, -0.0003, -0.0257,  0.0267,  0.0496]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0140, grad_fn=<MinBackward1>), tensor(0.9816, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.24443018436431885\n",
      "@sample 311: tensor([[-0.0247, -0.0053,  0.0115,  0.0056, -0.0314],\n",
      "        [ 0.0479,  0.0005,  0.0913, -0.0461,  0.0760],\n",
      "        [-0.0208,  0.0189,  0.0359,  0.0164,  0.0063],\n",
      "        [-0.0063,  0.0381,  0.0145, -0.0846,  0.0243],\n",
      "        [-0.0147, -0.0627, -0.0368,  0.0368,  0.0056],\n",
      "        [-0.0107, -0.0198, -0.0125,  0.0031,  0.0282],\n",
      "        [-0.0034,  0.0236,  0.0131, -0.0540,  0.0097],\n",
      "        [-0.0011,  0.0202,  0.0364, -0.0422,  0.0005]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-6.0107e-03, -1.1906e-02, -3.8266e-02,  6.5626e-03,  9.1744e-03],\n",
      "        [ 1.9967e-02, -4.8155e-02, -4.3996e-02,  4.4899e-02, -2.5779e-02],\n",
      "        [-1.1888e-02,  3.9067e-02, -9.6129e-02, -7.8143e-03, -2.6070e-02],\n",
      "        [-8.9513e-02, -8.4202e-02, -8.7260e-02, -9.7966e-03, -1.4693e-02],\n",
      "        [ 7.5352e-02,  3.1620e-02,  4.7446e-02, -2.4900e-02,  4.6004e-02],\n",
      "        [ 2.5393e-02,  1.1497e-02, -1.0118e-01,  1.7121e-02,  5.5695e-02],\n",
      "        [-5.2776e-02, -3.5650e-03, -7.5024e-02,  5.6204e-02, -1.2405e-06],\n",
      "        [-5.1943e-03, -1.0946e-02, -8.9990e-02,  3.1788e-02, -4.7399e-04]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0196, grad_fn=<MinBackward1>), tensor(0.9607, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.23939652740955353\n",
      "@sample 312: tensor([[ 0.0377,  0.0143, -0.0081, -0.0465,  0.0284],\n",
      "        [-0.0379,  0.0500,  0.0201, -0.0893,  0.0288],\n",
      "        [ 0.0304, -0.0171,  0.0489, -0.0376, -0.0599],\n",
      "        [-0.0019,  0.0404,  0.0121, -0.0690,  0.0370],\n",
      "        [-0.0341,  0.0238, -0.0654,  0.0708, -0.0610],\n",
      "        [ 0.0075,  0.0407,  0.0083, -0.0073,  0.0263],\n",
      "        [-0.0149, -0.0187, -0.0318, -0.0240,  0.0284],\n",
      "        [-0.0311,  0.0213, -0.0461, -0.0025,  0.0142]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0014, -0.0345, -0.1056,  0.0146,  0.0174],\n",
      "        [-0.0340, -0.0579, -0.0885,  0.0464, -0.0158],\n",
      "        [-0.0818, -0.0100, -0.0070,  0.0468,  0.0131],\n",
      "        [-0.0144, -0.0417, -0.0400,  0.0394, -0.0246],\n",
      "        [ 0.0072,  0.0088,  0.0260, -0.0632,  0.0473],\n",
      "        [-0.0239, -0.0074, -0.0873,  0.0468,  0.0057],\n",
      "        [ 0.0192, -0.0630, -0.0160, -0.0603, -0.0047],\n",
      "        [-0.0246, -0.0606, -0.0089, -0.0553, -0.0642]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0265, grad_fn=<MinBackward1>), tensor(0.9853, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.24012206494808197\n",
      "@sample 313: tensor([[ 0.0038, -0.0228, -0.0646,  0.0680,  0.0085],\n",
      "        [ 0.0264, -0.0239,  0.0157,  0.0024, -0.0244],\n",
      "        [ 0.0336, -0.0055, -0.0052,  0.0382, -0.0508],\n",
      "        [-0.0198,  0.1446,  0.0269, -0.0562,  0.0205],\n",
      "        [-0.0037, -0.0174, -0.0002,  0.0569,  0.0114],\n",
      "        [-0.0090,  0.0039, -0.0175, -0.0078,  0.0289],\n",
      "        [-0.0236, -0.0270,  0.0190,  0.0027,  0.0691],\n",
      "        [ 0.0051,  0.0578,  0.0243, -0.0817, -0.0009]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0172, -0.0162,  0.0169,  0.0324, -0.0140],\n",
      "        [ 0.0082, -0.0468,  0.0227, -0.0279,  0.0445],\n",
      "        [-0.0092,  0.0787, -0.0310,  0.0407,  0.0071],\n",
      "        [-0.0176, -0.0775, -0.1728,  0.0208, -0.0249],\n",
      "        [ 0.0247,  0.0204, -0.0048, -0.0104, -0.0190],\n",
      "        [-0.0221, -0.0017, -0.0935, -0.0078,  0.0118],\n",
      "        [ 0.0462, -0.0246, -0.0078, -0.0341, -0.0175],\n",
      "        [-0.0303, -0.0486, -0.0797,  0.0218, -0.0020]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0167, grad_fn=<MinBackward1>), tensor(0.9603, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.23796580731868744\n",
      "@sample 314: tensor([[-0.0261,  0.0043,  0.0038,  0.0209, -0.0244],\n",
      "        [ 0.0346,  0.0004,  0.0035,  0.0132,  0.0440],\n",
      "        [-0.0129, -0.0089,  0.0661, -0.0396,  0.0607],\n",
      "        [-0.0349, -0.0122,  0.0195, -0.0157,  0.0110],\n",
      "        [-0.0028,  0.0259,  0.0043,  0.0252,  0.0322],\n",
      "        [ 0.0013,  0.0279,  0.0191, -0.0037, -0.0015],\n",
      "        [-0.0074,  0.0204,  0.0237, -0.0268,  0.0114],\n",
      "        [ 0.0082, -0.0155, -0.0056,  0.0492, -0.0887]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0517,  0.0443,  0.0596, -0.0060, -0.0069],\n",
      "        [-0.0225,  0.0422, -0.1027,  0.1175,  0.0187],\n",
      "        [ 0.0157, -0.0305, -0.0600,  0.1072,  0.0266],\n",
      "        [-0.0092,  0.0120, -0.0128,  0.0153,  0.0153],\n",
      "        [ 0.1175,  0.0354, -0.0184, -0.0864, -0.0408],\n",
      "        [ 0.0035, -0.0061,  0.0258,  0.0048, -0.0074],\n",
      "        [-0.0268,  0.0196, -0.0562,  0.0399,  0.0166],\n",
      "        [ 0.0200,  0.0166,  0.1393, -0.1244, -0.0806]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0204, grad_fn=<MinBackward1>), tensor(0.9894, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2408236712217331\n",
      "@sample 315: tensor([[-0.0427, -0.0640,  0.0051,  0.0311,  0.0335],\n",
      "        [-0.0114,  0.0218,  0.0059, -0.0392,  0.0132],\n",
      "        [-0.0256,  0.0547,  0.0026, -0.0542,  0.0432],\n",
      "        [ 0.0098,  0.0467,  0.0237, -0.1114,  0.0432],\n",
      "        [ 0.0365,  0.0737,  0.0250, -0.0692,  0.0248],\n",
      "        [ 0.0261,  0.0179, -0.0010,  0.0039, -0.0096],\n",
      "        [-0.0140,  0.1073,  0.0495, -0.0670,  0.0085],\n",
      "        [-0.0236,  0.0545, -0.0361,  0.0178, -0.0010]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0456,  0.0262,  0.0793,  0.0155, -0.0199],\n",
      "        [-0.0296, -0.0527, -0.0512,  0.0192, -0.0323],\n",
      "        [-0.0450,  0.0232, -0.0398,  0.0235,  0.0481],\n",
      "        [-0.0871, -0.0675, -0.0485,  0.0117, -0.0379],\n",
      "        [-0.0725, -0.0709, -0.1347,  0.0891, -0.0005],\n",
      "        [-0.0341,  0.0234, -0.0220, -0.0104, -0.0331],\n",
      "        [-0.0075, -0.0649, -0.1104,  0.0551,  0.0246],\n",
      "        [ 0.0138,  0.0082, -0.1403,  0.0485, -0.0135]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0098, grad_fn=<MinBackward1>), tensor(0.9528, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.25499778985977173\n",
      "@sample 316: tensor([[ 0.0014,  0.0006,  0.0297, -0.0027,  0.0110],\n",
      "        [-0.0269, -0.0488,  0.0261,  0.0078, -0.0272],\n",
      "        [-0.0216,  0.0053,  0.0243, -0.1269,  0.0292],\n",
      "        [ 0.0208, -0.0488, -0.0485,  0.1065, -0.0479],\n",
      "        [-0.0259,  0.0473,  0.0042, -0.0833,  0.0338],\n",
      "        [ 0.0079,  0.0423,  0.0019,  0.0032,  0.0127],\n",
      "        [ 0.0321,  0.0236, -0.0796,  0.0712,  0.0067],\n",
      "        [ 0.0361, -0.0137,  0.0383, -0.0426, -0.0282]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0081,  0.0101,  0.0100,  0.0266,  0.0321],\n",
      "        [ 0.0337,  0.0213,  0.0425,  0.0009,  0.0304],\n",
      "        [-0.0509, -0.1175, -0.0354,  0.0645, -0.0652],\n",
      "        [ 0.0432,  0.0675,  0.0458, -0.0688, -0.0052],\n",
      "        [-0.0199, -0.1025, -0.0749,  0.0392,  0.0306],\n",
      "        [ 0.0006, -0.0053, -0.0416,  0.0369,  0.0192],\n",
      "        [ 0.0737,  0.0545, -0.0130,  0.0323,  0.0332],\n",
      "        [-0.0646, -0.0367, -0.0059, -0.0766, -0.0435]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0130, grad_fn=<MinBackward1>), tensor(0.9395, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.24415335059165955\n",
      "@sample 317: tensor([[-0.0142,  0.0823,  0.0270, -0.0648,  0.0121],\n",
      "        [-0.0468,  0.0525,  0.0499, -0.0091,  0.0072],\n",
      "        [ 0.0253,  0.0370,  0.0127,  0.0173, -0.0051],\n",
      "        [-0.0347, -0.0050,  0.0097, -0.0179,  0.0386],\n",
      "        [ 0.0130,  0.0382,  0.0058, -0.0615,  0.0434],\n",
      "        [-0.0951, -0.0003, -0.0129, -0.1185,  0.0454],\n",
      "        [-0.0168, -0.0527,  0.0010,  0.0212,  0.0113],\n",
      "        [-0.0074,  0.0119,  0.0211,  0.0223,  0.0194]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0313, -0.0237, -0.0415,  0.0680, -0.0132],\n",
      "        [-0.0661, -0.0224, -0.0891, -0.0050, -0.0334],\n",
      "        [ 0.0003,  0.0060, -0.0340, -0.0090, -0.0454],\n",
      "        [-0.0076, -0.0235,  0.0454, -0.0705, -0.0048],\n",
      "        [-0.0153, -0.0409, -0.0809,  0.0581, -0.0287],\n",
      "        [-0.1256, -0.0806, -0.0287,  0.0683, -0.0050],\n",
      "        [ 0.0680,  0.0490,  0.0122, -0.0165,  0.0526],\n",
      "        [ 0.0797,  0.0270, -0.0401, -0.0140,  0.0172]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0248, grad_fn=<MinBackward1>), tensor(0.9546, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.24368450045585632\n",
      "@sample 318: tensor([[-0.0155,  0.0187,  0.0324, -0.0536, -0.0202],\n",
      "        [-0.0198,  0.0519,  0.0140, -0.0448,  0.0238],\n",
      "        [-0.0074, -0.0460, -0.0314,  0.0550, -0.0440],\n",
      "        [-0.0739,  0.0089,  0.0222, -0.0594,  0.0207],\n",
      "        [-0.0107,  0.0459,  0.0157, -0.0362,  0.0258],\n",
      "        [-0.0151, -0.0715, -0.0039,  0.0268,  0.0028],\n",
      "        [-0.0155,  0.0185, -0.0572,  0.0020, -0.0390],\n",
      "        [-0.0098,  0.0189,  0.0341, -0.0049,  0.0268]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0418, -0.0070, -0.0055,  0.0054, -0.0317],\n",
      "        [-0.0151, -0.0374, -0.0395,  0.0366, -0.0160],\n",
      "        [ 0.0246,  0.0384,  0.0340, -0.0619, -0.0371],\n",
      "        [-0.0854, -0.0221,  0.0597, -0.0610, -0.0172],\n",
      "        [-0.0387, -0.0667, -0.1018,  0.0672,  0.0281],\n",
      "        [-0.0016,  0.0094,  0.0641, -0.0228,  0.0430],\n",
      "        [-0.0050,  0.0307, -0.0711, -0.0403, -0.0969],\n",
      "        [-0.0247,  0.0150, -0.0090,  0.0166, -0.0285]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0096, grad_fn=<MinBackward1>), tensor(0.9622, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.23321418464183807\n",
      "@sample 319: tensor([[ 0.0534,  0.0761, -0.0183, -0.1239, -0.0067],\n",
      "        [-0.0171,  0.1368, -0.0035, -0.0644,  0.0536],\n",
      "        [ 0.0134, -0.0670, -0.0067,  0.0182,  0.0200],\n",
      "        [-0.0169, -0.0055, -0.0268,  0.0399, -0.0469],\n",
      "        [-0.0302, -0.0538, -0.0074,  0.0210, -0.0136],\n",
      "        [-0.0245,  0.0370, -0.0611,  0.0125, -0.0010],\n",
      "        [-0.0121, -0.0149, -0.0147,  0.0092, -0.0132],\n",
      "        [-0.0029,  0.0386,  0.0325, -0.0134,  0.0621]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.1020, -0.1001, -0.0058, -0.0210,  0.0327],\n",
      "        [-0.0483,  0.0029, -0.1605,  0.0493, -0.0009],\n",
      "        [ 0.0044, -0.0395, -0.0531,  0.0425,  0.0420],\n",
      "        [ 0.0101,  0.0408,  0.1239, -0.0881, -0.0811],\n",
      "        [ 0.0570, -0.0294,  0.0119, -0.0196,  0.0339],\n",
      "        [ 0.0198, -0.0091, -0.1235,  0.0082, -0.0688],\n",
      "        [-0.0441,  0.0385,  0.0002, -0.0705, -0.0186],\n",
      "        [-0.0168, -0.0205, -0.0622,  0.0794, -0.0150]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0110, grad_fn=<MinBackward1>), tensor(0.9407, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.26601430773735046\n",
      "@sample 320: tensor([[-0.0251,  0.0160,  0.0018, -0.0294, -0.0334],\n",
      "        [ 0.0240,  0.0383,  0.0027, -0.0094,  0.0023],\n",
      "        [-0.0125,  0.0669,  0.0581, -0.0398,  0.0737],\n",
      "        [-0.0094,  0.0358,  0.0117,  0.0356, -0.0311],\n",
      "        [-0.0244, -0.0365, -0.0313,  0.0249, -0.0591],\n",
      "        [-0.0135,  0.0551,  0.0522, -0.0409, -0.0136],\n",
      "        [ 0.0256,  0.0122, -0.0254,  0.0113, -0.0004],\n",
      "        [-0.0138,  0.0084,  0.0336,  0.0213,  0.0057]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0287,  0.0333,  0.0462, -0.0220, -0.0151],\n",
      "        [-0.0071, -0.0022, -0.0289,  0.0402,  0.0075],\n",
      "        [-0.0131, -0.0413, -0.0884,  0.0552,  0.0047],\n",
      "        [ 0.0257,  0.0610, -0.0311, -0.0148,  0.0141],\n",
      "        [ 0.0153, -0.0244,  0.0382, -0.0130,  0.0308],\n",
      "        [-0.0761, -0.0381, -0.0908,  0.0161, -0.0011],\n",
      "        [-0.0114,  0.0164,  0.0457, -0.0423, -0.0073],\n",
      "        [-0.0178,  0.0193,  0.0185,  0.0259,  0.0025]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0230, grad_fn=<MinBackward1>), tensor(0.9585, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.22284463047981262\n",
      "@sample 321: tensor([[ 0.0142,  0.0466,  0.0152, -0.0112,  0.0081],\n",
      "        [-0.0470, -0.0583, -0.0331,  0.0294, -0.0159],\n",
      "        [ 0.0557, -0.0493, -0.0515,  0.0614, -0.0206],\n",
      "        [ 0.0277, -0.0023,  0.0604, -0.0451,  0.0257],\n",
      "        [ 0.0119, -0.0698,  0.0198,  0.1035, -0.0811],\n",
      "        [ 0.0213, -0.0548, -0.0422,  0.0953, -0.0106],\n",
      "        [ 0.0054, -0.0350, -0.0364,  0.0566, -0.0251],\n",
      "        [ 0.0232,  0.0242, -0.0533,  0.0182, -0.0172]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-3.9872e-02, -2.4091e-02, -9.8121e-02,  4.3695e-02,  1.8875e-02],\n",
      "        [ 1.7694e-02,  1.2522e-04,  1.1677e-01, -4.8087e-02,  1.2393e-02],\n",
      "        [ 8.3347e-02,  4.5657e-02, -1.7005e-02,  7.7824e-02,  3.7686e-02],\n",
      "        [-3.0843e-02, -4.0769e-02, -6.1710e-02,  1.0452e-01,  3.6567e-02],\n",
      "        [ 2.3530e-02,  8.5571e-02,  1.2746e-01, -9.0352e-02, -3.6237e-02],\n",
      "        [ 6.1322e-02,  6.3051e-02,  1.1249e-02, -3.5505e-02, -6.3152e-03],\n",
      "        [ 6.8694e-02,  5.2453e-02,  6.5316e-03, -2.0880e-02,  5.7338e-03],\n",
      "        [ 5.1650e-03,  4.2242e-02,  5.3298e-02, -4.9935e-02, -7.2098e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0185, grad_fn=<MinBackward1>), tensor(0.9570, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2555188536643982\n",
      "@sample 322: tensor([[ 0.0095, -0.0256, -0.0322,  0.0440, -0.0512],\n",
      "        [-0.0191,  0.0526,  0.0118, -0.0260,  0.0319],\n",
      "        [-0.0103, -0.0005,  0.0088,  0.0084, -0.0247],\n",
      "        [ 0.0091,  0.0233,  0.0241, -0.0137, -0.0210],\n",
      "        [-0.0481,  0.0302,  0.0322, -0.0514,  0.0321],\n",
      "        [ 0.0590, -0.0420, -0.0392,  0.0643,  0.0229],\n",
      "        [-0.0120,  0.0006,  0.0278, -0.0382, -0.0106],\n",
      "        [-0.0356,  0.0179, -0.0385,  0.0117,  0.0714]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0061,  0.0188,  0.0707, -0.0656,  0.0273],\n",
      "        [-0.0327, -0.0225, -0.0793,  0.0474,  0.0643],\n",
      "        [-0.0201,  0.0224, -0.0101, -0.0443, -0.0172],\n",
      "        [-0.0123,  0.0149,  0.0923, -0.0595,  0.0020],\n",
      "        [-0.0244, -0.0618, -0.1073,  0.0651,  0.0265],\n",
      "        [ 0.0493,  0.0563, -0.0470,  0.0981,  0.0461],\n",
      "        [-0.0450, -0.0500, -0.0320, -0.0015, -0.0372],\n",
      "        [-0.0131,  0.0323, -0.0150,  0.0073,  0.0259]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0144, grad_fn=<MinBackward1>), tensor(0.9308, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.22614921629428864\n",
      "@sample 323: tensor([[ 0.0713, -0.0609, -0.0805,  0.0909, -0.0413],\n",
      "        [-0.0145,  0.0136,  0.0146,  0.0044,  0.0298],\n",
      "        [-0.0050, -0.0240,  0.0359, -0.0657, -0.0167],\n",
      "        [ 0.0478,  0.0175, -0.0257,  0.0355,  0.0132],\n",
      "        [ 0.0063,  0.0182,  0.0117,  0.0163,  0.0114],\n",
      "        [ 0.0178, -0.0019, -0.0590, -0.0127,  0.0500],\n",
      "        [ 0.0113,  0.0243, -0.0232, -0.0097, -0.0096],\n",
      "        [-0.0008, -0.0246,  0.0146, -0.0486,  0.0338]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0716,  0.0766,  0.0622, -0.0832,  0.0147],\n",
      "        [-0.0096, -0.0172, -0.0056,  0.0287, -0.0887],\n",
      "        [ 0.0443, -0.0166, -0.0052,  0.0471, -0.0158],\n",
      "        [ 0.0698,  0.0236, -0.1012,  0.0264,  0.0122],\n",
      "        [ 0.0731,  0.0512,  0.0188,  0.0622,  0.0607],\n",
      "        [-0.0129, -0.0245, -0.0575, -0.0072, -0.0383],\n",
      "        [-0.0171, -0.0122,  0.0193,  0.0172,  0.0049],\n",
      "        [ 0.0088, -0.0239, -0.1479,  0.1121, -0.0104]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0212, grad_fn=<MinBackward1>), tensor(0.9881, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.24211011826992035\n",
      "@sample 324: tensor([[-0.0007, -0.0083,  0.0228, -0.0224, -0.0424],\n",
      "        [ 0.0050,  0.0859,  0.0507, -0.0658,  0.0226],\n",
      "        [-0.0027,  0.0532, -0.0261, -0.0745, -0.0026],\n",
      "        [-0.0307,  0.0155, -0.0735, -0.0154,  0.0139],\n",
      "        [ 0.0379,  0.0104,  0.0032, -0.0429, -0.0369],\n",
      "        [-0.0209,  0.0133, -0.0114, -0.0335, -0.0049],\n",
      "        [-0.0293,  0.0168,  0.0232,  0.0107,  0.0186],\n",
      "        [ 0.0086,  0.0352, -0.0440, -0.0030,  0.0018]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0047,  0.0024,  0.0143,  0.0244, -0.0314],\n",
      "        [-0.0247, -0.0227, -0.0569,  0.0255, -0.0085],\n",
      "        [-0.0212, -0.0617, -0.0281,  0.0661,  0.0299],\n",
      "        [-0.0301,  0.0068,  0.0069,  0.0180,  0.0298],\n",
      "        [-0.0366, -0.0060,  0.0261, -0.0264, -0.0398],\n",
      "        [ 0.0349,  0.0096,  0.0434, -0.0119, -0.0325],\n",
      "        [ 0.0113,  0.0044,  0.0213,  0.0291,  0.0178],\n",
      "        [-0.0032,  0.0512, -0.0279,  0.0397, -0.0587]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0231, grad_fn=<MinBackward1>), tensor(0.9487, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.215629443526268\n",
      "@sample 325: tensor([[-0.0164,  0.0470, -0.0065, -0.0821,  0.0441],\n",
      "        [-0.0081,  0.0548,  0.0146, -0.0993,  0.0424],\n",
      "        [ 0.0217, -0.0543, -0.0318,  0.0689, -0.0433],\n",
      "        [ 0.0077,  0.0152, -0.0080, -0.0683,  0.0327],\n",
      "        [ 0.0327,  0.0358, -0.0331,  0.0398, -0.0188],\n",
      "        [ 0.0107, -0.0185, -0.0095,  0.0257, -0.0005],\n",
      "        [ 0.0230,  0.0534, -0.0127, -0.0583,  0.0418],\n",
      "        [-0.0124, -0.0463,  0.0050,  0.0158,  0.0445]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0331, -0.0103,  0.0172,  0.0170,  0.0473],\n",
      "        [-0.0641, -0.0225, -0.0349,  0.0204, -0.0082],\n",
      "        [-0.0050,  0.0230,  0.0346, -0.0120, -0.0112],\n",
      "        [ 0.0569, -0.0190, -0.0352,  0.0408,  0.0354],\n",
      "        [-0.0587, -0.0624, -0.0236, -0.0356, -0.0168],\n",
      "        [ 0.0016,  0.0290,  0.0083,  0.0122, -0.0181],\n",
      "        [-0.0370, -0.0498, -0.1034,  0.0986, -0.0190],\n",
      "        [ 0.0313,  0.0116,  0.0270,  0.0124,  0.0128]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0120, grad_fn=<MinBackward1>), tensor(0.9373, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.23369376361370087\n",
      "@sample 326: tensor([[-0.0045,  0.0194, -0.0314, -0.0538,  0.0457],\n",
      "        [-0.0016, -0.0360, -0.0117,  0.0561, -0.0143],\n",
      "        [ 0.0160,  0.0194,  0.0434, -0.0585,  0.0079],\n",
      "        [ 0.0041,  0.0853,  0.0025, -0.1034,  0.0643],\n",
      "        [-0.0312,  0.0254,  0.0323, -0.0099,  0.0052],\n",
      "        [ 0.0126, -0.0138,  0.0037,  0.0059, -0.0010],\n",
      "        [ 0.0172,  0.0121,  0.0045, -0.0096,  0.0178],\n",
      "        [ 0.0228,  0.0051,  0.0022, -0.0198,  0.0162]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0330, -0.0439, -0.1008,  0.0697,  0.0491],\n",
      "        [ 0.0360,  0.0434,  0.0082, -0.0444, -0.0419],\n",
      "        [-0.0541, -0.0539, -0.0745,  0.0464,  0.0059],\n",
      "        [-0.0995, -0.1150, -0.0965,  0.1032, -0.0462],\n",
      "        [-0.0463,  0.0206, -0.0765,  0.0651, -0.0015],\n",
      "        [-0.0193, -0.0179, -0.0163,  0.0121, -0.0038],\n",
      "        [ 0.0050, -0.0058, -0.0378,  0.0402, -0.0378],\n",
      "        [-0.0224,  0.0033, -0.0156, -0.0492, -0.0092]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0180, grad_fn=<MinBackward1>), tensor(0.9576, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.23506630957126617\n",
      "@sample 327: tensor([[ 4.6301e-04, -1.3583e-01, -7.6492e-02,  1.0832e-01,  1.6000e-02],\n",
      "        [ 2.5617e-02,  1.3602e-01,  1.3682e-04, -4.0661e-02,  4.5925e-03],\n",
      "        [ 1.7926e-02, -7.5788e-02, -1.3060e-03,  2.1136e-02, -3.7076e-04],\n",
      "        [ 1.7040e-02, -2.8941e-02,  1.0552e-04, -2.2472e-02, -5.4363e-02],\n",
      "        [ 4.7245e-02, -2.5952e-02, -3.0231e-02,  1.1049e-02,  4.7803e-03],\n",
      "        [-1.0300e-02,  1.4055e-03, -1.7388e-02,  3.2952e-02, -4.3858e-02],\n",
      "        [ 1.4267e-02, -6.9010e-02,  1.3509e-02,  2.9056e-02, -8.5357e-03],\n",
      "        [ 1.0785e-02, -4.8259e-02, -4.0882e-02,  5.6100e-02, -5.8810e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0416,  0.0208,  0.1478, -0.1030, -0.0172],\n",
      "        [ 0.0268, -0.0591, -0.1337,  0.1026, -0.0159],\n",
      "        [ 0.0281,  0.0305,  0.0805, -0.0155,  0.0073],\n",
      "        [-0.0031, -0.0032,  0.0431,  0.0046, -0.0505],\n",
      "        [ 0.0243, -0.0010, -0.0257,  0.0396, -0.0099],\n",
      "        [-0.0486,  0.0668,  0.0041, -0.0283, -0.0494],\n",
      "        [ 0.0219,  0.0569,  0.0185,  0.0011,  0.0980],\n",
      "        [ 0.0111, -0.0090,  0.0993, -0.0619,  0.0281]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0172, grad_fn=<MinBackward1>), tensor(0.9455, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2676093578338623\n",
      "@sample 328: tensor([[-0.0177,  0.0227,  0.0146, -0.0171,  0.0208],\n",
      "        [ 0.0049,  0.0654,  0.0742, -0.1596,  0.0240],\n",
      "        [ 0.0362, -0.0393, -0.0329,  0.0436, -0.0081],\n",
      "        [ 0.0323,  0.0070,  0.0536, -0.0150,  0.0174],\n",
      "        [-0.0037, -0.0591, -0.0142,  0.0114,  0.0189],\n",
      "        [ 0.0172, -0.0211, -0.0362,  0.0273, -0.0063],\n",
      "        [ 0.0201, -0.0147,  0.0533, -0.0555,  0.0007],\n",
      "        [-0.0483,  0.1142,  0.0496, -0.0488, -0.0035]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0177, -0.0466, -0.0687,  0.0605,  0.0203],\n",
      "        [-0.0953, -0.0981, -0.0332,  0.0786, -0.0419],\n",
      "        [-0.0144, -0.0115,  0.0118, -0.0339, -0.0378],\n",
      "        [-0.0569,  0.0142, -0.0387,  0.0366,  0.0137],\n",
      "        [ 0.0097, -0.0519,  0.0490,  0.0002, -0.0155],\n",
      "        [ 0.0062, -0.0341,  0.0093, -0.0707, -0.0197],\n",
      "        [-0.0033, -0.0280, -0.0305,  0.0326,  0.0393],\n",
      "        [-0.0194, -0.0516, -0.0680,  0.0502,  0.0038]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0179, grad_fn=<MinBackward1>), tensor(0.9488, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.23483358323574066\n",
      "@sample 329: tensor([[ 0.0135, -0.0044, -0.0231,  0.0108, -0.0146],\n",
      "        [-0.0025,  0.0643, -0.0035, -0.0420,  0.0547],\n",
      "        [ 0.0345, -0.0219, -0.0084,  0.0187, -0.0376],\n",
      "        [ 0.0654, -0.0473, -0.0395,  0.1273, -0.0650],\n",
      "        [ 0.0059,  0.0003, -0.0166, -0.0121, -0.0190],\n",
      "        [ 0.0265,  0.0067,  0.0024,  0.0281,  0.0371],\n",
      "        [ 0.0371, -0.0257,  0.0105,  0.0114, -0.0372],\n",
      "        [-0.0039,  0.0242, -0.0415, -0.0177, -0.0319]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0175, -0.0352, -0.0281,  0.0352,  0.0125],\n",
      "        [-0.0018, -0.0138, -0.0422,  0.0067, -0.0222],\n",
      "        [ 0.0213,  0.0042,  0.0281, -0.0096, -0.0126],\n",
      "        [ 0.0478,  0.0899,  0.0214, -0.0542, -0.0492],\n",
      "        [ 0.0231,  0.0019, -0.0133,  0.0189, -0.0156],\n",
      "        [ 0.0607, -0.0280,  0.0526, -0.0605, -0.0343],\n",
      "        [-0.0460, -0.0434, -0.0163,  0.0693,  0.0105],\n",
      "        [ 0.0306, -0.0082,  0.0390, -0.0146,  0.0175]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0177, grad_fn=<MinBackward1>), tensor(0.9605, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2195255607366562\n",
      "@sample 330: tensor([[ 0.0098, -0.0207,  0.0466,  0.0261,  0.0041],\n",
      "        [ 0.0176, -0.0626, -0.0048,  0.0179, -0.0022],\n",
      "        [-0.0352,  0.0173, -0.0177, -0.0197, -0.0129],\n",
      "        [ 0.0236, -0.0202, -0.0635,  0.0645, -0.0310],\n",
      "        [ 0.0066, -0.0280, -0.0184,  0.0201,  0.0017],\n",
      "        [ 0.0106,  0.0081, -0.0468, -0.0157, -0.0374],\n",
      "        [ 0.0453, -0.0540, -0.0162, -0.0023,  0.0030],\n",
      "        [ 0.0070, -0.0539,  0.0383,  0.0634,  0.0069]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0601,  0.0217, -0.0627,  0.0156,  0.0349],\n",
      "        [ 0.0635, -0.0084,  0.0418, -0.0347, -0.0277],\n",
      "        [ 0.0196, -0.0196,  0.0492, -0.0120, -0.0143],\n",
      "        [ 0.0754,  0.0372,  0.0885, -0.0406, -0.0283],\n",
      "        [-0.0010,  0.0044,  0.0295, -0.0172, -0.0024],\n",
      "        [-0.0354,  0.0201, -0.0175, -0.0788,  0.0105],\n",
      "        [-0.0085, -0.0201, -0.0168,  0.0106, -0.0132],\n",
      "        [ 0.0636,  0.0592,  0.0362,  0.0053, -0.0105]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0164, grad_fn=<MinBackward1>), tensor(0.9638, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2194647639989853\n",
      "@sample 331: tensor([[ 0.0155,  0.0397, -0.0089, -0.0295,  0.0084],\n",
      "        [ 0.0286, -0.0568,  0.0259,  0.0110, -0.0051],\n",
      "        [ 0.0331, -0.0225, -0.0077,  0.0313, -0.0299],\n",
      "        [ 0.0139,  0.0543, -0.0230, -0.0030,  0.0398],\n",
      "        [ 0.0297, -0.0282, -0.0169,  0.0291, -0.0372],\n",
      "        [ 0.0184, -0.0278,  0.0380,  0.0256,  0.0141],\n",
      "        [ 0.0093,  0.0603,  0.0732, -0.0207,  0.0270],\n",
      "        [-0.0211,  0.0124,  0.0276, -0.0487,  0.0513]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0096, -0.0304, -0.0289,  0.0237, -0.0127],\n",
      "        [ 0.0323, -0.0205, -0.0355,  0.0185, -0.0156],\n",
      "        [ 0.0055, -0.0146,  0.0111,  0.0522, -0.0035],\n",
      "        [-0.0108, -0.0768, -0.1348,  0.0783,  0.0040],\n",
      "        [ 0.0308, -0.0328,  0.0342, -0.0074,  0.0011],\n",
      "        [ 0.0181,  0.0515,  0.0259,  0.0374, -0.0098],\n",
      "        [-0.0009, -0.0338, -0.0382,  0.0141, -0.0388],\n",
      "        [-0.0055,  0.0092,  0.0305,  0.0287, -0.0363]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0144, grad_fn=<MinBackward1>), tensor(0.9513, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.20418336987495422\n",
      "@sample 332: tensor([[ 0.0163, -0.0331, -0.0269,  0.0247,  0.0133],\n",
      "        [ 0.0106,  0.0074, -0.0060,  0.0097, -0.0142],\n",
      "        [ 0.0011,  0.0424, -0.0306, -0.0288,  0.0110],\n",
      "        [-0.0120,  0.0178,  0.0010, -0.0404, -0.0104],\n",
      "        [-0.0056, -0.0149,  0.0531, -0.0099,  0.0187],\n",
      "        [ 0.0269, -0.0592, -0.0738,  0.0917, -0.0887],\n",
      "        [ 0.0152, -0.0268,  0.0199,  0.0393, -0.0250],\n",
      "        [ 0.0043, -0.0421, -0.0054, -0.0370, -0.0099]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0216,  0.0383, -0.0469, -0.0489, -0.0555],\n",
      "        [-0.0029, -0.0100,  0.0346, -0.0104, -0.0029],\n",
      "        [-0.0370, -0.0275, -0.0332,  0.0499,  0.0119],\n",
      "        [-0.0352, -0.0425,  0.0361,  0.0430,  0.0080],\n",
      "        [ 0.0103, -0.0144,  0.0005,  0.0350, -0.0076],\n",
      "        [-0.0447,  0.0123,  0.0235,  0.0360,  0.0222],\n",
      "        [-0.0188,  0.0331, -0.0118,  0.0183, -0.0030],\n",
      "        [-0.0223, -0.0630,  0.0229, -0.0048,  0.0038]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0150, grad_fn=<MinBackward1>), tensor(0.9361, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.21099650859832764\n",
      "@sample 333: tensor([[ 0.0308, -0.0232, -0.0072, -0.0419, -0.0018],\n",
      "        [ 0.0019, -0.0349, -0.0161,  0.0113,  0.0144],\n",
      "        [ 0.0353,  0.0575,  0.0178,  0.0125, -0.0163],\n",
      "        [-0.0033,  0.0289,  0.0401, -0.0228, -0.0112],\n",
      "        [ 0.0178, -0.0169,  0.0297,  0.0245, -0.0089],\n",
      "        [-0.0160,  0.0268,  0.0199, -0.0570,  0.0521],\n",
      "        [ 0.0055, -0.0561,  0.0043,  0.0220,  0.0077],\n",
      "        [-0.0196, -0.0199, -0.0417,  0.0302, -0.0044]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0339, -0.0149,  0.0211,  0.0769,  0.0305],\n",
      "        [ 0.0226, -0.0098, -0.0181, -0.0361, -0.0625],\n",
      "        [-0.0250,  0.0343, -0.1525, -0.0374,  0.0404],\n",
      "        [-0.0208,  0.0298, -0.0901, -0.0010, -0.0070],\n",
      "        [-0.0086,  0.0094, -0.0104,  0.0065, -0.0091],\n",
      "        [ 0.0004,  0.0042,  0.0250,  0.0054, -0.0846],\n",
      "        [ 0.0329,  0.0098,  0.1353, -0.0911, -0.0489],\n",
      "        [ 0.0850,  0.0098, -0.0180, -0.0475,  0.0222]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0215, grad_fn=<MinBackward1>), tensor(0.9555, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2307363897562027\n",
      "@sample 334: tensor([[-0.0209, -0.0324,  0.0196,  0.0690, -0.0236],\n",
      "        [ 0.0071,  0.0475,  0.0075, -0.0105,  0.0239],\n",
      "        [-0.0059,  0.0197,  0.0170,  0.0099, -0.0288],\n",
      "        [ 0.0330,  0.0291,  0.0588, -0.0532, -0.0137],\n",
      "        [ 0.0038,  0.0497,  0.0102, -0.0364,  0.0306],\n",
      "        [-0.0250,  0.0670,  0.0269, -0.1224,  0.0843],\n",
      "        [-0.0265,  0.0510, -0.0232, -0.0443,  0.0275],\n",
      "        [-0.0090, -0.0641, -0.0480,  0.1171, -0.0340]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0892,  0.0829,  0.1099, -0.0073, -0.0461],\n",
      "        [-0.0147, -0.0258, -0.0782,  0.0201, -0.0181],\n",
      "        [-0.0521, -0.0585,  0.0028,  0.0466,  0.0281],\n",
      "        [-0.0084, -0.0537,  0.0358,  0.0006, -0.0138],\n",
      "        [-0.0669, -0.0810, -0.0416,  0.0399, -0.0728],\n",
      "        [-0.0476, -0.0471, -0.0965,  0.0317, -0.0593],\n",
      "        [-0.1136,  0.0051, -0.1141, -0.0194,  0.0387],\n",
      "        [ 0.0301,  0.0558,  0.0734, -0.0248, -0.0004]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0300, grad_fn=<MinBackward1>), tensor(0.9555, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.25103244185447693\n",
      "@sample 335: tensor([[-0.0461, -0.0027,  0.0094,  0.0114,  0.0097],\n",
      "        [-0.0113, -0.0678,  0.0562,  0.0400, -0.0030],\n",
      "        [-0.0158,  0.0292,  0.0033, -0.0486,  0.0768],\n",
      "        [-0.0256,  0.0240,  0.0063, -0.0790,  0.0516],\n",
      "        [-0.0009,  0.0119,  0.0481, -0.0379,  0.0519],\n",
      "        [-0.0255,  0.0170, -0.0028,  0.0434,  0.0517],\n",
      "        [-0.0053,  0.0049,  0.0078,  0.0012,  0.0379],\n",
      "        [ 0.0176, -0.0338, -0.0134, -0.0212,  0.0045]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0153,  0.0206,  0.0658, -0.0465, -0.0044],\n",
      "        [ 0.0214, -0.0276,  0.0736, -0.0432,  0.0334],\n",
      "        [-0.0093, -0.0603, -0.0587,  0.0500,  0.0095],\n",
      "        [ 0.0016, -0.0184, -0.0132,  0.0337, -0.0179],\n",
      "        [-0.0222, -0.0299, -0.0197,  0.0725, -0.0040],\n",
      "        [-0.0008,  0.0168, -0.0887,  0.0165, -0.0553],\n",
      "        [-0.0211, -0.0267, -0.0233,  0.0025,  0.0176],\n",
      "        [-0.0204, -0.0613,  0.0315, -0.0022, -0.0195]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0177, grad_fn=<MinBackward1>), tensor(0.9509, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2164984941482544\n",
      "@sample 336: tensor([[ 1.6334e-02,  1.9493e-03,  4.5295e-02,  2.8014e-02,  2.5873e-02],\n",
      "        [ 9.2233e-06, -7.0465e-02,  2.0710e-02,  9.0359e-03,  3.5342e-02],\n",
      "        [ 5.9590e-03,  9.2085e-03,  2.4981e-02, -4.4468e-03,  6.0019e-02],\n",
      "        [-2.9573e-02, -2.5171e-03,  8.2121e-03, -6.4159e-02,  2.7825e-02],\n",
      "        [ 8.9160e-03, -8.4936e-02,  2.6272e-03,  4.0940e-02,  3.2736e-03],\n",
      "        [-1.8905e-02, -5.6348e-02, -3.5543e-02,  5.5625e-02,  2.2247e-04],\n",
      "        [ 1.1676e-02, -3.3014e-02, -4.2325e-02,  9.1312e-02, -8.6125e-02],\n",
      "        [ 5.2477e-02, -7.2680e-02, -3.3089e-02,  6.6577e-02, -3.9081e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0123,  0.0151, -0.0812,  0.0383,  0.0613],\n",
      "        [-0.0083,  0.0220,  0.0812, -0.0179, -0.0075],\n",
      "        [-0.0105,  0.0225,  0.0185,  0.0237, -0.0034],\n",
      "        [-0.0082, -0.0289,  0.0282, -0.0356, -0.0649],\n",
      "        [ 0.0298, -0.0042,  0.1189, -0.0426, -0.0161],\n",
      "        [-0.0099,  0.0471,  0.0034, -0.0206,  0.0057],\n",
      "        [ 0.0187,  0.0042,  0.0222, -0.0809, -0.0398],\n",
      "        [ 0.0603,  0.0374,  0.0562, -0.0336,  0.0172]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0211, grad_fn=<MinBackward1>), tensor(0.9672, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.23158122599124908\n",
      "@sample 337: tensor([[-0.0313, -0.0891, -0.0173,  0.0589,  0.0055],\n",
      "        [ 0.0129,  0.0268,  0.0197, -0.0204, -0.0050],\n",
      "        [ 0.0305, -0.0010,  0.0200,  0.0313, -0.0077],\n",
      "        [-0.0070,  0.0082,  0.0187, -0.0091,  0.0126],\n",
      "        [-0.0091,  0.0109,  0.0223,  0.0331,  0.0286],\n",
      "        [-0.0372,  0.0269,  0.0067, -0.0219, -0.0077],\n",
      "        [-0.0058,  0.0587,  0.0384,  0.0005,  0.0358],\n",
      "        [-0.0535,  0.0958,  0.0520, -0.0888,  0.0331]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0639, -0.0269,  0.0678, -0.0862,  0.0204],\n",
      "        [-0.0339,  0.0361, -0.0512,  0.0261,  0.0226],\n",
      "        [ 0.0276, -0.0350, -0.0181,  0.0187, -0.0480],\n",
      "        [-0.0135,  0.0178, -0.0499,  0.0357,  0.0287],\n",
      "        [ 0.0637,  0.0529,  0.0657, -0.0488, -0.0291],\n",
      "        [ 0.0366, -0.0128, -0.0657,  0.0751,  0.0420],\n",
      "        [ 0.0169, -0.0303, -0.0673, -0.0087,  0.0137],\n",
      "        [-0.0767, -0.0627, -0.0557,  0.0526, -0.0384]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0193, grad_fn=<MinBackward1>), tensor(0.9863, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2233213186264038\n",
      "@sample 338: tensor([[-0.0582,  0.0444, -0.0054, -0.0730,  0.0430],\n",
      "        [-0.0277, -0.0354, -0.0073, -0.0027, -0.0169],\n",
      "        [-0.0098, -0.0119, -0.0086, -0.0264,  0.0166],\n",
      "        [ 0.0203, -0.0416,  0.0153,  0.0487,  0.0384],\n",
      "        [-0.0328, -0.0502,  0.0128,  0.0344,  0.0120],\n",
      "        [-0.0227,  0.0326, -0.0059, -0.0700, -0.0266],\n",
      "        [ 0.0035, -0.0191,  0.0105,  0.0296, -0.0281],\n",
      "        [-0.0080,  0.0052,  0.0204, -0.0109,  0.0374]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0316, -0.0668, -0.0968,  0.0309, -0.0254],\n",
      "        [-0.0176, -0.0292,  0.0298, -0.0031,  0.0008],\n",
      "        [-0.0302, -0.0734, -0.0203, -0.0527, -0.0586],\n",
      "        [ 0.0228,  0.0475, -0.0278, -0.0129, -0.0320],\n",
      "        [ 0.0306, -0.0255,  0.0420, -0.0890, -0.0175],\n",
      "        [-0.0104, -0.0193, -0.0005,  0.0369,  0.0607],\n",
      "        [ 0.0072, -0.0015,  0.0501, -0.0374, -0.0104],\n",
      "        [ 0.0260,  0.0270, -0.0185,  0.0443,  0.0665]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0099, grad_fn=<MinBackward1>), tensor(0.9615, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.21690267324447632\n",
      "@sample 339: tensor([[-0.0234, -0.0211,  0.0662, -0.0221, -0.0291],\n",
      "        [ 0.0065, -0.0168, -0.0151,  0.0362,  0.0432],\n",
      "        [ 0.0110,  0.0069, -0.0520,  0.0188, -0.0241],\n",
      "        [-0.0082,  0.0664,  0.0605, -0.0996,  0.0249],\n",
      "        [-0.0216,  0.1057, -0.0204, -0.0425,  0.0087],\n",
      "        [-0.0726,  0.0117,  0.0405,  0.0068, -0.0143],\n",
      "        [-0.0587,  0.0849,  0.0218, -0.0975,  0.0638],\n",
      "        [-0.0162, -0.0419,  0.0321, -0.0072,  0.0333]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0390, -0.0054,  0.0302, -0.0014, -0.0054],\n",
      "        [ 0.0105,  0.0138, -0.0331,  0.0067, -0.0229],\n",
      "        [ 0.0224,  0.0121,  0.0076, -0.0408,  0.0098],\n",
      "        [-0.0071, -0.0123, -0.0451,  0.0776,  0.0128],\n",
      "        [-0.0322, -0.0311, -0.1077,  0.0564,  0.0327],\n",
      "        [ 0.0281,  0.0353,  0.0439, -0.0238,  0.0270],\n",
      "        [ 0.0106, -0.0453, -0.1289,  0.0475,  0.0178],\n",
      "        [ 0.0367, -0.0323,  0.0755,  0.0343, -0.0173]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0148, grad_fn=<MinBackward1>), tensor(0.9536, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.23850277066230774\n",
      "@sample 340: tensor([[ 0.0550,  0.0897,  0.0026, -0.0264, -0.0084],\n",
      "        [-0.0057, -0.0393,  0.0038,  0.0606,  0.0201],\n",
      "        [ 0.0062,  0.0518,  0.0623, -0.0362,  0.0284],\n",
      "        [-0.0533, -0.0448,  0.0037,  0.0702, -0.0068],\n",
      "        [ 0.0122,  0.0081,  0.0095, -0.0053,  0.0119],\n",
      "        [ 0.0179,  0.0432,  0.0102,  0.0041,  0.0022],\n",
      "        [-0.0310, -0.0201, -0.0003, -0.0036,  0.0031],\n",
      "        [-0.0076,  0.0149,  0.0110,  0.0120,  0.0277]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0336, -0.0519, -0.0098,  0.0049, -0.0135],\n",
      "        [ 0.0370,  0.0348,  0.0507, -0.0596, -0.0310],\n",
      "        [ 0.0013,  0.0029, -0.0428,  0.0468,  0.0039],\n",
      "        [ 0.0652,  0.0820,  0.0615, -0.0553, -0.0090],\n",
      "        [ 0.0014, -0.0091, -0.0269,  0.0222, -0.0287],\n",
      "        [-0.0403, -0.0015, -0.0352,  0.0286, -0.0502],\n",
      "        [-0.0297, -0.0460,  0.0109, -0.0093, -0.0435],\n",
      "        [ 0.0338, -0.0080, -0.0314,  0.0105,  0.0001]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0137, grad_fn=<MinBackward1>), tensor(0.9422, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.21070559322834015\n",
      "@sample 341: tensor([[ 0.0244,  0.0197,  0.0257,  0.0057,  0.0154],\n",
      "        [-0.0326, -0.0034, -0.0482,  0.0198, -0.0607],\n",
      "        [ 0.0152,  0.0877,  0.0523, -0.0466,  0.0486],\n",
      "        [ 0.0337, -0.0235, -0.0065,  0.0290, -0.0247],\n",
      "        [-0.0009, -0.0112,  0.0221, -0.0039,  0.0173],\n",
      "        [ 0.0049, -0.0059,  0.0136, -0.0071, -0.0705],\n",
      "        [-0.0076,  0.0106,  0.0283, -0.0296, -0.0043],\n",
      "        [-0.0383,  0.0054,  0.0115, -0.0518,  0.0159]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0403,  0.0712,  0.0193,  0.0124,  0.0123],\n",
      "        [ 0.0027, -0.0142,  0.0239, -0.0464,  0.0056],\n",
      "        [-0.0278, -0.0102, -0.0723,  0.0650,  0.0051],\n",
      "        [-0.0149, -0.0392,  0.0172, -0.0209, -0.0102],\n",
      "        [ 0.0769, -0.0074, -0.0060, -0.0032,  0.0095],\n",
      "        [-0.0620, -0.0265,  0.0163, -0.0442, -0.0169],\n",
      "        [-0.0272, -0.0030, -0.0075,  0.0160,  0.0006],\n",
      "        [-0.0603, -0.0989, -0.0386,  0.0171, -0.0249]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0181, grad_fn=<MinBackward1>), tensor(0.9895, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.21368135511875153\n",
      "@sample 342: tensor([[-0.0200,  0.0177,  0.0155,  0.0147,  0.0351],\n",
      "        [-0.0450,  0.0365,  0.0316, -0.0228,  0.0158],\n",
      "        [-0.0017,  0.0663, -0.0215, -0.0047,  0.0120],\n",
      "        [-0.0150, -0.0084, -0.0189,  0.0228, -0.0476],\n",
      "        [ 0.0159,  0.0032, -0.0265, -0.0234, -0.0068],\n",
      "        [ 0.0499, -0.0619,  0.0010, -0.0103,  0.0189],\n",
      "        [ 0.0138,  0.0344,  0.0352, -0.0296,  0.0572],\n",
      "        [ 0.0041, -0.0647, -0.0458,  0.0803, -0.0130]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0019,  0.0559, -0.0671, -0.0244,  0.0293],\n",
      "        [-0.0081,  0.0643,  0.0178, -0.0259,  0.0014],\n",
      "        [-0.0411, -0.0247, -0.0472,  0.0283,  0.0094],\n",
      "        [-0.0132,  0.0076,  0.0240, -0.0580, -0.0489],\n",
      "        [-0.0238, -0.0113, -0.0226,  0.0212, -0.0039],\n",
      "        [-0.0038,  0.0058,  0.0307, -0.0100, -0.0041],\n",
      "        [-0.0497, -0.0408, -0.1379,  0.0232,  0.0131],\n",
      "        [ 0.0248,  0.0477,  0.0570, -0.1003, -0.0225]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0113, grad_fn=<MinBackward1>), tensor(0.9495, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.21646429598331451\n",
      "@sample 343: tensor([[-0.0356, -0.0147, -0.0229,  0.0358,  0.0152],\n",
      "        [ 0.0039,  0.0288,  0.0350, -0.0688,  0.0539],\n",
      "        [ 0.0012,  0.0121,  0.0100,  0.0137, -0.0150],\n",
      "        [ 0.0160, -0.0522, -0.0423, -0.0202, -0.0054],\n",
      "        [-0.0281,  0.0107,  0.0304, -0.0106, -0.0092],\n",
      "        [ 0.0212, -0.0261, -0.0224, -0.0556,  0.0132],\n",
      "        [-0.0158, -0.0115, -0.0037,  0.0059,  0.0165],\n",
      "        [ 0.0020,  0.0836,  0.0100, -0.1457,  0.0500]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0157,  0.0518,  0.0386,  0.0009,  0.0370],\n",
      "        [-0.0329, -0.0247, -0.0581,  0.0073,  0.0154],\n",
      "        [ 0.0322,  0.0506,  0.0495, -0.0797,  0.0132],\n",
      "        [ 0.0831, -0.0454,  0.0464, -0.0479,  0.0123],\n",
      "        [ 0.0097, -0.0102, -0.0317, -0.0409, -0.0486],\n",
      "        [ 0.0423, -0.0397,  0.0257, -0.0271,  0.0768],\n",
      "        [ 0.0443,  0.0459, -0.0200, -0.0428,  0.0039],\n",
      "        [-0.0510, -0.0619,  0.0175, -0.0076,  0.0062]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0190, grad_fn=<MinBackward1>), tensor(0.9305, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2198319286108017\n",
      "@sample 344: tensor([[-0.0292, -0.0401, -0.0369,  0.0143,  0.0173],\n",
      "        [ 0.0110, -0.0187, -0.0847,  0.0595, -0.0327],\n",
      "        [ 0.0170,  0.0271,  0.0147, -0.0383,  0.0422],\n",
      "        [ 0.0178,  0.1233, -0.0011, -0.0629, -0.0199],\n",
      "        [ 0.0088, -0.0997, -0.0538,  0.0280, -0.0298],\n",
      "        [ 0.0209,  0.0150, -0.0280,  0.0002, -0.0029],\n",
      "        [-0.0180, -0.0121, -0.0477,  0.0878, -0.0273],\n",
      "        [ 0.0007,  0.0060,  0.0004,  0.0465,  0.0081]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0587, -0.0176,  0.0138, -0.0502, -0.0042],\n",
      "        [ 0.0380,  0.0566,  0.0195, -0.0386,  0.0319],\n",
      "        [-0.0519, -0.0099, -0.1132,  0.0734,  0.0054],\n",
      "        [-0.0654, -0.0204, -0.0883,  0.0519, -0.0079],\n",
      "        [ 0.0580, -0.0464,  0.1447, -0.0792, -0.0418],\n",
      "        [-0.0109, -0.0385, -0.0107,  0.0467, -0.0737],\n",
      "        [ 0.0499,  0.0559,  0.1118, -0.0776, -0.0113],\n",
      "        [ 0.0401,  0.0678, -0.0217, -0.0343,  0.0586]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0125, grad_fn=<MinBackward1>), tensor(0.9688, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2498118281364441\n",
      "@sample 345: tensor([[ 0.0308,  0.0636,  0.0091, -0.0322,  0.0490],\n",
      "        [-0.0120, -0.0239,  0.0226, -0.0288, -0.0054],\n",
      "        [-0.0342,  0.0383,  0.0125, -0.0427, -0.0077],\n",
      "        [-0.0081,  0.0879,  0.0424, -0.0897,  0.0400],\n",
      "        [-0.0392,  0.0015, -0.0113,  0.0218,  0.0069],\n",
      "        [ 0.0183,  0.0002, -0.0124,  0.0582, -0.0299],\n",
      "        [ 0.0088,  0.0405,  0.0253,  0.0068,  0.0046],\n",
      "        [ 0.0420,  0.0393,  0.0046, -0.0004, -0.0173]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0094, -0.0324, -0.1309,  0.0621,  0.0248],\n",
      "        [-0.0135, -0.0503, -0.0348, -0.0049,  0.0269],\n",
      "        [ 0.0167, -0.0116,  0.0402, -0.0320, -0.0184],\n",
      "        [-0.0227, -0.0810, -0.0782,  0.0409, -0.0209],\n",
      "        [ 0.0048,  0.0214,  0.0763, -0.0847, -0.0413],\n",
      "        [-0.0103,  0.0453, -0.0478,  0.0235, -0.0063],\n",
      "        [ 0.0041,  0.0182, -0.0805, -0.0344,  0.0511],\n",
      "        [-0.0147,  0.0117, -0.0377,  0.0286, -0.0046]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0267, grad_fn=<MinBackward1>), tensor(0.9563, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.21470192074775696\n",
      "@sample 346: tensor([[-0.0050, -0.0422, -0.0525,  0.0773, -0.0461],\n",
      "        [ 0.0211, -0.0427, -0.0016,  0.0606, -0.0607],\n",
      "        [-0.0326,  0.0160, -0.0244, -0.0546,  0.0592],\n",
      "        [ 0.0111,  0.0115,  0.0084,  0.0154, -0.0072],\n",
      "        [-0.0184,  0.0107,  0.0162, -0.0153, -0.0024],\n",
      "        [-0.0219, -0.0128, -0.0196, -0.0227,  0.0058],\n",
      "        [ 0.0098, -0.0141,  0.0102,  0.0488, -0.0329],\n",
      "        [-0.0106,  0.0151, -0.0270, -0.0569, -0.0027]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0340,  0.0508,  0.1105, -0.0583,  0.0059],\n",
      "        [ 0.0021,  0.0846,  0.0121, -0.0300,  0.0224],\n",
      "        [ 0.0356, -0.0318, -0.0503,  0.0047,  0.0910],\n",
      "        [ 0.0265,  0.0050, -0.0120,  0.0200,  0.0185],\n",
      "        [-0.0391, -0.0105, -0.0022,  0.0340,  0.0124],\n",
      "        [-0.0040,  0.0175,  0.0145,  0.0091,  0.0560],\n",
      "        [-0.0076,  0.0602, -0.0021, -0.0220, -0.0109],\n",
      "        [-0.0775, -0.0107, -0.1170,  0.0480, -0.0051]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0196, grad_fn=<MinBackward1>), tensor(0.9633, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.22538499534130096\n",
      "@sample 347: tensor([[-0.0012, -0.0054, -0.0555,  0.0788, -0.0060],\n",
      "        [-0.0248,  0.0160,  0.0042,  0.0054, -0.0003],\n",
      "        [ 0.0342,  0.0689, -0.0296,  0.0051, -0.0566],\n",
      "        [ 0.0127,  0.0098, -0.0474, -0.0019,  0.0081],\n",
      "        [ 0.0177,  0.0434, -0.0049,  0.0026, -0.0104],\n",
      "        [-0.0315, -0.0551, -0.0304,  0.0533, -0.0694],\n",
      "        [ 0.0028, -0.0117, -0.0605,  0.0313, -0.0124],\n",
      "        [-0.0456, -0.0536, -0.0715,  0.0344, -0.0182]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0509,  0.0801, -0.0185, -0.0482,  0.0280],\n",
      "        [ 0.0354, -0.0047,  0.0380, -0.0488, -0.0336],\n",
      "        [ 0.0598,  0.0417, -0.0194,  0.0183,  0.0064],\n",
      "        [ 0.0286, -0.0196,  0.0292, -0.0084,  0.0514],\n",
      "        [ 0.0152,  0.0693,  0.0191,  0.0007,  0.0300],\n",
      "        [ 0.0340, -0.0002,  0.1655, -0.0977, -0.0155],\n",
      "        [ 0.0324,  0.0516,  0.0661, -0.0618, -0.0086],\n",
      "        [ 0.0514, -0.0062,  0.0618, -0.0257,  0.0432]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0074, grad_fn=<MinBackward1>), tensor(0.9639, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.23620834946632385\n",
      "@sample 348: tensor([[ 3.7521e-05, -3.4366e-02, -2.5374e-02,  1.3289e-02,  4.4450e-03],\n",
      "        [-5.8489e-02,  1.4047e-02,  3.0342e-02,  1.0736e-02, -4.0862e-03],\n",
      "        [-1.4272e-03, -5.4770e-02, -4.2658e-03,  1.1977e-02,  8.0811e-03],\n",
      "        [ 2.3900e-02,  7.0320e-03,  7.4903e-03,  4.6083e-02, -2.4333e-03],\n",
      "        [-3.3492e-02,  4.0096e-02, -8.7800e-03, -5.5459e-02,  1.6018e-04],\n",
      "        [ 5.1739e-03, -5.6894e-02, -2.2427e-02,  2.1432e-03,  4.6035e-02],\n",
      "        [ 1.4251e-04, -2.4248e-02, -2.3740e-02,  2.7676e-03,  1.8946e-02],\n",
      "        [ 7.4498e-03,  1.9463e-02, -1.8059e-02, -7.8041e-03, -7.4654e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0040, -0.0726,  0.0128,  0.0473,  0.0101],\n",
      "        [-0.0180,  0.0387, -0.0560,  0.0017, -0.0346],\n",
      "        [-0.0177,  0.0133, -0.0316,  0.0044,  0.0119],\n",
      "        [ 0.0164,  0.0701,  0.0186, -0.0144,  0.0191],\n",
      "        [-0.0879, -0.0016, -0.0592,  0.0187,  0.0165],\n",
      "        [-0.0262, -0.0314,  0.0422, -0.0658, -0.0592],\n",
      "        [ 0.0390, -0.0003, -0.0212,  0.0046, -0.0007],\n",
      "        [-0.0392,  0.0196,  0.0424, -0.0692,  0.0573]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0183, grad_fn=<MinBackward1>), tensor(0.9720, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.21705283224582672\n",
      "@sample 349: tensor([[-0.0438,  0.0492,  0.0257, -0.0059, -0.0526],\n",
      "        [ 0.0133,  0.0005,  0.0337, -0.0266,  0.0280],\n",
      "        [ 0.0072,  0.0216,  0.0100, -0.0533,  0.0628],\n",
      "        [-0.0194, -0.0176,  0.0334, -0.0203,  0.0063],\n",
      "        [-0.0172,  0.0304, -0.0003, -0.0424,  0.0441],\n",
      "        [-0.0057,  0.0318,  0.0421, -0.0409,  0.0222],\n",
      "        [ 0.0165, -0.0182, -0.0289, -0.0241,  0.0046],\n",
      "        [ 0.0241, -0.0848,  0.0077,  0.0657, -0.0512]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0340, -0.0505, -0.0943, -0.0602,  0.0364],\n",
      "        [-0.0114, -0.0121, -0.0547,  0.0179,  0.0284],\n",
      "        [ 0.0312, -0.0797, -0.0627,  0.0261,  0.0254],\n",
      "        [-0.0177, -0.0218, -0.0587,  0.0219,  0.0220],\n",
      "        [-0.0605, -0.0555, -0.0675,  0.0407,  0.0017],\n",
      "        [-0.0288, -0.0179, -0.0168,  0.0154,  0.0107],\n",
      "        [ 0.0202, -0.0343,  0.0101, -0.0201,  0.0201],\n",
      "        [-0.0050,  0.0424,  0.0934, -0.0708,  0.0243]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0239, grad_fn=<MinBackward1>), tensor(0.9722, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.22971880435943604\n",
      "@sample 350: tensor([[-0.0207,  0.0251, -0.0239, -0.0129,  0.0444],\n",
      "        [-0.0628,  0.0174,  0.0713, -0.0569,  0.0447],\n",
      "        [-0.0052, -0.0088, -0.0636,  0.0737, -0.0026],\n",
      "        [-0.0264,  0.0647,  0.0195,  0.0036, -0.0281],\n",
      "        [ 0.0266,  0.0141,  0.0431, -0.0566, -0.0057],\n",
      "        [-0.0508, -0.0042,  0.0365, -0.0467,  0.0541],\n",
      "        [-0.0282,  0.0099,  0.0206, -0.0093, -0.0016],\n",
      "        [ 0.0150,  0.0708,  0.0236, -0.0166,  0.0197]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0445, -0.0121,  0.0092, -0.0181,  0.0203],\n",
      "        [ 0.0012,  0.0102, -0.0100,  0.0087,  0.0262],\n",
      "        [ 0.0635,  0.0341, -0.0748,  0.0080,  0.0292],\n",
      "        [ 0.0231,  0.0229, -0.0240,  0.0546, -0.0031],\n",
      "        [-0.0836, -0.0263, -0.0077,  0.0301,  0.0189],\n",
      "        [-0.0424,  0.0143,  0.0054, -0.0137, -0.0271],\n",
      "        [-0.0170, -0.0301,  0.0158, -0.0539, -0.0150],\n",
      "        [-0.0075, -0.0073, -0.0524,  0.0461,  0.0126]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0188, grad_fn=<MinBackward1>), tensor(0.9443, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.21576842665672302\n",
      "@sample 351: tensor([[ 0.0061, -0.0143, -0.0322,  0.0135,  0.0314],\n",
      "        [-0.0266, -0.0357, -0.0165,  0.0094, -0.0562],\n",
      "        [-0.0042,  0.0537,  0.0034, -0.0305,  0.0036],\n",
      "        [-0.0078,  0.0654,  0.0105, -0.0411,  0.0145],\n",
      "        [-0.0037,  0.0020, -0.0453, -0.0413, -0.0181],\n",
      "        [-0.0190,  0.0072, -0.0376, -0.0710,  0.0132],\n",
      "        [-0.0368, -0.0202, -0.0303,  0.0049, -0.0258],\n",
      "        [-0.0033, -0.0623, -0.0532,  0.0561, -0.0376]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0177,  0.0322, -0.0547,  0.0162, -0.0045],\n",
      "        [-0.0384,  0.0697,  0.1052, -0.0906, -0.0148],\n",
      "        [-0.0237, -0.0321, -0.0529,  0.0075, -0.0347],\n",
      "        [-0.0341, -0.0383, -0.0780,  0.0579, -0.0007],\n",
      "        [ 0.0218, -0.0429,  0.0759, -0.0383,  0.0167],\n",
      "        [-0.0461, -0.0684, -0.0414,  0.0369,  0.0242],\n",
      "        [ 0.0361, -0.0009,  0.0511, -0.0501,  0.0079],\n",
      "        [ 0.0593,  0.0409,  0.0485, -0.0102,  0.0632]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0143, grad_fn=<MinBackward1>), tensor(0.9475, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.21559616923332214\n",
      "@sample 352: tensor([[-0.0115, -0.0297,  0.0311,  0.0226, -0.0750],\n",
      "        [ 0.0169,  0.0473,  0.0164, -0.0128, -0.0055],\n",
      "        [ 0.0245, -0.0274, -0.0330,  0.0249, -0.0421],\n",
      "        [ 0.0228,  0.0149, -0.0276, -0.0544,  0.0476],\n",
      "        [ 0.0341,  0.0405,  0.0141, -0.0078, -0.0310],\n",
      "        [-0.0024,  0.0177, -0.0222, -0.0094, -0.0057],\n",
      "        [ 0.0313, -0.0329, -0.0075,  0.0453, -0.0302],\n",
      "        [-0.0204,  0.0760,  0.0350, -0.0555,  0.0413]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0212, -0.0306,  0.0782, -0.0036,  0.0436],\n",
      "        [-0.0057,  0.0088, -0.0162,  0.0201,  0.0075],\n",
      "        [ 0.0132,  0.0308,  0.0096, -0.0226,  0.0073],\n",
      "        [-0.0262, -0.0564, -0.0660,  0.0361, -0.0514],\n",
      "        [-0.0286,  0.0585,  0.0431,  0.0210,  0.0095],\n",
      "        [ 0.0177,  0.0066, -0.0292,  0.0179,  0.0427],\n",
      "        [ 0.0428,  0.0772,  0.0252, -0.0359,  0.0387],\n",
      "        [ 0.0234,  0.0015, -0.0337,  0.0865,  0.0264]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0192, grad_fn=<MinBackward1>), tensor(0.9653, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.21632882952690125\n",
      "@sample 353: tensor([[ 0.0043,  0.0177,  0.0255, -0.0526,  0.0222],\n",
      "        [-0.0400,  0.0212, -0.0140, -0.0496,  0.0645],\n",
      "        [-0.0040, -0.0694,  0.0023,  0.0445, -0.0190],\n",
      "        [ 0.0085,  0.0365,  0.0067, -0.0088,  0.0143],\n",
      "        [ 0.0098, -0.0172, -0.0380,  0.0666, -0.0063],\n",
      "        [-0.0323, -0.0042,  0.0111,  0.0019,  0.0393],\n",
      "        [ 0.0058,  0.0155, -0.0192,  0.0155, -0.0935],\n",
      "        [-0.0391,  0.0582, -0.0199, -0.0482,  0.0043]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0497, -0.0321, -0.0817,  0.0960, -0.0161],\n",
      "        [-0.0231, -0.0621, -0.0632,  0.0250, -0.0013],\n",
      "        [-0.0194,  0.0669, -0.0286,  0.0094, -0.0083],\n",
      "        [-0.0152, -0.0043, -0.0378,  0.0215,  0.0271],\n",
      "        [-0.0052,  0.0671,  0.0653, -0.0581,  0.0340],\n",
      "        [ 0.0114, -0.0079, -0.0751,  0.0286,  0.0512],\n",
      "        [-0.0212,  0.0192,  0.0897, -0.0558,  0.0190],\n",
      "        [-0.0370, -0.0824, -0.1085,  0.0722,  0.0064]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0206, grad_fn=<MinBackward1>), tensor(0.9511, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.22434057295322418\n",
      "@sample 354: tensor([[ 3.7858e-02,  4.4525e-02, -1.1675e-02, -2.7343e-02,  1.8221e-02],\n",
      "        [ 5.5872e-03, -1.4901e-02, -3.3581e-03,  3.8261e-02, -3.6005e-03],\n",
      "        [ 2.5340e-02,  2.0742e-02,  1.4409e-02, -2.5258e-02,  3.2275e-02],\n",
      "        [-5.5799e-02, -2.8086e-02,  3.2343e-02, -1.8297e-02, -1.0533e-02],\n",
      "        [-4.3081e-02,  8.5200e-02,  8.6956e-02, -1.1542e-01,  4.1677e-02],\n",
      "        [-3.4123e-03,  2.3285e-02,  1.0711e-02,  4.2969e-03, -2.0117e-02],\n",
      "        [ 5.9477e-03,  5.8734e-03, -2.7583e-02,  2.1919e-02, -1.8699e-02],\n",
      "        [ 1.5692e-02,  3.0562e-02, -9.3654e-05, -3.6050e-02, -4.5338e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-4.7862e-02, -2.6238e-02, -1.1580e-01,  7.1409e-02,  1.5203e-02],\n",
      "        [-5.0282e-03,  7.2043e-02,  1.1442e-03,  4.9961e-03,  3.0683e-02],\n",
      "        [ 2.4158e-02, -1.4490e-02, -1.0802e-01,  6.7176e-02,  1.2851e-01],\n",
      "        [ 6.4273e-02,  4.2758e-02,  1.4481e-02, -2.9727e-02, -3.2147e-02],\n",
      "        [-8.2566e-02, -8.9306e-02, -1.9038e-02,  9.5841e-02, -1.4763e-02],\n",
      "        [ 6.9700e-05, -3.1290e-03, -3.4703e-02,  7.4374e-03,  2.0969e-02],\n",
      "        [-1.9720e-02,  3.8612e-02, -2.2925e-02, -6.0885e-03,  1.8169e-02],\n",
      "        [-7.7147e-02, -2.8414e-02,  4.1845e-02,  4.5281e-02, -4.5391e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0229, grad_fn=<MinBackward1>), tensor(0.9253, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.22444909811019897\n",
      "@sample 355: tensor([[ 0.0006, -0.0151, -0.0445,  0.0033, -0.0675],\n",
      "        [-0.0127,  0.0281, -0.0109,  0.0108,  0.0005],\n",
      "        [-0.0013,  0.0398, -0.0455, -0.0060, -0.0165],\n",
      "        [-0.0377, -0.0510, -0.0449,  0.0301,  0.0295],\n",
      "        [ 0.0279, -0.0077,  0.0168, -0.0237,  0.0458],\n",
      "        [ 0.0044, -0.0488, -0.0468, -0.0297, -0.0468],\n",
      "        [-0.0412,  0.0934,  0.0452, -0.0585, -0.0055],\n",
      "        [-0.0216,  0.0305,  0.0192, -0.0749,  0.0331]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0091, -0.0041,  0.0054, -0.0588,  0.0121],\n",
      "        [ 0.0516, -0.0013,  0.0752, -0.0285,  0.0085],\n",
      "        [-0.0607, -0.0602,  0.0155,  0.0032, -0.0311],\n",
      "        [-0.0241,  0.0294, -0.0318, -0.0022, -0.0203],\n",
      "        [-0.0014, -0.0158, -0.0014,  0.0376,  0.0282],\n",
      "        [-0.0489, -0.0148, -0.0064, -0.0406, -0.0389],\n",
      "        [-0.0222,  0.0155, -0.0437,  0.0433,  0.0111],\n",
      "        [-0.0069, -0.0479, -0.0207,  0.0545, -0.0132]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0141, grad_fn=<MinBackward1>), tensor(0.9504, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.21572206914424896\n",
      "@sample 356: tensor([[ 0.0276,  0.0308, -0.0122, -0.0172,  0.0307],\n",
      "        [ 0.0002, -0.0707, -0.0449,  0.0637, -0.0584],\n",
      "        [ 0.0111,  0.0045,  0.0222,  0.0047, -0.0153],\n",
      "        [-0.0143,  0.0586, -0.0125, -0.0350, -0.0104],\n",
      "        [ 0.0175, -0.0166, -0.0161, -0.0032, -0.0226],\n",
      "        [ 0.0161,  0.0251, -0.0054, -0.0193, -0.0119],\n",
      "        [ 0.0042, -0.0124, -0.0285, -0.0172, -0.0157],\n",
      "        [ 0.0062, -0.0086,  0.0396,  0.0077, -0.0318]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0187, -0.0219, -0.0620,  0.0603, -0.0197],\n",
      "        [ 0.0649,  0.0445,  0.1493, -0.1052, -0.0065],\n",
      "        [-0.0026,  0.0146,  0.1101, -0.0188, -0.0238],\n",
      "        [-0.0173, -0.0684, -0.0542, -0.0033, -0.0227],\n",
      "        [-0.0059, -0.0054,  0.0244, -0.0130,  0.0326],\n",
      "        [-0.0380,  0.0007,  0.0069,  0.0113,  0.0318],\n",
      "        [-0.0155, -0.0234,  0.0030,  0.0244, -0.0253],\n",
      "        [-0.0354,  0.0730,  0.0134, -0.0175,  0.0054]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0200, grad_fn=<MinBackward1>), tensor(0.9741, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.21520671248435974\n",
      "@sample 357: tensor([[-0.0226, -0.0314,  0.0799, -0.0245,  0.0060],\n",
      "        [ 0.0138,  0.0178,  0.0248, -0.0169,  0.0278],\n",
      "        [-0.0336, -0.0130,  0.0034, -0.0387,  0.0090],\n",
      "        [-0.0049,  0.0402,  0.0493, -0.0473,  0.0720],\n",
      "        [-0.0111,  0.0046,  0.0335, -0.0042,  0.0216],\n",
      "        [ 0.0098, -0.0024, -0.0174,  0.0508, -0.0483],\n",
      "        [-0.0233,  0.0158,  0.0146, -0.0736,  0.0742],\n",
      "        [-0.0063,  0.0021,  0.0163,  0.0020,  0.0355]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0390,  0.0477,  0.0452, -0.0137,  0.0326],\n",
      "        [ 0.0120, -0.0078, -0.0055,  0.0080, -0.0325],\n",
      "        [-0.0139,  0.0018, -0.0662,  0.0082,  0.0441],\n",
      "        [-0.0077, -0.0237, -0.0733,  0.0601,  0.0205],\n",
      "        [-0.0277,  0.0193, -0.0165, -0.0410, -0.0200],\n",
      "        [ 0.0113,  0.0374,  0.0341, -0.0109,  0.0076],\n",
      "        [-0.0620, -0.0559, -0.0664,  0.0269, -0.0223],\n",
      "        [-0.0102,  0.0103, -0.0009, -0.0005, -0.0082]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0213, grad_fn=<MinBackward1>), tensor(0.9620, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2118159383535385\n",
      "@sample 358: tensor([[ 0.0095,  0.0074,  0.0716, -0.0298, -0.0299],\n",
      "        [-0.0079,  0.0279,  0.0008, -0.0189,  0.0195],\n",
      "        [-0.0170,  0.0395,  0.0152,  0.0085, -0.0254],\n",
      "        [ 0.0028,  0.0606, -0.0206, -0.0066, -0.0172],\n",
      "        [-0.0631,  0.0079, -0.0287, -0.0410,  0.0170],\n",
      "        [ 0.0101,  0.0046, -0.0486,  0.0192,  0.0060],\n",
      "        [-0.0162,  0.0216, -0.0310, -0.0437,  0.0023],\n",
      "        [ 0.0147, -0.0603, -0.0038,  0.0404,  0.0127]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.1425, -0.0529,  0.0247,  0.0252, -0.0179],\n",
      "        [-0.0385, -0.0388, -0.0904,  0.0144, -0.0171],\n",
      "        [-0.0252,  0.0477, -0.0423,  0.0019,  0.0567],\n",
      "        [ 0.0222, -0.0059, -0.0127,  0.0059, -0.0404],\n",
      "        [ 0.0021, -0.0510, -0.0243, -0.0084,  0.0014],\n",
      "        [-0.0300,  0.0157, -0.0007,  0.0262,  0.0499],\n",
      "        [ 0.0167, -0.0471, -0.1151,  0.0524,  0.0193],\n",
      "        [ 0.0312,  0.0328,  0.0758, -0.0280, -0.0312]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0231, grad_fn=<MinBackward1>), tensor(0.9517, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.21634311974048615\n",
      "@sample 359: tensor([[ 0.0060,  0.0299, -0.0090, -0.0410, -0.0131],\n",
      "        [ 0.0221, -0.0652,  0.0196,  0.0363, -0.0007],\n",
      "        [-0.0210, -0.0620,  0.0226, -0.0515,  0.0383],\n",
      "        [-0.0025,  0.0841,  0.0261, -0.0277,  0.0246],\n",
      "        [ 0.0011,  0.0230, -0.0061,  0.0184,  0.0036],\n",
      "        [-0.0165, -0.0514, -0.0057, -0.0365,  0.0531],\n",
      "        [ 0.0167, -0.0540,  0.0521, -0.0261, -0.0401],\n",
      "        [ 0.0532, -0.0618, -0.0020,  0.0865, -0.0639]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0242, -0.0260,  0.0102, -0.0014, -0.0230],\n",
      "        [ 0.0231,  0.0089,  0.0245, -0.0403, -0.0060],\n",
      "        [-0.0232, -0.0153,  0.0849, -0.0341, -0.0246],\n",
      "        [-0.0537, -0.0132, -0.1094,  0.0641, -0.0208],\n",
      "        [ 0.0336,  0.0193,  0.0052, -0.0160,  0.0237],\n",
      "        [ 0.0015, -0.0731, -0.0498,  0.0123,  0.0248],\n",
      "        [-0.0958, -0.0416,  0.0388, -0.0445,  0.0356],\n",
      "        [ 0.0298,  0.0634,  0.0834, -0.1027, -0.0401]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0175, grad_fn=<MinBackward1>), tensor(0.9462, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2287367880344391\n",
      "@sample 360: tensor([[ 0.0213, -0.0017,  0.0431, -0.0082,  0.0302],\n",
      "        [-0.0298,  0.0130,  0.0139, -0.0415,  0.0059],\n",
      "        [-0.0469, -0.0899,  0.0417,  0.0325,  0.0284],\n",
      "        [-0.0246,  0.0395,  0.0633, -0.0083,  0.0305],\n",
      "        [-0.0253,  0.0466,  0.0358, -0.0628,  0.0275],\n",
      "        [-0.0044, -0.0081,  0.0096, -0.0489,  0.0249],\n",
      "        [ 0.0154,  0.0151,  0.0264,  0.0740,  0.0126],\n",
      "        [ 0.0048,  0.0528,  0.0126, -0.0613,  0.0418]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0385,  0.0333, -0.0773,  0.0771,  0.0497],\n",
      "        [-0.0258, -0.0156,  0.0150,  0.0189, -0.0194],\n",
      "        [ 0.0684, -0.0167,  0.0596, -0.0179, -0.0041],\n",
      "        [-0.0508, -0.0080, -0.0381, -0.0601, -0.0583],\n",
      "        [-0.0514, -0.0713, -0.0768,  0.0729, -0.0113],\n",
      "        [-0.0587, -0.0352, -0.0192, -0.0140,  0.0082],\n",
      "        [ 0.0116,  0.0559,  0.0234, -0.0292, -0.0839],\n",
      "        [-0.0222, -0.0134, -0.0394,  0.0213, -0.0211]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0275, grad_fn=<MinBackward1>), tensor(0.9514, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2148950695991516\n",
      "@sample 361: tensor([[-0.0184, -0.0546,  0.0170,  0.0364,  0.0070],\n",
      "        [-0.0090, -0.0391, -0.0057,  0.0139,  0.0051],\n",
      "        [-0.0310, -0.0032,  0.0091,  0.0330,  0.0315],\n",
      "        [-0.0363,  0.0273,  0.0308, -0.0126,  0.0231],\n",
      "        [-0.0095,  0.0384,  0.0616, -0.0404,  0.0383],\n",
      "        [-0.0236, -0.0376,  0.0018,  0.0194,  0.0061],\n",
      "        [ 0.0052,  0.0236,  0.0158, -0.0579,  0.0491],\n",
      "        [-0.0131, -0.0431,  0.0302,  0.0010,  0.0554]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0318, -0.0230,  0.0190,  0.0069,  0.0604],\n",
      "        [-0.0037,  0.0063,  0.0777, -0.0026, -0.0009],\n",
      "        [ 0.0553,  0.0421,  0.0562,  0.0348,  0.0297],\n",
      "        [ 0.0050,  0.0455,  0.0360, -0.0553, -0.0260],\n",
      "        [-0.0477, -0.0085, -0.0404,  0.0668,  0.0061],\n",
      "        [ 0.0171,  0.0266,  0.0362, -0.0403, -0.0035],\n",
      "        [-0.0414, -0.0175, -0.0863,  0.0494, -0.0312],\n",
      "        [ 0.0229, -0.0178,  0.0289, -0.0106, -0.0305]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0098, grad_fn=<MinBackward1>), tensor(0.9470, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.20695918798446655\n",
      "@sample 362: tensor([[-0.0431, -0.0046,  0.0322,  0.0332, -0.0038],\n",
      "        [-0.0157,  0.0580,  0.0521, -0.0776,  0.0921],\n",
      "        [-0.0020, -0.0069,  0.0308,  0.0405, -0.0109],\n",
      "        [-0.0170, -0.0082,  0.0399, -0.0337,  0.0563],\n",
      "        [-0.0364,  0.0037,  0.0509, -0.0054,  0.0291],\n",
      "        [-0.0294,  0.0157,  0.0768,  0.0149,  0.0300],\n",
      "        [ 0.0175, -0.0001,  0.0155, -0.0021,  0.0255],\n",
      "        [-0.0418,  0.1113,  0.0541, -0.0409,  0.0882]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0495,  0.0360,  0.1003, -0.1067,  0.0039],\n",
      "        [ 0.0463, -0.0125,  0.0043, -0.0419,  0.0014],\n",
      "        [ 0.0210,  0.0410, -0.0122, -0.0257, -0.0416],\n",
      "        [-0.0067, -0.0481,  0.0081,  0.0061,  0.0234],\n",
      "        [ 0.0351,  0.0099, -0.0306, -0.0355,  0.0538],\n",
      "        [ 0.0232,  0.0197,  0.0353, -0.0136, -0.0016],\n",
      "        [-0.0258, -0.0018, -0.0056,  0.0341,  0.0004],\n",
      "        [ 0.0085, -0.0105, -0.1113,  0.1287, -0.0131]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0044, grad_fn=<MinBackward1>), tensor(0.9370, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.22621546685695648\n",
      "@sample 363: tensor([[-0.0128,  0.0221,  0.0472, -0.0302,  0.0010],\n",
      "        [-0.0038, -0.0577, -0.0074, -0.0015, -0.0018],\n",
      "        [-0.0185, -0.0188,  0.0411,  0.0304, -0.0130],\n",
      "        [ 0.0085, -0.0374,  0.0119,  0.0225, -0.0194],\n",
      "        [-0.0399,  0.0253, -0.0038, -0.0152,  0.0181],\n",
      "        [ 0.0004, -0.0156, -0.0155, -0.0002,  0.0459],\n",
      "        [ 0.0151, -0.0229, -0.0380,  0.0456, -0.0071],\n",
      "        [ 0.0011,  0.0260, -0.0193, -0.0674,  0.0335]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0230, -0.0127, -0.0356,  0.0240, -0.0113],\n",
      "        [ 0.0031, -0.0247,  0.0543, -0.0414, -0.0377],\n",
      "        [ 0.0293,  0.0207,  0.1046, -0.0694,  0.0092],\n",
      "        [-0.0380,  0.0385,  0.0510, -0.0791, -0.0390],\n",
      "        [-0.0053, -0.0246,  0.0095, -0.0322, -0.0311],\n",
      "        [ 0.0271, -0.0260, -0.0681, -0.0164, -0.0221],\n",
      "        [ 0.0504,  0.0807,  0.0290,  0.0237,  0.0328],\n",
      "        [-0.0469, -0.0634, -0.0710,  0.0421, -0.0343]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0205, grad_fn=<MinBackward1>), tensor(0.9367, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.20448501408100128\n",
      "@sample 364: tensor([[-0.0074,  0.0015, -0.0217,  0.0040,  0.0255],\n",
      "        [ 0.0095,  0.0396,  0.0776, -0.0135,  0.1013],\n",
      "        [ 0.0403, -0.0290,  0.0345,  0.0293,  0.0047],\n",
      "        [-0.0258,  0.0136, -0.0366,  0.0103,  0.0349],\n",
      "        [ 0.0059, -0.0070, -0.0356,  0.0017, -0.0039],\n",
      "        [ 0.0372, -0.0448, -0.0410,  0.0494,  0.0082],\n",
      "        [ 0.0133,  0.0451, -0.0164, -0.0001, -0.0106],\n",
      "        [-0.0443, -0.0450, -0.0103,  0.0401, -0.0037]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 5.0829e-02, -2.1872e-02, -1.1654e-02,  4.3991e-02,  3.8389e-02],\n",
      "        [-7.8765e-03,  4.2791e-02, -7.0418e-02,  7.2370e-02, -4.1494e-03],\n",
      "        [-2.1526e-02,  5.3245e-02,  2.3277e-02, -1.9989e-02, -6.2159e-03],\n",
      "        [ 4.2220e-02,  3.0301e-02,  1.0330e-02,  3.7589e-03, -3.6642e-02],\n",
      "        [-3.7832e-02, -4.6674e-02, -6.8991e-02,  1.5659e-02,  7.5251e-05],\n",
      "        [-3.4367e-02,  2.7807e-03,  5.4230e-02, -4.4108e-02,  2.7876e-02],\n",
      "        [-6.7242e-02,  1.4808e-02, -8.2270e-02,  4.2186e-02, -7.6099e-03],\n",
      "        [ 2.7020e-02, -2.4060e-03,  7.4155e-02, -5.0637e-02, -2.7316e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0194, grad_fn=<MinBackward1>), tensor(0.9687, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.21985144913196564\n",
      "@sample 365: tensor([[ 0.0380,  0.0034,  0.0023,  0.0002, -0.0047],\n",
      "        [ 0.0165,  0.0448, -0.0028, -0.0392,  0.0601],\n",
      "        [ 0.0158, -0.0438, -0.0315,  0.0124, -0.0179],\n",
      "        [-0.0131,  0.0110, -0.0073, -0.0077,  0.0300],\n",
      "        [ 0.0240,  0.0268, -0.0042, -0.0049,  0.0333],\n",
      "        [-0.0022, -0.0183, -0.0069,  0.0530, -0.0477],\n",
      "        [ 0.0120, -0.0248, -0.0212,  0.0652, -0.0154],\n",
      "        [-0.0345,  0.0316, -0.0668,  0.0071, -0.0331]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0197, -0.0360, -0.0324, -0.0011, -0.0170],\n",
      "        [-0.0140, -0.0141, -0.0491,  0.0049,  0.0179],\n",
      "        [-0.0148,  0.0108, -0.0104,  0.0022,  0.0014],\n",
      "        [ 0.0356,  0.0484, -0.0082, -0.0116, -0.0862],\n",
      "        [ 0.0019,  0.0230, -0.0997,  0.0723,  0.0218],\n",
      "        [ 0.0143,  0.0507,  0.0358,  0.0232, -0.0245],\n",
      "        [ 0.0019,  0.0404, -0.0083, -0.0155, -0.0244],\n",
      "        [ 0.0012,  0.0145,  0.0409, -0.0147, -0.0030]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0301, grad_fn=<MinBackward1>), tensor(0.9388, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.20442524552345276\n",
      "@sample 366: tensor([[-0.0014, -0.0003, -0.0395,  0.0009, -0.0085],\n",
      "        [ 0.0277, -0.0221, -0.0009,  0.0480, -0.0154],\n",
      "        [-0.0180,  0.0381,  0.0254,  0.0117, -0.0172],\n",
      "        [ 0.0214, -0.0560, -0.0479,  0.0899, -0.0512],\n",
      "        [ 0.0176,  0.0622,  0.0090, -0.0629,  0.0220],\n",
      "        [ 0.0214,  0.0483, -0.0084,  0.0130, -0.0426],\n",
      "        [ 0.0047,  0.0342, -0.0020, -0.0063,  0.0088],\n",
      "        [ 0.0130, -0.0020,  0.0203, -0.0006, -0.0169]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0164, -0.0172, -0.0103, -0.0085,  0.0036],\n",
      "        [-0.0218,  0.0162,  0.0059, -0.0144,  0.0184],\n",
      "        [ 0.0490,  0.0251,  0.0231, -0.0040, -0.0431],\n",
      "        [-0.0058,  0.0365,  0.0599, -0.0504, -0.0239],\n",
      "        [-0.0839, -0.0474, -0.0891,  0.0529, -0.0042],\n",
      "        [-0.0385, -0.0032, -0.0667,  0.0640,  0.0289],\n",
      "        [-0.0033, -0.0108, -0.0415,  0.0131, -0.0182],\n",
      "        [-0.0055, -0.0126,  0.0106,  0.0189, -0.0020]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0230, grad_fn=<MinBackward1>), tensor(0.9563, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1995343565940857\n",
      "@sample 367: tensor([[ 0.0218,  0.0342, -0.0141, -0.0482,  0.0157],\n",
      "        [ 0.0074,  0.0255, -0.0408, -0.0188, -0.0424],\n",
      "        [ 0.0248,  0.0408, -0.0153, -0.0182, -0.0015],\n",
      "        [ 0.0156,  0.0355,  0.0332, -0.0136,  0.0470],\n",
      "        [ 0.0113,  0.0139,  0.0070, -0.0437,  0.0380],\n",
      "        [-0.0205,  0.0012, -0.0157,  0.0033,  0.0396],\n",
      "        [ 0.0187,  0.0449,  0.0284, -0.0810,  0.0432],\n",
      "        [ 0.0113,  0.0135, -0.0063,  0.0215,  0.0026]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0416, -0.0400, -0.0560,  0.0867,  0.0225],\n",
      "        [-0.0159, -0.0191,  0.0034,  0.0474, -0.0023],\n",
      "        [-0.0072, -0.0268,  0.0014, -0.0534, -0.0675],\n",
      "        [-0.0014, -0.0471,  0.0361,  0.0655, -0.0161],\n",
      "        [-0.0083, -0.0522,  0.0044, -0.0173, -0.0334],\n",
      "        [ 0.0329,  0.0006, -0.0430,  0.0451, -0.0065],\n",
      "        [-0.0419, -0.0426, -0.0507,  0.0648, -0.0152],\n",
      "        [-0.0079,  0.0207, -0.0224,  0.0392,  0.0421]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0124, grad_fn=<MinBackward1>), tensor(0.9708, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2065320611000061\n",
      "@sample 368: tensor([[-1.4700e-02,  2.7790e-02, -3.1089e-02,  1.4428e-02, -3.6601e-02],\n",
      "        [-2.6012e-02, -4.6271e-02, -2.5064e-02,  4.6483e-02, -1.2114e-02],\n",
      "        [ 1.1633e-02,  6.2936e-03,  3.0215e-02, -5.2214e-05, -1.4883e-02],\n",
      "        [-1.8260e-02, -4.2925e-02,  2.3742e-02,  3.7772e-02, -3.0919e-02],\n",
      "        [-7.0041e-03, -5.8319e-02,  1.3276e-02,  5.7503e-02,  7.4218e-04],\n",
      "        [-3.1886e-02,  1.7412e-02,  1.1966e-02, -1.3220e-02,  2.5855e-02],\n",
      "        [-1.9821e-02,  7.9116e-02, -1.1474e-02, -3.0249e-02,  1.6584e-03],\n",
      "        [-2.7275e-02,  3.6021e-03,  7.0863e-02,  6.6946e-03, -1.9155e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0147, -0.0126,  0.0109, -0.0031, -0.0411],\n",
      "        [-0.0134, -0.0035,  0.0061, -0.0179,  0.0283],\n",
      "        [ 0.0031, -0.0135, -0.0523,  0.0191,  0.0024],\n",
      "        [-0.0542,  0.0395,  0.0694, -0.0267,  0.0343],\n",
      "        [ 0.0453,  0.0500,  0.0109,  0.0314,  0.0005],\n",
      "        [-0.0294, -0.0587, -0.0801, -0.0376,  0.0127],\n",
      "        [-0.0275, -0.0505, -0.1021,  0.0439, -0.0062],\n",
      "        [-0.0328,  0.0149, -0.0065,  0.0230,  0.0076]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0110, grad_fn=<MinBackward1>), tensor(0.9642, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.20439089834690094\n",
      "@sample 369: tensor([[ 0.0330, -0.0072, -0.0226,  0.0257, -0.0366],\n",
      "        [-0.0182, -0.0214, -0.0149, -0.0527,  0.0216],\n",
      "        [ 0.0183, -0.0129, -0.0689,  0.0487, -0.0121],\n",
      "        [-0.0141,  0.0865,  0.0402, -0.1039,  0.0217],\n",
      "        [ 0.0282,  0.0151,  0.0021,  0.0319,  0.0011],\n",
      "        [-0.0326,  0.0403, -0.0064, -0.0431,  0.0215],\n",
      "        [-0.0050, -0.0076,  0.0055, -0.0027, -0.0597],\n",
      "        [-0.0031, -0.0079, -0.0069, -0.0354,  0.0302]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0373,  0.0228,  0.0012,  0.0411,  0.0158],\n",
      "        [ 0.0101, -0.0521, -0.0913,  0.0743, -0.0375],\n",
      "        [ 0.0523,  0.0165,  0.0613, -0.0595,  0.0430],\n",
      "        [-0.0197, -0.0765, -0.0590,  0.0307, -0.0187],\n",
      "        [ 0.0183,  0.0389, -0.0395,  0.0588,  0.0270],\n",
      "        [ 0.0049, -0.0124, -0.0393,  0.0521,  0.0396],\n",
      "        [-0.0403, -0.0183, -0.0308, -0.0108, -0.0148],\n",
      "        [ 0.0184, -0.0151,  0.0036, -0.0273,  0.0338]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0132, grad_fn=<MinBackward1>), tensor(0.9235, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.22068065404891968\n",
      "@sample 370: tensor([[-0.0054,  0.0009,  0.0134,  0.0326, -0.0195],\n",
      "        [ 0.0343,  0.0128,  0.0086,  0.0182, -0.0026],\n",
      "        [-0.0366,  0.0484,  0.0230, -0.0772, -0.0176],\n",
      "        [ 0.0046,  0.0457, -0.0335,  0.0229, -0.0331],\n",
      "        [ 0.0359, -0.0145, -0.0160,  0.0143, -0.0077],\n",
      "        [-0.0081, -0.0408, -0.0285,  0.0265, -0.0437],\n",
      "        [ 0.0366, -0.0261,  0.0087,  0.0221,  0.0066],\n",
      "        [-0.0121,  0.1005,  0.0663, -0.1169,  0.0330]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0129,  0.0167, -0.0804,  0.0589,  0.0120],\n",
      "        [-0.0078, -0.0083, -0.0511,  0.0079, -0.0347],\n",
      "        [ 0.0048, -0.0278, -0.0910,  0.0527,  0.0520],\n",
      "        [ 0.0161,  0.0777, -0.0748,  0.0058, -0.0157],\n",
      "        [-0.0100, -0.0323, -0.0125,  0.0213, -0.0154],\n",
      "        [ 0.0176, -0.0335,  0.0551, -0.0260, -0.0202],\n",
      "        [ 0.0056,  0.0136, -0.0086,  0.0479,  0.0610],\n",
      "        [-0.0574, -0.0981, -0.0819,  0.0549, -0.0244]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0223, grad_fn=<MinBackward1>), tensor(0.9333, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2221883088350296\n",
      "@sample 371: tensor([[ 3.2362e-02,  3.5538e-02, -4.9176e-02, -4.5004e-02,  1.9905e-03],\n",
      "        [-4.3039e-03,  1.7938e-02, -2.3415e-02,  5.7398e-02, -1.4715e-05],\n",
      "        [-3.4277e-03, -1.7062e-02, -1.5993e-02,  3.1281e-02, -1.3235e-02],\n",
      "        [-1.1434e-02, -1.0947e-02, -3.1185e-03,  1.9568e-02,  3.6474e-04],\n",
      "        [-4.3146e-02, -4.0053e-02,  8.8084e-03,  1.4961e-03, -2.0093e-02],\n",
      "        [-1.0677e-02,  6.2953e-02, -1.4219e-02, -1.0213e-01,  3.0794e-02],\n",
      "        [ 9.4640e-03, -6.2504e-02, -3.4759e-02,  3.3219e-02,  7.0901e-03],\n",
      "        [-6.9873e-02,  6.7845e-02,  6.8522e-03, -9.2961e-02,  6.1225e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0530, -0.0136, -0.0364,  0.0609, -0.0135],\n",
      "        [ 0.0255,  0.0480,  0.0045, -0.0508, -0.0243],\n",
      "        [ 0.0150, -0.0224,  0.0043, -0.0175, -0.0075],\n",
      "        [ 0.0219,  0.0412,  0.0322, -0.0406, -0.0038],\n",
      "        [-0.0231, -0.0440,  0.1073, -0.0837, -0.0081],\n",
      "        [-0.0343, -0.0909, -0.0980,  0.0174, -0.0257],\n",
      "        [ 0.0299,  0.0096,  0.0657,  0.0082,  0.0124],\n",
      "        [-0.0375, -0.0564, -0.0790,  0.0385,  0.0106]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0178, grad_fn=<MinBackward1>), tensor(0.9500, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.21913595497608185\n",
      "@sample 372: tensor([[ 0.0238, -0.0001, -0.0356,  0.0018,  0.0287],\n",
      "        [ 0.0515, -0.0068, -0.0216,  0.0235, -0.0449],\n",
      "        [-0.0179, -0.0310, -0.0210,  0.0451, -0.0098],\n",
      "        [ 0.0058,  0.0363, -0.0518,  0.0066,  0.0125],\n",
      "        [ 0.0095, -0.0203,  0.0233,  0.0462, -0.0100],\n",
      "        [ 0.0248,  0.0461,  0.0024, -0.0391,  0.0073],\n",
      "        [-0.0030,  0.0185,  0.0257, -0.0137,  0.0090],\n",
      "        [ 0.0091, -0.0223, -0.0036,  0.0083, -0.0238]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0076, -0.0201, -0.0206, -0.0701, -0.0560],\n",
      "        [ 0.0208, -0.0180,  0.0130,  0.0222, -0.0260],\n",
      "        [ 0.0316,  0.0765,  0.0817, -0.0363, -0.0567],\n",
      "        [ 0.0261, -0.0046, -0.0358, -0.0264, -0.0383],\n",
      "        [ 0.0026,  0.0192, -0.0350,  0.0470,  0.0715],\n",
      "        [-0.0557, -0.0525, -0.0529,  0.0421,  0.0395],\n",
      "        [-0.0351, -0.0116, -0.0334,  0.0152,  0.0024],\n",
      "        [-0.0343, -0.0020,  0.0282, -0.0848, -0.0661]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0222, grad_fn=<MinBackward1>), tensor(0.9472, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.20390543341636658\n",
      "@sample 373: tensor([[-0.0018, -0.0632, -0.0183,  0.0301, -0.0631],\n",
      "        [ 0.0102,  0.0319,  0.0111, -0.0522,  0.0497],\n",
      "        [-0.0127,  0.0002,  0.0301, -0.0603, -0.0146],\n",
      "        [-0.0120, -0.0360, -0.0527, -0.0399, -0.0369],\n",
      "        [-0.0106, -0.0187,  0.0073,  0.0511, -0.0130],\n",
      "        [ 0.0292,  0.0442,  0.0367, -0.0585,  0.0139],\n",
      "        [ 0.0105, -0.0167, -0.0272,  0.0224, -0.0502],\n",
      "        [-0.0231,  0.0095,  0.0286, -0.0238, -0.0215]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0024, -0.0067,  0.0632, -0.0662, -0.0170],\n",
      "        [-0.0064, -0.0262, -0.0024,  0.0199, -0.0022],\n",
      "        [-0.0281, -0.0762,  0.0228,  0.0194, -0.0085],\n",
      "        [-0.0518, -0.0761, -0.0006, -0.0098, -0.0214],\n",
      "        [ 0.0225,  0.0375,  0.1015, -0.0546, -0.0216],\n",
      "        [-0.0284, -0.0147, -0.0300,  0.0510,  0.0056],\n",
      "        [ 0.0139,  0.0441,  0.0454,  0.0201,  0.0138],\n",
      "        [-0.0487, -0.0114, -0.0250,  0.0127, -0.0316]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0167, grad_fn=<MinBackward1>), tensor(0.9562, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.205967515707016\n",
      "@sample 374: tensor([[ 0.0194, -0.0636, -0.0535,  0.0525, -0.0175],\n",
      "        [ 0.0401, -0.0423, -0.0205, -0.0223,  0.0019],\n",
      "        [-0.0032,  0.0159,  0.0454, -0.0182, -0.0126],\n",
      "        [-0.0040,  0.0028, -0.0340,  0.0016, -0.0076],\n",
      "        [-0.0081,  0.0049, -0.0351, -0.0044,  0.0095],\n",
      "        [-0.0135,  0.0481, -0.0189, -0.0372,  0.0399],\n",
      "        [-0.0218, -0.0061, -0.0071,  0.0255, -0.0719],\n",
      "        [-0.0141, -0.0597, -0.0212, -0.0010,  0.0109]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0045,  0.0026,  0.0187, -0.0141,  0.0320],\n",
      "        [-0.0489, -0.0219, -0.0733,  0.0317, -0.0034],\n",
      "        [-0.0574,  0.0380, -0.0284,  0.0297,  0.0130],\n",
      "        [ 0.0267,  0.0068, -0.0318,  0.0366,  0.0043],\n",
      "        [ 0.0200,  0.0052,  0.0569, -0.0050,  0.0142],\n",
      "        [-0.0414, -0.0211, -0.0625,  0.0079, -0.0034],\n",
      "        [ 0.0251,  0.0107,  0.0480, -0.0556,  0.0094],\n",
      "        [-0.0049, -0.0076,  0.0459, -0.0027,  0.0468]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0126, grad_fn=<MinBackward1>), tensor(0.9586, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.20024392008781433\n",
      "@sample 375: tensor([[ 0.0131, -0.0589, -0.0390,  0.0689, -0.0315],\n",
      "        [ 0.0081, -0.0229, -0.0216,  0.0199, -0.0018],\n",
      "        [ 0.0121,  0.0237,  0.0152, -0.0372,  0.0256],\n",
      "        [ 0.0034,  0.0028, -0.0057, -0.0005, -0.0326],\n",
      "        [ 0.0157,  0.0283,  0.0042, -0.0292, -0.0132],\n",
      "        [-0.0016,  0.0635, -0.0205, -0.0234,  0.0367],\n",
      "        [ 0.0057, -0.0211, -0.0312, -0.0125, -0.0054],\n",
      "        [ 0.0199, -0.0487, -0.0556,  0.0142, -0.0219]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0681,  0.0567,  0.0726, -0.0379, -0.0333],\n",
      "        [ 0.0365, -0.0121,  0.0023, -0.0020, -0.0273],\n",
      "        [-0.0273, -0.0670,  0.0544,  0.0050, -0.0303],\n",
      "        [-0.0032, -0.0071,  0.0652, -0.0403, -0.0549],\n",
      "        [-0.0595, -0.0072, -0.0521,  0.0221, -0.0203],\n",
      "        [ 0.0302, -0.0380, -0.1575,  0.0620, -0.0018],\n",
      "        [-0.0007, -0.0679,  0.0165, -0.0040,  0.0108],\n",
      "        [ 0.0273,  0.0380, -0.0039, -0.0591,  0.0316]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0128, grad_fn=<MinBackward1>), tensor(0.9429, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.21443192660808563\n",
      "@sample 376: tensor([[-0.0217,  0.0032,  0.0086, -0.0144, -0.0019],\n",
      "        [-0.0292, -0.0430, -0.0653,  0.0603, -0.0397],\n",
      "        [ 0.0009, -0.0119, -0.0218,  0.0137, -0.0328],\n",
      "        [-0.0337,  0.0752,  0.0178, -0.0656,  0.0625],\n",
      "        [-0.0301,  0.0204,  0.0209, -0.0177,  0.0485],\n",
      "        [-0.0198, -0.0677, -0.0159,  0.0307, -0.0049],\n",
      "        [ 0.0172,  0.0154,  0.0064,  0.0007, -0.0341],\n",
      "        [-0.0203, -0.0082, -0.0017,  0.0114, -0.0073]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 6.8750e-02, -2.9391e-02,  5.5273e-02, -7.1919e-02, -1.0292e-02],\n",
      "        [ 2.1694e-03, -5.9406e-03,  9.5389e-02, -8.3951e-02, -4.9024e-03],\n",
      "        [-2.5356e-02, -1.7266e-03,  3.4724e-02, -2.6356e-02, -4.7515e-03],\n",
      "        [-1.3822e-02, -2.4264e-02, -1.5262e-01,  4.6245e-02,  1.2545e-02],\n",
      "        [-2.4864e-02, -3.5308e-05, -4.2190e-02,  1.2799e-02,  4.8653e-02],\n",
      "        [ 3.9104e-02, -2.1005e-03,  8.6835e-02, -9.2385e-02,  2.9325e-02],\n",
      "        [-1.9878e-02,  2.5283e-02, -2.4591e-03,  4.3729e-04,  6.1236e-03],\n",
      "        [ 2.9549e-02,  4.6486e-02,  5.1739e-02, -3.6540e-02, -4.7407e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0293, grad_fn=<MinBackward1>), tensor(0.9264, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2148827314376831\n",
      "@sample 377: tensor([[ 0.0047,  0.0184,  0.0143,  0.0079,  0.0340],\n",
      "        [-0.0705,  0.0856, -0.0269, -0.0670,  0.0331],\n",
      "        [ 0.0044, -0.0211,  0.0036,  0.0267, -0.0536],\n",
      "        [ 0.0167,  0.0123,  0.0211, -0.0498,  0.0084],\n",
      "        [-0.0245, -0.0359,  0.0034,  0.0270,  0.0067],\n",
      "        [ 0.0300,  0.0179, -0.0212, -0.0399,  0.0531],\n",
      "        [ 0.0016, -0.0636, -0.0324,  0.0583, -0.0208],\n",
      "        [-0.0037, -0.0356,  0.0091,  0.0660, -0.0176]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0098,  0.0007, -0.0408, -0.0071, -0.0246],\n",
      "        [-0.0051,  0.0016, -0.0441, -0.0098, -0.0053],\n",
      "        [-0.0374,  0.0207, -0.0559,  0.0123,  0.0020],\n",
      "        [-0.0473, -0.0347,  0.0696,  0.0435, -0.0290],\n",
      "        [-0.0206,  0.0128,  0.0710, -0.0369,  0.0264],\n",
      "        [ 0.0114, -0.0234, -0.0165,  0.0057, -0.0013],\n",
      "        [ 0.0351,  0.0327,  0.0875, -0.0734, -0.0056],\n",
      "        [ 0.0435,  0.0156,  0.0574, -0.0659,  0.0174]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0245, grad_fn=<MinBackward1>), tensor(0.9540, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.21858914196491241\n",
      "@sample 378: tensor([[-0.0268, -0.0562, -0.0024, -0.0143, -0.0005],\n",
      "        [ 0.0417, -0.0180,  0.0105,  0.0350,  0.0309],\n",
      "        [ 0.0330,  0.0594,  0.0092, -0.0300,  0.0167],\n",
      "        [ 0.0151,  0.0577,  0.0218,  0.0006,  0.0203],\n",
      "        [ 0.0169, -0.0239,  0.0285, -0.0358,  0.1090],\n",
      "        [ 0.0247, -0.0182,  0.0002, -0.0203,  0.0433],\n",
      "        [ 0.0183, -0.0089,  0.0056, -0.0363,  0.0552],\n",
      "        [-0.0045,  0.0231,  0.0563, -0.0169, -0.0140]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0214, -0.0124, -0.0285,  0.0867,  0.0059],\n",
      "        [ 0.0323,  0.0347,  0.0021, -0.0092, -0.0305],\n",
      "        [ 0.0444, -0.0045, -0.1338,  0.0621, -0.0304],\n",
      "        [-0.0057, -0.0115, -0.1140,  0.0258, -0.0093],\n",
      "        [ 0.0205, -0.0013, -0.0498, -0.0180, -0.0469],\n",
      "        [-0.0201, -0.0089, -0.0229,  0.0557, -0.0040],\n",
      "        [-0.0079, -0.0622, -0.0075, -0.0478, -0.0455],\n",
      "        [-0.0216,  0.0208, -0.0103,  0.0075,  0.0043]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0171, grad_fn=<MinBackward1>), tensor(0.9527, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.20963610708713531\n",
      "@sample 379: tensor([[-0.0090,  0.0034,  0.0009, -0.0370,  0.0401],\n",
      "        [-0.0111, -0.0445,  0.0134,  0.0180, -0.0115],\n",
      "        [ 0.0339, -0.0121,  0.0010,  0.0217,  0.0054],\n",
      "        [-0.0509, -0.0151,  0.0419,  0.0252, -0.0281],\n",
      "        [ 0.0148,  0.0571,  0.0228, -0.0134,  0.0278],\n",
      "        [ 0.0006,  0.0060,  0.0261, -0.0335,  0.0089],\n",
      "        [-0.0031, -0.0216,  0.0395, -0.0480,  0.0161],\n",
      "        [ 0.0177,  0.0511,  0.0423, -0.0899,  0.0737]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0377,  0.0439, -0.0402,  0.0455,  0.0157],\n",
      "        [ 0.0254, -0.0221,  0.0814, -0.0433, -0.0240],\n",
      "        [ 0.0264,  0.0515, -0.0003,  0.0309, -0.0065],\n",
      "        [-0.0152,  0.0438,  0.0185,  0.0097,  0.0240],\n",
      "        [-0.0036, -0.0083, -0.0581,  0.0355, -0.0030],\n",
      "        [-0.1206, -0.0003, -0.0908,  0.0826,  0.0618],\n",
      "        [-0.0231, -0.0709, -0.0460,  0.0160, -0.0585],\n",
      "        [-0.0846, -0.0464, -0.0586,  0.0895,  0.0044]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0117, grad_fn=<MinBackward1>), tensor(0.9523, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2137204259634018\n",
      "@sample 380: tensor([[-0.0002,  0.0577,  0.0286, -0.0543,  0.0531],\n",
      "        [-0.0005,  0.0479,  0.0156, -0.0443,  0.0172],\n",
      "        [-0.0023,  0.0152,  0.0407,  0.0317,  0.0220],\n",
      "        [ 0.0239,  0.0225,  0.0051, -0.0562,  0.0366],\n",
      "        [-0.0012, -0.0049,  0.0302, -0.0047,  0.0129],\n",
      "        [-0.0184, -0.0071, -0.0140,  0.0143,  0.0255],\n",
      "        [-0.0247, -0.0175,  0.0281, -0.0285,  0.0235],\n",
      "        [-0.0239,  0.0218,  0.0246,  0.0243,  0.0065]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0121, -0.0421, -0.0747,  0.0646,  0.0063],\n",
      "        [-0.0476, -0.0186, -0.0650,  0.0023, -0.0356],\n",
      "        [ 0.0041,  0.0151,  0.0250, -0.0090, -0.0003],\n",
      "        [-0.0496, -0.0394, -0.0967,  0.0305, -0.0174],\n",
      "        [-0.0033,  0.0186,  0.0047,  0.0654,  0.0470],\n",
      "        [ 0.0188, -0.0062,  0.0566, -0.0057, -0.0515],\n",
      "        [ 0.0215, -0.0643,  0.0407, -0.0132, -0.0142],\n",
      "        [ 0.0245,  0.0219, -0.0184,  0.0041, -0.0405]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0244, grad_fn=<MinBackward1>), tensor(0.9484, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.19123587012290955\n",
      "@sample 381: tensor([[ 0.0364, -0.0190, -0.0029,  0.0359,  0.0326],\n",
      "        [-0.0235,  0.0241, -0.0175, -0.0653,  0.0379],\n",
      "        [-0.0154,  0.0570,  0.0227, -0.0566,  0.0551],\n",
      "        [-0.0237, -0.0088, -0.0128,  0.0045, -0.0275],\n",
      "        [ 0.0175,  0.0454,  0.0052, -0.0138, -0.0015],\n",
      "        [-0.0175, -0.0260,  0.0161,  0.0165,  0.0154],\n",
      "        [-0.0304, -0.0193, -0.0011, -0.0499,  0.0256],\n",
      "        [-0.0251,  0.0560,  0.0233, -0.0337,  0.0599]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0505, -0.0084, -0.0300,  0.0101,  0.0112],\n",
      "        [-0.0203, -0.0427, -0.1477,  0.0768,  0.0279],\n",
      "        [-0.0166, -0.0264, -0.1306,  0.0677,  0.0356],\n",
      "        [-0.0593,  0.0367,  0.0433, -0.0331, -0.0068],\n",
      "        [-0.0096,  0.0004,  0.0497, -0.0533, -0.0271],\n",
      "        [-0.0007,  0.0465, -0.0054, -0.0021,  0.0095],\n",
      "        [-0.0202, -0.0748,  0.0240, -0.0622, -0.0131],\n",
      "        [ 0.0535, -0.0183, -0.0009, -0.0587, -0.0105]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0167, grad_fn=<MinBackward1>), tensor(0.9550, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.21227450668811798\n",
      "@sample 382: tensor([[-0.0284,  0.1045,  0.0554, -0.0537,  0.0336],\n",
      "        [ 0.0182, -0.0333,  0.0259,  0.0165, -0.0169],\n",
      "        [-0.0233,  0.0833,  0.0280, -0.1245,  0.0398],\n",
      "        [-0.0238,  0.0412,  0.0070, -0.0377, -0.0159],\n",
      "        [-0.0079,  0.0013,  0.0305, -0.0077,  0.0225],\n",
      "        [ 0.0325, -0.0296, -0.0252,  0.0404, -0.0508],\n",
      "        [-0.0145,  0.0180,  0.0352, -0.0452,  0.0397],\n",
      "        [-0.0287,  0.0297, -0.0194, -0.0074, -0.0160]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0949, -0.0665, -0.1442,  0.0488, -0.0226],\n",
      "        [-0.0109,  0.0379, -0.0367, -0.0118,  0.0184],\n",
      "        [-0.0656, -0.1225, -0.1211,  0.0463, -0.0725],\n",
      "        [-0.0133,  0.0019, -0.0059,  0.0917, -0.0130],\n",
      "        [-0.0059,  0.0068,  0.0398, -0.0514, -0.0359],\n",
      "        [-0.0089,  0.0765,  0.0080, -0.0373,  0.0004],\n",
      "        [-0.0112, -0.0137,  0.0248, -0.0112,  0.0431],\n",
      "        [ 0.0557,  0.0291,  0.0090, -0.0614,  0.0070]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0267, grad_fn=<MinBackward1>), tensor(0.9529, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2297435849905014\n",
      "@sample 383: tensor([[ 0.0156,  0.0523,  0.0035, -0.0560,  0.0076],\n",
      "        [ 0.0113, -0.0004,  0.0472, -0.0046, -0.0205],\n",
      "        [ 0.0419,  0.0388,  0.0250, -0.0516,  0.0348],\n",
      "        [ 0.0078,  0.0228,  0.0696, -0.0144,  0.0170],\n",
      "        [ 0.0108,  0.0376, -0.0134,  0.0436, -0.0143],\n",
      "        [-0.0086, -0.0113,  0.0311,  0.0401,  0.0154],\n",
      "        [ 0.0177, -0.0454, -0.0187,  0.0114, -0.0117],\n",
      "        [ 0.0320, -0.0181,  0.0271,  0.0836, -0.0255]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0296, -0.0530, -0.0628,  0.0236, -0.0598],\n",
      "        [ 0.0040,  0.0319,  0.0005,  0.0581, -0.0259],\n",
      "        [-0.0291, -0.0296, -0.0258,  0.0632, -0.0811],\n",
      "        [-0.0131,  0.0129, -0.0438, -0.0123, -0.0168],\n",
      "        [-0.0088,  0.0354,  0.0005, -0.0231,  0.0334],\n",
      "        [-0.0262,  0.0208,  0.0086, -0.0576, -0.0055],\n",
      "        [-0.0276,  0.0018,  0.0467, -0.0638, -0.0577],\n",
      "        [ 0.0348,  0.1014,  0.0438, -0.0639, -0.0007]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0185, grad_fn=<MinBackward1>), tensor(0.9302, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2120678722858429\n",
      "@sample 384: tensor([[ 0.0086,  0.0155,  0.0314, -0.0060, -0.0079],\n",
      "        [-0.0170,  0.0375,  0.0246, -0.0106, -0.0094],\n",
      "        [-0.0323, -0.0094,  0.0117,  0.0019,  0.0129],\n",
      "        [-0.0087, -0.0089, -0.0045, -0.0069, -0.0283],\n",
      "        [-0.0133,  0.0549,  0.0355, -0.0714,  0.0189],\n",
      "        [ 0.0036, -0.0039, -0.0153,  0.0279, -0.0401],\n",
      "        [-0.0397,  0.0345,  0.0373, -0.0218,  0.0142],\n",
      "        [-0.0081,  0.0191, -0.0074, -0.0356,  0.0236]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-1.8158e-02,  2.4085e-02, -2.7303e-02,  2.6070e-03, -1.8406e-02],\n",
      "        [ 3.2263e-02,  3.5734e-02, -1.3272e-02,  1.8742e-02, -2.0909e-03],\n",
      "        [ 7.8145e-03,  1.6570e-02,  3.0148e-02, -3.8890e-02,  5.3104e-03],\n",
      "        [-1.0407e-02, -6.3309e-03,  3.0326e-02,  8.0153e-05, -5.6894e-03],\n",
      "        [-2.9521e-02, -2.6490e-02, -6.6774e-02,  4.4251e-02, -1.3075e-02],\n",
      "        [ 3.3507e-02,  7.1834e-02,  4.1149e-02, -8.4398e-02, -2.6054e-02],\n",
      "        [-1.1602e-02, -9.5475e-03, -5.8382e-02,  8.8935e-02,  2.8993e-02],\n",
      "        [-4.6618e-02, -3.1272e-03,  5.4485e-04, -2.2901e-02, -1.8587e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0237, grad_fn=<MinBackward1>), tensor(0.9781, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.19257844984531403\n",
      "@sample 385: tensor([[-1.5972e-02,  2.0640e-02,  4.5703e-02, -5.1909e-02,  5.6819e-02],\n",
      "        [-1.6906e-02,  1.3184e-02,  2.9762e-02, -2.1758e-02,  2.8737e-02],\n",
      "        [-2.5223e-02,  4.8060e-03,  1.4430e-04,  2.4149e-02, -1.2345e-02],\n",
      "        [-2.6317e-02, -7.3910e-05,  2.9170e-02, -4.0659e-02, -4.9799e-03],\n",
      "        [ 3.5045e-02, -2.2943e-02,  1.5172e-02,  1.4418e-02, -2.2931e-02],\n",
      "        [-2.8962e-02,  2.6409e-02,  4.1038e-02,  8.4955e-03,  7.5278e-03],\n",
      "        [ 2.8110e-02, -4.4047e-02, -1.4089e-02,  3.8624e-02, -1.6086e-02],\n",
      "        [ 2.8415e-02,  5.7837e-03,  2.3614e-02, -6.3302e-02,  3.6871e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0147, -0.0740, -0.0199,  0.0020,  0.0042],\n",
      "        [-0.0097, -0.0120, -0.0612,  0.0233, -0.0147],\n",
      "        [ 0.0526,  0.0544,  0.0896, -0.0234,  0.0431],\n",
      "        [-0.0013, -0.0083, -0.0435,  0.0163, -0.0146],\n",
      "        [-0.0285,  0.0525, -0.0615,  0.0169, -0.0176],\n",
      "        [ 0.0121,  0.0517,  0.0145, -0.0060, -0.0303],\n",
      "        [-0.0201,  0.0370, -0.0238, -0.0054, -0.0040],\n",
      "        [-0.0429, -0.0385, -0.0851,  0.0335, -0.0133]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0220, grad_fn=<MinBackward1>), tensor(0.9634, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.20141544938087463\n",
      "@sample 386: tensor([[ 0.0047, -0.0202, -0.0199,  0.0110,  0.0309],\n",
      "        [ 0.0298,  0.0088,  0.0391,  0.0218,  0.0156],\n",
      "        [ 0.0135, -0.0185, -0.0158,  0.0244, -0.0127],\n",
      "        [ 0.0102,  0.0305,  0.0059,  0.0246,  0.0051],\n",
      "        [-0.0187,  0.0025,  0.0087,  0.0137,  0.0178],\n",
      "        [ 0.0052,  0.0045,  0.0032, -0.0248,  0.0349],\n",
      "        [ 0.0028, -0.0151,  0.0059, -0.0262,  0.0185],\n",
      "        [-0.0176, -0.0051,  0.0023, -0.0066,  0.0190]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0527, -0.0487,  0.0131,  0.0103,  0.0246],\n",
      "        [-0.0496,  0.0036, -0.0747,  0.0146,  0.0239],\n",
      "        [ 0.0250, -0.0020, -0.0239, -0.0127, -0.0010],\n",
      "        [-0.0221,  0.0265, -0.0134, -0.0104,  0.0094],\n",
      "        [ 0.0546,  0.0129, -0.0393, -0.0007, -0.0025],\n",
      "        [ 0.0358,  0.0270, -0.0110,  0.0574,  0.0587],\n",
      "        [-0.0233, -0.0562, -0.1229,  0.0411,  0.0694],\n",
      "        [ 0.0131,  0.0006,  0.0729, -0.0205,  0.0023]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0191, grad_fn=<MinBackward1>), tensor(0.9557, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1907900720834732\n",
      "@sample 387: tensor([[ 0.0214, -0.0370, -0.0079,  0.0637, -0.0214],\n",
      "        [-0.0257, -0.0063, -0.0514, -0.0367,  0.0425],\n",
      "        [-0.0139, -0.0144, -0.0306,  0.0918, -0.0620],\n",
      "        [-0.0201,  0.0033, -0.0461,  0.0189,  0.0077],\n",
      "        [-0.0531,  0.0377, -0.0468, -0.0034,  0.0183],\n",
      "        [ 0.0067,  0.0205,  0.0095,  0.0253, -0.0515],\n",
      "        [-0.0058, -0.0287, -0.0622,  0.1307, -0.0563],\n",
      "        [-0.0337, -0.0085, -0.0145, -0.0243,  0.0141]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0407,  0.0439,  0.0225, -0.0005,  0.0243],\n",
      "        [ 0.0267, -0.0468,  0.0124,  0.0026, -0.0166],\n",
      "        [ 0.0492,  0.0325,  0.0611, -0.1028, -0.0066],\n",
      "        [ 0.0064, -0.0012,  0.0091, -0.0146, -0.0252],\n",
      "        [ 0.0179, -0.0166,  0.0148, -0.0191,  0.0175],\n",
      "        [ 0.0254,  0.0268, -0.0355,  0.0091,  0.0447],\n",
      "        [ 0.0976,  0.0690,  0.0475, -0.0284,  0.0275],\n",
      "        [-0.0139,  0.0146, -0.0529,  0.0463,  0.0697]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0149, grad_fn=<MinBackward1>), tensor(0.9330, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.22471828758716583\n",
      "@sample 388: tensor([[ 0.0122,  0.1163,  0.0295, -0.0380,  0.0875],\n",
      "        [-0.0187,  0.0588,  0.0067, -0.1038,  0.0764],\n",
      "        [ 0.0062, -0.0084,  0.0097, -0.0245,  0.0173],\n",
      "        [ 0.0085,  0.0283,  0.0227,  0.0271,  0.0171],\n",
      "        [-0.0265, -0.0065, -0.0369, -0.0250,  0.0379],\n",
      "        [ 0.0167, -0.0065,  0.0432, -0.0248,  0.0445],\n",
      "        [ 0.0280,  0.0409,  0.0086,  0.0232,  0.0089],\n",
      "        [-0.0030, -0.0060,  0.0064,  0.0344, -0.0399]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0041, -0.0077, -0.0841,  0.0095, -0.0127],\n",
      "        [-0.0397, -0.1137, -0.1253,  0.0980, -0.0113],\n",
      "        [-0.0054,  0.0017, -0.0449,  0.0438,  0.0560],\n",
      "        [-0.0004,  0.0115, -0.0505,  0.0478,  0.0335],\n",
      "        [ 0.0581, -0.0430, -0.0162, -0.0020, -0.0505],\n",
      "        [ 0.0406, -0.0401, -0.0570,  0.0791,  0.0571],\n",
      "        [ 0.0055,  0.0181, -0.0371,  0.0161,  0.0391],\n",
      "        [-0.0155,  0.0408,  0.0211, -0.0126, -0.0119]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0182, grad_fn=<MinBackward1>), tensor(0.9291, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.21453367173671722\n",
      "@sample 389: tensor([[-3.7003e-02, -1.5483e-02, -6.7531e-02,  1.6679e-02, -8.2475e-03],\n",
      "        [ 2.1795e-02,  1.8475e-02,  2.0310e-02, -2.1651e-02, -2.5134e-03],\n",
      "        [-5.4130e-03, -5.2286e-02, -3.6107e-02,  9.6165e-02, -4.0189e-02],\n",
      "        [-2.5846e-02, -2.1328e-02, -7.2777e-03,  2.6948e-02, -7.7273e-03],\n",
      "        [ 4.8833e-02,  5.4651e-02, -1.4623e-02, -2.4447e-02,  2.9093e-02],\n",
      "        [-1.5361e-03, -3.4469e-02, -9.4324e-06,  6.1123e-02, -3.1833e-02],\n",
      "        [ 4.0213e-03,  2.3433e-02,  3.2126e-03, -2.5477e-02,  1.8232e-02],\n",
      "        [-4.3067e-02, -9.4774e-03,  7.2109e-03,  9.5531e-03,  1.0592e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0023, -0.0101,  0.0303, -0.0113,  0.0251],\n",
      "        [-0.0464, -0.0515, -0.0646,  0.0403,  0.0141],\n",
      "        [ 0.0595,  0.0454,  0.0797, -0.0417,  0.0488],\n",
      "        [ 0.0414,  0.0028,  0.0057, -0.0045,  0.0327],\n",
      "        [-0.0572, -0.0276, -0.0528,  0.0626,  0.0089],\n",
      "        [ 0.0632,  0.0994,  0.0166, -0.0437,  0.0264],\n",
      "        [ 0.0039, -0.0198, -0.0286,  0.0478,  0.0075],\n",
      "        [ 0.0189,  0.0064,  0.0282,  0.0056, -0.0024]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0083, grad_fn=<MinBackward1>), tensor(0.9766, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.215998575091362\n",
      "@sample 390: tensor([[ 0.0025, -0.0667, -0.0054,  0.0048, -0.0164],\n",
      "        [ 0.0033, -0.0089,  0.0260,  0.0115, -0.0239],\n",
      "        [ 0.0209,  0.0131,  0.0020,  0.0089,  0.0102],\n",
      "        [-0.0199,  0.0325, -0.0215, -0.0301, -0.0095],\n",
      "        [-0.0097,  0.0565, -0.0003, -0.0450,  0.0299],\n",
      "        [ 0.0195, -0.0169, -0.0064,  0.0875, -0.0172],\n",
      "        [-0.0319,  0.0325, -0.0107, -0.0315,  0.0266],\n",
      "        [ 0.0164,  0.0175,  0.0034, -0.0254, -0.0140]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0443,  0.0039, -0.0814,  0.0670, -0.0548],\n",
      "        [-0.0196, -0.0064,  0.0288, -0.0081,  0.0042],\n",
      "        [ 0.0039, -0.0143, -0.0330, -0.0053,  0.0069],\n",
      "        [-0.0074, -0.0196, -0.0380,  0.0119, -0.0088],\n",
      "        [ 0.0054, -0.0460, -0.0575,  0.0461,  0.0180],\n",
      "        [ 0.0957,  0.0964, -0.0670,  0.0569,  0.0115],\n",
      "        [-0.0538, -0.0370,  0.0127,  0.0016, -0.0358],\n",
      "        [-0.0400,  0.0060, -0.0379,  0.0606, -0.0111]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0127, grad_fn=<MinBackward1>), tensor(0.9556, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.19966284930706024\n",
      "@sample 391: tensor([[-0.0027,  0.0307,  0.0879, -0.0911,  0.0677],\n",
      "        [ 0.0039,  0.0810,  0.0307, -0.0249,  0.0177],\n",
      "        [-0.0060, -0.0045,  0.0017,  0.0158, -0.0305],\n",
      "        [-0.0354,  0.0456, -0.0192, -0.0988,  0.0107],\n",
      "        [-0.0347, -0.0272, -0.0029, -0.0508,  0.0028],\n",
      "        [-0.0087, -0.0046, -0.0426, -0.0210, -0.0452],\n",
      "        [-0.0153, -0.0401, -0.0182, -0.0278,  0.0114],\n",
      "        [-0.0100, -0.0204, -0.0037,  0.0345, -0.0152]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0502, -0.0646, -0.1024,  0.0540,  0.0026],\n",
      "        [-0.0627, -0.0250, -0.0817,  0.0710,  0.0060],\n",
      "        [-0.0345,  0.0163, -0.0180,  0.0009, -0.0165],\n",
      "        [-0.0904, -0.0808, -0.1143,  0.0306, -0.0014],\n",
      "        [-0.0074, -0.0691,  0.0241, -0.0276, -0.0334],\n",
      "        [ 0.0075, -0.0203, -0.0123, -0.0124,  0.0161],\n",
      "        [ 0.0272,  0.0003,  0.0121, -0.0345, -0.0032],\n",
      "        [-0.0208, -0.0070,  0.0130,  0.0067,  0.0125]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0244, grad_fn=<MinBackward1>), tensor(0.9054, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2118576467037201\n",
      "@sample 392: tensor([[ 0.0026, -0.0377, -0.0431,  0.0461, -0.0669],\n",
      "        [ 0.0029, -0.0221, -0.0287,  0.0137, -0.0476],\n",
      "        [ 0.0119, -0.0578, -0.0239,  0.0723, -0.0333],\n",
      "        [-0.0220, -0.0190,  0.0165,  0.0313,  0.0189],\n",
      "        [-0.0004, -0.0714, -0.0334,  0.0749, -0.0501],\n",
      "        [ 0.0253,  0.0243, -0.0190,  0.0298,  0.0053],\n",
      "        [-0.0723,  0.0210,  0.0063, -0.0208,  0.0179],\n",
      "        [-0.0110,  0.0585, -0.0047, -0.0654,  0.0075]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0324, -0.0255,  0.0945, -0.0465, -0.0339],\n",
      "        [ 0.0159,  0.0050,  0.0293, -0.0101, -0.0074],\n",
      "        [ 0.0392,  0.0601,  0.1123, -0.1054, -0.0041],\n",
      "        [ 0.0205,  0.0328,  0.0746, -0.0313, -0.0116],\n",
      "        [ 0.0836,  0.0262,  0.1225, -0.0787,  0.0620],\n",
      "        [ 0.0153,  0.0230, -0.0261,  0.0150,  0.0086],\n",
      "        [ 0.0237, -0.0287,  0.0353, -0.0199, -0.0201],\n",
      "        [ 0.0156, -0.0367, -0.0686,  0.0517,  0.0579]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0159, grad_fn=<MinBackward1>), tensor(0.9563, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.22957424819469452\n",
      "@sample 393: tensor([[-0.0090, -0.0235, -0.0593,  0.0463, -0.0275],\n",
      "        [-0.0189, -0.0117, -0.0474,  0.0186, -0.0049],\n",
      "        [-0.0428,  0.0615,  0.0394, -0.0937,  0.0060],\n",
      "        [-0.0111, -0.0397, -0.0255,  0.0235, -0.0241],\n",
      "        [ 0.0032,  0.0576,  0.0353, -0.0756,  0.0510],\n",
      "        [ 0.0224,  0.0172,  0.0211, -0.0258,  0.0347],\n",
      "        [-0.0281, -0.0355, -0.0040,  0.0063,  0.0235],\n",
      "        [-0.0097, -0.0440, -0.0217,  0.0559, -0.0086]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0091,  0.0044,  0.0463, -0.0357, -0.0088],\n",
      "        [-0.0028, -0.0318, -0.0105, -0.0310, -0.0077],\n",
      "        [-0.0451, -0.0237,  0.0189,  0.0244, -0.0144],\n",
      "        [-0.0082, -0.0416,  0.0348,  0.0128,  0.0203],\n",
      "        [-0.0193, -0.0316, -0.0696,  0.0653,  0.0262],\n",
      "        [-0.0028,  0.0292, -0.0940,  0.0486,  0.0323],\n",
      "        [ 0.0259,  0.0075, -0.0346,  0.0300, -0.0015],\n",
      "        [ 0.0264, -0.0333, -0.0054, -0.0213, -0.0042]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0271, grad_fn=<MinBackward1>), tensor(0.9445, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.20725655555725098\n",
      "@sample 394: tensor([[-0.0126,  0.0445,  0.0147, -0.0824,  0.0417],\n",
      "        [-0.0252,  0.0279,  0.0035, -0.0395,  0.0059],\n",
      "        [ 0.0004, -0.0258,  0.0329,  0.0584, -0.0146],\n",
      "        [-0.0039,  0.0235, -0.0226,  0.0160, -0.0550],\n",
      "        [ 0.0171,  0.0334,  0.0209, -0.0766,  0.0360],\n",
      "        [-0.0122,  0.0044,  0.0275, -0.0190, -0.0445],\n",
      "        [-0.0213,  0.0237, -0.0012, -0.0294,  0.0109],\n",
      "        [-0.0032,  0.0093,  0.0002, -0.0364, -0.0038]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0623, -0.0718, -0.1187,  0.1193, -0.0024],\n",
      "        [-0.0288, -0.0220, -0.0632,  0.0339, -0.0068],\n",
      "        [ 0.0942,  0.0851,  0.0302, -0.0101,  0.0347],\n",
      "        [ 0.0283,  0.0014,  0.0957, -0.0032,  0.0559],\n",
      "        [-0.0300, -0.0827, -0.1235,  0.0595, -0.0342],\n",
      "        [-0.0280,  0.0277,  0.0438, -0.0249,  0.0337],\n",
      "        [ 0.0503, -0.0621, -0.0485,  0.0245,  0.0216],\n",
      "        [-0.0638, -0.0325, -0.0249,  0.0261, -0.0084]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0162, grad_fn=<MinBackward1>), tensor(0.9774, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.21352408826351166\n",
      "@sample 395: tensor([[ 0.0110, -0.0147,  0.0101, -0.0895,  0.0653],\n",
      "        [-0.0102, -0.0523, -0.0130,  0.0688, -0.0514],\n",
      "        [ 0.0236,  0.0057, -0.0469, -0.0348,  0.0025],\n",
      "        [ 0.0092,  0.0076,  0.0334, -0.0439, -0.0269],\n",
      "        [-0.0149,  0.0323, -0.0233, -0.0035, -0.0189],\n",
      "        [-0.0020,  0.0012, -0.0137,  0.0436, -0.0288],\n",
      "        [ 0.0365,  0.0106,  0.0114, -0.0084,  0.0067],\n",
      "        [ 0.0234,  0.0063, -0.0502, -0.0142, -0.0048]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-2.3742e-02, -8.1022e-02, -9.7908e-02,  7.2366e-02, -6.1025e-03],\n",
      "        [ 5.3228e-02,  3.1306e-02,  7.0321e-02, -4.8851e-02,  4.1395e-02],\n",
      "        [-5.8519e-02, -5.6957e-02,  1.1209e-02,  8.7872e-05, -5.3333e-02],\n",
      "        [-4.4565e-02, -5.4035e-02,  2.8064e-03,  4.5430e-02,  2.4884e-03],\n",
      "        [-4.6195e-02, -7.7815e-03,  3.5235e-02, -5.1005e-02, -4.7936e-02],\n",
      "        [ 6.8302e-02,  4.1827e-03, -5.4874e-03, -3.6382e-02,  4.6301e-02],\n",
      "        [-6.5313e-02,  2.3720e-02, -3.4398e-02, -2.1662e-03, -2.1300e-03],\n",
      "        [-6.6059e-04, -1.2661e-02, -5.9633e-03,  1.8907e-02,  4.3650e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0190, grad_fn=<MinBackward1>), tensor(0.9665, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2188325673341751\n",
      "@sample 396: tensor([[-0.0257,  0.0235, -0.0539,  0.0470, -0.0431],\n",
      "        [ 0.0024,  0.0157,  0.0254, -0.0045, -0.0047],\n",
      "        [-0.0007,  0.0514, -0.0581, -0.0837,  0.0057],\n",
      "        [ 0.0136, -0.0080, -0.0005, -0.0240,  0.0131],\n",
      "        [-0.0354,  0.0170, -0.0117, -0.0401,  0.0587],\n",
      "        [ 0.0061,  0.0538, -0.0078, -0.0332, -0.0067],\n",
      "        [ 0.0031, -0.0460, -0.0211, -0.0185, -0.0164],\n",
      "        [ 0.0007, -0.0229,  0.0033, -0.0338, -0.0003]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0097,  0.0172, -0.0097, -0.0133,  0.0339],\n",
      "        [-0.0200,  0.0051, -0.0342, -0.0006,  0.0104],\n",
      "        [-0.0272, -0.0931, -0.0606,  0.0547,  0.0103],\n",
      "        [ 0.0041,  0.0037,  0.0058, -0.0061, -0.0285],\n",
      "        [-0.0482, -0.0768, -0.0503,  0.0251, -0.0041],\n",
      "        [-0.0064, -0.0175, -0.0931,  0.0312,  0.0430],\n",
      "        [-0.0471, -0.0838,  0.0234, -0.0297, -0.0786],\n",
      "        [-0.0414,  0.0228, -0.0400,  0.0286,  0.0371]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0163, grad_fn=<MinBackward1>), tensor(0.9434, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2047743946313858\n",
      "@sample 397: tensor([[ 0.0178,  0.0342,  0.0176, -0.0368,  0.0074],\n",
      "        [-0.0104,  0.0310,  0.0133, -0.0179,  0.0376],\n",
      "        [ 0.0100, -0.0689, -0.0040,  0.0263, -0.0100],\n",
      "        [ 0.0256,  0.0298,  0.0272, -0.0329,  0.0327],\n",
      "        [ 0.0285, -0.0078, -0.0176,  0.0379, -0.0824],\n",
      "        [-0.0118,  0.0038, -0.0309,  0.0271, -0.0317],\n",
      "        [ 0.0427, -0.0126,  0.0374, -0.0435,  0.0206],\n",
      "        [-0.0110, -0.0224,  0.0244, -0.0335,  0.0371]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0257, -0.0362, -0.0395,  0.0246, -0.0340],\n",
      "        [-0.0294, -0.0483, -0.0687,  0.0391, -0.0017],\n",
      "        [-0.0214, -0.0210,  0.0416, -0.0344, -0.0220],\n",
      "        [-0.0423, -0.0304, -0.0496,  0.0506,  0.0023],\n",
      "        [-0.0174, -0.0144,  0.0443,  0.0148, -0.0707],\n",
      "        [-0.0449,  0.0217, -0.0412, -0.0019, -0.0079],\n",
      "        [-0.0502,  0.0095,  0.0413, -0.0527, -0.0396],\n",
      "        [ 0.0048, -0.0234,  0.0273, -0.0317,  0.0079]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0205, grad_fn=<MinBackward1>), tensor(0.9336, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.20135030150413513\n",
      "@sample 398: tensor([[ 0.0096,  0.0116, -0.0381,  0.0034, -0.0037],\n",
      "        [ 0.0354, -0.0264, -0.0336,  0.0410,  0.0090],\n",
      "        [-0.0087,  0.0146, -0.0103, -0.0294, -0.0103],\n",
      "        [ 0.0037, -0.0575,  0.0097,  0.0367,  0.0205],\n",
      "        [ 0.0215,  0.0094,  0.0012,  0.0026,  0.0053],\n",
      "        [ 0.0234, -0.0096, -0.0679, -0.0248,  0.0052],\n",
      "        [-0.0316, -0.0208, -0.0460, -0.0093, -0.0359],\n",
      "        [-0.0178, -0.0191, -0.0396,  0.0455,  0.0172]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0072, -0.0043, -0.0057, -0.0068, -0.0189],\n",
      "        [ 0.0480,  0.0161,  0.0139, -0.0278, -0.0131],\n",
      "        [-0.0314, -0.0334, -0.0460,  0.0507,  0.0179],\n",
      "        [ 0.0201,  0.0008,  0.0813, -0.0583, -0.0123],\n",
      "        [ 0.0053, -0.0365, -0.0328,  0.0069, -0.0950],\n",
      "        [ 0.0175, -0.0521,  0.0301,  0.0223,  0.0188],\n",
      "        [ 0.0142, -0.0206,  0.0947, -0.0574,  0.0024],\n",
      "        [ 0.0370,  0.0048,  0.0240, -0.0357, -0.0334]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0177, grad_fn=<MinBackward1>), tensor(0.9486, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.19733738899230957\n",
      "@sample 399: tensor([[ 0.0227, -0.0148, -0.0298,  0.0035, -0.0175],\n",
      "        [ 0.0242,  0.0666,  0.0527, -0.1412,  0.1095],\n",
      "        [-0.0323,  0.0361,  0.0345, -0.0458, -0.0060],\n",
      "        [-0.0171,  0.0612,  0.0407, -0.0782,  0.0557],\n",
      "        [ 0.0114,  0.0144,  0.0122,  0.0394,  0.0148],\n",
      "        [-0.0142,  0.0126, -0.0128, -0.0460, -0.0087],\n",
      "        [ 0.0188,  0.0559,  0.0356, -0.0178,  0.0614],\n",
      "        [ 0.0041,  0.0386,  0.0074, -0.0073,  0.0114]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0489, -0.0282,  0.0054,  0.0164,  0.0425],\n",
      "        [-0.1052, -0.0561, -0.0092,  0.1028, -0.0789],\n",
      "        [-0.0761, -0.0716, -0.0141,  0.0213,  0.0199],\n",
      "        [-0.0094, -0.0454, -0.0590,  0.0521,  0.0014],\n",
      "        [ 0.0248, -0.0019, -0.0468,  0.0079,  0.0139],\n",
      "        [ 0.0161, -0.0104,  0.0686, -0.0061, -0.0314],\n",
      "        [-0.0177, -0.0418, -0.0314,  0.0895,  0.0090],\n",
      "        [ 0.0362,  0.0100, -0.0161,  0.0283,  0.0548]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0187, grad_fn=<MinBackward1>), tensor(0.9472, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2215074896812439\n",
      "@sample 400: tensor([[-0.0318, -0.0295, -0.0062, -0.0117,  0.0302],\n",
      "        [-0.0068, -0.0418, -0.0078,  0.0002, -0.0148],\n",
      "        [ 0.0042,  0.0229,  0.0187, -0.0168,  0.0202],\n",
      "        [ 0.0376,  0.0111, -0.0300, -0.0907,  0.0450],\n",
      "        [ 0.0033,  0.0255, -0.0026, -0.0939, -0.0174],\n",
      "        [-0.0071, -0.0305, -0.0331,  0.0055, -0.0241],\n",
      "        [-0.0200,  0.0002, -0.0003, -0.0071, -0.0127],\n",
      "        [-0.0085, -0.0150,  0.0119, -0.0138,  0.0188]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0484,  0.0060,  0.0318,  0.0042, -0.0382],\n",
      "        [-0.0452, -0.0009,  0.0191, -0.0007, -0.0691],\n",
      "        [-0.0305,  0.0546,  0.0072,  0.0234,  0.0135],\n",
      "        [-0.0853, -0.1421, -0.1462,  0.1182,  0.0305],\n",
      "        [-0.0693, -0.0426, -0.0579,  0.0610,  0.0153],\n",
      "        [ 0.0027, -0.0015,  0.2149, -0.1094,  0.0087],\n",
      "        [-0.0201,  0.0146,  0.0249,  0.0306, -0.0151],\n",
      "        [-0.0180,  0.0226,  0.0008, -0.0290, -0.0158]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0156, grad_fn=<MinBackward1>), tensor(0.9313, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.23044411838054657\n",
      "@sample 401: tensor([[ 0.0098,  0.0079,  0.0348,  0.0063, -0.0106],\n",
      "        [-0.0363,  0.0419, -0.0053,  0.0222, -0.0341],\n",
      "        [ 0.0078, -0.0010,  0.0262,  0.0270,  0.0209],\n",
      "        [ 0.0206,  0.0139, -0.0340,  0.0567,  0.0004],\n",
      "        [ 0.0144, -0.0871, -0.0136,  0.0202,  0.0319],\n",
      "        [ 0.0087,  0.0181,  0.0181, -0.0066,  0.0312],\n",
      "        [-0.0219,  0.0038, -0.0212, -0.0320,  0.0054],\n",
      "        [ 0.0657, -0.0582,  0.0070,  0.0762, -0.0596]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0071,  0.0219,  0.0259, -0.0202,  0.0078],\n",
      "        [ 0.0027,  0.0360,  0.0528, -0.0323, -0.0199],\n",
      "        [ 0.0218,  0.0258, -0.0186, -0.0363, -0.0369],\n",
      "        [ 0.0249,  0.0430,  0.0273, -0.0204, -0.0120],\n",
      "        [-0.0024,  0.0283,  0.0590, -0.0626,  0.0009],\n",
      "        [ 0.0397, -0.0248, -0.0223,  0.0358,  0.0362],\n",
      "        [-0.0099, -0.0159,  0.0016,  0.0241, -0.0562],\n",
      "        [ 0.0226,  0.0309,  0.0056,  0.0023, -0.0258]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0215, grad_fn=<MinBackward1>), tensor(0.9653, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2019250988960266\n",
      "@sample 402: tensor([[ 0.0043,  0.0142, -0.0204,  0.0190, -0.0125],\n",
      "        [-0.0391,  0.0147,  0.0188,  0.0279, -0.0172],\n",
      "        [ 0.0098, -0.0746, -0.0223,  0.1053,  0.0289],\n",
      "        [ 0.0375, -0.0266, -0.0026,  0.0124, -0.0424],\n",
      "        [ 0.0048,  0.0095,  0.0211,  0.0114, -0.0197],\n",
      "        [-0.0419,  0.0491,  0.0687, -0.0335,  0.0047],\n",
      "        [-0.0441,  0.0113,  0.0669, -0.0784,  0.0095],\n",
      "        [-0.0218, -0.0507, -0.0119,  0.0052, -0.0284]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0264,  0.0302,  0.0016,  0.0049,  0.0036],\n",
      "        [ 0.0440,  0.0172,  0.0362, -0.0348,  0.0137],\n",
      "        [ 0.0451,  0.0448,  0.1178, -0.0504, -0.0597],\n",
      "        [ 0.0130, -0.0089,  0.0478,  0.0171, -0.0028],\n",
      "        [-0.0160,  0.0400,  0.0142, -0.0104, -0.0093],\n",
      "        [-0.0295, -0.0679,  0.0220,  0.0039, -0.0273],\n",
      "        [-0.0305, -0.0572,  0.0586,  0.0067, -0.0288],\n",
      "        [-0.0294, -0.0143,  0.0717, -0.0496,  0.0084]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0160, grad_fn=<MinBackward1>), tensor(0.9538, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.21128657460212708\n",
      "@sample 403: tensor([[ 1.3514e-02, -4.5579e-02,  5.7611e-03,  6.6261e-03, -3.9812e-02],\n",
      "        [ 3.7756e-02,  7.5435e-03,  2.7638e-02,  3.1199e-03, -1.7703e-02],\n",
      "        [ 8.6893e-04,  1.5160e-02,  2.2440e-02, -2.1295e-02,  4.2212e-02],\n",
      "        [ 2.0176e-02, -1.3268e-02,  7.8958e-04,  5.5031e-03, -3.7164e-04],\n",
      "        [-3.1287e-03, -6.0103e-03,  8.8021e-02, -6.6975e-03,  5.2930e-02],\n",
      "        [-2.0628e-02, -5.8771e-02,  2.0646e-02,  4.9792e-02,  2.2950e-03],\n",
      "        [ 2.7828e-02,  8.2999e-03, -1.8690e-02,  1.5362e-02, -8.5518e-05],\n",
      "        [ 2.2708e-02, -2.2682e-02,  8.6405e-03, -5.5036e-03,  7.9741e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0060, -0.0352,  0.0481,  0.0114,  0.0123],\n",
      "        [-0.0249,  0.0443,  0.0485,  0.0074,  0.0243],\n",
      "        [ 0.0110, -0.0073, -0.0494,  0.0515, -0.0515],\n",
      "        [ 0.0066, -0.0104, -0.0092, -0.0100, -0.0245],\n",
      "        [ 0.0312,  0.0220, -0.0296,  0.0269,  0.0334],\n",
      "        [ 0.0048,  0.0066,  0.0568,  0.0316, -0.0536],\n",
      "        [-0.0079, -0.0118,  0.0217,  0.0142, -0.0326],\n",
      "        [-0.0605, -0.0106, -0.0406,  0.0249, -0.0005]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0080, grad_fn=<MinBackward1>), tensor(0.9568, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.18794213235378265\n",
      "@sample 404: tensor([[-2.0040e-05,  1.0394e-01,  3.0243e-02, -3.9954e-02,  7.0884e-03],\n",
      "        [ 7.5146e-03,  4.5431e-03, -1.4715e-02,  1.5553e-02,  1.9468e-02],\n",
      "        [-3.3094e-03,  5.5005e-03,  6.3550e-02,  1.0737e-02, -4.8829e-03],\n",
      "        [ 1.2782e-02, -1.0028e-01, -3.1218e-02,  1.1175e-01, -1.7474e-02],\n",
      "        [ 8.0862e-03,  4.2089e-02,  7.7111e-02, -1.3591e-02, -2.9339e-02],\n",
      "        [ 5.9726e-03,  4.1768e-02, -9.8265e-03,  6.2763e-02, -5.8172e-02],\n",
      "        [ 2.4796e-03, -1.9500e-02,  4.8217e-02,  7.9350e-03, -3.0292e-02],\n",
      "        [ 2.6683e-02,  2.1556e-03,  7.5661e-04, -2.3864e-02,  3.0220e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-2.0800e-02,  1.1472e-02, -3.1561e-02,  2.3397e-03,  4.3728e-03],\n",
      "        [ 1.8045e-02,  5.8482e-03, -3.4995e-02,  6.6172e-03, -7.5192e-02],\n",
      "        [ 3.3364e-03,  3.2406e-02, -7.4269e-02,  3.6506e-02,  3.7027e-02],\n",
      "        [ 6.7335e-02,  5.4621e-02,  4.0219e-02, -1.0006e-02,  2.5234e-02],\n",
      "        [-5.6343e-02,  4.6481e-02,  4.3125e-02, -3.1697e-03, -2.8615e-02],\n",
      "        [-1.9413e-02,  1.2654e-01,  5.8381e-02, -2.2226e-02,  3.6079e-05],\n",
      "        [-3.3640e-02,  3.6741e-02,  6.0622e-02,  5.7218e-03,  3.2227e-02],\n",
      "        [-3.0205e-02, -1.2851e-02, -1.2573e-03,  3.1906e-02, -4.2728e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0202, grad_fn=<MinBackward1>), tensor(0.9362, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.21722549200057983\n",
      "@sample 405: tensor([[-0.0010,  0.0116, -0.0022, -0.0256, -0.0106],\n",
      "        [ 0.0040,  0.0257,  0.0658,  0.0760, -0.0144],\n",
      "        [ 0.0239, -0.0012, -0.0122,  0.0333, -0.0082],\n",
      "        [-0.0017,  0.0441,  0.0424, -0.0464,  0.0314],\n",
      "        [ 0.0028, -0.0296, -0.0181,  0.0153, -0.0078],\n",
      "        [ 0.0529,  0.0361,  0.0427, -0.0335,  0.0226],\n",
      "        [ 0.0420,  0.0497,  0.0062,  0.0466, -0.0128],\n",
      "        [-0.0103, -0.0526, -0.0181,  0.0336, -0.0292]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0208,  0.0076, -0.0239,  0.0052, -0.0067],\n",
      "        [ 0.0234,  0.0947,  0.0240, -0.0704,  0.0387],\n",
      "        [ 0.0170,  0.0360,  0.0121, -0.0551, -0.0372],\n",
      "        [-0.0360, -0.0133, -0.0714,  0.0766, -0.0176],\n",
      "        [-0.0379, -0.0391, -0.0069, -0.0222, -0.0018],\n",
      "        [-0.0876, -0.0063, -0.0551,  0.0400, -0.0030],\n",
      "        [ 0.0077,  0.0375, -0.0449,  0.0634,  0.0216],\n",
      "        [ 0.0185,  0.0363,  0.0571, -0.0747, -0.0027]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0199, grad_fn=<MinBackward1>), tensor(0.9350, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1947052776813507\n",
      "@sample 406: tensor([[-0.0351, -0.0054, -0.0163, -0.0724,  0.0725],\n",
      "        [ 0.0088, -0.0655,  0.0252,  0.0507, -0.0102],\n",
      "        [-0.0115, -0.0277, -0.0430,  0.0334, -0.0344],\n",
      "        [ 0.0440, -0.0322, -0.0641, -0.0075, -0.0342],\n",
      "        [-0.0247,  0.0182,  0.0170, -0.0232,  0.0745],\n",
      "        [-0.0149, -0.0049,  0.0159, -0.0223, -0.0141],\n",
      "        [ 0.0156,  0.0982,  0.0381, -0.0798, -0.0279],\n",
      "        [ 0.0227, -0.0010,  0.0063,  0.0294,  0.0105]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0217, -0.0685, -0.1032,  0.0384,  0.0242],\n",
      "        [ 0.0187,  0.0323, -0.0068, -0.0306, -0.0117],\n",
      "        [ 0.0036,  0.0177, -0.0313, -0.0187, -0.0012],\n",
      "        [-0.0872, -0.0707, -0.0206,  0.0033,  0.0256],\n",
      "        [-0.0019, -0.0199, -0.0856,  0.0034,  0.0731],\n",
      "        [-0.0483, -0.0373, -0.0471,  0.0069, -0.0109],\n",
      "        [-0.1374, -0.0908, -0.0920,  0.0496, -0.0344],\n",
      "        [ 0.0252,  0.0175, -0.0258, -0.0103, -0.0107]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0209, grad_fn=<MinBackward1>), tensor(0.9487, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.21316108107566833\n",
      "@sample 407: tensor([[ 0.0028, -0.0303,  0.0129,  0.0048, -0.0064],\n",
      "        [-0.0304,  0.0051, -0.0097,  0.0173,  0.0396],\n",
      "        [-0.0058, -0.0227,  0.0124,  0.0182,  0.0322],\n",
      "        [-0.0059, -0.0457, -0.0005,  0.0289,  0.0284],\n",
      "        [-0.0135,  0.0691,  0.0492, -0.0161, -0.0143],\n",
      "        [ 0.0061,  0.0702,  0.0019, -0.0096,  0.0079],\n",
      "        [ 0.0330,  0.0108,  0.0086,  0.0125, -0.0283],\n",
      "        [ 0.0206, -0.0407,  0.0025,  0.0239, -0.0258]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0052, -0.0067, -0.0049, -0.0088, -0.0097],\n",
      "        [-0.0292,  0.0035,  0.0190, -0.0354,  0.0278],\n",
      "        [ 0.0278,  0.0194, -0.0661,  0.0393,  0.0189],\n",
      "        [-0.0024,  0.0091,  0.0248, -0.0595, -0.0454],\n",
      "        [-0.0312,  0.0161, -0.0198, -0.0010, -0.0092],\n",
      "        [-0.0737,  0.0013, -0.0654,  0.0013, -0.0035],\n",
      "        [-0.0179,  0.0468,  0.0259, -0.0410, -0.0414],\n",
      "        [-0.0096,  0.0056, -0.0061, -0.0227, -0.0361]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0113, grad_fn=<MinBackward1>), tensor(0.9145, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.18849553167819977\n",
      "@sample 408: tensor([[-0.0158,  0.0683, -0.0072, -0.0138,  0.0429],\n",
      "        [-0.0103,  0.0311, -0.0226, -0.0080,  0.0161],\n",
      "        [-0.0013,  0.0134, -0.0266,  0.0359, -0.0115],\n",
      "        [-0.0222, -0.0055,  0.0518, -0.0021,  0.0397],\n",
      "        [ 0.0208,  0.0035,  0.0028,  0.0084, -0.0039],\n",
      "        [-0.0446,  0.0257, -0.0091,  0.0397, -0.0036],\n",
      "        [ 0.0125,  0.0395, -0.0011,  0.0001, -0.0067],\n",
      "        [ 0.0069,  0.0039, -0.0024, -0.0170,  0.0121]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0405,  0.0093, -0.1185,  0.0512, -0.0316],\n",
      "        [-0.0085,  0.0360, -0.0477, -0.0727,  0.0051],\n",
      "        [ 0.0129,  0.0312, -0.0423, -0.0187,  0.0449],\n",
      "        [-0.0018,  0.0315, -0.0691,  0.0917,  0.0525],\n",
      "        [-0.0142,  0.0145, -0.0479,  0.0074,  0.0168],\n",
      "        [ 0.0384,  0.0320,  0.0825, -0.0998, -0.0333],\n",
      "        [ 0.0090, -0.0189, -0.0703,  0.0143,  0.0139],\n",
      "        [ 0.0196,  0.0009,  0.0004,  0.0277, -0.0157]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0155, grad_fn=<MinBackward1>), tensor(0.9371, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.20050087571144104\n",
      "@sample 409: tensor([[-0.0058, -0.0135,  0.0387, -0.0377,  0.0360],\n",
      "        [ 0.0463, -0.0083,  0.0169, -0.0492, -0.0128],\n",
      "        [ 0.0413,  0.0105, -0.0332,  0.0189,  0.0098],\n",
      "        [-0.0147, -0.0504, -0.0026,  0.0508,  0.0171],\n",
      "        [-0.0016,  0.0348,  0.0350, -0.0015, -0.0210],\n",
      "        [ 0.0037,  0.0561,  0.0076, -0.0245,  0.0216],\n",
      "        [-0.0202,  0.0317,  0.0231, -0.0419,  0.0314],\n",
      "        [-0.0154,  0.0010,  0.0026,  0.0392,  0.0006]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0438, -0.0468, -0.0841,  0.0424,  0.0014],\n",
      "        [-0.0600, -0.0513, -0.0730,  0.0412, -0.0007],\n",
      "        [-0.0234,  0.0031,  0.0349,  0.0013,  0.0070],\n",
      "        [ 0.0666,  0.0402,  0.0898, -0.0436, -0.0214],\n",
      "        [-0.0400,  0.0229, -0.0217,  0.0370,  0.0307],\n",
      "        [-0.0099, -0.0209, -0.0575,  0.0423, -0.0060],\n",
      "        [-0.0102, -0.0738, -0.0661,  0.0359,  0.0030],\n",
      "        [ 0.0327,  0.0443, -0.0009, -0.0003,  0.0156]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0126, grad_fn=<MinBackward1>), tensor(0.9737, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.19901244342327118\n",
      "@sample 410: tensor([[-4.9180e-02,  7.1406e-02,  1.9548e-02, -1.1822e-02, -5.7648e-03],\n",
      "        [-3.5546e-02,  5.8357e-03, -1.8450e-02, -3.5352e-03,  1.6256e-02],\n",
      "        [ 1.2012e-02,  7.9303e-02, -2.3163e-02,  5.1832e-03,  7.8322e-02],\n",
      "        [ 5.7246e-03,  1.6522e-02, -6.3496e-04,  2.9234e-02,  3.7194e-03],\n",
      "        [-3.5180e-02,  1.0229e-02,  2.5443e-02, -3.5055e-02,  1.2884e-02],\n",
      "        [ 6.0611e-03, -2.3394e-02,  1.8076e-02,  9.3096e-03,  2.9194e-02],\n",
      "        [ 2.6805e-02,  4.4668e-02, -1.1276e-03, -3.9521e-02,  3.0219e-02],\n",
      "        [ 8.2135e-03,  2.6881e-02,  2.5193e-03, -2.8357e-02, -4.5612e-05]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0438,  0.0332,  0.0176,  0.0240,  0.0153],\n",
      "        [ 0.0106, -0.0141,  0.0127, -0.0395, -0.0129],\n",
      "        [ 0.0167,  0.0248, -0.0808,  0.0619,  0.0494],\n",
      "        [ 0.0182,  0.0109,  0.0317,  0.0102,  0.0314],\n",
      "        [ 0.0445, -0.0147, -0.0630,  0.0637,  0.0409],\n",
      "        [-0.0257, -0.0026,  0.0792, -0.0571,  0.0011],\n",
      "        [-0.0412, -0.0203, -0.0931,  0.0788,  0.0119],\n",
      "        [-0.0422, -0.0126, -0.0607,  0.0491,  0.0297]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0159, grad_fn=<MinBackward1>), tensor(0.9322, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.20074713230133057\n",
      "@sample 411: tensor([[-4.9478e-03, -2.0120e-02,  3.8010e-02,  2.1753e-02, -3.1097e-02],\n",
      "        [-2.0140e-03,  6.0183e-02,  5.6677e-02, -1.8879e-02,  6.9548e-02],\n",
      "        [ 1.0440e-02, -3.9388e-02,  5.8222e-02,  6.4585e-02, -1.6428e-02],\n",
      "        [-2.2324e-02, -3.9610e-02,  1.1076e-02,  3.2461e-04, -1.8882e-02],\n",
      "        [-2.8454e-02,  4.5549e-02, -1.3885e-02,  7.3092e-03, -1.1377e-03],\n",
      "        [-4.9921e-03, -1.1638e-05, -2.1515e-03, -3.9191e-02, -1.2749e-03],\n",
      "        [-1.1714e-02, -1.1931e-02, -1.8022e-02,  3.7428e-02, -2.8192e-02],\n",
      "        [-9.0971e-03,  7.4611e-03, -3.3190e-02,  6.8373e-02, -1.6665e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0188, -0.0109, -0.0206,  0.0443,  0.0368],\n",
      "        [-0.0640,  0.0138, -0.0480,  0.0265, -0.0435],\n",
      "        [ 0.0191,  0.0349,  0.0006, -0.0181,  0.0510],\n",
      "        [ 0.0032, -0.0031,  0.0503, -0.0087,  0.0033],\n",
      "        [-0.0019,  0.0119, -0.0302, -0.0586, -0.0161],\n",
      "        [-0.0575,  0.0145,  0.0860, -0.0529, -0.0163],\n",
      "        [ 0.0310, -0.0025,  0.0936, -0.0527,  0.0117],\n",
      "        [ 0.0353,  0.0450,  0.0292, -0.0574, -0.0417]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0171, grad_fn=<MinBackward1>), tensor(0.9234, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.19888260960578918\n",
      "@sample 412: tensor([[ 1.0766e-02,  2.4622e-02,  2.4212e-02, -2.2347e-02, -1.6066e-02],\n",
      "        [-1.7309e-02, -5.9199e-02, -3.2839e-02,  3.1132e-02, -3.8016e-02],\n",
      "        [-4.2269e-02, -1.6048e-02, -2.3684e-02,  3.7694e-03, -3.6817e-02],\n",
      "        [-1.2703e-02, -2.5049e-03, -1.6655e-02, -4.3030e-03, -3.6182e-02],\n",
      "        [-1.8347e-02, -9.3427e-03,  3.1634e-02, -3.0740e-03, -1.1732e-02],\n",
      "        [-5.9068e-03,  1.2549e-02, -6.9315e-04, -1.2589e-02, -2.7220e-02],\n",
      "        [ 4.1328e-03,  8.8543e-05,  6.9302e-03,  9.1303e-03,  2.1739e-02],\n",
      "        [-5.2844e-03, -3.5117e-02, -7.2707e-04,  6.8539e-03, -7.6239e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0318, -0.0101, -0.0402,  0.0242,  0.0493],\n",
      "        [ 0.0412, -0.0005,  0.0481, -0.0330,  0.0225],\n",
      "        [ 0.0409, -0.0165,  0.0799, -0.0458,  0.0256],\n",
      "        [ 0.0019,  0.0125, -0.0119, -0.0146,  0.0015],\n",
      "        [-0.0257, -0.0020,  0.0102,  0.0046, -0.0179],\n",
      "        [-0.0128,  0.0364,  0.0641, -0.0297, -0.0042],\n",
      "        [-0.0217, -0.0011,  0.0153, -0.0319, -0.0281],\n",
      "        [ 0.0132, -0.0127,  0.0263, -0.0149, -0.0158]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0160, grad_fn=<MinBackward1>), tensor(0.9560, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.17688411474227905\n",
      "@sample 413: tensor([[ 1.9585e-02,  6.3264e-03,  5.8077e-03,  4.5508e-03,  3.0882e-02],\n",
      "        [-1.1599e-02,  1.6413e-02,  1.3059e-02,  1.9659e-02, -3.1367e-05],\n",
      "        [ 6.0941e-03,  2.4842e-02, -3.8501e-03, -5.3755e-02,  1.0616e-02],\n",
      "        [ 2.3136e-03,  2.9263e-02, -7.0024e-03,  7.7504e-03,  5.7983e-02],\n",
      "        [ 2.2855e-02, -1.1705e-02, -3.9336e-02,  5.3129e-02, -2.6867e-02],\n",
      "        [ 2.5524e-02,  2.2160e-02, -1.7007e-02, -3.5388e-02,  3.9844e-02],\n",
      "        [-2.8937e-02,  3.5416e-02,  2.3053e-02, -4.9502e-03,  3.1007e-03],\n",
      "        [-2.3642e-02,  1.0214e-03, -5.9990e-03, -1.2839e-02, -1.8259e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0032, -0.0170, -0.0238,  0.0169,  0.0458],\n",
      "        [ 0.0106,  0.0585, -0.0324, -0.0558, -0.0236],\n",
      "        [-0.0300, -0.0328, -0.0593,  0.0375, -0.0271],\n",
      "        [ 0.0367,  0.0080,  0.0061,  0.0490,  0.0711],\n",
      "        [ 0.0307,  0.0300, -0.0190, -0.0714, -0.0154],\n",
      "        [ 0.0260, -0.0681, -0.0279,  0.0135,  0.0598],\n",
      "        [-0.0484, -0.0055, -0.0668, -0.0030,  0.0389],\n",
      "        [ 0.0220,  0.0189,  0.0419, -0.0161, -0.0221]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0194, grad_fn=<MinBackward1>), tensor(0.9301, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1744307577610016\n",
      "@sample 414: tensor([[-0.0065, -0.0423,  0.0017, -0.0288, -0.0201],\n",
      "        [-0.0364, -0.0277,  0.0067, -0.0123,  0.0037],\n",
      "        [ 0.0013, -0.0206, -0.0130,  0.0259,  0.0177],\n",
      "        [-0.0206, -0.0066, -0.0037,  0.0302, -0.0030],\n",
      "        [ 0.0122, -0.0195, -0.0357, -0.0218,  0.0038],\n",
      "        [ 0.0117,  0.0283,  0.0372, -0.0496,  0.0338],\n",
      "        [ 0.0112, -0.0329, -0.0165,  0.0310, -0.0167],\n",
      "        [-0.0438, -0.0543, -0.0232,  0.0262,  0.0026]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0178, -0.0753,  0.0732,  0.0184, -0.0086],\n",
      "        [ 0.0178, -0.0284,  0.0326, -0.0257,  0.0243],\n",
      "        [ 0.0076,  0.0167,  0.0144,  0.0251,  0.0392],\n",
      "        [ 0.0417,  0.0345, -0.0086,  0.0017, -0.0008],\n",
      "        [ 0.0132, -0.0020, -0.0204,  0.0158, -0.0052],\n",
      "        [-0.0012, -0.0150, -0.0224,  0.0377, -0.0210],\n",
      "        [-0.0021,  0.0246,  0.0224, -0.0414, -0.0209],\n",
      "        [ 0.0517, -0.0285,  0.0870, -0.0783, -0.0408]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0197, grad_fn=<MinBackward1>), tensor(0.9255, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1909695416688919\n",
      "@sample 415: tensor([[-9.0199e-03, -9.0329e-03,  6.5506e-05, -1.4134e-02,  2.0962e-02],\n",
      "        [ 5.5328e-03,  2.7956e-02,  2.4233e-02,  3.2237e-02, -2.3978e-03],\n",
      "        [-3.0851e-02,  3.7721e-02,  4.9180e-03, -6.8204e-02,  1.8655e-02],\n",
      "        [-9.1925e-03, -1.0688e-02, -1.5609e-02, -1.7379e-02, -3.8717e-02],\n",
      "        [ 3.3537e-02, -1.0378e-02, -3.4247e-03,  4.2065e-02, -6.2492e-02],\n",
      "        [-1.0105e-02,  3.0869e-02, -3.5290e-02, -5.1740e-02,  6.0753e-04],\n",
      "        [-4.8512e-03,  1.1826e-03,  1.0018e-02,  1.7227e-02,  2.7018e-03],\n",
      "        [ 7.6426e-03,  7.3258e-02,  4.7543e-02, -1.3392e-01,  2.5542e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0300,  0.0132,  0.0202,  0.0186, -0.0097],\n",
      "        [-0.0187, -0.0245, -0.0601,  0.0374,  0.0033],\n",
      "        [-0.0958, -0.0404, -0.0058, -0.0036, -0.0595],\n",
      "        [ 0.0188, -0.0203,  0.0610, -0.0713, -0.0105],\n",
      "        [ 0.0260,  0.0025,  0.0526, -0.0201, -0.0235],\n",
      "        [-0.0302, -0.0492, -0.0495,  0.0261, -0.0145],\n",
      "        [-0.0085,  0.0289, -0.0232, -0.0425,  0.0081],\n",
      "        [-0.0508, -0.1063, -0.0649,  0.0303,  0.0226]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0303, grad_fn=<MinBackward1>), tensor(0.9685, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1942504644393921\n",
      "@sample 416: tensor([[ 0.0316, -0.0274,  0.0002,  0.0098,  0.0045],\n",
      "        [-0.0074,  0.0157, -0.0280, -0.0166, -0.0307],\n",
      "        [-0.0051,  0.0074, -0.0176, -0.0079, -0.0064],\n",
      "        [-0.0041,  0.0007, -0.0247, -0.0074,  0.0079],\n",
      "        [ 0.0178, -0.0584, -0.0336,  0.0049, -0.0423],\n",
      "        [-0.0245, -0.0357, -0.0434, -0.0178,  0.0141],\n",
      "        [ 0.0009,  0.0251, -0.0164, -0.0365,  0.0158],\n",
      "        [ 0.0147,  0.0252, -0.0056, -0.0375,  0.0358]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0011, -0.0055, -0.0234, -0.0057,  0.0292],\n",
      "        [-0.0460,  0.0336, -0.0178, -0.0213, -0.0250],\n",
      "        [ 0.0061,  0.0235,  0.0567, -0.0426,  0.0323],\n",
      "        [-0.0327,  0.0167,  0.0368,  0.0088, -0.0090],\n",
      "        [-0.0438, -0.0653,  0.0088, -0.0385, -0.0302],\n",
      "        [ 0.0420, -0.0502,  0.0387, -0.0137,  0.0529],\n",
      "        [-0.0166, -0.0389, -0.0342,  0.0352, -0.0113],\n",
      "        [ 0.0036, -0.0380, -0.0482,  0.0497, -0.0251]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0147, grad_fn=<MinBackward1>), tensor(0.9260, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.18436656892299652\n",
      "@sample 417: tensor([[ 1.8421e-02,  1.3064e-02, -3.4096e-02,  3.0244e-02, -2.1393e-03],\n",
      "        [-4.9240e-02,  2.1761e-02, -4.3064e-02, -5.8698e-02, -2.5587e-03],\n",
      "        [-5.4865e-02,  8.6198e-02,  6.0442e-02, -4.8688e-02,  4.0657e-02],\n",
      "        [-3.7731e-02, -3.7384e-02, -6.6350e-03, -1.0641e-02,  1.1638e-02],\n",
      "        [ 3.7758e-02,  6.9107e-02,  4.4918e-02, -5.4334e-02,  8.9659e-02],\n",
      "        [-2.9329e-03,  9.8920e-03, -4.0933e-03, -6.4346e-03, -2.4996e-03],\n",
      "        [-1.1019e-03,  1.0913e-02,  7.3410e-03, -4.7909e-02, -1.0579e-02],\n",
      "        [-3.4737e-02, -1.2340e-02, -1.8132e-02,  1.7127e-02, -3.2619e-05]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0025,  0.0121, -0.0268,  0.0576,  0.0233],\n",
      "        [-0.0240, -0.0382, -0.0003, -0.0680, -0.0337],\n",
      "        [-0.0221, -0.0147, -0.0461,  0.0057, -0.0106],\n",
      "        [-0.0205, -0.0541,  0.0645, -0.0157,  0.0192],\n",
      "        [-0.0018, -0.0106, -0.1390,  0.0472, -0.0546],\n",
      "        [-0.0082,  0.0058,  0.0113,  0.0100, -0.0314],\n",
      "        [-0.0218, -0.0522,  0.0040,  0.0150, -0.0185],\n",
      "        [-0.0116,  0.0106, -0.0112, -0.0058, -0.0210]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0178, grad_fn=<MinBackward1>), tensor(0.9312, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.207460418343544\n",
      "@sample 418: tensor([[-0.0006,  0.0014, -0.0062, -0.0495,  0.0431],\n",
      "        [-0.0267,  0.0526, -0.0348, -0.0565, -0.0411],\n",
      "        [-0.0084, -0.0024,  0.0472, -0.0586,  0.0812],\n",
      "        [-0.0004, -0.0145, -0.0717,  0.0255, -0.0434],\n",
      "        [-0.0214, -0.0104,  0.0536, -0.0820,  0.0226],\n",
      "        [ 0.0123, -0.0382, -0.0031,  0.0358, -0.0381],\n",
      "        [ 0.0055,  0.0058,  0.0138, -0.0150,  0.0273],\n",
      "        [-0.0203, -0.0205, -0.0009,  0.0219, -0.0085]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0085, -0.0644, -0.0257,  0.0459,  0.0187],\n",
      "        [ 0.0267, -0.0506, -0.0540,  0.0281,  0.0649],\n",
      "        [ 0.0332, -0.0124, -0.0805,  0.0863,  0.0504],\n",
      "        [ 0.0044, -0.0295, -0.0084, -0.0313, -0.0141],\n",
      "        [-0.0457, -0.0207, -0.0627,  0.0431,  0.0169],\n",
      "        [-0.0470,  0.0499, -0.0155, -0.0261, -0.0545],\n",
      "        [ 0.0164,  0.0018, -0.0296,  0.0052, -0.0139],\n",
      "        [ 0.0147,  0.0352,  0.0160,  0.0042,  0.0074]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0189, grad_fn=<MinBackward1>), tensor(0.9520, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.19218604266643524\n",
      "@sample 419: tensor([[-0.0158,  0.0466,  0.0080, -0.0124, -0.0099],\n",
      "        [-0.0200,  0.0254,  0.0010, -0.0620,  0.0303],\n",
      "        [-0.0086,  0.0211,  0.0105,  0.0347,  0.0241],\n",
      "        [ 0.0172, -0.0568, -0.0271,  0.0405,  0.0180],\n",
      "        [ 0.0034,  0.0172, -0.0006, -0.0336,  0.0122],\n",
      "        [-0.0575,  0.0091, -0.0230, -0.0209, -0.0008],\n",
      "        [-0.0140,  0.0152,  0.0153, -0.0369,  0.0209],\n",
      "        [-0.0213, -0.0382,  0.0123, -0.0211,  0.0107]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0178,  0.0173, -0.0342,  0.0021, -0.0264],\n",
      "        [-0.0023, -0.0575, -0.0871,  0.0150, -0.0094],\n",
      "        [ 0.0621,  0.0304,  0.0158,  0.0084,  0.0227],\n",
      "        [-0.0020, -0.0274,  0.0799, -0.0581, -0.0329],\n",
      "        [-0.0660, -0.0208, -0.0498,  0.0063, -0.0194],\n",
      "        [ 0.0168, -0.0764, -0.0289, -0.0049,  0.0105],\n",
      "        [ 0.0217, -0.0071, -0.0341,  0.0337,  0.0295],\n",
      "        [ 0.0330, -0.0478,  0.0567, -0.0398, -0.0431]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0186, grad_fn=<MinBackward1>), tensor(0.9593, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.18143236637115479\n",
      "@sample 420: tensor([[-0.0100, -0.0480, -0.0064, -0.0007, -0.0362],\n",
      "        [-0.0023,  0.0038, -0.0004, -0.0113,  0.0129],\n",
      "        [ 0.0078,  0.0183, -0.0330,  0.0364, -0.0214],\n",
      "        [ 0.0010, -0.0326, -0.0132,  0.0305, -0.0181],\n",
      "        [ 0.0039,  0.0075, -0.0036, -0.0227,  0.0059],\n",
      "        [-0.0068, -0.0225, -0.0100,  0.0079, -0.0252],\n",
      "        [-0.0453,  0.0375, -0.0101, -0.0118, -0.0328],\n",
      "        [-0.0340,  0.0414, -0.0113, -0.0777,  0.0562]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0521, -0.0036, -0.0318,  0.0171,  0.0341],\n",
      "        [-0.0026, -0.0064, -0.0136,  0.0004, -0.0078],\n",
      "        [ 0.0521,  0.0244,  0.0007, -0.0384,  0.0512],\n",
      "        [ 0.0185,  0.0045, -0.0102, -0.0013,  0.0095],\n",
      "        [-0.0099, -0.0382, -0.0667,  0.0096,  0.0034],\n",
      "        [ 0.0160,  0.0188, -0.0201,  0.0612,  0.0332],\n",
      "        [-0.0161, -0.0043, -0.0205,  0.0103,  0.0153],\n",
      "        [-0.0329, -0.0622, -0.0600, -0.0110,  0.0077]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0228, grad_fn=<MinBackward1>), tensor(0.9685, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.18703019618988037\n",
      "@sample 421: tensor([[ 0.0111, -0.0050,  0.0258,  0.0086,  0.0400],\n",
      "        [-0.0131,  0.0138, -0.0219,  0.0108,  0.0112],\n",
      "        [-0.0198,  0.0252,  0.0326, -0.0369,  0.0301],\n",
      "        [-0.0151,  0.0138,  0.0402, -0.0443,  0.0325],\n",
      "        [-0.0335, -0.0537, -0.0046,  0.0260, -0.0129],\n",
      "        [-0.0087,  0.0504, -0.0265, -0.1173,  0.0547],\n",
      "        [-0.0361, -0.0256,  0.0168,  0.0033,  0.0039],\n",
      "        [ 0.0442, -0.0158,  0.0069,  0.0514, -0.0580]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0283, -0.0042, -0.0112,  0.0225,  0.0604],\n",
      "        [ 0.0579, -0.0033,  0.0288, -0.0344, -0.0054],\n",
      "        [-0.0149, -0.0199, -0.0356,  0.0334,  0.0014],\n",
      "        [-0.0309, -0.0098,  0.0082,  0.0346,  0.0052],\n",
      "        [ 0.0603,  0.0132,  0.1045, -0.0496, -0.0454],\n",
      "        [-0.0719, -0.0981, -0.0983,  0.0634, -0.0055],\n",
      "        [-0.0012,  0.0140, -0.0186, -0.0720, -0.0299],\n",
      "        [ 0.0102,  0.0715,  0.0148,  0.0434,  0.0412]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0171, grad_fn=<MinBackward1>), tensor(0.9186, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2002086341381073\n",
      "@sample 422: tensor([[-0.0041, -0.0128, -0.0051,  0.0158,  0.0013],\n",
      "        [-0.0423,  0.0636, -0.0031, -0.0940,  0.0464],\n",
      "        [ 0.0162,  0.0053, -0.0055,  0.0364, -0.0593],\n",
      "        [-0.0005, -0.0040, -0.0118,  0.0041,  0.0111],\n",
      "        [ 0.0138, -0.0531, -0.0244,  0.0396,  0.0210],\n",
      "        [-0.0189,  0.0047,  0.0198,  0.0063,  0.0514],\n",
      "        [ 0.0168, -0.0760,  0.0375,  0.0505, -0.0218],\n",
      "        [ 0.0185, -0.0597, -0.0065,  0.0515,  0.0199]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0298,  0.0271,  0.0204,  0.0036,  0.0398],\n",
      "        [-0.0281, -0.0228, -0.0769,  0.0136,  0.0495],\n",
      "        [ 0.0094,  0.0223,  0.0184, -0.0047,  0.0390],\n",
      "        [ 0.0295,  0.0457,  0.0091, -0.0168, -0.0106],\n",
      "        [ 0.0077,  0.0287, -0.0946,  0.0040, -0.0084],\n",
      "        [ 0.0326,  0.0124,  0.0163,  0.0136,  0.0276],\n",
      "        [ 0.0185,  0.0385,  0.0649, -0.0663, -0.0150],\n",
      "        [ 0.0662, -0.0098,  0.0132, -0.0068,  0.0435]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0205, grad_fn=<MinBackward1>), tensor(0.9415, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.2021036297082901\n",
      "@sample 423: tensor([[ 0.0285, -0.0151,  0.0173,  0.0390,  0.0042],\n",
      "        [ 0.0285,  0.0006, -0.0225,  0.0482,  0.0116],\n",
      "        [-0.0106,  0.0291, -0.0180, -0.0469,  0.0293],\n",
      "        [-0.0307, -0.0172,  0.0221, -0.0009,  0.0210],\n",
      "        [ 0.0074,  0.0418,  0.0203, -0.0305,  0.0433],\n",
      "        [-0.0407,  0.0587,  0.0246, -0.0692,  0.0267],\n",
      "        [ 0.0198,  0.0342,  0.0275, -0.0119,  0.0209],\n",
      "        [-0.0054, -0.0269, -0.0207,  0.0329, -0.0391]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0120,  0.0476, -0.0050,  0.0224,  0.0153],\n",
      "        [ 0.0058,  0.0321, -0.1049,  0.0424,  0.0259],\n",
      "        [-0.0321, -0.0261,  0.0282, -0.0453, -0.0262],\n",
      "        [ 0.0366, -0.0851,  0.0422, -0.0230, -0.0107],\n",
      "        [-0.0032, -0.0141, -0.0310,  0.0279,  0.0122],\n",
      "        [-0.0124, -0.0414, -0.0902,  0.0492, -0.0134],\n",
      "        [ 0.0321, -0.0099, -0.0509,  0.0349,  0.0208],\n",
      "        [ 0.0051, -0.0048,  0.0515, -0.0479, -0.0009]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0095, grad_fn=<MinBackward1>), tensor(0.9629, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.19312646985054016\n",
      "@sample 424: tensor([[ 0.0086, -0.0392, -0.0506,  0.0708, -0.0426],\n",
      "        [-0.0131,  0.0009, -0.0112, -0.0113,  0.0054],\n",
      "        [-0.0155,  0.0008, -0.0054,  0.0051,  0.0027],\n",
      "        [ 0.0111,  0.0079,  0.0163, -0.0358,  0.0532],\n",
      "        [ 0.0004,  0.0596, -0.0055, -0.0151,  0.0155],\n",
      "        [ 0.0070, -0.0813,  0.0251,  0.0335, -0.0353],\n",
      "        [-0.0228,  0.0062, -0.0439,  0.0104,  0.0247],\n",
      "        [ 0.0034, -0.0151,  0.0287, -0.0200, -0.0031]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0302, -0.0025, -0.0253, -0.0325,  0.0154],\n",
      "        [-0.0340, -0.0169, -0.0275,  0.0282,  0.0199],\n",
      "        [-0.0124,  0.0500, -0.0031, -0.0042,  0.0011],\n",
      "        [-0.0034, -0.0417, -0.0581,  0.0477,  0.0207],\n",
      "        [-0.0081,  0.0224, -0.0112,  0.0146, -0.0287],\n",
      "        [ 0.0522,  0.0336,  0.0636, -0.0162, -0.0054],\n",
      "        [ 0.0424,  0.0324,  0.0601, -0.0069,  0.0136],\n",
      "        [-0.0483,  0.0563, -0.0493, -0.0374,  0.0038]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0172, grad_fn=<MinBackward1>), tensor(0.9519, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.19732165336608887\n",
      "@sample 425: tensor([[-0.0280,  0.0591,  0.0167, -0.0338,  0.0173],\n",
      "        [-0.0559,  0.0824,  0.0172, -0.0753,  0.0023],\n",
      "        [ 0.0276, -0.0476,  0.0225,  0.0961, -0.0287],\n",
      "        [ 0.0295,  0.0040,  0.0075,  0.0406, -0.0092],\n",
      "        [-0.0019,  0.0197,  0.0384,  0.0374, -0.0128],\n",
      "        [-0.0173, -0.0495, -0.0102,  0.0703, -0.0152],\n",
      "        [-0.0239, -0.0577, -0.0094,  0.0396,  0.0239],\n",
      "        [-0.0057, -0.0084, -0.0310,  0.0459,  0.0021]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0069,  0.0204, -0.0742,  0.0353, -0.0158],\n",
      "        [-0.0219, -0.0375, -0.0803,  0.0161,  0.0307],\n",
      "        [ 0.0445,  0.0539, -0.0082, -0.0383,  0.0215],\n",
      "        [ 0.0288,  0.0467, -0.0454,  0.0018,  0.0277],\n",
      "        [ 0.0007,  0.0315, -0.0442, -0.0234, -0.0323],\n",
      "        [ 0.0213,  0.0346,  0.0440,  0.0072,  0.0256],\n",
      "        [ 0.0181, -0.0124,  0.0319, -0.0106, -0.0222],\n",
      "        [ 0.0159,  0.0428, -0.0182,  0.0323,  0.0246]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0203, grad_fn=<MinBackward1>), tensor(0.9489, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.20019492506980896\n",
      "@sample 426: tensor([[-0.0192,  0.0078,  0.0123, -0.0472,  0.0163],\n",
      "        [-0.0012,  0.0308, -0.0129, -0.0469,  0.0514],\n",
      "        [ 0.0106,  0.0319,  0.0328,  0.0155,  0.0086],\n",
      "        [-0.0073,  0.0530,  0.0091, -0.0082, -0.0104],\n",
      "        [-0.0191, -0.0236, -0.0243,  0.0515, -0.0137],\n",
      "        [-0.0382, -0.0045,  0.0022, -0.0467,  0.0690],\n",
      "        [-0.0161, -0.0449,  0.0173,  0.0399, -0.0603],\n",
      "        [ 0.0065, -0.0512,  0.0104,  0.0183, -0.0242]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0112, -0.0160,  0.0649, -0.0055, -0.0280],\n",
      "        [ 0.0066, -0.0264, -0.0722,  0.0630, -0.0385],\n",
      "        [-0.0040,  0.0198, -0.0280,  0.0103, -0.0028],\n",
      "        [-0.0520,  0.0768, -0.0475,  0.0023,  0.0116],\n",
      "        [ 0.0689,  0.0147,  0.0035, -0.0164,  0.0050],\n",
      "        [ 0.0533, -0.0143, -0.0472,  0.0542, -0.0280],\n",
      "        [ 0.0373,  0.0453,  0.1101, -0.0189,  0.0291],\n",
      "        [ 0.0599,  0.0289,  0.1037,  0.0024,  0.0298]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0136, grad_fn=<MinBackward1>), tensor(0.9363, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.20781069993972778\n",
      "@sample 427: tensor([[ 0.0278,  0.0191, -0.0024, -0.0106, -0.0106],\n",
      "        [ 0.0232,  0.0752, -0.0002,  0.0256, -0.0102],\n",
      "        [-0.0257,  0.0179, -0.0128,  0.0073,  0.0112],\n",
      "        [ 0.0125,  0.0162,  0.0121,  0.0034, -0.0065],\n",
      "        [ 0.0211, -0.0117,  0.0009, -0.0168,  0.0126],\n",
      "        [ 0.0157,  0.0402,  0.0221, -0.0185, -0.0008],\n",
      "        [-0.0172,  0.0545,  0.0291, -0.0887,  0.0202],\n",
      "        [ 0.0109, -0.0434,  0.0192,  0.0020,  0.0256]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0083, -0.0136,  0.0206,  0.0475,  0.0119],\n",
      "        [-0.0052,  0.0784, -0.0459,  0.0132,  0.0028],\n",
      "        [ 0.0250,  0.0208,  0.0419, -0.0343, -0.0211],\n",
      "        [ 0.0043, -0.0099, -0.0198,  0.0133,  0.0005],\n",
      "        [-0.0052, -0.0194, -0.0199,  0.0181, -0.0020],\n",
      "        [-0.0373, -0.0016, -0.0367,  0.0441, -0.0230],\n",
      "        [-0.0359, -0.0591, -0.0702,  0.0382,  0.0110],\n",
      "        [ 0.0294,  0.0415, -0.0305,  0.0006,  0.0597]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0140, grad_fn=<MinBackward1>), tensor(0.9763, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.18945077061653137\n",
      "@sample 428: tensor([[ 1.3825e-02, -2.4822e-02,  1.6253e-02, -1.0443e-02, -1.0538e-02],\n",
      "        [-9.2141e-03, -3.4738e-02, -3.4751e-02,  6.3994e-03,  3.7526e-03],\n",
      "        [-1.1683e-02,  2.6182e-02, -9.7855e-03, -4.1279e-02,  1.0393e-02],\n",
      "        [-2.9931e-02,  1.1535e-02,  2.2300e-02,  1.4449e-02,  2.2941e-02],\n",
      "        [ 5.7850e-03,  1.6969e-02,  5.7176e-05, -5.2473e-02,  1.8754e-02],\n",
      "        [ 8.4878e-03,  7.4538e-02, -1.9051e-02, -6.0097e-02, -8.7182e-03],\n",
      "        [ 2.9303e-02,  1.2764e-02,  4.7890e-04,  7.5218e-03,  7.8634e-03],\n",
      "        [ 1.9008e-02,  1.4322e-02, -1.2888e-02, -2.4491e-02,  4.1705e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0495, -0.0372, -0.0141, -0.0669, -0.0621],\n",
      "        [-0.0224, -0.0325,  0.1113, -0.0391, -0.0232],\n",
      "        [-0.0570, -0.0107, -0.0151,  0.0093,  0.0023],\n",
      "        [-0.0594,  0.0091, -0.0781, -0.0086,  0.0024],\n",
      "        [-0.0830, -0.0598, -0.0066,  0.0073, -0.0194],\n",
      "        [-0.0264, -0.0309, -0.0853,  0.0223,  0.0410],\n",
      "        [-0.0107,  0.0095, -0.0608,  0.0457, -0.0512],\n",
      "        [-0.0357, -0.0461,  0.0075,  0.0552, -0.0017]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0301, grad_fn=<MinBackward1>), tensor(0.9211, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.18714675307273865\n",
      "@sample 429: tensor([[ 0.0043, -0.0294,  0.0092, -0.0189, -0.0083],\n",
      "        [ 0.0078,  0.0484,  0.0170, -0.0420, -0.0082],\n",
      "        [-0.0029,  0.0461,  0.0031, -0.0459, -0.0005],\n",
      "        [ 0.0298,  0.0195,  0.0278, -0.0564,  0.0079],\n",
      "        [-0.0114,  0.0145,  0.0248, -0.0222, -0.0113],\n",
      "        [ 0.0041,  0.0083,  0.0087,  0.0341, -0.0305],\n",
      "        [ 0.0325, -0.0428, -0.0147,  0.0809, -0.0582],\n",
      "        [-0.0046,  0.0017,  0.0059, -0.0112,  0.0131]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0057, -0.0252, -0.0156,  0.0247, -0.0329],\n",
      "        [-0.0242, -0.0248,  0.0078, -0.0262, -0.0035],\n",
      "        [-0.0010, -0.0093, -0.0058, -0.0010,  0.0054],\n",
      "        [ 0.0082, -0.0562, -0.0244,  0.0369, -0.0192],\n",
      "        [-0.0430, -0.0046, -0.0271,  0.0459, -0.0356],\n",
      "        [ 0.0183,  0.0537,  0.0542, -0.0100,  0.0131],\n",
      "        [ 0.0484,  0.0326,  0.1053, -0.0604, -0.0108],\n",
      "        [-0.0156, -0.0151, -0.0321,  0.0228, -0.0126]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0185, grad_fn=<MinBackward1>), tensor(0.9733, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.17897334694862366\n",
      "@sample 430: tensor([[ 0.0174,  0.0121, -0.0132,  0.0025, -0.0261],\n",
      "        [ 0.0372,  0.0265,  0.0104,  0.0171,  0.0167],\n",
      "        [ 0.0100, -0.0261, -0.0069,  0.0202, -0.0033],\n",
      "        [-0.0250, -0.0143,  0.0068,  0.0049, -0.0091],\n",
      "        [-0.0288,  0.0236, -0.0399,  0.0047, -0.0041],\n",
      "        [ 0.0062,  0.0018,  0.0348,  0.0091,  0.0364],\n",
      "        [-0.0260,  0.0243, -0.0122,  0.0157, -0.0358],\n",
      "        [ 0.0079, -0.0184, -0.0475,  0.0257, -0.0060]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-1.2418e-02,  1.9945e-02,  3.0253e-02, -2.0303e-02,  1.0905e-04],\n",
      "        [ 8.2960e-03,  4.7453e-02, -9.5956e-02,  1.1210e-01, -4.1220e-04],\n",
      "        [-2.7655e-02,  8.1824e-03, -8.7195e-02,  1.4521e-02, -4.7218e-03],\n",
      "        [ 1.3596e-02,  3.4130e-02,  4.1859e-02, -3.2691e-03,  1.2880e-02],\n",
      "        [ 1.6845e-02,  1.1334e-02,  6.5231e-02,  7.1510e-03, -1.0623e-02],\n",
      "        [ 1.1887e-03,  7.1944e-02, -1.2297e-02,  4.1422e-02, -3.4593e-02],\n",
      "        [ 7.4242e-04,  2.7024e-02,  3.2538e-02, -3.9228e-02, -5.3527e-02],\n",
      "        [ 7.0247e-03, -2.1699e-02, -3.0533e-03, -3.9362e-02, -1.7964e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0185, grad_fn=<MinBackward1>), tensor(0.9437, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.17888858914375305\n",
      "@sample 431: tensor([[-0.0240,  0.0111,  0.0498, -0.0202,  0.0138],\n",
      "        [-0.0376,  0.0688, -0.0004, -0.0224,  0.0178],\n",
      "        [ 0.0069, -0.0822, -0.0056, -0.0164, -0.0211],\n",
      "        [-0.0436,  0.0024,  0.0310, -0.0240, -0.0156],\n",
      "        [ 0.0058,  0.0216, -0.0359, -0.0001,  0.0095],\n",
      "        [-0.0276,  0.0359,  0.0276, -0.0338,  0.0197],\n",
      "        [ 0.0386,  0.0470,  0.0134, -0.0296,  0.0303],\n",
      "        [ 0.0093, -0.0352, -0.0248,  0.0208,  0.0334]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0067, -0.0014, -0.0843,  0.0044,  0.0131],\n",
      "        [-0.0463,  0.0043, -0.1077,  0.0750, -0.0004],\n",
      "        [-0.0465, -0.0190,  0.0354, -0.0100, -0.0304],\n",
      "        [ 0.0207, -0.0276,  0.0179,  0.0177, -0.0057],\n",
      "        [-0.0325,  0.0369,  0.0351, -0.0631, -0.0387],\n",
      "        [-0.0030,  0.0044,  0.0174,  0.0236, -0.0251],\n",
      "        [-0.0536, -0.0284, -0.0741,  0.0243, -0.0527],\n",
      "        [-0.0179, -0.0072,  0.0194,  0.0313,  0.0009]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0167, grad_fn=<MinBackward1>), tensor(0.9140, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.188534215092659\n",
      "@sample 432: tensor([[-0.0318,  0.0200,  0.0261, -0.0291,  0.0394],\n",
      "        [-0.0514, -0.0050, -0.0046, -0.0294,  0.0233],\n",
      "        [ 0.0427, -0.0094,  0.0735, -0.0376,  0.0216],\n",
      "        [-0.0149, -0.0118, -0.0191,  0.0007, -0.0495],\n",
      "        [ 0.0181, -0.0092, -0.0002,  0.0148, -0.0058],\n",
      "        [ 0.0139,  0.0225,  0.0209, -0.0116,  0.0287],\n",
      "        [-0.0117, -0.0410, -0.0416,  0.0097, -0.0402],\n",
      "        [ 0.0057,  0.0193,  0.0294, -0.0099, -0.0319]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0203, -0.0152, -0.0519, -0.0316, -0.0107],\n",
      "        [-0.0190,  0.0008, -0.0328, -0.0188, -0.0282],\n",
      "        [-0.0390, -0.0249, -0.0082,  0.0911,  0.0589],\n",
      "        [ 0.0019, -0.0228,  0.0182,  0.0216,  0.0256],\n",
      "        [ 0.0263,  0.0036,  0.0117, -0.0104,  0.0133],\n",
      "        [-0.0031,  0.0449, -0.0143,  0.0219, -0.0106],\n",
      "        [ 0.0053, -0.0105,  0.0226, -0.0751,  0.0031],\n",
      "        [-0.0562, -0.0062, -0.0066,  0.0208, -0.0252]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0104, grad_fn=<MinBackward1>), tensor(0.9132, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1850566864013672\n",
      "@sample 433: tensor([[-0.0187,  0.0315,  0.0181, -0.0262, -0.0030],\n",
      "        [ 0.0227, -0.0027,  0.0018, -0.0116,  0.0231],\n",
      "        [ 0.0477, -0.1009, -0.0149, -0.0556, -0.0214],\n",
      "        [ 0.0095, -0.0472,  0.0168,  0.0096, -0.0004],\n",
      "        [-0.0011,  0.0360,  0.0582, -0.0079, -0.0065],\n",
      "        [-0.0135, -0.0438, -0.0293, -0.0052,  0.0164],\n",
      "        [-0.0040,  0.0009, -0.0157,  0.0134, -0.0456],\n",
      "        [-0.0133, -0.0599, -0.0248,  0.0277, -0.0112]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0664, -0.0146,  0.0243, -0.0668, -0.0585],\n",
      "        [-0.0237, -0.0341, -0.0603,  0.0367, -0.0371],\n",
      "        [-0.0566,  0.0130, -0.0063,  0.0112, -0.0487],\n",
      "        [-0.0012,  0.0295,  0.0959, -0.0609, -0.0187],\n",
      "        [-0.0227, -0.0083, -0.1110,  0.0374,  0.0281],\n",
      "        [-0.0084, -0.0578,  0.0667, -0.0739, -0.0435],\n",
      "        [-0.0203, -0.0444,  0.0842, -0.0563, -0.0242],\n",
      "        [ 0.0302, -0.0035,  0.0696, -0.0629,  0.0170]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0173, grad_fn=<MinBackward1>), tensor(0.9552, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.19378571212291718\n",
      "@sample 434: tensor([[-0.0181, -0.0149, -0.0051, -0.0366,  0.0153],\n",
      "        [-0.0103,  0.0753,  0.0217, -0.0279,  0.0114],\n",
      "        [-0.0083,  0.0194, -0.0015,  0.0231, -0.0147],\n",
      "        [-0.0071,  0.0560, -0.0102, -0.0472,  0.0730],\n",
      "        [ 0.0143, -0.0141, -0.0197,  0.0315, -0.0072],\n",
      "        [ 0.0011,  0.0566,  0.0314, -0.0269, -0.0264],\n",
      "        [ 0.0287,  0.0230, -0.0344,  0.0431, -0.0009],\n",
      "        [-0.0339,  0.0322,  0.0014, -0.0221,  0.0067]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0146, -0.0323, -0.0245, -0.0225, -0.0292],\n",
      "        [-0.0612,  0.0068, -0.1018,  0.0763,  0.0270],\n",
      "        [-0.0112,  0.0193, -0.0156,  0.0173, -0.0165],\n",
      "        [ 0.0245, -0.0345, -0.0819,  0.0215, -0.0277],\n",
      "        [-0.0066,  0.0022,  0.0043, -0.0164,  0.0309],\n",
      "        [-0.0143, -0.0459,  0.0565,  0.0157,  0.0386],\n",
      "        [ 0.0447,  0.0366,  0.0087, -0.0010, -0.0012],\n",
      "        [ 0.0461, -0.0417, -0.0282, -0.0066, -0.0655]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0220, grad_fn=<MinBackward1>), tensor(0.9425, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.19264855980873108\n",
      "@sample 435: tensor([[ 0.0116,  0.0415,  0.0313, -0.0298,  0.0061],\n",
      "        [-0.0140,  0.0300,  0.0144, -0.0052,  0.0191],\n",
      "        [ 0.0189, -0.0015, -0.0237,  0.0065, -0.0126],\n",
      "        [-0.0417,  0.0618,  0.0180, -0.0677,  0.0295],\n",
      "        [-0.0159,  0.0178,  0.0289, -0.0068,  0.0227],\n",
      "        [ 0.0345,  0.0150,  0.1114, -0.0299,  0.0428],\n",
      "        [ 0.0215,  0.0329,  0.0031,  0.0273, -0.0028],\n",
      "        [ 0.0041,  0.0625,  0.0275, -0.0025, -0.0125]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0313,  0.0046, -0.0370, -0.0094, -0.0432],\n",
      "        [ 0.0266,  0.0130, -0.0508, -0.0215, -0.0354],\n",
      "        [-0.0303, -0.0191, -0.0003, -0.0465, -0.0481],\n",
      "        [-0.0266, -0.0248, -0.0494,  0.0466, -0.0365],\n",
      "        [-0.0211,  0.0053, -0.0172, -0.0038, -0.0375],\n",
      "        [-0.0501, -0.0077, -0.0819,  0.1107,  0.0081],\n",
      "        [ 0.0068,  0.0750, -0.0696,  0.0404,  0.0024],\n",
      "        [-0.0360,  0.0344, -0.0518,  0.0109, -0.0610]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0188, grad_fn=<MinBackward1>), tensor(0.9364, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.18349097669124603\n",
      "@sample 436: tensor([[-0.0120, -0.0050, -0.0038,  0.0044, -0.0577],\n",
      "        [-0.0100, -0.0377, -0.0034,  0.0078, -0.0075],\n",
      "        [ 0.0278,  0.0092,  0.0223,  0.0086,  0.0028],\n",
      "        [ 0.0077,  0.0469,  0.0159,  0.0077,  0.0183],\n",
      "        [ 0.0350,  0.0369, -0.0013, -0.0023, -0.0071],\n",
      "        [ 0.0402, -0.0353,  0.0149,  0.0568, -0.0350],\n",
      "        [-0.0148,  0.0088, -0.0076, -0.0228,  0.0097],\n",
      "        [ 0.0145,  0.0555,  0.0008,  0.0393,  0.0287]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0646, -0.0108,  0.0109, -0.0388, -0.0135],\n",
      "        [ 0.0262,  0.0058,  0.0870, -0.0304, -0.0068],\n",
      "        [ 0.0062,  0.0339,  0.0192,  0.0057,  0.0129],\n",
      "        [ 0.0098, -0.0045, -0.0338, -0.0133, -0.0086],\n",
      "        [ 0.0040, -0.0246, -0.0190, -0.0158, -0.0188],\n",
      "        [ 0.0387,  0.0481,  0.0423,  0.0241, -0.0159],\n",
      "        [-0.0017, -0.0016,  0.0393, -0.0145, -0.0530],\n",
      "        [-0.0184,  0.0146, -0.0594,  0.0199, -0.0582]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0235, grad_fn=<MinBackward1>), tensor(0.8991, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.18314611911773682\n",
      "@sample 437: tensor([[ 0.0291,  0.0212, -0.0207, -0.0031,  0.0030],\n",
      "        [-0.0050, -0.0128, -0.0168,  0.0191,  0.0120],\n",
      "        [-0.0128,  0.0312,  0.0860, -0.0641,  0.0357],\n",
      "        [ 0.0085, -0.0371,  0.0162,  0.0624, -0.0297],\n",
      "        [-0.0385, -0.0208,  0.0280,  0.0029,  0.0091],\n",
      "        [ 0.0181,  0.0004, -0.0056,  0.0175, -0.0011],\n",
      "        [-0.0078,  0.0362,  0.0018, -0.0115,  0.0064],\n",
      "        [-0.0362, -0.0145,  0.0099, -0.0098,  0.0074]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-1.0178e-02,  4.6160e-02,  5.4657e-05,  9.0116e-03, -2.6098e-03],\n",
      "        [ 4.7249e-02, -1.3723e-02,  2.4139e-02, -8.3579e-03,  2.2927e-02],\n",
      "        [-7.1131e-02, -3.5756e-02, -1.5036e-01,  2.1935e-02,  7.8495e-03],\n",
      "        [ 2.9059e-03,  7.7472e-03,  6.0707e-02, -5.2473e-03,  8.4556e-03],\n",
      "        [ 5.8571e-03,  2.0727e-02,  3.8417e-02, -7.7812e-03, -1.7408e-03],\n",
      "        [ 8.2040e-03,  2.3719e-02, -1.5754e-02, -9.1330e-03, -4.5250e-03],\n",
      "        [-2.0345e-02,  1.9167e-02, -6.1316e-02,  2.7297e-02, -1.2041e-02],\n",
      "        [ 1.1376e-02, -4.1710e-03,  2.2324e-02,  5.9606e-02,  9.0126e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0191, grad_fn=<MinBackward1>), tensor(0.9444, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.18549560010433197\n",
      "@sample 438: tensor([[-0.0025,  0.0081,  0.0313, -0.0118,  0.0169],\n",
      "        [-0.0047, -0.0178,  0.0113, -0.0026,  0.0038],\n",
      "        [-0.0369,  0.1019,  0.0073, -0.0902,  0.0314],\n",
      "        [ 0.0264, -0.0615,  0.0160,  0.0497, -0.0135],\n",
      "        [ 0.0209,  0.0816, -0.0075, -0.0589,  0.0592],\n",
      "        [ 0.0015,  0.0403,  0.0038,  0.0060, -0.0257],\n",
      "        [-0.0174,  0.0299, -0.0508,  0.0370, -0.0144],\n",
      "        [-0.0181,  0.0384,  0.0132, -0.0058,  0.0210]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0162,  0.0167, -0.0119,  0.0319,  0.0437],\n",
      "        [ 0.0197, -0.0330, -0.0242,  0.0168,  0.0325],\n",
      "        [-0.0521, -0.0022, -0.1093,  0.0527,  0.0642],\n",
      "        [ 0.0045,  0.0175,  0.0889,  0.0104,  0.0302],\n",
      "        [-0.0021, -0.0193, -0.0679, -0.0202, -0.0933],\n",
      "        [-0.0528,  0.0163, -0.0262, -0.0290, -0.0484],\n",
      "        [-0.0020,  0.0021,  0.0183, -0.0090, -0.0117],\n",
      "        [ 0.0139,  0.0345, -0.0969,  0.0308,  0.0676]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0222, grad_fn=<MinBackward1>), tensor(0.9455, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.20910760760307312\n",
      "@sample 439: tensor([[-0.0407,  0.0151,  0.0326, -0.0217, -0.0085],\n",
      "        [-0.0189, -0.0241, -0.0187,  0.0412, -0.0340],\n",
      "        [ 0.0119,  0.0408, -0.0622, -0.0068, -0.0215],\n",
      "        [ 0.0170,  0.0354,  0.0160, -0.0160, -0.0113],\n",
      "        [ 0.0203, -0.0310,  0.0155,  0.0769, -0.0081],\n",
      "        [ 0.0472, -0.0062, -0.0019,  0.0120, -0.0044],\n",
      "        [-0.0349,  0.0678, -0.0367, -0.0743,  0.0177],\n",
      "        [ 0.0006, -0.0016,  0.0044, -0.0632,  0.0351]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 2.3895e-02, -4.4264e-02, -1.9410e-02,  2.1872e-03,  3.0522e-03],\n",
      "        [ 5.5561e-02,  1.4808e-02,  9.3848e-02, -1.0614e-01,  2.6732e-02],\n",
      "        [-7.2044e-02, -4.6153e-02, -7.8595e-02,  4.5122e-02, -2.7605e-03],\n",
      "        [ 3.0135e-03, -4.2678e-02, -4.4196e-02,  2.8379e-02, -1.0423e-03],\n",
      "        [ 5.4125e-02,  2.8769e-02,  3.8547e-02,  1.4586e-02,  2.0355e-02],\n",
      "        [ 1.2790e-03, -1.0668e-02, -1.5557e-02, -6.8575e-02, -4.4674e-02],\n",
      "        [-5.6500e-02, -3.9863e-02, -5.2844e-02,  3.3382e-02, -1.4387e-02],\n",
      "        [-6.4000e-06,  8.1012e-04,  9.1263e-03, -3.5939e-02, -5.7619e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0195, grad_fn=<MinBackward1>), tensor(0.9433, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.19254642724990845\n",
      "@sample 440: tensor([[ 0.0201, -0.0221, -0.0314,  0.0135, -0.0095],\n",
      "        [ 0.0304, -0.0274, -0.0285,  0.0855, -0.0509],\n",
      "        [ 0.0225,  0.0488, -0.0465, -0.0276,  0.0025],\n",
      "        [-0.0035, -0.0655, -0.0231,  0.0582, -0.0058],\n",
      "        [-0.0062,  0.0386, -0.0100,  0.0301, -0.0030],\n",
      "        [ 0.0551, -0.0896, -0.0814,  0.0757, -0.0145],\n",
      "        [-0.0200, -0.0401, -0.0178,  0.0360, -0.0509],\n",
      "        [-0.0287, -0.0376, -0.0184,  0.0134,  0.0313]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0125,  0.0334, -0.0120,  0.0066,  0.0037],\n",
      "        [ 0.0420,  0.0411,  0.0518, -0.0314, -0.0126],\n",
      "        [ 0.0313, -0.0544, -0.0622,  0.0222, -0.0024],\n",
      "        [-0.0177,  0.1000,  0.0501, -0.0015,  0.0470],\n",
      "        [ 0.0034,  0.0030, -0.0134, -0.0069, -0.0324],\n",
      "        [ 0.0029,  0.0021,  0.0193,  0.0141,  0.0163],\n",
      "        [-0.0093, -0.0050,  0.0494, -0.0376,  0.0146],\n",
      "        [ 0.0444, -0.0356, -0.0063,  0.0025, -0.0040]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0180, grad_fn=<MinBackward1>), tensor(0.9458, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.20173966884613037\n",
      "@sample 441: tensor([[ 3.7471e-02, -3.7322e-02, -5.2024e-02,  5.3082e-02, -3.5089e-02],\n",
      "        [ 4.8498e-02, -4.4683e-02, -7.2342e-03,  3.6244e-02, -1.0277e-02],\n",
      "        [ 2.9712e-02, -1.6024e-02, -1.4130e-02,  6.3273e-02, -9.9501e-03],\n",
      "        [ 1.0944e-02,  5.2115e-02,  1.1146e-02, -4.3882e-02,  2.4595e-02],\n",
      "        [-1.8990e-02,  5.4998e-02,  5.5757e-02, -1.2279e-01,  3.4329e-02],\n",
      "        [ 2.0598e-02, -1.3541e-02, -7.4827e-02,  4.8883e-02, -4.2912e-02],\n",
      "        [ 4.7747e-05, -2.1475e-02, -1.4419e-02,  2.5174e-02,  3.6094e-02],\n",
      "        [-3.6696e-04, -7.6204e-03, -3.3530e-02,  2.7378e-02, -6.1720e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 5.4104e-02,  5.4474e-02,  6.8225e-02, -6.2717e-02, -7.2804e-03],\n",
      "        [ 1.5745e-02,  2.5433e-02,  2.0042e-02, -2.5587e-02, -1.0733e-02],\n",
      "        [ 6.8324e-02,  3.9100e-02,  1.7091e-02, -1.9470e-02,  2.0758e-02],\n",
      "        [-3.0912e-02, -3.5724e-02, -6.5201e-02,  4.0118e-02,  1.8182e-02],\n",
      "        [-1.0708e-01, -9.9991e-02, -6.2911e-02,  2.2574e-02, -2.8192e-02],\n",
      "        [ 5.6538e-02,  1.8120e-05,  2.8019e-02,  2.0846e-02,  6.5248e-02],\n",
      "        [ 1.2138e-02,  1.8702e-03,  1.9161e-02, -2.3936e-02,  1.6924e-02],\n",
      "        [ 3.2726e-03,  5.3206e-02, -1.7506e-02,  3.9870e-03, -8.4723e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0153, grad_fn=<MinBackward1>), tensor(0.9554, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.208003968000412\n",
      "@sample 442: tensor([[ 4.1673e-03, -3.2166e-02, -2.4535e-02,  6.7696e-02,  2.0405e-02],\n",
      "        [ 2.4772e-02,  2.7695e-03, -6.1922e-05, -2.0123e-02,  5.6867e-03],\n",
      "        [ 1.5285e-02, -2.6737e-02,  3.6420e-02,  6.5181e-03, -3.1720e-02],\n",
      "        [-4.1416e-03, -8.4786e-03, -1.4201e-02,  4.2148e-02, -3.4218e-02],\n",
      "        [-2.0876e-02,  1.9607e-03, -1.3826e-02, -3.5021e-02,  4.4821e-02],\n",
      "        [-1.3537e-02, -1.0367e-02, -1.6691e-02,  3.3358e-02, -3.4851e-02],\n",
      "        [-1.3620e-02,  3.1160e-02,  4.5454e-02, -5.1185e-02,  5.6673e-02],\n",
      "        [-1.4422e-02, -1.1065e-02,  2.6468e-02, -1.8080e-02,  4.6761e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.1264,  0.0155, -0.0006,  0.0173,  0.1055],\n",
      "        [-0.0297, -0.0318, -0.0350,  0.0112,  0.0130],\n",
      "        [-0.0239,  0.0495, -0.0023, -0.0247, -0.0244],\n",
      "        [ 0.0443,  0.0402,  0.0832, -0.0316, -0.0201],\n",
      "        [-0.0240, -0.0793, -0.0649,  0.0437, -0.0077],\n",
      "        [ 0.0085, -0.0258,  0.0456, -0.0670, -0.0042],\n",
      "        [-0.0501, -0.0232, -0.1096,  0.0369, -0.0274],\n",
      "        [-0.0356, -0.0197, -0.0017,  0.0104, -0.0269]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0279, grad_fn=<MinBackward1>), tensor(0.9496, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.19614437222480774\n",
      "@sample 443: tensor([[-0.0107, -0.0322,  0.0034, -0.0030, -0.0003],\n",
      "        [ 0.0088, -0.0279, -0.0511,  0.0534, -0.0119],\n",
      "        [ 0.0305,  0.0301,  0.0354, -0.0431, -0.0291],\n",
      "        [-0.0395, -0.0320, -0.0350,  0.0086, -0.0155],\n",
      "        [ 0.0381, -0.0140,  0.0188,  0.0265, -0.0464],\n",
      "        [ 0.0383,  0.0044,  0.0676, -0.0176, -0.0329],\n",
      "        [ 0.0312, -0.0443, -0.0196,  0.0572, -0.0509],\n",
      "        [-0.0010,  0.0173,  0.0125, -0.0088,  0.0294]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0057,  0.0418,  0.0490, -0.0192, -0.0224],\n",
      "        [ 0.0344,  0.0793, -0.0439,  0.0162,  0.0387],\n",
      "        [-0.0445, -0.0285, -0.0042,  0.0186,  0.0125],\n",
      "        [-0.0079,  0.0034,  0.0681, -0.0757,  0.0119],\n",
      "        [-0.0301,  0.0220,  0.0155, -0.0209,  0.0180],\n",
      "        [-0.0117, -0.0092, -0.0745, -0.0041,  0.0086],\n",
      "        [ 0.0308, -0.0033,  0.0233, -0.0364,  0.0235],\n",
      "        [-0.0247, -0.0287, -0.0729,  0.0698,  0.0381]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0190, grad_fn=<MinBackward1>), tensor(0.9396, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.19187593460083008\n",
      "@sample 444: tensor([[-0.0078, -0.0220, -0.0326, -0.0080, -0.0005],\n",
      "        [-0.0185, -0.0135, -0.0103, -0.0181, -0.0086],\n",
      "        [ 0.0087,  0.0593, -0.0231,  0.0061,  0.0279],\n",
      "        [ 0.0317,  0.0387,  0.0109, -0.0179,  0.0142],\n",
      "        [-0.0052, -0.0018, -0.0462,  0.0534, -0.0280],\n",
      "        [ 0.0038,  0.0040,  0.0112, -0.0233,  0.0430],\n",
      "        [-0.0328,  0.0068, -0.0147,  0.0135, -0.0189],\n",
      "        [-0.0092, -0.0017, -0.0254, -0.0057, -0.0064]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0284, -0.0131,  0.0173, -0.0272,  0.0054],\n",
      "        [ 0.0070, -0.0178,  0.0074,  0.0017,  0.0081],\n",
      "        [-0.0280, -0.0303, -0.1075,  0.0012,  0.0069],\n",
      "        [-0.0312, -0.0083, -0.0758,  0.0397,  0.0106],\n",
      "        [ 0.0153,  0.0671,  0.0221, -0.0175, -0.0048],\n",
      "        [ 0.0263, -0.0044, -0.0790,  0.0383, -0.0145],\n",
      "        [-0.0168, -0.0142, -0.0071, -0.0242, -0.0913],\n",
      "        [-0.0338, -0.0162, -0.0650, -0.0244,  0.0171]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0153, grad_fn=<MinBackward1>), tensor(0.9723, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.17136314511299133\n",
      "@sample 445: tensor([[-2.6991e-02, -5.1429e-02, -2.5810e-02, -6.1535e-03,  1.0405e-02],\n",
      "        [-1.3522e-02, -3.0904e-02, -2.7540e-02, -2.6119e-03,  3.7970e-02],\n",
      "        [-9.6692e-03,  1.7564e-02, -3.3705e-02,  2.2449e-02, -1.3685e-02],\n",
      "        [-1.2658e-02,  4.1515e-05, -4.7129e-02, -2.3054e-03,  1.6536e-02],\n",
      "        [-2.7670e-03, -5.7433e-03,  2.7709e-03, -4.4543e-02,  4.2424e-02],\n",
      "        [-1.0062e-03,  3.6731e-02,  8.6094e-04, -7.5177e-03,  9.3435e-03],\n",
      "        [ 1.6726e-02, -3.2128e-02, -5.5826e-03,  7.3831e-03,  1.8116e-02],\n",
      "        [-3.5746e-02, -4.8737e-03,  8.3447e-03,  3.2424e-02,  2.0314e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0349, -0.0256,  0.0309, -0.0413,  0.0087],\n",
      "        [-0.0229,  0.0172,  0.0225, -0.0404, -0.0213],\n",
      "        [-0.0077,  0.0024,  0.0283, -0.0453,  0.0176],\n",
      "        [-0.0267, -0.0368, -0.0262, -0.0083, -0.0326],\n",
      "        [ 0.0632,  0.0175, -0.0161,  0.0208,  0.0298],\n",
      "        [-0.0096, -0.0095, -0.0327, -0.0104,  0.0254],\n",
      "        [-0.0178,  0.0076, -0.0146,  0.0058,  0.0198],\n",
      "        [ 0.0192, -0.0072, -0.0007,  0.0050,  0.0038]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0145, grad_fn=<MinBackward1>), tensor(0.9149, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.17378643155097961\n",
      "@sample 446: tensor([[-0.0163, -0.0434, -0.0133,  0.0350, -0.0256],\n",
      "        [-0.0501,  0.0454,  0.0539, -0.0364, -0.0124],\n",
      "        [-0.0018, -0.0421,  0.0046, -0.0294,  0.0692],\n",
      "        [-0.0022, -0.0117,  0.0270, -0.0278, -0.0093],\n",
      "        [-0.0402, -0.0598,  0.0038, -0.0192,  0.0223],\n",
      "        [ 0.0679,  0.0483, -0.0029, -0.0694, -0.0013],\n",
      "        [-0.0141, -0.0198, -0.0104, -0.0024,  0.0257],\n",
      "        [-0.0057,  0.0358, -0.0385, -0.0685,  0.0281]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0021, -0.0034,  0.1144,  0.0011, -0.0092],\n",
      "        [-0.0259, -0.0160, -0.0124,  0.0280,  0.0346],\n",
      "        [-0.0329, -0.0569,  0.0363,  0.0061, -0.0186],\n",
      "        [-0.0279, -0.0371, -0.0146, -0.0514,  0.0014],\n",
      "        [-0.0306, -0.0461,  0.0670, -0.0783, -0.0426],\n",
      "        [-0.0979, -0.0537, -0.0575,  0.0521,  0.0197],\n",
      "        [-0.0368,  0.0537,  0.0412, -0.0313, -0.0096],\n",
      "        [-0.0236, -0.0679, -0.0542,  0.0569, -0.0416]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0140, grad_fn=<MinBackward1>), tensor(0.9481, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.19697988033294678\n",
      "@sample 447: tensor([[ 3.6529e-03,  9.8200e-03,  2.0842e-02, -2.7055e-02,  1.5334e-02],\n",
      "        [ 3.5525e-05,  1.2028e-02,  2.2020e-02, -6.8049e-02,  3.6357e-02],\n",
      "        [-8.9091e-03,  2.4701e-02,  3.3306e-02, -5.1220e-02,  4.8605e-02],\n",
      "        [ 2.1966e-02, -5.8358e-02, -8.6030e-03,  2.3179e-02, -4.4164e-02],\n",
      "        [ 2.7574e-02,  3.2378e-02,  5.2429e-03, -2.6358e-02,  3.5773e-02],\n",
      "        [-2.0139e-02, -7.2408e-03,  4.3173e-02,  1.4488e-03, -3.3495e-02],\n",
      "        [ 3.0004e-02,  5.2116e-02,  3.2044e-02, -4.9125e-02,  7.0124e-03],\n",
      "        [-2.0838e-02,  2.0143e-02, -1.2169e-02, -7.2846e-02,  5.3286e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0739,  0.0178, -0.0591,  0.0355,  0.0022],\n",
      "        [ 0.0077, -0.0774, -0.0157, -0.0015,  0.0009],\n",
      "        [-0.0017, -0.0273, -0.0433,  0.0236, -0.0388],\n",
      "        [ 0.0057, -0.0307,  0.0295, -0.0081, -0.0020],\n",
      "        [-0.0577, -0.0121, -0.0813,  0.0354,  0.0030],\n",
      "        [-0.0180,  0.0124,  0.0051,  0.0069,  0.0060],\n",
      "        [-0.0192, -0.0032, -0.0131,  0.0319,  0.0017],\n",
      "        [ 0.0099, -0.0535, -0.0322,  0.0232, -0.0072]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0185, grad_fn=<MinBackward1>), tensor(0.9436, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.18504899740219116\n",
      "@sample 448: tensor([[-0.0241,  0.0600,  0.0226, -0.0884,  0.0060],\n",
      "        [-0.0071, -0.0224,  0.0148, -0.0347,  0.0452],\n",
      "        [-0.0114,  0.0553,  0.0402, -0.0894,  0.0439],\n",
      "        [-0.0020,  0.0077,  0.0103, -0.0627,  0.0412],\n",
      "        [ 0.0059,  0.0002,  0.0222, -0.0465, -0.0215],\n",
      "        [-0.0239, -0.0131, -0.0229, -0.0403,  0.0556],\n",
      "        [ 0.0183,  0.0010, -0.0028,  0.0250,  0.0350],\n",
      "        [ 0.0049,  0.0204,  0.0185,  0.0131,  0.0070]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0425, -0.0927, -0.0906,  0.0517,  0.0134],\n",
      "        [-0.0169, -0.0640,  0.0132,  0.0418,  0.0051],\n",
      "        [-0.0555, -0.0542, -0.0367,  0.0398, -0.0305],\n",
      "        [-0.0381, -0.0602, -0.0869,  0.0411, -0.0079],\n",
      "        [-0.0306, -0.0373, -0.0273,  0.0296, -0.0218],\n",
      "        [-0.0051, -0.0589, -0.0188,  0.0129,  0.0143],\n",
      "        [ 0.0739,  0.0443, -0.0983,  0.0640, -0.0051],\n",
      "        [-0.0116, -0.0456,  0.0076,  0.0086, -0.0303]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0192, grad_fn=<MinBackward1>), tensor(0.8996, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.19754643738269806\n",
      "@sample 449: tensor([[ 0.0155, -0.0055, -0.0243,  0.0420, -0.0158],\n",
      "        [-0.0033, -0.0173, -0.0118,  0.0131, -0.0208],\n",
      "        [ 0.0188, -0.0308, -0.0057,  0.0201, -0.0117],\n",
      "        [-0.0154, -0.0744,  0.0227, -0.0123,  0.0280],\n",
      "        [-0.0087, -0.0235, -0.0216,  0.0200, -0.0402],\n",
      "        [ 0.0021, -0.0376,  0.0377, -0.0044, -0.0146],\n",
      "        [ 0.0059,  0.0380, -0.0211, -0.0228,  0.0116],\n",
      "        [-0.0213,  0.0226,  0.0181,  0.0025, -0.0055]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0289,  0.0235,  0.0339, -0.0365, -0.0055],\n",
      "        [ 0.0256,  0.0212, -0.0080, -0.0205,  0.0154],\n",
      "        [ 0.0201,  0.0264,  0.0173, -0.0451,  0.0018],\n",
      "        [-0.0006, -0.0094,  0.0377,  0.0076, -0.0124],\n",
      "        [ 0.0022, -0.0028,  0.0358, -0.0527,  0.0362],\n",
      "        [ 0.0140,  0.0270,  0.0687, -0.0130,  0.0431],\n",
      "        [-0.0212, -0.0553, -0.1058,  0.0384,  0.0272],\n",
      "        [-0.0061, -0.0091, -0.0055,  0.0116,  0.0105]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0111, grad_fn=<MinBackward1>), tensor(0.9303, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1933162808418274\n",
      "@sample 450: tensor([[ 1.4876e-02,  3.5202e-02,  2.7565e-03, -9.4681e-02, -7.1134e-04],\n",
      "        [ 3.8517e-02, -1.7073e-02,  1.5807e-03, -1.7474e-02,  2.6355e-02],\n",
      "        [-7.4025e-05, -2.4373e-02,  2.9512e-02,  2.1227e-02,  1.8372e-02],\n",
      "        [-1.5998e-02,  6.0200e-02,  4.8832e-02, -9.6307e-02,  4.1393e-02],\n",
      "        [-4.7591e-02, -1.1316e-02, -1.1026e-02, -1.1271e-02,  2.8130e-02],\n",
      "        [-1.7589e-02,  2.6442e-02,  5.0012e-02, -2.1057e-03,  1.2638e-02],\n",
      "        [-2.6421e-02,  5.3390e-02,  6.3645e-02, -7.3655e-02,  3.8122e-02],\n",
      "        [ 6.8073e-03,  2.4006e-02,  2.3832e-02, -3.2074e-02,  3.3740e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0389, -0.0373, -0.0414, -0.0148, -0.0365],\n",
      "        [ 0.0022, -0.0516, -0.0072,  0.0145,  0.0162],\n",
      "        [ 0.0114,  0.0532,  0.0134, -0.0341, -0.0455],\n",
      "        [-0.0307, -0.0688, -0.0934,  0.0574, -0.0137],\n",
      "        [-0.0013, -0.0155, -0.0234,  0.0433,  0.0242],\n",
      "        [ 0.0091,  0.0346,  0.0116, -0.0254,  0.0048],\n",
      "        [-0.0314, -0.0295, -0.0842,  0.0470,  0.0219],\n",
      "        [-0.0455, -0.0378, -0.0970,  0.0822,  0.0043]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0211, grad_fn=<MinBackward1>), tensor(0.9281, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.20056375861167908\n",
      "@sample 451: tensor([[-0.0277,  0.1380,  0.0530, -0.0766,  0.0220],\n",
      "        [ 0.0218,  0.0401,  0.0060, -0.0586, -0.0111],\n",
      "        [ 0.0034, -0.0124,  0.0113,  0.0075,  0.0078],\n",
      "        [ 0.0014,  0.0045,  0.0621, -0.0019, -0.0067],\n",
      "        [-0.0020, -0.0348,  0.0393, -0.0087,  0.0648],\n",
      "        [-0.0038,  0.1029,  0.0397, -0.0634,  0.0596],\n",
      "        [-0.0326,  0.0252, -0.0379, -0.0497,  0.0234],\n",
      "        [ 0.0127, -0.0429,  0.0116,  0.0172, -0.0010]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0534, -0.0269, -0.0802,  0.1020, -0.0125],\n",
      "        [-0.0349, -0.0396, -0.0090,  0.0722,  0.0347],\n",
      "        [-0.0232,  0.0120, -0.0068, -0.0121,  0.0125],\n",
      "        [ 0.0012, -0.0148, -0.0812, -0.0134, -0.0029],\n",
      "        [-0.0380,  0.0105,  0.0029,  0.0460, -0.0144],\n",
      "        [-0.0399, -0.0404, -0.1256,  0.0967,  0.0035],\n",
      "        [-0.0179, -0.0426, -0.0114, -0.0243,  0.0028],\n",
      "        [ 0.0303,  0.0342,  0.1337, -0.1017, -0.0489]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0216, grad_fn=<MinBackward1>), tensor(0.9390, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.21327650547027588\n",
      "@sample 452: tensor([[-0.0219,  0.0489, -0.0231,  0.0110, -0.0053],\n",
      "        [ 0.0055,  0.0006,  0.0083, -0.0196, -0.0113],\n",
      "        [-0.0010, -0.0333,  0.0097, -0.0258,  0.0384],\n",
      "        [-0.0069,  0.0345,  0.0264, -0.0180,  0.0221],\n",
      "        [-0.0184, -0.0201,  0.0165,  0.0091, -0.0085],\n",
      "        [ 0.0237, -0.0186,  0.0250,  0.0406,  0.0061],\n",
      "        [ 0.0088,  0.0175,  0.0452, -0.0185, -0.0169],\n",
      "        [ 0.0044, -0.0069,  0.0299, -0.0076,  0.0244]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0030,  0.0096, -0.0133, -0.0391, -0.0273],\n",
      "        [ 0.0092,  0.0012,  0.0068,  0.0280,  0.0274],\n",
      "        [-0.0309, -0.0269, -0.0837,  0.0446,  0.0291],\n",
      "        [-0.0592,  0.0037, -0.0528, -0.0161, -0.0504],\n",
      "        [ 0.0186, -0.0203,  0.0264, -0.0152,  0.0009],\n",
      "        [ 0.0162,  0.0280, -0.0818,  0.0205, -0.0252],\n",
      "        [ 0.0049, -0.0031, -0.0027,  0.0731,  0.0560],\n",
      "        [-0.0304,  0.0009, -0.0145,  0.0147, -0.0015]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0197, grad_fn=<MinBackward1>), tensor(0.9566, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.180498406291008\n",
      "@sample 453: tensor([[-0.0111, -0.0480, -0.0150,  0.0108,  0.0239],\n",
      "        [-0.0164, -0.0365, -0.0191,  0.0135,  0.0038],\n",
      "        [ 0.0047, -0.0435,  0.0087,  0.0007,  0.0037],\n",
      "        [ 0.0115, -0.0110,  0.0037,  0.0453,  0.0024],\n",
      "        [-0.0012,  0.0277,  0.0235, -0.0190,  0.0121],\n",
      "        [ 0.0063, -0.0006,  0.0011, -0.0201,  0.0146],\n",
      "        [-0.0156, -0.0583, -0.0049,  0.0193, -0.0021],\n",
      "        [-0.0122,  0.0048,  0.0076,  0.0208, -0.0321]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0075, -0.0122,  0.0822, -0.0299, -0.0584],\n",
      "        [ 0.0477, -0.0370,  0.0383,  0.0082,  0.0142],\n",
      "        [-0.0239,  0.0014, -0.0031, -0.0006, -0.0120],\n",
      "        [ 0.0311,  0.0296,  0.0788, -0.0640, -0.0482],\n",
      "        [-0.0339, -0.0286, -0.0470,  0.0157, -0.0145],\n",
      "        [ 0.0074, -0.0201,  0.0395, -0.0031,  0.0157],\n",
      "        [ 0.0110,  0.0073,  0.0374, -0.0545, -0.0226],\n",
      "        [ 0.0148,  0.0440, -0.0484,  0.0116,  0.0184]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0170, grad_fn=<MinBackward1>), tensor(0.9507, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.17770284414291382\n",
      "@sample 454: tensor([[ 0.0381,  0.0236,  0.0115, -0.0559,  0.0320],\n",
      "        [ 0.0302, -0.0425,  0.0020, -0.0148, -0.0110],\n",
      "        [-0.0188,  0.0122, -0.0090, -0.0073, -0.0155],\n",
      "        [ 0.0233, -0.0182,  0.0001, -0.0063, -0.0005],\n",
      "        [ 0.0232, -0.0302, -0.0061,  0.0284, -0.0067],\n",
      "        [ 0.0037, -0.0258, -0.0310, -0.0083, -0.0049],\n",
      "        [-0.0090, -0.0246, -0.0131, -0.0115, -0.0169],\n",
      "        [-0.0119, -0.0282,  0.0036,  0.0586, -0.0261]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0083, -0.0692, -0.0413,  0.0289, -0.0065],\n",
      "        [ 0.0089,  0.0331,  0.0228, -0.0352, -0.0023],\n",
      "        [-0.0154,  0.0005,  0.0657, -0.0003, -0.0049],\n",
      "        [-0.0498, -0.0023, -0.0209,  0.0235, -0.0196],\n",
      "        [-0.0132, -0.0117, -0.0109, -0.0046, -0.0170],\n",
      "        [ 0.0109, -0.0152,  0.1115, -0.0140,  0.0233],\n",
      "        [ 0.0301, -0.0465,  0.0284, -0.0171, -0.0006],\n",
      "        [ 0.0250,  0.0349,  0.1107, -0.0606,  0.0175]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0175, grad_fn=<MinBackward1>), tensor(0.9268, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1839004009962082\n",
      "@sample 455: tensor([[ 0.0171,  0.0574,  0.0182, -0.0251,  0.0127],\n",
      "        [-0.0157,  0.0242,  0.0155, -0.0103,  0.0107],\n",
      "        [ 0.0293, -0.0545, -0.0097,  0.0380, -0.0214],\n",
      "        [ 0.0013,  0.0515,  0.0240,  0.0056,  0.0192],\n",
      "        [ 0.0147,  0.0232,  0.0283, -0.0005,  0.0041],\n",
      "        [ 0.0282, -0.0223, -0.0065,  0.0249, -0.0188],\n",
      "        [-0.0287, -0.0222, -0.0262, -0.0348,  0.0431],\n",
      "        [ 0.0081, -0.0622, -0.0150,  0.0649, -0.0399]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0004, -0.0187, -0.0934, -0.0126, -0.0652],\n",
      "        [-0.0164,  0.0027,  0.0074,  0.0182,  0.0105],\n",
      "        [ 0.0058,  0.0021,  0.0773, -0.0110,  0.0036],\n",
      "        [-0.0033,  0.0495, -0.0087,  0.0213, -0.0344],\n",
      "        [ 0.0398,  0.0015, -0.0563,  0.0307,  0.0115],\n",
      "        [-0.0592, -0.0504, -0.0304,  0.0554, -0.0217],\n",
      "        [-0.0048, -0.0782, -0.0799,  0.0469,  0.0197],\n",
      "        [ 0.0193, -0.0044,  0.0760, -0.0293,  0.0344]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0262, grad_fn=<MinBackward1>), tensor(0.9195, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.18240855634212494\n",
      "@sample 456: tensor([[-0.0375,  0.0156, -0.0435, -0.0019,  0.0195],\n",
      "        [ 0.0002, -0.0181, -0.0270,  0.0276, -0.0046],\n",
      "        [ 0.0115, -0.0052,  0.0023,  0.0146,  0.0045],\n",
      "        [-0.0573,  0.0615,  0.0244, -0.0732,  0.0010],\n",
      "        [ 0.0140, -0.0433, -0.0070,  0.0110,  0.0144],\n",
      "        [-0.0058,  0.0605,  0.0465, -0.0267,  0.0465],\n",
      "        [ 0.0141,  0.0149,  0.0235, -0.0171,  0.0111],\n",
      "        [ 0.0062, -0.0102, -0.0078,  0.0474, -0.0322]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0381, -0.0293, -0.0568,  0.0440,  0.0344],\n",
      "        [-0.0036,  0.0260, -0.0020, -0.0163,  0.0353],\n",
      "        [-0.0010,  0.0209, -0.0003,  0.0052,  0.0011],\n",
      "        [-0.0159, -0.0082, -0.0376,  0.0409,  0.0014],\n",
      "        [ 0.0109, -0.0284,  0.0602, -0.0654, -0.0116],\n",
      "        [-0.0289, -0.0132, -0.0815,  0.0558,  0.0074],\n",
      "        [-0.0016, -0.0073, -0.0426,  0.0601,  0.0071],\n",
      "        [ 0.0400, -0.0245, -0.0352, -0.0115,  0.0218]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0195, grad_fn=<MinBackward1>), tensor(0.9466, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1800088733434677\n",
      "@sample 457: tensor([[-0.0069,  0.0259,  0.0142,  0.0133,  0.0161],\n",
      "        [ 0.0308,  0.0014, -0.0097, -0.0005, -0.0046],\n",
      "        [-0.0140, -0.0080,  0.0313,  0.0194,  0.0068],\n",
      "        [-0.0264,  0.0058, -0.0172,  0.0216, -0.0189],\n",
      "        [ 0.0084, -0.0112, -0.0548,  0.0342, -0.0241],\n",
      "        [ 0.0156,  0.0235, -0.0185,  0.0203, -0.0038],\n",
      "        [-0.0105,  0.0536,  0.0335, -0.0105, -0.0250],\n",
      "        [ 0.0062, -0.0606,  0.0045,  0.0106,  0.0029]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0119,  0.0217,  0.0256,  0.0058, -0.0358],\n",
      "        [-0.0262, -0.0033, -0.0174,  0.0516, -0.0017],\n",
      "        [-0.0150,  0.0036,  0.0055,  0.0036,  0.0069],\n",
      "        [-0.0033,  0.0355,  0.0716, -0.0235, -0.0348],\n",
      "        [ 0.0004, -0.0026, -0.0012,  0.0445, -0.0398],\n",
      "        [-0.0196,  0.0198, -0.0011, -0.0354, -0.0146],\n",
      "        [ 0.0320,  0.0154,  0.0335, -0.0201,  0.0355],\n",
      "        [ 0.0018, -0.0181,  0.0058, -0.0413, -0.0673]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0185, grad_fn=<MinBackward1>), tensor(0.9181, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.16983047127723694\n",
      "@sample 458: tensor([[ 0.0303,  0.0266,  0.0172, -0.0204, -0.0271],\n",
      "        [-0.0141,  0.0599,  0.0199, -0.0502, -0.0004],\n",
      "        [ 0.0251, -0.0410, -0.0288,  0.0273, -0.0139],\n",
      "        [-0.0236,  0.0122,  0.0544, -0.0233,  0.0414],\n",
      "        [-0.0182, -0.0202, -0.0108, -0.0100, -0.0150],\n",
      "        [-0.0180, -0.0074, -0.0109,  0.0262,  0.0050],\n",
      "        [-0.0012, -0.0113,  0.0125,  0.0414, -0.0116],\n",
      "        [ 0.0260, -0.0047, -0.0064,  0.0113, -0.0113]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0026,  0.0300, -0.0282,  0.0143, -0.0239],\n",
      "        [-0.0514, -0.0354, -0.0594,  0.0422, -0.0306],\n",
      "        [ 0.0142,  0.0343,  0.0364, -0.0380, -0.0142],\n",
      "        [ 0.0330,  0.0648, -0.0179,  0.0160,  0.0195],\n",
      "        [-0.0093,  0.0336,  0.0446, -0.0675, -0.0351],\n",
      "        [-0.0181, -0.0019,  0.0047, -0.0215,  0.0083],\n",
      "        [ 0.0417,  0.0269,  0.1148, -0.0354, -0.0177],\n",
      "        [-0.0031,  0.0115,  0.0168, -0.0138,  0.0055]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0152, grad_fn=<MinBackward1>), tensor(0.9259, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.18285003304481506\n",
      "@sample 459: tensor([[ 0.0151,  0.0294,  0.0122, -0.0125,  0.0134],\n",
      "        [-0.0078,  0.0512, -0.0018, -0.0489, -0.0310],\n",
      "        [ 0.0263,  0.0537,  0.0031, -0.0088,  0.0350],\n",
      "        [ 0.0156, -0.0154, -0.0098, -0.0092,  0.0122],\n",
      "        [ 0.0118, -0.0248, -0.0154,  0.0322, -0.0499],\n",
      "        [ 0.0233,  0.0251, -0.0142,  0.0170, -0.0100],\n",
      "        [ 0.0072,  0.0072, -0.0048,  0.0135,  0.0179],\n",
      "        [-0.0141, -0.0158,  0.0209,  0.0012,  0.0187]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0013,  0.0165, -0.0075, -0.0007, -0.0274],\n",
      "        [-0.0477,  0.0042, -0.0152, -0.0105,  0.0129],\n",
      "        [-0.0109,  0.0083, -0.0702,  0.0378,  0.0241],\n",
      "        [-0.0413,  0.0067, -0.1059,  0.0741,  0.0484],\n",
      "        [-0.0106,  0.0094,  0.0079, -0.0106,  0.0197],\n",
      "        [ 0.0038,  0.0090, -0.0262,  0.0079,  0.0275],\n",
      "        [ 0.0275,  0.0488,  0.0041, -0.0014,  0.0252],\n",
      "        [ 0.0290,  0.0051,  0.0988, -0.0718, -0.0422]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0215, grad_fn=<MinBackward1>), tensor(0.9246, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.163713276386261\n",
      "@sample 460: tensor([[ 0.0034,  0.0235,  0.0454,  0.0133,  0.0104],\n",
      "        [ 0.0052,  0.0328, -0.0118,  0.0145, -0.0566],\n",
      "        [-0.0286,  0.0218, -0.0094, -0.0495, -0.0203],\n",
      "        [ 0.0410, -0.0132,  0.0157,  0.0483, -0.0155],\n",
      "        [ 0.0044, -0.0237, -0.0172,  0.0411, -0.0355],\n",
      "        [-0.0008, -0.0285, -0.0382,  0.0496, -0.0374],\n",
      "        [-0.0183,  0.0069,  0.0029,  0.0096, -0.0159],\n",
      "        [ 0.0043,  0.0120, -0.0154, -0.0015,  0.0216]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0120,  0.0051,  0.0329, -0.0414, -0.0389],\n",
      "        [-0.0197,  0.0783, -0.0099,  0.0139,  0.0254],\n",
      "        [ 0.0159, -0.0196, -0.0121,  0.0121,  0.0420],\n",
      "        [-0.0043,  0.0424,  0.0518,  0.0094, -0.0062],\n",
      "        [ 0.0077, -0.0056,  0.0246, -0.0210,  0.0087],\n",
      "        [-0.0675,  0.0278, -0.0410, -0.0262, -0.0184],\n",
      "        [ 0.0150,  0.0517,  0.0779, -0.0100, -0.0229],\n",
      "        [-0.0123,  0.0033, -0.0445, -0.0007, -0.0549]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0228, grad_fn=<MinBackward1>), tensor(0.9335, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.17283685505390167\n",
      "@sample 461: tensor([[-0.0180,  0.0155,  0.0051,  0.0057, -0.0180],\n",
      "        [ 0.0117,  0.0013, -0.0195,  0.0583, -0.0008],\n",
      "        [ 0.0111, -0.0263, -0.0248,  0.0143,  0.0281],\n",
      "        [ 0.0068, -0.0311,  0.0651, -0.0068, -0.0369],\n",
      "        [-0.0292, -0.0425, -0.0004,  0.0264, -0.0034],\n",
      "        [ 0.0014, -0.0199, -0.0147, -0.0146,  0.0136],\n",
      "        [-0.0208, -0.0090, -0.0467,  0.0363, -0.0021],\n",
      "        [ 0.0424,  0.0054, -0.0399,  0.0174,  0.0050]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0055,  0.0228,  0.0291, -0.0149, -0.0030],\n",
      "        [ 0.0627,  0.0471,  0.0257, -0.0520, -0.0603],\n",
      "        [ 0.0305, -0.0229, -0.0687,  0.0308,  0.0071],\n",
      "        [-0.0152, -0.0129, -0.0212,  0.0042,  0.0213],\n",
      "        [-0.0123, -0.0378, -0.0138, -0.0162, -0.0350],\n",
      "        [-0.0012, -0.0194,  0.0170, -0.0036, -0.0107],\n",
      "        [ 0.0477, -0.0085,  0.0496, -0.0092,  0.0207],\n",
      "        [ 0.0225,  0.0229, -0.0429,  0.0070, -0.0299]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0253, grad_fn=<MinBackward1>), tensor(0.9178, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.16940730810165405\n",
      "@sample 462: tensor([[-0.0437, -0.0130, -0.0316, -0.0275, -0.0063],\n",
      "        [-0.0067,  0.0292, -0.0393,  0.0003,  0.0036],\n",
      "        [ 0.0043,  0.0526,  0.0091, -0.0418,  0.0165],\n",
      "        [-0.0068,  0.0673,  0.0213, -0.0574,  0.0038],\n",
      "        [ 0.0149,  0.0056, -0.0207, -0.0179, -0.0361],\n",
      "        [ 0.0248, -0.0469, -0.0096,  0.0571, -0.0323],\n",
      "        [-0.0028,  0.0186, -0.0350,  0.0561, -0.0236],\n",
      "        [ 0.0016,  0.0110, -0.0039,  0.0246, -0.0108]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0158, -0.0312,  0.0212, -0.0093,  0.0103],\n",
      "        [-0.0120,  0.0141, -0.0183, -0.0141, -0.0354],\n",
      "        [-0.0318, -0.0088, -0.0572,  0.0660,  0.0162],\n",
      "        [-0.0301, -0.0298, -0.0906,  0.0526,  0.0205],\n",
      "        [ 0.0169, -0.0211, -0.0641,  0.0617,  0.0309],\n",
      "        [ 0.0103,  0.0215,  0.0466, -0.0521, -0.0176],\n",
      "        [ 0.0757,  0.0638,  0.0619,  0.0062,  0.0175],\n",
      "        [ 0.0620,  0.0397,  0.0161, -0.0849, -0.0427]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0197, grad_fn=<MinBackward1>), tensor(0.8951, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.18936319649219513\n",
      "@sample 463: tensor([[ 0.0206, -0.0077, -0.0207, -0.0068,  0.0074],\n",
      "        [-0.0226, -0.0327, -0.0342,  0.0315, -0.0162],\n",
      "        [-0.0098, -0.0541,  0.0107,  0.0538,  0.0121],\n",
      "        [-0.0121,  0.0328,  0.0280, -0.0122,  0.0173],\n",
      "        [ 0.0096,  0.0048, -0.0025, -0.0157,  0.0126],\n",
      "        [-0.0105, -0.0264, -0.0198,  0.0194, -0.0039],\n",
      "        [-0.0125, -0.0303, -0.0091,  0.0108, -0.0063],\n",
      "        [-0.0084,  0.0494, -0.0039, -0.0308,  0.0330]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0368, -0.0118,  0.0082, -0.0068,  0.0182],\n",
      "        [-0.0400, -0.0049, -0.0757, -0.0235,  0.0092],\n",
      "        [ 0.0276, -0.0119,  0.0417, -0.0605, -0.0131],\n",
      "        [-0.0300,  0.0009, -0.0149,  0.0384, -0.0967],\n",
      "        [ 0.0126, -0.0532, -0.0522,  0.0481, -0.0043],\n",
      "        [ 0.0009, -0.0003,  0.0515, -0.0081, -0.0027],\n",
      "        [ 0.0099,  0.0559,  0.0341, -0.0156,  0.0350],\n",
      "        [-0.0049,  0.0098, -0.0830,  0.0047,  0.0098]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0106, grad_fn=<MinBackward1>), tensor(0.9572, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1833667755126953\n",
      "@sample 464: tensor([[-0.0443,  0.0564,  0.0035, -0.0354,  0.0528],\n",
      "        [-0.0179,  0.0500,  0.0382, -0.0310,  0.0367],\n",
      "        [-0.0018, -0.0058, -0.0074,  0.0294,  0.0165],\n",
      "        [-0.0062, -0.0337,  0.0159,  0.0114, -0.0376],\n",
      "        [ 0.0027,  0.0049, -0.0143, -0.0502,  0.0456],\n",
      "        [-0.0002,  0.0089,  0.0139,  0.0324, -0.0300],\n",
      "        [-0.0320,  0.0164, -0.0074, -0.0384, -0.0016],\n",
      "        [-0.0151, -0.0136, -0.0017,  0.0295, -0.0300]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0067, -0.0352, -0.0795,  0.0239, -0.0026],\n",
      "        [-0.0326, -0.0132, -0.0597,  0.0646,  0.0137],\n",
      "        [ 0.0204,  0.0214,  0.0213, -0.0150, -0.0301],\n",
      "        [ 0.0215,  0.0088,  0.0366, -0.0377,  0.0028],\n",
      "        [ 0.0032, -0.0372, -0.0578,  0.0287, -0.0247],\n",
      "        [ 0.0275,  0.0405,  0.0679, -0.0533,  0.0053],\n",
      "        [ 0.0267, -0.0501, -0.1045,  0.0734,  0.0732],\n",
      "        [ 0.0464,  0.0224,  0.0694, -0.0583,  0.0422]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0200, grad_fn=<MinBackward1>), tensor(0.8994, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.17425519227981567\n",
      "@sample 465: tensor([[ 0.0007, -0.0302, -0.0005,  0.0784, -0.0212],\n",
      "        [-0.0215,  0.0270,  0.0073, -0.0261,  0.0349],\n",
      "        [-0.0097, -0.0190,  0.0060,  0.0272,  0.0089],\n",
      "        [-0.0002,  0.0113,  0.0006, -0.0096,  0.0114],\n",
      "        [-0.0025,  0.0531,  0.0360, -0.0616,  0.0470],\n",
      "        [-0.0025, -0.0315,  0.0138, -0.0378,  0.0469],\n",
      "        [ 0.0209,  0.0246, -0.0147, -0.0257,  0.0390],\n",
      "        [ 0.0087,  0.0239,  0.0485,  0.0052, -0.0317]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0358,  0.0419, -0.0042, -0.0341, -0.0314],\n",
      "        [-0.0170,  0.0067, -0.0353, -0.0028,  0.0266],\n",
      "        [ 0.0209, -0.0175,  0.0130, -0.0284, -0.0018],\n",
      "        [ 0.0004,  0.0067,  0.0077, -0.0005, -0.0094],\n",
      "        [-0.0163, -0.0224, -0.0821,  0.0894,  0.0231],\n",
      "        [-0.0360, -0.0407, -0.0868,  0.0049,  0.0406],\n",
      "        [ 0.0014, -0.0340, -0.0457,  0.0041, -0.0013],\n",
      "        [-0.0734,  0.0018, -0.0191,  0.0135,  0.0165]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0135, grad_fn=<MinBackward1>), tensor(0.9438, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.17528460919857025\n",
      "@sample 466: tensor([[ 0.0108,  0.0562, -0.0166,  0.0078, -0.0045],\n",
      "        [-0.0415,  0.0585,  0.0194, -0.0471,  0.0142],\n",
      "        [-0.0589, -0.0249,  0.0380,  0.0049, -0.0125],\n",
      "        [-0.0190,  0.0940,  0.0638, -0.0348,  0.0050],\n",
      "        [-0.0141, -0.0054,  0.0218, -0.0209,  0.0360],\n",
      "        [-0.0189, -0.0362, -0.0264, -0.0011, -0.0184],\n",
      "        [ 0.0005, -0.0076,  0.0312, -0.0623,  0.0544],\n",
      "        [-0.0442,  0.0016,  0.0127, -0.0388,  0.0004]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0120, -0.0389, -0.0598,  0.0370, -0.0004],\n",
      "        [ 0.0121, -0.0381, -0.0663,  0.0032,  0.0292],\n",
      "        [ 0.0400,  0.0319,  0.0836, -0.0928, -0.0153],\n",
      "        [-0.0294, -0.0484, -0.0967,  0.0091, -0.0655],\n",
      "        [ 0.0057, -0.0059,  0.0156, -0.0352,  0.0548],\n",
      "        [-0.0272, -0.0299,  0.0125, -0.0416,  0.0003],\n",
      "        [-0.0139, -0.0864, -0.1111,  0.0022, -0.0358],\n",
      "        [ 0.0016, -0.0356,  0.0484,  0.0061,  0.0081]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0120, grad_fn=<MinBackward1>), tensor(0.9025, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.19152598083019257\n",
      "@sample 467: tensor([[-0.0064, -0.0002,  0.0323, -0.0810,  0.0348],\n",
      "        [ 0.0209,  0.0549,  0.0006, -0.0289,  0.0065],\n",
      "        [-0.0415,  0.0418,  0.0282, -0.0564,  0.0666],\n",
      "        [-0.0298, -0.0200,  0.0086, -0.0234,  0.0147],\n",
      "        [-0.0313,  0.0461,  0.0246, -0.0348,  0.0231],\n",
      "        [-0.0071,  0.0439,  0.0249, -0.0391,  0.0390],\n",
      "        [-0.0338,  0.0142, -0.0193, -0.0215, -0.0253],\n",
      "        [-0.0056, -0.0304,  0.0204,  0.0038, -0.0337]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0458, -0.0584, -0.0261,  0.0216,  0.0066],\n",
      "        [-0.0631, -0.0262, -0.1184,  0.0542,  0.0257],\n",
      "        [ 0.0047, -0.0238,  0.0077,  0.0452, -0.0059],\n",
      "        [-0.0481,  0.0133, -0.0180, -0.0237,  0.0196],\n",
      "        [-0.0291, -0.0742, -0.1354,  0.0402, -0.0184],\n",
      "        [-0.0400, -0.0161,  0.0027,  0.0183, -0.0491],\n",
      "        [-0.0003, -0.0090,  0.0829, -0.0483,  0.0078],\n",
      "        [-0.0237, -0.0122,  0.0460,  0.0004, -0.0032]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0151, grad_fn=<MinBackward1>), tensor(0.9281, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1884801983833313\n",
      "@sample 468: tensor([[ 0.0024, -0.0070, -0.0099, -0.0362, -0.0023],\n",
      "        [-0.0206, -0.0164,  0.0204,  0.0117,  0.0064],\n",
      "        [ 0.0128,  0.0294,  0.0196, -0.0216,  0.0011],\n",
      "        [-0.0047,  0.0107, -0.0039, -0.0044,  0.0039],\n",
      "        [-0.0104, -0.0178,  0.0136, -0.0348,  0.0124],\n",
      "        [-0.0074,  0.0724,  0.0019, -0.0347, -0.0199],\n",
      "        [-0.0029,  0.0143,  0.0146, -0.0143,  0.0005],\n",
      "        [-0.0183,  0.0098,  0.0158,  0.0183,  0.0339]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0014,  0.0043,  0.0209,  0.0118,  0.0219],\n",
      "        [-0.0170, -0.0186,  0.0153, -0.0045, -0.0106],\n",
      "        [-0.0195, -0.0263, -0.0173,  0.0012,  0.0091],\n",
      "        [-0.0022,  0.0266,  0.0441, -0.0276,  0.0122],\n",
      "        [-0.0264, -0.0338,  0.0096, -0.0098, -0.0231],\n",
      "        [-0.0486, -0.0192, -0.0800, -0.0268, -0.0313],\n",
      "        [-0.0171, -0.0141, -0.0287,  0.0168,  0.0364],\n",
      "        [ 0.0311,  0.0371, -0.0109,  0.0137,  0.0254]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0184, grad_fn=<MinBackward1>), tensor(0.9502, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.16216813027858734\n",
      "@sample 469: tensor([[-0.0270,  0.0685, -0.0222, -0.0318, -0.0441],\n",
      "        [ 0.0014,  0.0165, -0.0210, -0.0298, -0.0187],\n",
      "        [ 0.0150, -0.0065,  0.0207, -0.0197, -0.0003],\n",
      "        [-0.0203,  0.0103,  0.0418, -0.0443,  0.0107],\n",
      "        [-0.0161,  0.0519,  0.0277, -0.0844,  0.0071],\n",
      "        [-0.0213,  0.0053,  0.0222,  0.0258, -0.0150],\n",
      "        [-0.0419,  0.0477, -0.0142, -0.0112, -0.0239],\n",
      "        [ 0.0057,  0.0350,  0.0213, -0.0457,  0.0321]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0030,  0.0123, -0.0438,  0.0157,  0.0384],\n",
      "        [-0.0206, -0.0065,  0.0737, -0.0661,  0.0048],\n",
      "        [-0.0404,  0.0238, -0.0013,  0.0137,  0.0225],\n",
      "        [-0.0116, -0.0185, -0.0278,  0.0087,  0.0118],\n",
      "        [-0.0196, -0.0210, -0.0262,  0.0104, -0.0332],\n",
      "        [-0.0311,  0.0287, -0.0704,  0.0132, -0.0207],\n",
      "        [-0.0476, -0.0064, -0.0725,  0.0523,  0.0101],\n",
      "        [-0.0050, -0.0377, -0.0624,  0.0341,  0.0114]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0196, grad_fn=<MinBackward1>), tensor(0.9410, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1839270293712616\n",
      "@sample 470: tensor([[-0.0138, -0.0180, -0.0272,  0.0350, -0.0324],\n",
      "        [ 0.0059,  0.0179,  0.0148, -0.0307,  0.0140],\n",
      "        [ 0.0210,  0.0771,  0.0087, -0.0604,  0.0112],\n",
      "        [ 0.0098,  0.0247,  0.0060, -0.0311,  0.0272],\n",
      "        [-0.0167,  0.0163,  0.0367, -0.0061,  0.0346],\n",
      "        [ 0.0089,  0.0230, -0.0332,  0.0221, -0.0480],\n",
      "        [-0.0035, -0.0194, -0.0105,  0.0357, -0.0120],\n",
      "        [-0.0287,  0.0162,  0.0240, -0.0419, -0.0427]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0219,  0.0437,  0.0307, -0.0613, -0.0543],\n",
      "        [-0.0252, -0.0159, -0.0264,  0.0225,  0.0132],\n",
      "        [-0.0405,  0.0103, -0.1080,  0.0564,  0.0146],\n",
      "        [ 0.0219, -0.0371, -0.0542,  0.0148,  0.0194],\n",
      "        [-0.0430,  0.0175, -0.0408, -0.0214,  0.0071],\n",
      "        [-0.0427,  0.0103,  0.0221,  0.0317, -0.0164],\n",
      "        [-0.0035,  0.0005, -0.0456,  0.0486,  0.0697],\n",
      "        [-0.0418, -0.0416,  0.0087,  0.0076,  0.0251]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0176, grad_fn=<MinBackward1>), tensor(0.9477, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.17915090918540955\n",
      "@sample 471: tensor([[-0.0395, -0.0446,  0.0258,  0.0347, -0.0035],\n",
      "        [-0.0014, -0.0091,  0.0066, -0.0193,  0.0124],\n",
      "        [-0.0229,  0.0284,  0.0404, -0.0861,  0.1004],\n",
      "        [-0.0039,  0.0476,  0.0033, -0.0343,  0.0110],\n",
      "        [ 0.0061,  0.0368,  0.0178, -0.0123, -0.0191],\n",
      "        [-0.0101, -0.0081, -0.0027, -0.0750,  0.0035],\n",
      "        [-0.0023,  0.0048,  0.0126,  0.0130,  0.0100],\n",
      "        [-0.0126, -0.0026, -0.0096,  0.0261,  0.0209]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0032,  0.0161,  0.0472,  0.0204, -0.0003],\n",
      "        [-0.0116, -0.0173,  0.0021, -0.0021,  0.0132],\n",
      "        [-0.0498, -0.0341, -0.0967,  0.0362, -0.0265],\n",
      "        [-0.0331, -0.0490, -0.0895,  0.0259,  0.0024],\n",
      "        [-0.0605,  0.0144, -0.0127, -0.0097, -0.0186],\n",
      "        [-0.0312, -0.0428, -0.0050, -0.0228, -0.0092],\n",
      "        [ 0.0210,  0.0213,  0.0601, -0.0536,  0.0434],\n",
      "        [ 0.0103, -0.0250, -0.0399,  0.0303, -0.0209]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0126, grad_fn=<MinBackward1>), tensor(0.9484, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1863258332014084\n",
      "@sample 472: tensor([[ 4.4312e-03,  2.9063e-02,  2.5836e-02,  8.0823e-03,  7.5191e-05],\n",
      "        [ 7.7565e-03,  1.2420e-02, -8.2291e-03,  3.8305e-02, -4.7974e-02],\n",
      "        [-1.5259e-02,  2.3458e-02, -6.0086e-03, -2.2726e-02,  1.7234e-02],\n",
      "        [ 1.4767e-02,  2.8478e-02,  8.5589e-03, -1.8515e-02,  2.5886e-02],\n",
      "        [-9.9511e-03, -1.0935e-02, -1.6906e-02, -3.8218e-02,  3.8775e-03],\n",
      "        [ 8.2828e-03,  1.3639e-02, -3.5348e-03, -1.4204e-02, -7.0684e-03],\n",
      "        [-4.0940e-03,  1.5117e-02,  4.0797e-02, -4.1507e-02,  3.0277e-02],\n",
      "        [ 1.1476e-02,  4.0282e-03,  9.4515e-04, -5.6542e-02,  4.3654e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0662,  0.0157, -0.0911,  0.0278,  0.0025],\n",
      "        [ 0.0066,  0.0445,  0.0189, -0.0292,  0.0409],\n",
      "        [-0.0200,  0.0162,  0.0573, -0.0248, -0.0240],\n",
      "        [-0.0202, -0.0256, -0.0474,  0.0398,  0.0224],\n",
      "        [ 0.0045, -0.0252,  0.0194, -0.0218, -0.0029],\n",
      "        [-0.0315,  0.0136, -0.0147,  0.0400,  0.0262],\n",
      "        [-0.0003, -0.0496, -0.0255,  0.0211,  0.0412],\n",
      "        [ 0.0005, -0.0232, -0.0403,  0.0185, -0.0210]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0216, grad_fn=<MinBackward1>), tensor(0.9510, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.16174772381782532\n",
      "@sample 473: tensor([[ 1.3576e-02, -4.4068e-02, -3.1389e-02,  4.6075e-02, -2.7357e-02],\n",
      "        [-1.6057e-02, -1.3009e-02, -1.2520e-02,  1.3673e-02,  2.8659e-02],\n",
      "        [-1.6076e-02,  3.9144e-02,  1.3133e-02, -6.8874e-02, -3.9948e-03],\n",
      "        [-2.9337e-02,  8.6793e-02,  7.1316e-02, -2.8294e-02,  1.0783e-02],\n",
      "        [-1.0257e-02,  2.7484e-02, -1.0314e-02, -2.0547e-02,  2.0834e-03],\n",
      "        [ 7.8205e-05,  5.0507e-02,  2.4331e-02, -3.5886e-02, -2.0003e-02],\n",
      "        [ 2.0651e-02, -3.2505e-02,  4.2795e-03, -2.5651e-02, -8.0050e-03],\n",
      "        [-2.7571e-02,  1.2035e-02,  1.0806e-03, -5.6236e-03,  1.6568e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0056, -0.0089,  0.0735, -0.0491, -0.0226],\n",
      "        [ 0.0533,  0.0090, -0.0284, -0.0056,  0.0256],\n",
      "        [-0.0336, -0.0349, -0.0360,  0.0588,  0.0198],\n",
      "        [-0.0145, -0.0500, -0.1152,  0.0148, -0.0328],\n",
      "        [-0.0210, -0.0160, -0.0279, -0.0406, -0.0232],\n",
      "        [-0.0356,  0.0045, -0.0428, -0.0014, -0.0119],\n",
      "        [-0.0380, -0.0083,  0.0593, -0.0048, -0.0191],\n",
      "        [-0.0128,  0.0051,  0.0112,  0.0236, -0.0571]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0175, grad_fn=<MinBackward1>), tensor(0.9499, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.17502444982528687\n",
      "@sample 474: tensor([[ 5.2331e-02, -2.5231e-02,  1.5696e-02, -1.5106e-02, -2.2866e-05],\n",
      "        [ 7.2208e-04,  2.3803e-02,  2.2584e-02,  1.2385e-03, -7.8172e-03],\n",
      "        [-3.8551e-02,  3.4406e-02, -3.9556e-03, -5.1816e-02, -1.4342e-02],\n",
      "        [ 4.5551e-03,  2.0794e-02, -2.5763e-02,  3.9385e-02, -4.6075e-02],\n",
      "        [-1.5449e-02,  1.4721e-02,  1.7095e-02,  3.4035e-02, -2.3077e-02],\n",
      "        [ 1.7856e-02, -3.5816e-03, -3.5774e-02,  5.5158e-02, -1.8554e-02],\n",
      "        [ 9.7968e-03, -8.1046e-02,  6.3449e-03,  3.2958e-02, -1.8103e-02],\n",
      "        [-2.8976e-02, -3.3761e-03,  7.1764e-03, -8.3082e-03, -1.7081e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0236, -0.0054, -0.0369,  0.0076, -0.0044],\n",
      "        [-0.0139,  0.0487, -0.0219,  0.0245, -0.0011],\n",
      "        [ 0.0155, -0.0374, -0.0240, -0.0230, -0.0187],\n",
      "        [ 0.0193,  0.0202,  0.0240,  0.0272,  0.0137],\n",
      "        [ 0.0048,  0.0365,  0.0078, -0.0100, -0.0011],\n",
      "        [ 0.0200,  0.0349, -0.0859,  0.0049,  0.0441],\n",
      "        [ 0.0158,  0.0469,  0.0973, -0.0183, -0.0027],\n",
      "        [-0.0257, -0.0097,  0.0630, -0.0556, -0.0346]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0173, grad_fn=<MinBackward1>), tensor(0.9261, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.17795926332473755\n",
      "@sample 475: tensor([[-0.0101,  0.0577, -0.0033, -0.0169,  0.0090],\n",
      "        [-0.0159,  0.0101, -0.0423, -0.0175,  0.0210],\n",
      "        [ 0.0136,  0.0400, -0.0401,  0.0116, -0.0142],\n",
      "        [ 0.0044, -0.0334, -0.0166, -0.0131, -0.0044],\n",
      "        [-0.0184,  0.0650, -0.0245,  0.0293,  0.0349],\n",
      "        [ 0.0044,  0.0994,  0.0040, -0.0558, -0.0070],\n",
      "        [ 0.0144,  0.0081,  0.0152, -0.0066,  0.0528],\n",
      "        [-0.0376,  0.0185, -0.0527,  0.0539, -0.0065]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0147,  0.0065, -0.0985,  0.0677,  0.0428],\n",
      "        [ 0.0464,  0.0043, -0.0102, -0.0502,  0.0119],\n",
      "        [ 0.0205, -0.0033, -0.0561,  0.0158,  0.0384],\n",
      "        [-0.0009, -0.0205, -0.0046, -0.0412, -0.0045],\n",
      "        [ 0.0323, -0.0061, -0.0312, -0.0330,  0.0235],\n",
      "        [-0.0352, -0.0380, -0.0451,  0.0762, -0.0279],\n",
      "        [-0.0233, -0.0247, -0.0410,  0.0362, -0.0084],\n",
      "        [ 0.0249,  0.0317,  0.0031, -0.0048, -0.0202]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0287, grad_fn=<MinBackward1>), tensor(0.9125, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.17754270136356354\n",
      "@sample 476: tensor([[-0.0083,  0.0059,  0.0249,  0.0213,  0.0312],\n",
      "        [ 0.0061,  0.0023, -0.0815,  0.0132,  0.0144],\n",
      "        [ 0.0208, -0.0117, -0.0302,  0.0179,  0.0007],\n",
      "        [ 0.0097, -0.0019, -0.0022, -0.0408, -0.0021],\n",
      "        [ 0.0154,  0.0295,  0.0307, -0.0058,  0.0231],\n",
      "        [-0.0173, -0.0180, -0.0589,  0.0029,  0.0018],\n",
      "        [ 0.0120,  0.0286, -0.0275, -0.0404,  0.0312],\n",
      "        [-0.0067,  0.0202,  0.0070, -0.0373, -0.0048]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0142,  0.0428, -0.0281,  0.0295,  0.0049],\n",
      "        [ 0.0436, -0.0264, -0.0350, -0.0036, -0.0217],\n",
      "        [-0.0112,  0.0007, -0.0226,  0.0378,  0.0371],\n",
      "        [-0.0396, -0.0214, -0.0398,  0.0261, -0.0216],\n",
      "        [-0.0031, -0.0115, -0.0509, -0.0073, -0.0170],\n",
      "        [-0.0173,  0.0156, -0.0829, -0.0122,  0.0008],\n",
      "        [-0.0248, -0.0359, -0.0875,  0.0493, -0.0042],\n",
      "        [-0.0197,  0.0015, -0.0044,  0.0237, -0.0017]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0192, grad_fn=<MinBackward1>), tensor(0.9240, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.16327151656150818\n",
      "@sample 477: tensor([[-0.0121,  0.0274,  0.0331, -0.0052, -0.0382],\n",
      "        [ 0.0272,  0.0158, -0.0253, -0.0474,  0.0153],\n",
      "        [-0.0182, -0.0067, -0.0081,  0.0107,  0.0084],\n",
      "        [-0.0246,  0.0166,  0.0163,  0.0037,  0.0126],\n",
      "        [ 0.0080, -0.0360, -0.0576,  0.0661, -0.0050],\n",
      "        [-0.0092, -0.0440, -0.0476,  0.0498,  0.0076],\n",
      "        [ 0.0108,  0.0099, -0.0030,  0.0223, -0.0612],\n",
      "        [ 0.0004, -0.0182, -0.0471,  0.0069,  0.0003]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0326,  0.0065,  0.0124, -0.0024, -0.0002],\n",
      "        [ 0.0139, -0.0245, -0.0138,  0.0019, -0.0605],\n",
      "        [-0.0009,  0.0049,  0.0101, -0.0295,  0.0248],\n",
      "        [-0.0110, -0.0022, -0.0297, -0.0003,  0.0197],\n",
      "        [ 0.0009,  0.0318,  0.0944, -0.0661,  0.0034],\n",
      "        [ 0.0311,  0.0188,  0.0448, -0.0016,  0.0054],\n",
      "        [-0.0140,  0.0281, -0.0192,  0.0185, -0.0080],\n",
      "        [ 0.0163, -0.0218,  0.0363, -0.0159, -0.0256]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0215, grad_fn=<MinBackward1>), tensor(0.9605, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.17469573020935059\n",
      "@sample 478: tensor([[ 0.0202, -0.0413, -0.0242,  0.0461, -0.0091],\n",
      "        [-0.0094, -0.0574, -0.0407,  0.0375,  0.0143],\n",
      "        [-0.0225,  0.0450, -0.0054, -0.0593, -0.0155],\n",
      "        [ 0.0210, -0.0199, -0.0469,  0.0648, -0.0205],\n",
      "        [-0.0296, -0.0098,  0.0448, -0.0134,  0.0026],\n",
      "        [-0.0301,  0.0253,  0.0102, -0.0449,  0.0301],\n",
      "        [-0.0130, -0.0019, -0.0317, -0.0093,  0.0172],\n",
      "        [-0.0043,  0.0157, -0.0082, -0.0073,  0.0010]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0039,  0.0284,  0.0595, -0.0708, -0.0389],\n",
      "        [ 0.0681, -0.0300, -0.0190, -0.0352, -0.0236],\n",
      "        [-0.0564, -0.0600, -0.0258,  0.0249, -0.0209],\n",
      "        [ 0.0079,  0.0698,  0.0335, -0.0300, -0.0132],\n",
      "        [-0.0246,  0.0229,  0.0052,  0.0214,  0.0058],\n",
      "        [ 0.0129,  0.0034,  0.0316, -0.0318, -0.0006],\n",
      "        [ 0.0008, -0.0232,  0.0182, -0.0143, -0.0208],\n",
      "        [ 0.0250, -0.0183, -0.0167, -0.0043, -0.0097]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0190, grad_fn=<MinBackward1>), tensor(0.9443, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.17397436499595642\n",
      "@sample 479: tensor([[ 0.0182, -0.0381, -0.0109, -0.0433,  0.0284],\n",
      "        [-0.0121,  0.0559,  0.0405, -0.0903,  0.0439],\n",
      "        [ 0.0194, -0.0089,  0.0506, -0.0299,  0.0379],\n",
      "        [ 0.0081, -0.0710, -0.0027,  0.0293,  0.0100],\n",
      "        [-0.0155,  0.0152,  0.0561, -0.0665,  0.0327],\n",
      "        [-0.0044, -0.0255, -0.0124, -0.0182,  0.0310],\n",
      "        [-0.0075,  0.0358, -0.0076, -0.0476,  0.0133],\n",
      "        [-0.0189, -0.0292,  0.0318, -0.0440,  0.0307]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0051, -0.0374, -0.0261,  0.0308, -0.0238],\n",
      "        [-0.0566, -0.0378, -0.0882,  0.0299, -0.0183],\n",
      "        [ 0.0195,  0.0018, -0.1027,  0.0353, -0.0195],\n",
      "        [-0.0038,  0.0219,  0.1162, -0.0530, -0.0462],\n",
      "        [-0.0396, -0.0382, -0.0330,  0.0438,  0.0229],\n",
      "        [ 0.0084, -0.0078,  0.0331, -0.0110,  0.0018],\n",
      "        [-0.0136, -0.0447, -0.0352, -0.0318, -0.0435],\n",
      "        [-0.0457, -0.0215,  0.0043, -0.0072, -0.0429]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0223, grad_fn=<MinBackward1>), tensor(0.9212, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.17672497034072876\n",
      "@sample 480: tensor([[-4.0255e-02, -1.8843e-02, -1.2371e-02,  2.3148e-02, -1.4898e-02],\n",
      "        [ 5.1555e-02,  3.5075e-02,  3.7125e-02,  1.5360e-04,  5.0809e-03],\n",
      "        [ 2.7128e-02,  1.0650e-02, -2.0219e-02,  8.5226e-04, -7.6980e-03],\n",
      "        [-9.3039e-03,  1.7577e-02,  1.5563e-02, -4.3052e-02,  5.8507e-03],\n",
      "        [ 1.8956e-02,  5.4227e-02,  1.2103e-02, -7.4252e-02,  5.7826e-02],\n",
      "        [ 1.8389e-02,  4.9213e-02,  1.9864e-02, -1.2993e-02,  2.2695e-02],\n",
      "        [ 2.0014e-02, -2.7148e-02, -7.5869e-05, -6.9694e-03, -1.1527e-03],\n",
      "        [ 5.8276e-02, -3.3054e-03,  2.1673e-02, -7.3589e-03, -3.8465e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0492,  0.0104,  0.0351, -0.0461,  0.0278],\n",
      "        [-0.0285,  0.0141, -0.0517,  0.0626,  0.0550],\n",
      "        [ 0.0077,  0.0059,  0.0304,  0.0171,  0.0084],\n",
      "        [-0.0058, -0.0464, -0.0206,  0.0058, -0.0643],\n",
      "        [-0.0619, -0.0431, -0.0529,  0.1202,  0.0519],\n",
      "        [-0.0054, -0.0266, -0.0250,  0.0480,  0.0046],\n",
      "        [-0.0078,  0.0491,  0.0125, -0.0316, -0.0482],\n",
      "        [-0.0178,  0.0236, -0.0615,  0.0624, -0.0246]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0141, grad_fn=<MinBackward1>), tensor(0.9013, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.17808520793914795\n",
      "@sample 481: tensor([[ 0.0103, -0.0658,  0.0231,  0.0166, -0.0017],\n",
      "        [ 0.0117, -0.0792, -0.0349,  0.1324, -0.0272],\n",
      "        [ 0.0075, -0.0241, -0.0155,  0.0358,  0.0287],\n",
      "        [ 0.0307,  0.0076, -0.0265,  0.0091,  0.0131],\n",
      "        [ 0.0072, -0.0389, -0.0138,  0.0448,  0.0062],\n",
      "        [ 0.0332,  0.0463, -0.0099,  0.0450, -0.0126],\n",
      "        [-0.0030, -0.0296,  0.0107,  0.0514, -0.0051],\n",
      "        [ 0.0272,  0.0031,  0.0244, -0.0335,  0.0399]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0186,  0.0015,  0.0441, -0.0210, -0.0176],\n",
      "        [ 0.0750,  0.0691,  0.0577, -0.0502, -0.0558],\n",
      "        [ 0.0206,  0.0512, -0.0564,  0.0101, -0.0145],\n",
      "        [-0.0206,  0.0128, -0.0108, -0.0184, -0.0145],\n",
      "        [-0.0006,  0.0491, -0.0221, -0.0068,  0.0494],\n",
      "        [-0.0305,  0.0722, -0.0799, -0.0284, -0.0207],\n",
      "        [ 0.0333,  0.0424,  0.0144, -0.0276, -0.0543],\n",
      "        [-0.0119, -0.0380, -0.0436, -0.0268, -0.0754]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0262, grad_fn=<MinBackward1>), tensor(0.9438, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1930685192346573\n",
      "@sample 482: tensor([[ 0.0266,  0.0206,  0.0123, -0.0095,  0.0124],\n",
      "        [ 0.0242, -0.0111, -0.0072,  0.0191, -0.0038],\n",
      "        [ 0.0291, -0.0103, -0.0138, -0.0049,  0.0014],\n",
      "        [ 0.0112, -0.0398,  0.0070, -0.0020,  0.0312],\n",
      "        [ 0.0327, -0.0139,  0.0027, -0.0013, -0.0124],\n",
      "        [-0.0041, -0.0058, -0.0389,  0.0296, -0.0106],\n",
      "        [ 0.0311, -0.0214,  0.0040,  0.0253,  0.0221],\n",
      "        [ 0.0316, -0.0399, -0.0122,  0.0362, -0.0195]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0052,  0.0068, -0.0371,  0.0046,  0.0033],\n",
      "        [-0.0080, -0.0086, -0.0142,  0.0237,  0.0117],\n",
      "        [ 0.0102, -0.0155, -0.0450,  0.0091, -0.0122],\n",
      "        [-0.0278,  0.0263, -0.0491,  0.0517, -0.0501],\n",
      "        [-0.0254,  0.0246, -0.0696,  0.0611,  0.0160],\n",
      "        [ 0.0297,  0.0241, -0.0055, -0.0169,  0.0026],\n",
      "        [ 0.0133,  0.0614, -0.0079,  0.0491, -0.0130],\n",
      "        [ 0.0200,  0.0025,  0.0227, -0.0065,  0.0392]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0107, grad_fn=<MinBackward1>), tensor(0.9487, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.15888166427612305\n",
      "@sample 483: tensor([[ 0.0128,  0.0215,  0.0369,  0.0012,  0.0280],\n",
      "        [-0.0006, -0.0098,  0.0021, -0.0008,  0.0055],\n",
      "        [-0.0202, -0.0169, -0.0186, -0.0490,  0.0593],\n",
      "        [ 0.0071, -0.0224,  0.0232,  0.0657, -0.0483],\n",
      "        [-0.0002,  0.0485,  0.0400, -0.0238,  0.0519],\n",
      "        [ 0.0459, -0.0259,  0.0160,  0.0598, -0.0308],\n",
      "        [ 0.0130,  0.0160, -0.0158, -0.0078,  0.0079],\n",
      "        [ 0.0107,  0.0064, -0.0160, -0.0102,  0.0304]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0250,  0.0118, -0.0213,  0.0380,  0.0058],\n",
      "        [ 0.0190, -0.0082, -0.0092,  0.0143, -0.0118],\n",
      "        [-0.0541, -0.0758, -0.0655, -0.0055,  0.0005],\n",
      "        [ 0.0247,  0.0531,  0.0226, -0.0216,  0.0180],\n",
      "        [ 0.0009, -0.0389, -0.0670,  0.0553,  0.0346],\n",
      "        [ 0.0103,  0.0912,  0.0082, -0.0248,  0.0118],\n",
      "        [-0.0129, -0.0222, -0.0138,  0.0294,  0.0136],\n",
      "        [-0.0110, -0.0047, -0.0187,  0.0302, -0.0264]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0147, grad_fn=<MinBackward1>), tensor(0.9324, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.18107542395591736\n",
      "@sample 484: tensor([[ 0.0086,  0.0814,  0.0207, -0.0279,  0.0271],\n",
      "        [ 0.0007,  0.0261,  0.0159, -0.0287,  0.0566],\n",
      "        [ 0.0122,  0.0097,  0.0013,  0.0158, -0.0237],\n",
      "        [ 0.0388,  0.0371, -0.0907, -0.0055, -0.0097],\n",
      "        [-0.0214,  0.0027, -0.0059,  0.0197, -0.0150],\n",
      "        [ 0.0075, -0.0575, -0.0403,  0.0125,  0.0017],\n",
      "        [ 0.0243,  0.0031,  0.0054, -0.0390,  0.0154],\n",
      "        [-0.0203, -0.0180,  0.0349, -0.0030, -0.0065]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-1.7928e-02, -2.9591e-02, -8.7411e-02,  7.0583e-02,  6.8697e-03],\n",
      "        [-2.5721e-02, -1.7800e-02, -4.8102e-02,  1.6109e-02,  2.2296e-03],\n",
      "        [-1.3227e-02,  1.2510e-03,  2.3889e-02, -3.7677e-02,  3.5552e-03],\n",
      "        [ 1.7093e-02,  4.7651e-03, -2.4605e-02,  4.9413e-02, -1.5173e-02],\n",
      "        [-4.0147e-03,  1.8291e-02,  1.0477e-02,  2.2501e-06, -3.6731e-03],\n",
      "        [-6.4708e-03, -2.3769e-02,  5.3230e-02, -3.4286e-02, -4.0498e-02],\n",
      "        [-2.3401e-02,  3.4653e-03, -2.7938e-02,  1.9138e-02, -5.4522e-03],\n",
      "        [ 2.8541e-03,  3.1372e-02, -5.2422e-02,  3.6201e-03, -3.3591e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0093, grad_fn=<MinBackward1>), tensor(0.9415, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.17019058763980865\n",
      "@sample 485: tensor([[ 0.0078,  0.0390,  0.0426, -0.0017,  0.0045],\n",
      "        [-0.0139, -0.0117, -0.0141,  0.0088, -0.0613],\n",
      "        [-0.0326, -0.0327,  0.0341,  0.0282, -0.0083],\n",
      "        [-0.0107, -0.0546, -0.0438,  0.0016, -0.0051],\n",
      "        [ 0.0294, -0.0608, -0.0069,  0.0082,  0.0049],\n",
      "        [ 0.0072,  0.0109, -0.0121, -0.0487,  0.0254],\n",
      "        [ 0.0229,  0.0265, -0.0043, -0.0052,  0.0132],\n",
      "        [ 0.0127, -0.0544,  0.0151,  0.0057,  0.0425]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0043,  0.0297,  0.0040,  0.0429,  0.0282],\n",
      "        [-0.0099,  0.0131,  0.0262,  0.0020,  0.0118],\n",
      "        [-0.0156,  0.0158, -0.0157,  0.0158,  0.0030],\n",
      "        [-0.0262, -0.0451,  0.0371, -0.0539, -0.0159],\n",
      "        [-0.0352, -0.0268, -0.0483,  0.0594,  0.0114],\n",
      "        [-0.0247,  0.0027, -0.0219,  0.0097, -0.0064],\n",
      "        [ 0.0091, -0.0062, -0.0475,  0.0458,  0.0063],\n",
      "        [ 0.0459, -0.0437,  0.0089,  0.0200, -0.0045]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0137, grad_fn=<MinBackward1>), tensor(0.9310, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.17185808718204498\n",
      "@sample 486: tensor([[ 0.0121,  0.0092, -0.0130, -0.0098, -0.0170],\n",
      "        [ 0.0343, -0.0132, -0.0250,  0.0517, -0.0148],\n",
      "        [ 0.0463, -0.0454, -0.0183, -0.0272, -0.0023],\n",
      "        [-0.0150, -0.0064,  0.0001,  0.0092, -0.0086],\n",
      "        [ 0.0071, -0.0133,  0.0106,  0.0234, -0.0081],\n",
      "        [ 0.0162, -0.0062, -0.0024,  0.0125, -0.0182],\n",
      "        [ 0.0069, -0.0254,  0.0003, -0.0302,  0.0095],\n",
      "        [ 0.0357, -0.0393,  0.0289,  0.0190, -0.0134]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0380, -0.0493, -0.0569, -0.0097, -0.0368],\n",
      "        [ 0.0019,  0.0224, -0.0408, -0.0204, -0.0263],\n",
      "        [-0.0329,  0.0179, -0.0066, -0.0048,  0.0139],\n",
      "        [ 0.0435, -0.0104, -0.0470,  0.0052,  0.0307],\n",
      "        [ 0.0159, -0.0052,  0.0610, -0.0657, -0.0566],\n",
      "        [-0.0001, -0.0179,  0.0006, -0.0314,  0.0154],\n",
      "        [-0.0580, -0.0391, -0.0179,  0.0431, -0.0075],\n",
      "        [-0.0094, -0.0074,  0.0173,  0.0466,  0.0245]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0192, grad_fn=<MinBackward1>), tensor(0.9366, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.16299580037593842\n",
      "@sample 487: tensor([[ 0.0056, -0.0391, -0.0126, -0.0175,  0.0140],\n",
      "        [ 0.0290, -0.0022,  0.0443, -0.0325,  0.0173],\n",
      "        [-0.0127,  0.0002,  0.0198, -0.0011,  0.0140],\n",
      "        [-0.0034, -0.0222,  0.0039,  0.0370, -0.0156],\n",
      "        [-0.0117, -0.0319,  0.0179, -0.0243,  0.0082],\n",
      "        [ 0.0046,  0.0717,  0.0047, -0.0724, -0.0098],\n",
      "        [ 0.0198,  0.0037,  0.0069, -0.0255, -0.0030],\n",
      "        [ 0.0014, -0.0024,  0.0631, -0.0488,  0.0557]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0556, -0.0384, -0.0259, -0.0219,  0.0031],\n",
      "        [-0.0063, -0.0072, -0.0581,  0.0624,  0.0221],\n",
      "        [ 0.0065,  0.0290,  0.0292,  0.0010,  0.0095],\n",
      "        [ 0.0668,  0.0155,  0.0734, -0.0093,  0.0215],\n",
      "        [-0.0096, -0.0510,  0.0107, -0.0300, -0.0372],\n",
      "        [-0.0400, -0.0811, -0.0957,  0.0528, -0.0358],\n",
      "        [-0.0097,  0.0161, -0.0329,  0.0161, -0.0014],\n",
      "        [-0.0204, -0.0507, -0.0906,  0.0152, -0.0379]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0176, grad_fn=<MinBackward1>), tensor(0.9286, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.16771303117275238\n",
      "@sample 488: tensor([[-0.0168, -0.0325,  0.0184, -0.0232,  0.0147],\n",
      "        [-0.0129,  0.0195,  0.0326, -0.0311,  0.0408],\n",
      "        [-0.0178, -0.0316,  0.0102,  0.0115, -0.0263],\n",
      "        [-0.0071,  0.0131,  0.0375, -0.0155,  0.0385],\n",
      "        [-0.0018, -0.0120, -0.0172,  0.0231, -0.0420],\n",
      "        [-0.0105,  0.0030,  0.0332, -0.0125, -0.0189],\n",
      "        [ 0.0219,  0.0087, -0.0036, -0.0495,  0.0327],\n",
      "        [-0.0374, -0.0223, -0.0197,  0.0225,  0.0034]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0159, -0.0416, -0.0108,  0.0059, -0.0381],\n",
      "        [-0.0151, -0.0287, -0.0417,  0.0237, -0.0110],\n",
      "        [-0.0104, -0.0086,  0.0435, -0.0567, -0.0207],\n",
      "        [-0.0103,  0.0012, -0.0227,  0.0117, -0.0071],\n",
      "        [ 0.0088,  0.0278,  0.0176, -0.0346,  0.0315],\n",
      "        [-0.0057,  0.0018, -0.0116,  0.0009, -0.0060],\n",
      "        [ 0.0004, -0.0260, -0.0172, -0.0029, -0.0108],\n",
      "        [ 0.0155, -0.0392, -0.0251, -0.0233, -0.0241]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0180, grad_fn=<MinBackward1>), tensor(0.9161, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.15932360291481018\n",
      "@sample 489: tensor([[ 1.8423e-02, -4.5937e-02, -3.1462e-02,  2.4010e-02,  7.0691e-05],\n",
      "        [ 3.4839e-04, -1.9095e-02, -1.2420e-02, -3.4168e-02,  1.1707e-02],\n",
      "        [-5.9726e-03, -3.2960e-02,  1.8574e-03, -1.2029e-03,  2.4630e-03],\n",
      "        [ 2.4536e-02,  2.7105e-02,  1.4040e-02, -4.2948e-02,  3.9858e-03],\n",
      "        [ 1.9470e-02,  3.5206e-02,  2.5065e-02, -2.0151e-03,  7.1481e-03],\n",
      "        [-3.4853e-02,  7.2620e-03,  8.6519e-03, -2.3714e-02,  9.1846e-03],\n",
      "        [-1.3651e-02,  6.3663e-02,  1.7252e-02, -4.9329e-02,  6.8391e-03],\n",
      "        [ 2.0733e-03, -3.6702e-02,  3.5938e-03,  2.4337e-02,  5.1193e-04]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0694,  0.0051,  0.0881, -0.0932, -0.0261],\n",
      "        [-0.0261, -0.0120, -0.0140,  0.0202,  0.0008],\n",
      "        [-0.0003,  0.0010,  0.0428, -0.0169, -0.0241],\n",
      "        [-0.0199, -0.0554, -0.0932,  0.0351,  0.0018],\n",
      "        [-0.0241,  0.0166,  0.0131, -0.0071,  0.0119],\n",
      "        [ 0.0097, -0.0199,  0.0069,  0.0244,  0.0015],\n",
      "        [-0.0912,  0.0002, -0.0064,  0.0113, -0.0651],\n",
      "        [ 0.0256, -0.0091,  0.0795, -0.0065, -0.0064]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0260, grad_fn=<MinBackward1>), tensor(0.9105, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1675075739622116\n",
      "@sample 490: tensor([[ 0.0162,  0.0252, -0.0220, -0.0107,  0.0407],\n",
      "        [-0.0113, -0.0153,  0.0073,  0.0129, -0.0030],\n",
      "        [ 0.0114, -0.0712, -0.0265,  0.0462, -0.0035],\n",
      "        [ 0.0036, -0.0003, -0.0038,  0.0118, -0.0038],\n",
      "        [-0.0086,  0.0174, -0.0104, -0.0382, -0.0097],\n",
      "        [-0.0322, -0.0447, -0.0709,  0.0363, -0.0290],\n",
      "        [-0.0002, -0.0048, -0.0071, -0.0488,  0.0071],\n",
      "        [ 0.0012, -0.0422, -0.0200,  0.0174, -0.0673]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0450,  0.0268,  0.0094, -0.0379, -0.0419],\n",
      "        [ 0.0249,  0.0112,  0.0614, -0.0660, -0.0291],\n",
      "        [-0.0044, -0.0355,  0.0047, -0.0719, -0.0103],\n",
      "        [ 0.0093,  0.0088,  0.0358, -0.0156, -0.0157],\n",
      "        [-0.0332, -0.0032, -0.0298,  0.0102, -0.0166],\n",
      "        [ 0.0226, -0.0184,  0.0723, -0.0710, -0.0083],\n",
      "        [-0.0516, -0.0450, -0.0525,  0.0276, -0.0349],\n",
      "        [-0.0085,  0.0076,  0.0267, -0.0498,  0.0146]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0224, grad_fn=<MinBackward1>), tensor(0.9303, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.16768749058246613\n",
      "@sample 491: tensor([[-0.0216,  0.0216,  0.0096, -0.0235,  0.0355],\n",
      "        [ 0.0141, -0.0136, -0.0172, -0.0301,  0.0010],\n",
      "        [-0.0131,  0.0170, -0.0201, -0.0138,  0.0096],\n",
      "        [-0.0014,  0.0711,  0.0467, -0.0625,  0.0265],\n",
      "        [ 0.0056,  0.0321,  0.0019, -0.0003,  0.0152],\n",
      "        [-0.0355,  0.0008,  0.0170, -0.0039, -0.0258],\n",
      "        [-0.0072, -0.0007,  0.0002,  0.0155, -0.0120],\n",
      "        [-0.0029,  0.0132,  0.0195, -0.0171,  0.0069]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0210, -0.0123, -0.0336,  0.0052, -0.0097],\n",
      "        [-0.0562, -0.0197,  0.0366,  0.0062,  0.0173],\n",
      "        [-0.0348, -0.0317, -0.0451, -0.0138, -0.0025],\n",
      "        [-0.0504, -0.0496, -0.0617,  0.0399, -0.0270],\n",
      "        [ 0.0011,  0.0246, -0.0763,  0.0211,  0.0068],\n",
      "        [-0.0056, -0.0266,  0.0682, -0.0735, -0.0247],\n",
      "        [ 0.0271,  0.0159,  0.0210, -0.0033, -0.0071],\n",
      "        [-0.0394,  0.0150, -0.0022, -0.0069,  0.0373]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0192, grad_fn=<MinBackward1>), tensor(0.9330, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.16877293586730957\n",
      "@sample 492: tensor([[-0.0306,  0.0640,  0.0153, -0.0929,  0.0578],\n",
      "        [ 0.0104,  0.0370,  0.0072,  0.0269,  0.0189],\n",
      "        [-0.0356,  0.0082, -0.0044, -0.0315,  0.0118],\n",
      "        [ 0.0003,  0.0114, -0.0134,  0.0049,  0.0010],\n",
      "        [ 0.0704, -0.0167,  0.0004,  0.0232, -0.0059],\n",
      "        [ 0.0604, -0.0262, -0.0110,  0.0112,  0.0093],\n",
      "        [ 0.0355,  0.0374, -0.0146, -0.0051, -0.0663],\n",
      "        [ 0.0206, -0.0487, -0.0338,  0.0568, -0.0026]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0189, -0.0283,  0.0072,  0.0108, -0.0467],\n",
      "        [ 0.0320,  0.0009, -0.0317,  0.0338,  0.0094],\n",
      "        [ 0.0169, -0.0071, -0.0284, -0.0098,  0.0305],\n",
      "        [-0.0109, -0.0044,  0.0080,  0.0303,  0.0252],\n",
      "        [ 0.0157,  0.0282,  0.0005, -0.0274,  0.0086],\n",
      "        [-0.0069, -0.0141, -0.0529, -0.0101, -0.0137],\n",
      "        [-0.0265, -0.0003, -0.0397,  0.0058,  0.0474],\n",
      "        [ 0.0113,  0.0068,  0.0118, -0.0689,  0.0046]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0210, grad_fn=<MinBackward1>), tensor(0.9339, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1692332625389099\n",
      "@sample 493: tensor([[-0.0293,  0.0201, -0.0192,  0.0358, -0.0339],\n",
      "        [ 0.0489, -0.0121, -0.0040,  0.0440,  0.0206],\n",
      "        [ 0.0134,  0.0124, -0.0476,  0.0030, -0.0019],\n",
      "        [ 0.0139,  0.0006,  0.0039,  0.0077, -0.0077],\n",
      "        [-0.0158, -0.0096, -0.0112,  0.0221, -0.0288],\n",
      "        [ 0.0230,  0.0501, -0.0305, -0.0331,  0.0131],\n",
      "        [ 0.0132,  0.0294, -0.0025,  0.0221, -0.0131],\n",
      "        [-0.0067,  0.0322,  0.0153, -0.0421, -0.0115]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0126,  0.0187, -0.0168, -0.0254, -0.0055],\n",
      "        [ 0.0357,  0.0205,  0.0173,  0.0053, -0.0253],\n",
      "        [-0.0310, -0.0238, -0.0298,  0.0146, -0.0255],\n",
      "        [ 0.0761,  0.0202,  0.0578, -0.0594,  0.0027],\n",
      "        [ 0.0078,  0.0099,  0.0656, -0.0414, -0.0219],\n",
      "        [-0.0393, -0.0350, -0.0286,  0.0123, -0.0095],\n",
      "        [-0.0197,  0.0440, -0.0523, -0.0095, -0.0300],\n",
      "        [ 0.0406, -0.0095,  0.0058,  0.0044,  0.0357]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0199, grad_fn=<MinBackward1>), tensor(0.9231, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.16430220007896423\n",
      "@sample 494: tensor([[-0.0165,  0.0395,  0.0055, -0.0497,  0.0258],\n",
      "        [ 0.0206,  0.0275,  0.0289, -0.0284,  0.0098],\n",
      "        [-0.0136, -0.0453, -0.0341,  0.0073, -0.0238],\n",
      "        [-0.0109, -0.0212,  0.0116, -0.0023, -0.0355],\n",
      "        [-0.0043, -0.0124,  0.0158,  0.0389,  0.0314],\n",
      "        [-0.0215,  0.0036, -0.0049,  0.0054, -0.0057],\n",
      "        [-0.0327,  0.0285,  0.0170,  0.0161, -0.0303],\n",
      "        [-0.0218,  0.0726,  0.0200, -0.0711,  0.0241]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0125, -0.0290, -0.0591,  0.0650,  0.0276],\n",
      "        [-0.0187, -0.0001, -0.0657,  0.0320,  0.0217],\n",
      "        [-0.0082, -0.0255,  0.0198,  0.0166, -0.0255],\n",
      "        [-0.0373,  0.0090, -0.0011, -0.0112, -0.0331],\n",
      "        [ 0.0029,  0.0454, -0.0550, -0.0065, -0.0015],\n",
      "        [-0.0182,  0.0116, -0.0404,  0.0419,  0.0048],\n",
      "        [-0.0143,  0.0695,  0.0398, -0.0644, -0.0133],\n",
      "        [-0.0140,  0.0021, -0.1021,  0.0507,  0.0071]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0195, grad_fn=<MinBackward1>), tensor(0.9191, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.17121629416942596\n",
      "@sample 495: tensor([[-0.0167, -0.0031, -0.0300,  0.0334,  0.0045],\n",
      "        [-0.0172,  0.0106,  0.0003, -0.0219, -0.0054],\n",
      "        [-0.0314,  0.0553,  0.0095, -0.0111,  0.0245],\n",
      "        [-0.0604, -0.0002,  0.0050, -0.0202,  0.0117],\n",
      "        [-0.0069,  0.0222,  0.0184, -0.0307, -0.0123],\n",
      "        [-0.0086,  0.0073,  0.0229, -0.0135,  0.0188],\n",
      "        [-0.0047,  0.0528,  0.0041,  0.0090, -0.0382],\n",
      "        [ 0.0074, -0.0149, -0.0112,  0.0161,  0.0392]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0658,  0.0190,  0.0497, -0.0692,  0.0188],\n",
      "        [ 0.0129, -0.0550,  0.0578,  0.0358,  0.0184],\n",
      "        [ 0.0298,  0.0340, -0.0235, -0.0428,  0.0112],\n",
      "        [-0.0174, -0.0262,  0.0347, -0.0111,  0.0212],\n",
      "        [-0.0346,  0.0213,  0.0567, -0.0544, -0.0040],\n",
      "        [-0.0167, -0.0099,  0.0398, -0.0112,  0.0193],\n",
      "        [-0.0070,  0.0252, -0.0236, -0.0330,  0.0223],\n",
      "        [-0.0260,  0.0259, -0.0053,  0.0261,  0.0096]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0236, grad_fn=<MinBackward1>), tensor(0.9291, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.17243817448616028\n",
      "@sample 496: tensor([[-0.0302,  0.0156, -0.0453,  0.0332,  0.0121],\n",
      "        [-0.0146,  0.0433,  0.0134, -0.0298,  0.0384],\n",
      "        [-0.0088, -0.0277, -0.0475,  0.0256,  0.0021],\n",
      "        [-0.0260, -0.0170, -0.0198, -0.0282, -0.0017],\n",
      "        [-0.0149,  0.0098,  0.0286, -0.0238,  0.0087],\n",
      "        [-0.0161,  0.0657,  0.0318, -0.0823,  0.0155],\n",
      "        [ 0.0062, -0.0054,  0.0081, -0.0341, -0.0251],\n",
      "        [ 0.0062,  0.0128,  0.0059,  0.0364, -0.0194]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0123,  0.0317, -0.0213, -0.0312,  0.0019],\n",
      "        [ 0.0060, -0.0116,  0.0114,  0.0125, -0.0173],\n",
      "        [-0.0018,  0.0289,  0.0241, -0.0438, -0.0149],\n",
      "        [-0.0389, -0.0084,  0.0361,  0.0062,  0.0403],\n",
      "        [ 0.0185, -0.0236,  0.0074, -0.0300,  0.0016],\n",
      "        [-0.0757, -0.0143, -0.0627,  0.0245,  0.0027],\n",
      "        [-0.0344,  0.0309,  0.0143, -0.0059, -0.0070],\n",
      "        [ 0.0026,  0.0305,  0.0093, -0.0024,  0.0077]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0133, grad_fn=<MinBackward1>), tensor(0.9318, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.16713769733905792\n",
      "@sample 497: tensor([[ 0.0053,  0.0152, -0.0187, -0.0052, -0.0214],\n",
      "        [-0.0123,  0.0023,  0.0058, -0.0129, -0.0166],\n",
      "        [ 0.0020,  0.0613, -0.0197,  0.0697, -0.0364],\n",
      "        [-0.0358,  0.0205, -0.0328,  0.0399, -0.0113],\n",
      "        [-0.0436,  0.0264, -0.0071,  0.0616, -0.0118],\n",
      "        [-0.0154,  0.0058, -0.0023, -0.0173,  0.0471],\n",
      "        [-0.0436,  0.0576,  0.0378, -0.0618,  0.0411],\n",
      "        [ 0.0065, -0.0013, -0.0234, -0.0072, -0.0011]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-2.6198e-02,  3.2291e-02,  4.4954e-02,  9.2360e-02, -1.3624e-03],\n",
      "        [ 1.1697e-06,  1.6871e-02,  5.3208e-02,  1.8122e-02,  3.6647e-03],\n",
      "        [-1.2177e-02,  8.7944e-02, -4.9254e-02,  1.6510e-02,  1.2036e-02],\n",
      "        [ 5.3912e-02,  7.4963e-03,  3.4124e-02, -3.8058e-03,  5.0802e-02],\n",
      "        [ 9.5171e-02,  9.6444e-02,  6.6548e-02, -7.8546e-02,  5.1739e-02],\n",
      "        [ 2.1119e-02,  1.2000e-02,  2.4950e-03, -3.1137e-02,  2.6961e-03],\n",
      "        [-7.2171e-03, -1.5671e-02, -1.0879e-01,  2.6999e-02,  6.9911e-03],\n",
      "        [-7.4427e-03,  2.5477e-02, -3.4595e-02, -9.3091e-03,  1.9027e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0215, grad_fn=<MinBackward1>), tensor(0.8903, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1764681488275528\n",
      "@sample 498: tensor([[-0.0181, -0.0013,  0.0129, -0.0048, -0.0100],\n",
      "        [-0.0302, -0.0420, -0.0333,  0.0372, -0.0365],\n",
      "        [-0.0075,  0.0127,  0.0064, -0.0536, -0.0353],\n",
      "        [-0.0036, -0.0017,  0.0004, -0.0109,  0.0009],\n",
      "        [-0.0065,  0.0164,  0.0448, -0.0355, -0.0074],\n",
      "        [ 0.0010,  0.0027,  0.0034,  0.0521, -0.0069],\n",
      "        [ 0.0107,  0.0478,  0.0142, -0.0012,  0.0107],\n",
      "        [ 0.0298,  0.0058, -0.0003,  0.0154, -0.0100]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0377,  0.0086,  0.0360, -0.0234,  0.0105],\n",
      "        [ 0.0167, -0.0156,  0.0723, -0.0442, -0.0202],\n",
      "        [-0.0063, -0.0161,  0.0249,  0.0611,  0.0747],\n",
      "        [-0.0186, -0.0270,  0.0397,  0.0138, -0.0195],\n",
      "        [-0.0098,  0.0126,  0.0294, -0.0138,  0.0034],\n",
      "        [ 0.0255,  0.0243, -0.0194, -0.0004,  0.0579],\n",
      "        [ 0.0008,  0.0027, -0.0362,  0.0379,  0.0205],\n",
      "        [-0.0471,  0.0646, -0.0632,  0.0461, -0.0267]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0139, grad_fn=<MinBackward1>), tensor(0.9278, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1612934172153473\n",
      "@sample 499: tensor([[ 0.0090,  0.0161,  0.0053,  0.0077,  0.0111],\n",
      "        [ 0.0035, -0.0511, -0.0186,  0.0457, -0.0213],\n",
      "        [-0.0255, -0.0425,  0.0064,  0.0679, -0.0056],\n",
      "        [ 0.0114, -0.0519, -0.0180,  0.0415, -0.0249],\n",
      "        [-0.0165,  0.0064,  0.0161,  0.0079,  0.0364],\n",
      "        [ 0.0484,  0.0307, -0.0211, -0.0031, -0.0115],\n",
      "        [ 0.0145,  0.0156, -0.0008,  0.0190, -0.0311],\n",
      "        [-0.0445, -0.0034,  0.0138, -0.0001, -0.0177]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0343,  0.0281, -0.0162, -0.0082, -0.0134],\n",
      "        [ 0.0331, -0.0229,  0.0688, -0.0405, -0.0045],\n",
      "        [ 0.0543, -0.0132,  0.0828, -0.0440, -0.0012],\n",
      "        [-0.0011,  0.0348,  0.0593, -0.0062,  0.0084],\n",
      "        [ 0.0087,  0.0059,  0.0342, -0.0160, -0.0246],\n",
      "        [-0.0104, -0.0009, -0.0541,  0.0238,  0.0179],\n",
      "        [-0.0097,  0.0161, -0.0083,  0.0106,  0.0376],\n",
      "        [ 0.0203,  0.0256,  0.0303, -0.0082, -0.0076]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0131, grad_fn=<MinBackward1>), tensor(0.9686, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1624602973461151\n",
      "@sample 500: tensor([[-0.0504,  0.0235,  0.0293, -0.0260,  0.0495],\n",
      "        [ 0.0288,  0.0474,  0.0104, -0.0341,  0.0156],\n",
      "        [-0.0342, -0.0229, -0.0357,  0.0304, -0.0014],\n",
      "        [-0.0120, -0.0320, -0.0081,  0.0503, -0.0370],\n",
      "        [ 0.0668, -0.0376, -0.0214,  0.0606, -0.0383],\n",
      "        [ 0.0006, -0.0032,  0.0194, -0.0146,  0.0260],\n",
      "        [-0.0309, -0.0072, -0.0312,  0.0367, -0.0193],\n",
      "        [ 0.0060,  0.0240, -0.0002, -0.0113, -0.0105]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0430, -0.0039, -0.0069,  0.0584,  0.0116],\n",
      "        [-0.0002, -0.0029, -0.0207,  0.0025, -0.0052],\n",
      "        [ 0.0288,  0.0250,  0.0852, -0.0456, -0.0013],\n",
      "        [ 0.0306,  0.0420,  0.0438, -0.0576, -0.0009],\n",
      "        [ 0.0111,  0.0561,  0.0320, -0.0144,  0.0183],\n",
      "        [-0.0143, -0.0094, -0.0059,  0.0169,  0.0469],\n",
      "        [ 0.0059, -0.0172,  0.0006, -0.0094,  0.0517],\n",
      "        [-0.0252,  0.0056, -0.0148,  0.0455, -0.0300]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0116, grad_fn=<MinBackward1>), tensor(0.9419, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.17049716413021088\n",
      "@sample 501: tensor([[-0.0625,  0.0394,  0.0709, -0.0727,  0.0270],\n",
      "        [-0.0261, -0.0091, -0.0230,  0.0077,  0.0246],\n",
      "        [-0.0193, -0.0248,  0.0176, -0.0360,  0.0040],\n",
      "        [-0.0149, -0.0163,  0.0506, -0.0060,  0.0003],\n",
      "        [-0.0034, -0.0151, -0.0107, -0.0150, -0.0023],\n",
      "        [-0.0301, -0.0254,  0.0221,  0.0251, -0.0243],\n",
      "        [-0.0007, -0.0110, -0.0250,  0.0215,  0.0033],\n",
      "        [-0.0097, -0.0354,  0.0128,  0.0287, -0.0352]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0344, -0.0544, -0.0095, -0.0272, -0.0328],\n",
      "        [ 0.0404,  0.0237,  0.0648,  0.0130,  0.0148],\n",
      "        [ 0.0204, -0.0091,  0.0326, -0.0079,  0.0303],\n",
      "        [ 0.0194,  0.0414,  0.0683, -0.0156,  0.0032],\n",
      "        [-0.0135, -0.0119, -0.0094,  0.0088, -0.0016],\n",
      "        [-0.0134,  0.0201,  0.0360, -0.0013, -0.0067],\n",
      "        [ 0.0204, -0.0011,  0.0456, -0.0565, -0.0116],\n",
      "        [ 0.0097,  0.0066,  0.0617, -0.0743, -0.0670]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0145, grad_fn=<MinBackward1>), tensor(0.9486, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.16549095511436462\n",
      "@sample 502: tensor([[-0.0331, -0.0085, -0.0284, -0.0105,  0.0388],\n",
      "        [ 0.0298, -0.0169, -0.0179,  0.0466, -0.0067],\n",
      "        [ 0.0333,  0.0786,  0.0228, -0.0299,  0.0231],\n",
      "        [-0.0267,  0.0248,  0.0294, -0.0354,  0.0279],\n",
      "        [-0.0051, -0.0022,  0.0496,  0.0006, -0.0061],\n",
      "        [-0.0230, -0.0072,  0.0336,  0.0034, -0.0251],\n",
      "        [ 0.0255,  0.0158,  0.0186,  0.0001,  0.0038],\n",
      "        [-0.0134,  0.0454,  0.0173, -0.0428,  0.0361]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0460, -0.0057, -0.0784,  0.0170,  0.0222],\n",
      "        [ 0.0354,  0.0099,  0.0096, -0.0376,  0.0204],\n",
      "        [-0.0269,  0.0169, -0.1073,  0.0906,  0.0052],\n",
      "        [-0.0215,  0.0255,  0.0080,  0.0241,  0.0100],\n",
      "        [-0.0428,  0.0052, -0.0315, -0.0048, -0.0277],\n",
      "        [-0.0174, -0.0248,  0.0177,  0.0385, -0.0250],\n",
      "        [-0.0107, -0.0233,  0.0904,  0.0023, -0.0265],\n",
      "        [-0.0427, -0.0039, -0.0338,  0.0286,  0.0204]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0141, grad_fn=<MinBackward1>), tensor(0.8953, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.16307969391345978\n",
      "@sample 503: tensor([[-0.0261, -0.0287, -0.0140,  0.0351, -0.0080],\n",
      "        [ 0.0104, -0.0259, -0.0467,  0.0561, -0.0382],\n",
      "        [-0.0211,  0.0195, -0.0070, -0.0147,  0.0128],\n",
      "        [-0.0168, -0.0360,  0.0215,  0.0029, -0.0014],\n",
      "        [-0.0034, -0.0237,  0.0044, -0.0146, -0.0025],\n",
      "        [-0.0007, -0.0388,  0.0045,  0.0208, -0.0360],\n",
      "        [ 0.0125, -0.0304,  0.0107,  0.0310, -0.0183],\n",
      "        [ 0.0065, -0.0279,  0.0160,  0.0311,  0.0031]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 3.1564e-02, -5.6705e-03,  5.2984e-02, -5.3706e-02, -8.6438e-05],\n",
      "        [ 4.1459e-02,  5.6998e-02,  2.8489e-02, -2.2519e-02, -8.4702e-03],\n",
      "        [-2.8953e-02, -1.5695e-03,  9.5285e-02, -2.8130e-02, -4.3062e-02],\n",
      "        [ 2.0407e-02, -2.5286e-02,  4.1371e-02, -7.0337e-02, -4.8166e-02],\n",
      "        [-1.7792e-02, -4.1968e-02, -4.6447e-02,  2.3830e-02, -5.0070e-03],\n",
      "        [ 6.9252e-03, -2.3959e-02,  7.9156e-02, -6.6234e-03, -3.0632e-02],\n",
      "        [ 1.2489e-02,  3.6994e-02, -2.0137e-02, -1.7917e-02, -1.0252e-02],\n",
      "        [ 5.0656e-02,  1.7103e-02, -6.7265e-03,  1.9557e-02,  1.8898e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0148, grad_fn=<MinBackward1>), tensor(0.9273, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.15543434023857117\n",
      "@sample 504: tensor([[ 0.0205,  0.0349, -0.0076, -0.0172,  0.0403],\n",
      "        [-0.0138,  0.0259,  0.0113, -0.0600,  0.0008],\n",
      "        [ 0.0159,  0.0096,  0.0028,  0.0216, -0.0113],\n",
      "        [-0.0180, -0.0144,  0.0199, -0.0345,  0.0060],\n",
      "        [-0.0061, -0.0007, -0.0159,  0.0113, -0.0403],\n",
      "        [ 0.0127, -0.0316,  0.0680, -0.0298,  0.0386],\n",
      "        [ 0.0201, -0.0235,  0.0195, -0.0162,  0.0280],\n",
      "        [ 0.0147,  0.0045,  0.0114, -0.0073,  0.0258]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-1.4984e-02,  1.3405e-03, -8.2167e-02,  4.8194e-02,  2.1612e-02],\n",
      "        [ 1.6983e-03, -1.9556e-02, -3.8828e-02,  5.4869e-03,  1.5342e-02],\n",
      "        [ 1.4396e-02,  1.3689e-02, -2.9349e-02,  1.8049e-02,  2.3211e-02],\n",
      "        [-1.8443e-02,  1.2469e-02,  3.6513e-02,  1.6176e-02, -1.2523e-03],\n",
      "        [ 3.5986e-02, -1.3835e-02,  4.1082e-02, -2.1008e-02, -1.9585e-03],\n",
      "        [ 2.5395e-03, -3.2383e-02,  1.0793e-02,  2.0781e-02,  1.7766e-02],\n",
      "        [ 1.0460e-02, -3.5832e-02, -1.7624e-02, -1.7448e-02, -9.6499e-03],\n",
      "        [-9.9659e-05, -1.5531e-02, -3.7390e-02,  5.0271e-02,  2.4035e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0287, grad_fn=<MinBackward1>), tensor(0.9092, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.15816520154476166\n",
      "@sample 505: tensor([[ 0.0199, -0.0888, -0.0249,  0.0080,  0.0036],\n",
      "        [-0.0154, -0.0124,  0.0173, -0.0215, -0.0108],\n",
      "        [ 0.0065,  0.0143,  0.0497, -0.0301,  0.0486],\n",
      "        [ 0.0175, -0.0004,  0.0132, -0.0436,  0.0305],\n",
      "        [-0.0134,  0.0074,  0.0113, -0.0057,  0.0067],\n",
      "        [-0.0049,  0.0073, -0.0015,  0.0242,  0.0059],\n",
      "        [-0.0632, -0.0007, -0.0057,  0.0046, -0.0381],\n",
      "        [-0.0009, -0.0124,  0.0256, -0.0173,  0.0361]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0405, -0.0071,  0.0196,  0.0104,  0.0250],\n",
      "        [-0.0188, -0.0629,  0.0498,  0.0095,  0.0253],\n",
      "        [ 0.0296, -0.0836, -0.0966,  0.0743, -0.0162],\n",
      "        [-0.0358, -0.0534, -0.0246,  0.0116, -0.0061],\n",
      "        [-0.0306, -0.0444, -0.0498,  0.0113,  0.0336],\n",
      "        [-0.0006,  0.0269, -0.0569,  0.0548,  0.0207],\n",
      "        [-0.0141, -0.0499, -0.0010, -0.0453, -0.0468],\n",
      "        [ 0.0147, -0.0444, -0.0283,  0.0546,  0.0253]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0228, grad_fn=<MinBackward1>), tensor(0.9303, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.16939134895801544\n",
      "@sample 506: tensor([[ 0.0142,  0.0012, -0.0245,  0.0370,  0.0087],\n",
      "        [-0.0117, -0.0271, -0.0025,  0.0063,  0.0247],\n",
      "        [ 0.0036, -0.0303,  0.0007,  0.0417, -0.0473],\n",
      "        [ 0.0118, -0.0484, -0.0112,  0.0649,  0.0011],\n",
      "        [-0.0316, -0.0554, -0.0158,  0.0206,  0.0112],\n",
      "        [-0.0147, -0.0018,  0.0186, -0.0324, -0.0043],\n",
      "        [ 0.0091, -0.0859, -0.0065,  0.0432,  0.0197],\n",
      "        [ 0.0088, -0.0101,  0.0059, -0.0324,  0.0109]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0122, -0.0156, -0.0261,  0.0118,  0.0280],\n",
      "        [ 0.0276, -0.0237, -0.0361, -0.0313, -0.0080],\n",
      "        [-0.0066,  0.0297, -0.0084, -0.0139,  0.0150],\n",
      "        [ 0.0237,  0.0033, -0.0097,  0.0033,  0.0113],\n",
      "        [-0.0094, -0.0184,  0.0643, -0.0671,  0.0074],\n",
      "        [-0.0312, -0.0468,  0.0149, -0.0192, -0.0398],\n",
      "        [-0.0027, -0.0140,  0.0541, -0.0053,  0.0021],\n",
      "        [-0.0316, -0.0080,  0.0057, -0.0024,  0.0130]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0185, grad_fn=<MinBackward1>), tensor(0.9091, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.15922309458255768\n",
      "@sample 507: tensor([[ 0.0023, -0.0128, -0.0430,  0.0303, -0.0040],\n",
      "        [ 0.0245, -0.0153,  0.0407,  0.0164, -0.0134],\n",
      "        [ 0.0114,  0.0149, -0.0027,  0.0122, -0.0127],\n",
      "        [ 0.0272,  0.0357,  0.0258, -0.0238,  0.0147],\n",
      "        [-0.0499, -0.0449, -0.0278,  0.0370, -0.0066],\n",
      "        [-0.0102,  0.0002, -0.0358,  0.0082, -0.0269],\n",
      "        [ 0.0123, -0.0108, -0.0241,  0.0381, -0.0051],\n",
      "        [ 0.0071, -0.0188,  0.0223, -0.0379, -0.0033]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 4.4639e-02, -9.2184e-03, -1.4170e-03,  2.6215e-02,  2.4342e-02],\n",
      "        [-4.5535e-03,  1.7791e-02,  3.6729e-02,  1.1334e-02,  8.5900e-03],\n",
      "        [ 1.9181e-02,  3.1082e-02,  6.0337e-03, -8.3203e-03,  8.0997e-03],\n",
      "        [-1.1738e-02, -4.4370e-02, -6.8646e-02,  6.3630e-02,  3.0490e-02],\n",
      "        [ 4.5997e-02, -8.9306e-03,  9.0418e-02, -6.7425e-02, -5.4505e-02],\n",
      "        [ 4.5877e-03, -4.5085e-02, -3.3858e-02, -6.8396e-06,  4.6179e-02],\n",
      "        [ 3.5051e-02,  1.0434e-02, -5.0455e-03,  5.9651e-03,  1.4169e-02],\n",
      "        [-6.4622e-02,  7.6609e-03, -5.7498e-04,  7.2393e-02,  7.0684e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0171, grad_fn=<MinBackward1>), tensor(0.9165, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.16408611834049225\n",
      "@sample 508: tensor([[-0.0341, -0.0346, -0.0169,  0.0464, -0.0122],\n",
      "        [-0.0106, -0.0041, -0.0012, -0.0303,  0.0383],\n",
      "        [ 0.0294, -0.0094, -0.0196,  0.0237, -0.0071],\n",
      "        [ 0.0052, -0.0372, -0.0412,  0.0123, -0.0278],\n",
      "        [-0.0269,  0.0343, -0.0112,  0.0222, -0.0109],\n",
      "        [-0.0125,  0.0009, -0.0133, -0.0146,  0.0046],\n",
      "        [-0.0302, -0.0030,  0.0020, -0.0882,  0.0061],\n",
      "        [ 0.0077,  0.0199, -0.0104, -0.0274,  0.0110]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0641,  0.0332,  0.0542, -0.0504,  0.0211],\n",
      "        [-0.0145, -0.0368, -0.0350,  0.0184,  0.0270],\n",
      "        [-0.0093,  0.0457,  0.0382,  0.0273,  0.0453],\n",
      "        [ 0.0224,  0.0021,  0.0182, -0.0112, -0.0197],\n",
      "        [-0.0550,  0.0079, -0.1132,  0.0301,  0.0379],\n",
      "        [ 0.0141, -0.0045, -0.0067,  0.0278,  0.0008],\n",
      "        [-0.0385, -0.0568, -0.1042,  0.0625,  0.0707],\n",
      "        [-0.0249,  0.0050,  0.0059,  0.0034, -0.0576]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0178, grad_fn=<MinBackward1>), tensor(0.9254, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.16852101683616638\n",
      "@sample 509: tensor([[-2.6502e-02, -4.2825e-02, -1.5159e-02, -1.5476e-02,  5.9541e-02],\n",
      "        [ 1.0054e-02, -1.9614e-03,  9.5767e-03,  2.0252e-02,  1.0407e-02],\n",
      "        [-2.5727e-02,  1.8274e-02,  4.3489e-02,  2.0326e-02,  4.4202e-03],\n",
      "        [ 1.3963e-02,  2.2173e-02,  1.4992e-02, -2.3226e-02,  7.6216e-04],\n",
      "        [-4.0370e-02, -1.0896e-02,  4.3859e-03, -1.0983e-02, -1.1117e-02],\n",
      "        [-1.5694e-02,  7.6299e-02,  1.5729e-02, -5.8131e-02,  9.2935e-03],\n",
      "        [-1.2058e-02,  1.0100e-02, -2.0074e-02,  6.4163e-02, -1.2868e-02],\n",
      "        [-1.0086e-02, -1.6804e-02,  3.2329e-02,  6.2510e-05, -1.9872e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0371, -0.0832,  0.0213, -0.0141,  0.0015],\n",
      "        [ 0.0200, -0.0460,  0.0115,  0.0311, -0.0390],\n",
      "        [-0.0206, -0.0065, -0.0299, -0.0027,  0.0432],\n",
      "        [-0.0151,  0.0053, -0.0231,  0.0361,  0.0439],\n",
      "        [-0.0199, -0.0374,  0.0260, -0.0696,  0.0130],\n",
      "        [ 0.0236, -0.0309, -0.0544,  0.0311,  0.0272],\n",
      "        [ 0.0986,  0.0540,  0.0391, -0.0339,  0.0448],\n",
      "        [-0.0090,  0.0091, -0.0418,  0.0060, -0.0074]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0184, grad_fn=<MinBackward1>), tensor(0.9479, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.17317666113376617\n",
      "@sample 510: tensor([[ 0.0164, -0.0124,  0.0219, -0.0059,  0.0098],\n",
      "        [ 0.0085, -0.0188,  0.0120,  0.0301, -0.0041],\n",
      "        [-0.0048,  0.0005, -0.0350,  0.0685, -0.0660],\n",
      "        [-0.0149, -0.0041, -0.0097, -0.0004,  0.0171],\n",
      "        [ 0.0374,  0.0571,  0.0481, -0.0491, -0.0258],\n",
      "        [-0.0228,  0.0627,  0.0365, -0.0620, -0.0114],\n",
      "        [ 0.0226, -0.0110, -0.0230, -0.0120, -0.0050],\n",
      "        [ 0.0029,  0.0271, -0.0056, -0.0329, -0.0105]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0471, -0.0478,  0.0225,  0.0010,  0.0084],\n",
      "        [ 0.0261,  0.0394,  0.0674,  0.0315,  0.0103],\n",
      "        [ 0.0654,  0.0366,  0.0301,  0.0365,  0.0364],\n",
      "        [ 0.0242,  0.0074, -0.0298, -0.0114, -0.0256],\n",
      "        [-0.0551,  0.0332, -0.0381,  0.0229, -0.0529],\n",
      "        [-0.0027, -0.0150,  0.0073,  0.0034, -0.0033],\n",
      "        [-0.0102, -0.0356, -0.0945,  0.0459,  0.0358],\n",
      "        [-0.0420,  0.0239, -0.0385,  0.0311,  0.0140]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0247, grad_fn=<MinBackward1>), tensor(0.8928, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.17047902941703796\n",
      "@sample 511: tensor([[-3.5517e-02, -5.8233e-02,  2.0577e-02,  2.3567e-02,  2.3064e-02],\n",
      "        [-1.4601e-02, -4.2867e-02,  1.2417e-02,  8.0804e-03, -1.2534e-02],\n",
      "        [ 1.2480e-02,  2.8878e-02,  3.0940e-02, -3.8576e-02,  8.3666e-03],\n",
      "        [-3.2389e-03, -2.1052e-02, -7.1140e-03, -1.2091e-02,  5.6207e-02],\n",
      "        [ 1.3498e-02,  3.0397e-02,  9.4812e-03,  8.7396e-03, -1.0696e-02],\n",
      "        [-2.6892e-02, -7.8753e-05, -2.4645e-02, -1.1644e-02,  8.3368e-03],\n",
      "        [-1.5688e-02, -6.1339e-03, -1.0943e-02,  4.4027e-02, -2.0699e-02],\n",
      "        [ 2.6435e-02,  4.2139e-02,  4.6786e-03, -4.7010e-02,  4.0846e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 3.8068e-02, -6.8520e-02,  3.1575e-02, -7.9679e-02, -3.4188e-02],\n",
      "        [-1.6692e-02, -2.5444e-02,  7.4246e-02, -3.3648e-02, -1.3503e-02],\n",
      "        [-3.6785e-02, -2.6336e-02, -3.9512e-02,  3.2762e-02, -5.1378e-03],\n",
      "        [-1.3775e-02, -7.8524e-02, -3.8258e-02, -5.8568e-03, -2.1146e-02],\n",
      "        [-3.7555e-02, -3.6328e-03, -2.9478e-02,  4.2217e-02,  1.4322e-02],\n",
      "        [-1.9960e-02, -1.6411e-02, -9.6634e-05, -8.7473e-03,  2.8814e-03],\n",
      "        [ 1.9089e-02,  8.1296e-03,  4.7856e-02,  9.9131e-03,  2.8138e-02],\n",
      "        [-5.2571e-02,  5.1511e-03, -4.8018e-02,  7.2402e-02,  5.9425e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0113, grad_fn=<MinBackward1>), tensor(0.9441, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.16144926846027374\n",
      "@sample 512: tensor([[-0.0167,  0.1015,  0.0258, -0.0849,  0.0259],\n",
      "        [ 0.0563,  0.0270, -0.0105,  0.0067, -0.0196],\n",
      "        [ 0.0160, -0.0174, -0.0023,  0.0028,  0.0279],\n",
      "        [ 0.0039,  0.0135, -0.0006, -0.0099,  0.0097],\n",
      "        [-0.0156,  0.0096, -0.0292, -0.0080, -0.0258],\n",
      "        [ 0.0247,  0.0183,  0.0030, -0.0190, -0.0149],\n",
      "        [ 0.0013,  0.0072, -0.0214,  0.0045,  0.0536],\n",
      "        [-0.0320, -0.0121,  0.0284, -0.0625,  0.0384]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-5.7926e-02, -6.6412e-02, -1.0766e-01,  4.7421e-02, -3.9659e-02],\n",
      "        [-5.9123e-04,  2.2471e-02,  1.0787e-02,  3.4959e-02, -2.0878e-02],\n",
      "        [-1.8891e-02, -3.5240e-02,  9.0242e-03, -2.3170e-03, -3.4123e-02],\n",
      "        [-3.2090e-02, -1.9366e-02, -5.8051e-02, -1.0162e-02, -1.8958e-02],\n",
      "        [-1.4757e-02,  1.0773e-02, -1.2047e-02,  3.4750e-05,  2.0916e-02],\n",
      "        [-2.5940e-02,  4.0022e-02, -1.1857e-02, -1.0837e-02,  4.5146e-03],\n",
      "        [ 2.7638e-02,  1.5420e-02, -1.5681e-02, -1.7899e-02, -1.1383e-02],\n",
      "        [ 3.0133e-02, -5.9019e-02, -1.0526e-03,  6.1165e-02, -4.1408e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0183, grad_fn=<MinBackward1>), tensor(0.9722, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.16879531741142273\n",
      "@sample 513: tensor([[ 0.0172,  0.1258,  0.0334, -0.0611,  0.0117],\n",
      "        [-0.0051,  0.0402,  0.0751, -0.0792,  0.0472],\n",
      "        [-0.0187,  0.0531,  0.0119, -0.0070,  0.0114],\n",
      "        [-0.0152, -0.0156, -0.0123,  0.0165, -0.0039],\n",
      "        [-0.0192, -0.0142, -0.0014,  0.0266, -0.0073],\n",
      "        [-0.0215,  0.0762,  0.0379, -0.0065, -0.0089],\n",
      "        [ 0.0081,  0.0094,  0.0166, -0.0106,  0.0052],\n",
      "        [ 0.0443, -0.0221, -0.0240, -0.0148, -0.0134]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0796, -0.0513, -0.0602,  0.0143, -0.0620],\n",
      "        [-0.0324, -0.0394, -0.0983,  0.0074, -0.0531],\n",
      "        [ 0.0438,  0.0262,  0.0544, -0.0021,  0.0186],\n",
      "        [-0.0053, -0.0143,  0.0199, -0.0232, -0.0074],\n",
      "        [ 0.0205,  0.0132, -0.0081, -0.0290, -0.0025],\n",
      "        [-0.0838,  0.0045, -0.0343, -0.0268, -0.0095],\n",
      "        [-0.0297, -0.0295, -0.0466,  0.0316,  0.0159],\n",
      "        [-0.0161, -0.0359, -0.0255,  0.0120, -0.0262]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0195, grad_fn=<MinBackward1>), tensor(0.9690, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.17809134721755981\n",
      "@sample 514: tensor([[-0.0026,  0.0222,  0.0247, -0.0278,  0.0056],\n",
      "        [ 0.0136,  0.0057, -0.0154, -0.0102,  0.0596],\n",
      "        [ 0.0134,  0.0128,  0.0105, -0.0326, -0.0074],\n",
      "        [ 0.0125,  0.0144, -0.0252,  0.0103, -0.0068],\n",
      "        [-0.0264,  0.0321,  0.0783,  0.0075,  0.0277],\n",
      "        [ 0.0057, -0.0008,  0.0073,  0.0052, -0.0042],\n",
      "        [ 0.0315,  0.0295, -0.0258,  0.0165, -0.0288],\n",
      "        [ 0.0130, -0.0452, -0.0157,  0.0245, -0.0016]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0159, -0.0060, -0.0177,  0.0276,  0.0119],\n",
      "        [-0.0448, -0.0010, -0.0885,  0.0403, -0.0293],\n",
      "        [-0.0038,  0.0171, -0.0644,  0.0197,  0.0200],\n",
      "        [ 0.0141, -0.0075,  0.0090,  0.0147,  0.0084],\n",
      "        [ 0.0173,  0.0229, -0.0369,  0.0215,  0.0326],\n",
      "        [-0.0054, -0.0107, -0.0445, -0.0002, -0.0070],\n",
      "        [-0.0287, -0.0080, -0.0339, -0.0060, -0.0068],\n",
      "        [-0.0109,  0.0687,  0.0321,  0.0146,  0.0656]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0172, grad_fn=<MinBackward1>), tensor(0.9285, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1544816792011261\n",
      "@sample 515: tensor([[-0.0082,  0.0334, -0.0052, -0.0020,  0.0599],\n",
      "        [ 0.0272, -0.0051, -0.0542,  0.0667, -0.0600],\n",
      "        [-0.0031, -0.0075,  0.0062, -0.0053, -0.0101],\n",
      "        [-0.0144, -0.0501,  0.0084,  0.0702, -0.0199],\n",
      "        [ 0.0150,  0.0093, -0.0126, -0.0217, -0.0129],\n",
      "        [-0.0145,  0.0063,  0.0116, -0.0274, -0.0110],\n",
      "        [ 0.0059,  0.0180,  0.0103, -0.0084, -0.0038],\n",
      "        [ 0.0034, -0.0115, -0.0107,  0.0395,  0.0116]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0077, -0.0197, -0.0910, -0.0033,  0.0011],\n",
      "        [ 0.0133,  0.0387,  0.0248,  0.0012, -0.0144],\n",
      "        [-0.0654,  0.0101,  0.0006, -0.0248, -0.0070],\n",
      "        [ 0.0388,  0.0236,  0.0948, -0.0799, -0.0064],\n",
      "        [-0.0291, -0.0340, -0.0191,  0.0079, -0.0064],\n",
      "        [ 0.0093, -0.0156, -0.0400,  0.0528, -0.0184],\n",
      "        [-0.0096,  0.0793,  0.0346, -0.0208,  0.0262],\n",
      "        [ 0.0594,  0.0382, -0.0174,  0.0460,  0.0430]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0166, grad_fn=<MinBackward1>), tensor(0.9294, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1713002622127533\n",
      "@sample 516: tensor([[ 0.0126, -0.0192, -0.0455,  0.0367, -0.0106],\n",
      "        [-0.0026,  0.0470, -0.0236, -0.0714,  0.0346],\n",
      "        [-0.0118, -0.0277,  0.0109, -0.0147, -0.0021],\n",
      "        [-0.0127,  0.0560,  0.0608, -0.0825,  0.0377],\n",
      "        [-0.0008, -0.0072, -0.0221,  0.0098,  0.0044],\n",
      "        [-0.0100,  0.0435,  0.0108,  0.0363, -0.0065],\n",
      "        [-0.0293,  0.0398,  0.0191, -0.0110, -0.0188],\n",
      "        [ 0.0070,  0.0342, -0.0154, -0.0228, -0.0089]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0271,  0.0238,  0.0287, -0.0172,  0.0128],\n",
      "        [-0.0120, -0.0734, -0.0983,  0.0529,  0.0517],\n",
      "        [-0.0276, -0.0088,  0.0503, -0.0371,  0.0021],\n",
      "        [-0.0034, -0.0674, -0.0545,  0.0338, -0.0286],\n",
      "        [-0.0280, -0.0015, -0.0055, -0.0542, -0.0333],\n",
      "        [ 0.0096,  0.0330, -0.0449,  0.0249,  0.0131],\n",
      "        [-0.0160, -0.0268, -0.0349, -0.0188, -0.0035],\n",
      "        [-0.0091, -0.0524, -0.0660,  0.0113,  0.0287]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0185, grad_fn=<MinBackward1>), tensor(0.9415, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.16685116291046143\n",
      "@sample 517: tensor([[-0.0040, -0.0327, -0.0096,  0.0749, -0.0487],\n",
      "        [-0.0120, -0.0301, -0.0288,  0.0354, -0.0257],\n",
      "        [ 0.0047,  0.0374,  0.0034, -0.0090,  0.0371],\n",
      "        [-0.0078,  0.0594,  0.0056,  0.0026, -0.0170],\n",
      "        [ 0.0150,  0.0180, -0.0368, -0.0081,  0.0066],\n",
      "        [ 0.0135, -0.0450, -0.0259,  0.0691, -0.0281],\n",
      "        [-0.0043,  0.0128,  0.0407, -0.0140,  0.0155],\n",
      "        [-0.0381, -0.0141, -0.0204,  0.0386, -0.0054]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0210, -0.0086,  0.0545, -0.0426, -0.0259],\n",
      "        [ 0.0265, -0.0076,  0.0049, -0.0396, -0.0462],\n",
      "        [-0.0134, -0.0095, -0.0376, -0.0043, -0.0010],\n",
      "        [-0.0033,  0.0155, -0.0406,  0.0478,  0.0353],\n",
      "        [ 0.0065, -0.0211, -0.0422,  0.0158,  0.0029],\n",
      "        [ 0.0560,  0.0439,  0.0619, -0.0715,  0.0035],\n",
      "        [-0.0073,  0.0167,  0.0561, -0.0012,  0.0152],\n",
      "        [ 0.0178,  0.0525,  0.0523, -0.0972, -0.0046]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0262, grad_fn=<MinBackward1>), tensor(0.9277, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.16653089225292206\n",
      "@sample 518: tensor([[-9.4652e-03, -2.8046e-02, -3.2276e-03,  1.6275e-02, -8.6682e-03],\n",
      "        [ 1.5722e-05, -2.4883e-02, -9.8905e-03,  4.0239e-02,  4.0295e-02],\n",
      "        [ 1.2940e-02,  3.3021e-03, -1.9880e-02,  1.8330e-02, -3.7122e-02],\n",
      "        [-9.2557e-03,  1.6169e-03, -2.8181e-02,  3.2376e-02, -4.6111e-03],\n",
      "        [ 5.5745e-03,  2.5362e-02, -5.2715e-02, -7.1025e-03,  6.9647e-02],\n",
      "        [-1.5095e-02, -4.0434e-03,  5.3867e-03, -2.0876e-02,  3.5844e-02],\n",
      "        [ 6.1523e-03, -4.8702e-03,  2.7051e-02, -5.2500e-03,  2.8496e-02],\n",
      "        [ 5.1988e-02, -3.1798e-03,  1.2048e-02, -3.6438e-03, -2.2623e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0585,  0.0155,  0.0688, -0.0202,  0.0229],\n",
      "        [ 0.0341, -0.0043, -0.0409,  0.0095, -0.0331],\n",
      "        [ 0.0023,  0.0664,  0.0536, -0.0092,  0.0153],\n",
      "        [ 0.0298,  0.0075,  0.0219, -0.0175,  0.0070],\n",
      "        [ 0.0488, -0.0231, -0.0633,  0.0143,  0.0029],\n",
      "        [ 0.0031,  0.0094, -0.0675,  0.0068, -0.0051],\n",
      "        [-0.0014,  0.0242, -0.0112, -0.0379, -0.0381],\n",
      "        [-0.0144,  0.0389,  0.0061, -0.0411, -0.0258]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0200, grad_fn=<MinBackward1>), tensor(0.8918, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.16026616096496582\n",
      "@sample 519: tensor([[ 0.0276, -0.0038, -0.0534,  0.0481, -0.0170],\n",
      "        [-0.0029, -0.0485, -0.0339,  0.0163, -0.0128],\n",
      "        [-0.0147,  0.0402,  0.0004, -0.0547,  0.0140],\n",
      "        [-0.0468, -0.0473, -0.0429,  0.0476, -0.0235],\n",
      "        [-0.0191,  0.0069,  0.0039, -0.0444, -0.0024],\n",
      "        [ 0.0044,  0.0170,  0.0407,  0.0080, -0.0027],\n",
      "        [ 0.0228,  0.0188,  0.0180, -0.0267, -0.0215],\n",
      "        [ 0.0040, -0.0097,  0.0065,  0.0281, -0.0310]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0463,  0.0338,  0.0693, -0.0247, -0.0599],\n",
      "        [ 0.0241, -0.0166,  0.0663, -0.0526, -0.0157],\n",
      "        [-0.1103, -0.0301, -0.0577,  0.0230, -0.0281],\n",
      "        [-0.0020,  0.0115,  0.0757, -0.0557, -0.0125],\n",
      "        [-0.0343, -0.0272, -0.0241, -0.0059, -0.0409],\n",
      "        [ 0.0145, -0.0074, -0.0640, -0.0057, -0.0145],\n",
      "        [-0.0187, -0.0147,  0.0476,  0.0222, -0.0097],\n",
      "        [ 0.0107,  0.0498,  0.0617, -0.0334, -0.0147]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0233, grad_fn=<MinBackward1>), tensor(0.9220, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1655416488647461\n",
      "@sample 520: tensor([[ 0.0456, -0.0199, -0.0177,  0.0207, -0.0222],\n",
      "        [-0.0182,  0.0229,  0.0448, -0.0278,  0.0175],\n",
      "        [-0.0099,  0.0690,  0.0119, -0.0139,  0.0219],\n",
      "        [ 0.0202, -0.0374, -0.0257,  0.0598, -0.0580],\n",
      "        [-0.0029, -0.0071,  0.0353,  0.0110,  0.0125],\n",
      "        [ 0.0256, -0.0507, -0.0288,  0.0393, -0.0340],\n",
      "        [ 0.0269, -0.0094, -0.0319,  0.0012,  0.0204],\n",
      "        [-0.0156,  0.0041,  0.0337, -0.0037, -0.0304]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0327,  0.0364, -0.0057, -0.0253,  0.0088],\n",
      "        [-0.0257,  0.0100, -0.0987, -0.0054,  0.0010],\n",
      "        [-0.0427,  0.0211, -0.1292,  0.0279, -0.0121],\n",
      "        [ 0.0058,  0.0390,  0.0213, -0.0399, -0.0577],\n",
      "        [ 0.0107,  0.0259,  0.0173, -0.0612, -0.0201],\n",
      "        [ 0.0306,  0.0327,  0.0854, -0.0598, -0.0056],\n",
      "        [-0.0072, -0.0227,  0.0141,  0.0194, -0.0129],\n",
      "        [-0.0376,  0.0216,  0.0320, -0.0338, -0.0074]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0229, grad_fn=<MinBackward1>), tensor(0.9055, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.16505782306194305\n",
      "@sample 521: tensor([[ 2.2904e-02, -4.3702e-03,  3.5071e-02, -1.9693e-02,  1.6697e-02],\n",
      "        [-7.3490e-03, -3.1368e-02,  2.0812e-02, -1.6139e-02,  2.6513e-02],\n",
      "        [ 1.7450e-02,  3.4251e-03,  2.3780e-02,  5.2724e-03, -1.0025e-02],\n",
      "        [-8.2760e-03, -6.7003e-03,  2.5465e-02, -1.4454e-02,  3.2482e-03],\n",
      "        [-1.4327e-02, -7.7015e-03, -1.5675e-03, -5.9977e-03,  1.8030e-02],\n",
      "        [-5.8296e-03, -1.4200e-02, -2.0423e-02, -2.4285e-02,  4.9290e-02],\n",
      "        [ 5.2454e-02,  2.0139e-02,  1.3684e-02,  6.8533e-03, -2.9443e-02],\n",
      "        [-4.1335e-02, -2.0012e-05, -5.9682e-03,  3.4196e-02, -1.0919e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0208, -0.0500, -0.0308,  0.0413, -0.0159],\n",
      "        [-0.0306, -0.0085, -0.0267, -0.0094,  0.0312],\n",
      "        [ 0.0128,  0.0331, -0.0089, -0.0095,  0.0153],\n",
      "        [-0.0319, -0.0304,  0.0344, -0.0003, -0.0167],\n",
      "        [-0.0268,  0.0105,  0.0237, -0.0010, -0.0089],\n",
      "        [ 0.0345, -0.0280,  0.0083,  0.0197,  0.0484],\n",
      "        [-0.0538,  0.0226, -0.0291,  0.0183,  0.0158],\n",
      "        [ 0.0326,  0.0159,  0.0367, -0.0388, -0.0388]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0195, grad_fn=<MinBackward1>), tensor(0.9370, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.15472286939620972\n",
      "@sample 522: tensor([[-0.0073, -0.0164,  0.0227,  0.0197, -0.0202],\n",
      "        [-0.0370,  0.0036,  0.0361, -0.0470,  0.0384],\n",
      "        [ 0.0048, -0.0342,  0.0256, -0.0031,  0.0014],\n",
      "        [-0.0042,  0.0062,  0.0159, -0.0154,  0.0554],\n",
      "        [-0.0230,  0.0385,  0.0173,  0.0126,  0.0297],\n",
      "        [ 0.0204,  0.0062,  0.0167, -0.0250,  0.0454],\n",
      "        [ 0.0083, -0.0094, -0.0011,  0.0118, -0.0187],\n",
      "        [-0.0161,  0.0057, -0.0095, -0.0210,  0.0052]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0042, -0.0020,  0.0422,  0.0086, -0.0144],\n",
      "        [ 0.0020,  0.0150, -0.0097,  0.0506, -0.0023],\n",
      "        [ 0.0079,  0.0262,  0.0289, -0.0216, -0.0026],\n",
      "        [-0.0294, -0.0427, -0.0565,  0.0371,  0.0075],\n",
      "        [ 0.0165,  0.0983,  0.0215, -0.0050, -0.0127],\n",
      "        [-0.0482, -0.0357, -0.0726,  0.0227, -0.0215],\n",
      "        [-0.0078,  0.0195,  0.0390, -0.0620, -0.0329],\n",
      "        [-0.0317, -0.0090,  0.0231, -0.0356, -0.0282]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0222, grad_fn=<MinBackward1>), tensor(0.9015, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.14961379766464233\n",
      "@sample 523: tensor([[ 0.0227, -0.0268,  0.0116,  0.0128, -0.0155],\n",
      "        [ 0.0103,  0.0264,  0.0145, -0.0351, -0.0166],\n",
      "        [ 0.0095, -0.0078,  0.0263,  0.0250, -0.0137],\n",
      "        [-0.0013,  0.0279, -0.0412,  0.0107, -0.0022],\n",
      "        [ 0.0063, -0.0136, -0.0178,  0.0137,  0.0190],\n",
      "        [ 0.0258,  0.0342,  0.0186, -0.0449,  0.0418],\n",
      "        [ 0.0208,  0.0044, -0.0174,  0.0119, -0.0007],\n",
      "        [ 0.0011,  0.0066,  0.0217, -0.0161,  0.0285]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0224, -0.0053,  0.0364, -0.0062,  0.0238],\n",
      "        [-0.0140,  0.0136, -0.0325, -0.0274,  0.0018],\n",
      "        [-0.0045,  0.0249,  0.0270, -0.0248, -0.0359],\n",
      "        [ 0.0244,  0.0187, -0.0321, -0.0226,  0.0055],\n",
      "        [ 0.0058, -0.0435, -0.0215, -0.0097, -0.0226],\n",
      "        [-0.0563, -0.0384, -0.0748,  0.0516, -0.0046],\n",
      "        [-0.0140, -0.0075, -0.0163,  0.0210,  0.0050],\n",
      "        [-0.0217, -0.0035,  0.0419, -0.0079, -0.0076]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0238, grad_fn=<MinBackward1>), tensor(0.9129, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.149621844291687\n",
      "@sample 524: tensor([[-0.0078, -0.0401, -0.0229,  0.0177,  0.0117],\n",
      "        [ 0.0016,  0.0348,  0.0186,  0.0264, -0.0045],\n",
      "        [-0.0503,  0.0136,  0.0219, -0.0198,  0.0066],\n",
      "        [-0.0134,  0.0023,  0.0035, -0.0642, -0.0009],\n",
      "        [-0.0251, -0.0025, -0.0239,  0.0297, -0.0267],\n",
      "        [-0.0060, -0.0220, -0.0162, -0.0055, -0.0002],\n",
      "        [-0.0483,  0.0278,  0.0156, -0.0072,  0.0096],\n",
      "        [ 0.0098,  0.0120,  0.0083,  0.0227, -0.0355]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0303,  0.0170,  0.0035, -0.0114,  0.0045],\n",
      "        [-0.0263,  0.0393, -0.0226,  0.0160, -0.0074],\n",
      "        [-0.0006, -0.0052,  0.0089,  0.0104,  0.0188],\n",
      "        [-0.0205, -0.0472, -0.0568,  0.0159, -0.0237],\n",
      "        [ 0.0191,  0.0004,  0.0428, -0.0248,  0.0227],\n",
      "        [ 0.0049,  0.0111,  0.0323, -0.0304, -0.0288],\n",
      "        [-0.0112, -0.0249, -0.0105, -0.0146, -0.0407],\n",
      "        [ 0.0174, -0.0020,  0.0008, -0.0660, -0.0477]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0170, grad_fn=<MinBackward1>), tensor(0.9280, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.16365750133991241\n",
      "@sample 525: tensor([[-0.0078,  0.0325,  0.0175, -0.0352,  0.0107],\n",
      "        [-0.0243, -0.0227,  0.0042, -0.0098, -0.0144],\n",
      "        [-0.0281,  0.0511,  0.0286, -0.0653,  0.0115],\n",
      "        [ 0.0106, -0.0384,  0.0153,  0.0172,  0.0262],\n",
      "        [ 0.0272,  0.0288,  0.0021, -0.0405,  0.0253],\n",
      "        [ 0.0274, -0.0151,  0.0241, -0.0118, -0.0051],\n",
      "        [ 0.0169,  0.0086,  0.0060, -0.0013, -0.0164],\n",
      "        [ 0.0154, -0.0139, -0.0523,  0.0487, -0.0394]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-2.2593e-02, -2.8152e-02, -2.9201e-02,  1.7319e-02, -4.0975e-03],\n",
      "        [-1.7942e-02, -3.0687e-02,  4.9180e-02, -3.9495e-02, -2.7802e-02],\n",
      "        [ 3.6441e-05, -6.9981e-02, -6.5666e-02,  4.3207e-02, -1.1800e-03],\n",
      "        [ 2.1624e-02,  1.5009e-02,  6.2951e-03,  2.8487e-02,  2.0070e-02],\n",
      "        [-6.1342e-02, -5.4623e-02, -1.4636e-01,  9.8839e-02, -5.7835e-03],\n",
      "        [-2.1254e-02,  3.1375e-02, -1.2045e-01,  6.3157e-02,  8.2084e-03],\n",
      "        [-3.4238e-02,  2.7727e-03, -7.0471e-02,  2.7261e-02, -6.8938e-03],\n",
      "        [-1.2059e-02,  6.7172e-03,  4.8782e-02, -2.8442e-02,  1.9396e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0129, grad_fn=<MinBackward1>), tensor(0.9129, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.17040595412254333\n",
      "@sample 526: tensor([[-0.0013,  0.0147,  0.0010, -0.0202,  0.0186],\n",
      "        [ 0.0154,  0.0299, -0.0171, -0.0273,  0.0171],\n",
      "        [-0.0331, -0.0103, -0.0044,  0.0050,  0.0183],\n",
      "        [-0.0257,  0.0049, -0.0114, -0.0122, -0.0045],\n",
      "        [ 0.0173, -0.0674, -0.0512,  0.0551,  0.0350],\n",
      "        [-0.0058, -0.0264,  0.0067,  0.0097,  0.0342],\n",
      "        [-0.0072, -0.0316,  0.0037,  0.0334, -0.0240],\n",
      "        [-0.0128, -0.0318, -0.0740,  0.0204,  0.0013]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0126, -0.0201, -0.0453,  0.0506,  0.0341],\n",
      "        [-0.0322, -0.0261, -0.0763,  0.0180, -0.0245],\n",
      "        [ 0.0129,  0.0201, -0.0024,  0.0086,  0.0447],\n",
      "        [ 0.0116,  0.0137,  0.0117,  0.0131,  0.0158],\n",
      "        [ 0.0537,  0.0594,  0.0680, -0.0798, -0.0397],\n",
      "        [ 0.0287, -0.0360,  0.0301, -0.0043,  0.0131],\n",
      "        [ 0.0025,  0.0512,  0.0664,  0.0012, -0.0092],\n",
      "        [ 0.0050,  0.0177,  0.0288, -0.0038,  0.0202]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0142, grad_fn=<MinBackward1>), tensor(0.9158, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.17091575264930725\n",
      "@sample 527: tensor([[ 0.0294,  0.0045, -0.0280, -0.0182,  0.0105],\n",
      "        [ 0.0055,  0.0027,  0.0107,  0.0473, -0.0267],\n",
      "        [-0.0041, -0.0477, -0.0245, -0.0166, -0.0016],\n",
      "        [ 0.0345,  0.0019,  0.0043,  0.0119, -0.0130],\n",
      "        [-0.0257, -0.0029,  0.0253, -0.0014,  0.0350],\n",
      "        [ 0.0177,  0.0225, -0.0056, -0.0243,  0.0073],\n",
      "        [-0.0368, -0.0273, -0.0266,  0.0074, -0.0004],\n",
      "        [-0.0177, -0.0633, -0.0515,  0.0124, -0.0338]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0210, -0.0136, -0.0371,  0.0510, -0.0046],\n",
      "        [-0.0065,  0.0844,  0.0436,  0.0090,  0.0043],\n",
      "        [ 0.0046, -0.0696,  0.0164, -0.0257, -0.0271],\n",
      "        [-0.0119,  0.0425, -0.0417, -0.0418,  0.0017],\n",
      "        [ 0.0224,  0.0139, -0.0446,  0.0030,  0.0131],\n",
      "        [-0.0211,  0.0065, -0.0402,  0.0238,  0.0165],\n",
      "        [-0.0148, -0.0140,  0.0346, -0.0468, -0.0031],\n",
      "        [ 0.0076, -0.0253,  0.1139, -0.0858, -0.0291]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0133, grad_fn=<MinBackward1>), tensor(0.9519, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.17330056428909302\n",
      "@sample 528: tensor([[ 0.0270, -0.0782, -0.0394,  0.0512,  0.0018],\n",
      "        [-0.0074, -0.0396,  0.0003,  0.0232,  0.0343],\n",
      "        [-0.0115,  0.0089,  0.0201,  0.0204, -0.0171],\n",
      "        [-0.0121, -0.0208, -0.0186, -0.0306,  0.0083],\n",
      "        [-0.0091,  0.0134, -0.0020, -0.0039, -0.0102],\n",
      "        [-0.0092, -0.0132,  0.0022,  0.0278, -0.0316],\n",
      "        [-0.0183,  0.0650,  0.0907, -0.0381,  0.0582],\n",
      "        [ 0.0194, -0.0195,  0.0002, -0.0097,  0.0223]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 1.4081e-02, -4.1945e-04,  1.3744e-02,  2.1458e-06,  2.3425e-02],\n",
      "        [ 1.4769e-02,  1.6183e-02,  2.6661e-02, -2.6109e-02, -3.7745e-03],\n",
      "        [ 1.3041e-02, -4.9578e-03,  1.9835e-02,  3.2368e-02, -6.8852e-03],\n",
      "        [ 1.4120e-04, -2.2488e-02,  1.3860e-02,  1.3472e-02,  2.9344e-02],\n",
      "        [ 1.6123e-02, -2.7578e-02, -7.9299e-02, -2.8018e-03,  7.7968e-04],\n",
      "        [ 3.3080e-02,  3.2462e-02,  2.2456e-02,  5.5991e-03,  1.8588e-02],\n",
      "        [ 3.8331e-03, -1.5829e-02, -1.0089e-02,  1.3535e-02,  3.0475e-03],\n",
      "        [-2.9742e-03, -9.2600e-03, -3.0631e-02,  2.9583e-02,  6.9094e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0135, grad_fn=<MinBackward1>), tensor(0.8814, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1528567522764206\n",
      "@sample 529: tensor([[-0.0225, -0.0150,  0.0047,  0.0068, -0.0208],\n",
      "        [-0.0544,  0.0643,  0.0178, -0.1096,  0.0307],\n",
      "        [-0.0097,  0.0040, -0.0026, -0.0459,  0.0336],\n",
      "        [ 0.0093, -0.0094,  0.0035,  0.0086,  0.0113],\n",
      "        [ 0.0004,  0.0281, -0.0095, -0.0414,  0.0019],\n",
      "        [ 0.0080, -0.0363,  0.0148,  0.0286, -0.0536],\n",
      "        [ 0.0236,  0.0154,  0.0138, -0.0227,  0.0184],\n",
      "        [-0.0172, -0.0020, -0.0084, -0.0313,  0.0380]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0162, -0.0085,  0.0493, -0.0483, -0.0016],\n",
      "        [-0.0412, -0.0404, -0.0925,  0.0366,  0.0181],\n",
      "        [-0.0486, -0.0064, -0.0311,  0.0328, -0.0286],\n",
      "        [-0.0081,  0.0193, -0.0190,  0.0097,  0.0117],\n",
      "        [-0.0445, -0.0094, -0.0784,  0.0511, -0.0078],\n",
      "        [ 0.0003, -0.0025,  0.0080, -0.0583, -0.0096],\n",
      "        [ 0.0208,  0.0066, -0.0230,  0.0227, -0.0242],\n",
      "        [ 0.0070, -0.0233,  0.0128,  0.0019,  0.0328]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0270, grad_fn=<MinBackward1>), tensor(0.9467, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.17017756402492523\n",
      "@sample 530: tensor([[-0.0022,  0.0416, -0.0173,  0.0327, -0.0241],\n",
      "        [ 0.0142,  0.0432,  0.0583, -0.0720,  0.0126],\n",
      "        [-0.0169,  0.0441, -0.0075, -0.0013,  0.0152],\n",
      "        [ 0.0166,  0.0218, -0.0050, -0.0434,  0.0273],\n",
      "        [-0.0077,  0.0099,  0.0124, -0.0145,  0.0034],\n",
      "        [-0.0180,  0.0055,  0.0387, -0.0599, -0.0286],\n",
      "        [-0.0104, -0.0125,  0.0097, -0.0325,  0.0040],\n",
      "        [-0.0292, -0.0154,  0.0128, -0.0130,  0.0058]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0272, -0.0157, -0.0493,  0.0185,  0.0196],\n",
      "        [-0.0260, -0.0299, -0.0351,  0.0139, -0.0340],\n",
      "        [ 0.0011,  0.0064,  0.0525, -0.0227,  0.0212],\n",
      "        [ 0.0054, -0.0156, -0.0387,  0.0377, -0.0144],\n",
      "        [-0.0023,  0.0143,  0.0034, -0.0205, -0.0089],\n",
      "        [-0.0762, -0.0407, -0.0275,  0.0215,  0.0066],\n",
      "        [-0.0244, -0.0388,  0.0279, -0.0698, -0.0129],\n",
      "        [ 0.0085, -0.0216,  0.0447, -0.0924,  0.0035]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0275, grad_fn=<MinBackward1>), tensor(0.9007, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1591770499944687\n",
      "@sample 531: tensor([[-0.0062,  0.0471, -0.0032, -0.0453,  0.0037],\n",
      "        [ 0.0035,  0.0051,  0.0055, -0.0139,  0.0125],\n",
      "        [ 0.0051, -0.0007,  0.0070, -0.0117, -0.0135],\n",
      "        [-0.0059,  0.0039,  0.0238, -0.0095, -0.0299],\n",
      "        [-0.0034,  0.0149, -0.0136, -0.0192, -0.0021],\n",
      "        [-0.0223, -0.0708, -0.0087,  0.0318, -0.0077],\n",
      "        [ 0.0045,  0.0535, -0.0058, -0.0106, -0.0448],\n",
      "        [ 0.0126,  0.0041,  0.0198, -0.0101, -0.0452]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0098, -0.0205, -0.0039,  0.0328,  0.0417],\n",
      "        [ 0.0005,  0.0350, -0.0007, -0.0259,  0.0203],\n",
      "        [-0.0051, -0.0066,  0.0433, -0.0165,  0.0243],\n",
      "        [-0.0093,  0.0043, -0.0036, -0.0238,  0.0233],\n",
      "        [-0.0525, -0.0345, -0.0618,  0.0178, -0.0055],\n",
      "        [ 0.0209,  0.0086,  0.1136, -0.0760,  0.0020],\n",
      "        [-0.0087, -0.0200, -0.0443,  0.0116, -0.0167],\n",
      "        [-0.0042,  0.0200, -0.0543,  0.0231,  0.0321]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0188, grad_fn=<MinBackward1>), tensor(0.8987, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.15534566342830658\n",
      "@sample 532: tensor([[ 0.0076,  0.0255, -0.0095, -0.0353, -0.0095],\n",
      "        [ 0.0206,  0.0048,  0.0131, -0.0399, -0.0325],\n",
      "        [-0.0142,  0.0745,  0.0279, -0.0236,  0.0250],\n",
      "        [ 0.0232,  0.0169, -0.0112, -0.0332,  0.0190],\n",
      "        [ 0.0020,  0.0255,  0.0156,  0.0121,  0.0053],\n",
      "        [-0.0057,  0.0507,  0.0420, -0.0954,  0.0481],\n",
      "        [-0.0080,  0.0074, -0.0085, -0.0094, -0.0128],\n",
      "        [-0.0022,  0.0359,  0.0284, -0.0619,  0.0049]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0385, -0.0481,  0.0422,  0.0077,  0.0008],\n",
      "        [-0.0248,  0.0031, -0.0036,  0.0475, -0.0218],\n",
      "        [ 0.0127, -0.0144, -0.0521,  0.0128,  0.0059],\n",
      "        [-0.0242, -0.0043, -0.0441, -0.0090, -0.0338],\n",
      "        [ 0.0573, -0.0061, -0.0901,  0.0358,  0.0110],\n",
      "        [-0.0905, -0.0387, -0.0429,  0.0299, -0.0484],\n",
      "        [ 0.0140,  0.0377,  0.0347, -0.0164, -0.0079],\n",
      "        [-0.0669, -0.0468,  0.0174, -0.0175,  0.0080]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0186, grad_fn=<MinBackward1>), tensor(0.9069, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1657189130783081\n",
      "@sample 533: tensor([[ 0.0153,  0.0033, -0.0031, -0.0084,  0.0046],\n",
      "        [ 0.0085, -0.0009, -0.0084,  0.0119, -0.0056],\n",
      "        [ 0.0209,  0.0136,  0.0310, -0.0383,  0.0457],\n",
      "        [-0.0190,  0.0500,  0.0439, -0.0503,  0.0139],\n",
      "        [ 0.0147,  0.0730, -0.0084, -0.0434,  0.0137],\n",
      "        [-0.0002,  0.0169,  0.0375, -0.0219,  0.0165],\n",
      "        [ 0.0160, -0.0167,  0.0034, -0.0296,  0.0018],\n",
      "        [ 0.0153,  0.0035,  0.0052, -0.0262, -0.0136]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0040,  0.0053,  0.0022, -0.0269,  0.0039],\n",
      "        [ 0.0016,  0.0206, -0.0722,  0.0014,  0.0228],\n",
      "        [-0.0113,  0.0071, -0.0260,  0.0169, -0.0088],\n",
      "        [-0.0193,  0.0028, -0.0914,  0.0527, -0.0055],\n",
      "        [-0.0437, -0.0179, -0.0931,  0.0332, -0.0329],\n",
      "        [-0.0071, -0.0228, -0.0151, -0.0101, -0.0052],\n",
      "        [-0.0188, -0.0101, -0.0198, -0.0269, -0.0201],\n",
      "        [-0.0139,  0.0178,  0.0003, -0.0004,  0.0204]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0229, grad_fn=<MinBackward1>), tensor(0.9416, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.15673516690731049\n",
      "@sample 534: tensor([[-0.0154, -0.0200, -0.0079,  0.0188, -0.0118],\n",
      "        [ 0.0033,  0.0166, -0.0081,  0.0191,  0.0055],\n",
      "        [-0.0111, -0.0224,  0.0006,  0.0007, -0.0139],\n",
      "        [-0.0352,  0.0111,  0.0445, -0.0055,  0.0217],\n",
      "        [-0.0102,  0.0225, -0.0041, -0.0078, -0.0512],\n",
      "        [-0.0021,  0.0336, -0.0418, -0.0049, -0.0074],\n",
      "        [ 0.0080, -0.0180, -0.0209,  0.0125, -0.0335],\n",
      "        [-0.0061,  0.0250,  0.0215, -0.0120,  0.0066]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0018, -0.0219,  0.0248,  0.0017,  0.0307],\n",
      "        [ 0.0049,  0.0452, -0.0059, -0.0310, -0.0002],\n",
      "        [ 0.0029,  0.0276,  0.0240, -0.0007, -0.0166],\n",
      "        [ 0.0392, -0.0152, -0.0008,  0.0019, -0.0035],\n",
      "        [-0.0686, -0.0172,  0.0382, -0.0460, -0.0460],\n",
      "        [ 0.0146, -0.0106, -0.0109, -0.0073, -0.0017],\n",
      "        [ 0.0058,  0.0016,  0.0292, -0.0113,  0.0029],\n",
      "        [-0.0276, -0.0075, -0.0178,  0.0411, -0.0345]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0191, grad_fn=<MinBackward1>), tensor(0.9284, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1514822393655777\n",
      "@sample 535: tensor([[ 0.0025,  0.0018,  0.0174, -0.0036,  0.0167],\n",
      "        [-0.0097,  0.0189, -0.0036, -0.0214, -0.0422],\n",
      "        [-0.0135,  0.0512,  0.0059, -0.0898,  0.0232],\n",
      "        [-0.0181,  0.0021,  0.0025,  0.0401, -0.0125],\n",
      "        [ 0.0033, -0.0042, -0.0012,  0.0289, -0.0064],\n",
      "        [ 0.0320,  0.0751,  0.0358, -0.0488, -0.0054],\n",
      "        [-0.0132,  0.0081, -0.0562,  0.0273, -0.0194],\n",
      "        [-0.0084,  0.0008, -0.0029, -0.0600, -0.0298]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0102,  0.0158, -0.0241,  0.0120,  0.0121],\n",
      "        [-0.0206, -0.0022,  0.0456, -0.0324, -0.0413],\n",
      "        [-0.0434,  0.0068, -0.0300,  0.0356, -0.0123],\n",
      "        [ 0.0018,  0.0413,  0.0080,  0.0199,  0.0245],\n",
      "        [ 0.0054,  0.0203,  0.0357, -0.0190, -0.0229],\n",
      "        [-0.0058,  0.0172, -0.0093,  0.0096, -0.0253],\n",
      "        [ 0.0017,  0.0238, -0.0043,  0.0158,  0.0474],\n",
      "        [-0.0961, -0.1005, -0.0353,  0.0450,  0.0035]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0107, grad_fn=<MinBackward1>), tensor(0.9321, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1696525514125824\n",
      "@sample 536: tensor([[-0.0098,  0.0479,  0.0085, -0.0312,  0.0132],\n",
      "        [ 0.0300, -0.0155, -0.0164,  0.0451, -0.0171],\n",
      "        [ 0.0015,  0.0491, -0.0018, -0.0243,  0.0026],\n",
      "        [-0.0108, -0.0301, -0.0277,  0.0205, -0.0640],\n",
      "        [-0.0079,  0.0046, -0.0190,  0.0294,  0.0013],\n",
      "        [ 0.0028,  0.0087,  0.0101,  0.0369, -0.0287],\n",
      "        [ 0.0155,  0.0771, -0.0127, -0.0092, -0.0082],\n",
      "        [ 0.0335,  0.0104, -0.0096,  0.0187,  0.0144]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0083,  0.0045, -0.0593,  0.0144,  0.0019],\n",
      "        [ 0.0447,  0.0076, -0.0212,  0.0069,  0.0565],\n",
      "        [-0.0331, -0.0324, -0.0855,  0.0842,  0.0403],\n",
      "        [-0.0245, -0.0014, -0.0067,  0.0033, -0.0105],\n",
      "        [ 0.0200,  0.0282,  0.0310, -0.0373,  0.0058],\n",
      "        [ 0.0424,  0.0673,  0.0136,  0.0291,  0.0058],\n",
      "        [ 0.0243,  0.0032, -0.0531, -0.0008, -0.0518],\n",
      "        [ 0.0193,  0.0059, -0.0501,  0.0243, -0.0014]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0201, grad_fn=<MinBackward1>), tensor(0.9206, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.15828458964824677\n",
      "@sample 537: tensor([[ 0.0054,  0.1041,  0.0248, -0.0730,  0.0422],\n",
      "        [ 0.0112,  0.0521, -0.0004,  0.0260,  0.0241],\n",
      "        [ 0.0281,  0.0169, -0.0414,  0.0277,  0.0268],\n",
      "        [ 0.0052, -0.0129,  0.0184,  0.0138,  0.0070],\n",
      "        [-0.0038, -0.0167, -0.0167, -0.0314, -0.0218],\n",
      "        [ 0.0078, -0.0158, -0.0335,  0.0306,  0.0395],\n",
      "        [ 0.0014, -0.0347, -0.0244,  0.0054,  0.0152],\n",
      "        [ 0.0149,  0.0351,  0.0372, -0.0278,  0.0401]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0240, -0.0365, -0.0859,  0.0380, -0.0246],\n",
      "        [ 0.0294, -0.0014, -0.1174,  0.0755,  0.0096],\n",
      "        [ 0.0093,  0.0542, -0.0179, -0.0738, -0.0725],\n",
      "        [ 0.0172, -0.0177,  0.0420,  0.0076,  0.0047],\n",
      "        [ 0.0262, -0.0502,  0.0270, -0.0006,  0.0214],\n",
      "        [ 0.0219,  0.0143, -0.0444,  0.0158, -0.0279],\n",
      "        [ 0.0039,  0.0098,  0.0251, -0.0202, -0.0358],\n",
      "        [-0.0178, -0.0137, -0.0825,  0.1028,  0.0276]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0196, grad_fn=<MinBackward1>), tensor(0.9021, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.18244074285030365\n",
      "@sample 538: tensor([[ 0.0326, -0.0113, -0.0177,  0.0034,  0.0191],\n",
      "        [-0.0010,  0.0278,  0.0256, -0.0557,  0.0373],\n",
      "        [-0.0031, -0.0293, -0.0267,  0.0170,  0.0546],\n",
      "        [ 0.0077,  0.0011,  0.0194, -0.0110,  0.0012],\n",
      "        [ 0.0017,  0.0289, -0.0009, -0.0146,  0.0304],\n",
      "        [ 0.0415, -0.0305,  0.0029, -0.0016,  0.0038],\n",
      "        [-0.0024, -0.0083, -0.0165,  0.0004, -0.0185],\n",
      "        [ 0.0002, -0.0276, -0.0083,  0.0051, -0.0069]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0004, -0.0010,  0.0218,  0.0332,  0.0246],\n",
      "        [ 0.0103, -0.0100, -0.0459,  0.0099,  0.0248],\n",
      "        [ 0.0125,  0.0083, -0.0803,  0.0034, -0.0055],\n",
      "        [-0.0395, -0.0191, -0.0416,  0.0230,  0.0093],\n",
      "        [-0.0101, -0.0183, -0.0868,  0.0372, -0.0370],\n",
      "        [ 0.0043,  0.0133,  0.0096, -0.0077,  0.0597],\n",
      "        [ 0.0009, -0.0031, -0.0258, -0.0011, -0.0179],\n",
      "        [-0.0200,  0.0042, -0.0248,  0.0190,  0.0034]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0238, grad_fn=<MinBackward1>), tensor(0.9105, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.14332176744937897\n",
      "@sample 539: tensor([[ 0.0179, -0.0245, -0.0041,  0.0562, -0.0311],\n",
      "        [-0.0012,  0.0250,  0.0001, -0.0046, -0.0259],\n",
      "        [ 0.0148, -0.0348, -0.0134,  0.0004,  0.0285],\n",
      "        [ 0.0175, -0.0722, -0.0082,  0.0659,  0.0249],\n",
      "        [ 0.0015, -0.0193,  0.0327, -0.0268, -0.0049],\n",
      "        [ 0.0487, -0.0176,  0.0246, -0.0021,  0.0037],\n",
      "        [ 0.0312, -0.0052, -0.0170, -0.0038,  0.0025],\n",
      "        [-0.0291,  0.0319,  0.0057, -0.0580,  0.0471]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 3.9913e-02,  3.3410e-02,  2.2136e-02, -6.8203e-05,  5.2008e-03],\n",
      "        [-1.0712e-02,  3.4527e-02,  2.1042e-02, -3.1367e-02, -7.1275e-03],\n",
      "        [-1.5796e-02, -9.5922e-03,  2.3993e-02, -1.8725e-02, -2.0411e-02],\n",
      "        [ 1.0401e-02,  1.5334e-02,  3.8247e-02,  1.0061e-02, -1.2475e-02],\n",
      "        [-9.1576e-02,  7.0902e-03, -2.0086e-02,  4.3588e-02,  3.6231e-03],\n",
      "        [-3.8370e-02, -3.6863e-03, -1.0272e-02, -2.6520e-02,  9.9481e-04],\n",
      "        [ 1.0334e-02, -4.5527e-02, -5.1352e-02,  2.5064e-02,  3.9723e-02],\n",
      "        [-8.7448e-02, -6.2978e-02, -1.0935e-01,  9.5834e-02, -1.4711e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0102, grad_fn=<MinBackward1>), tensor(0.9435, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.16813203692436218\n",
      "@sample 540: tensor([[-0.0078,  0.0083,  0.0144, -0.0401,  0.0343],\n",
      "        [ 0.0138, -0.0267, -0.0082,  0.0413, -0.0224],\n",
      "        [-0.0469,  0.0066,  0.0148, -0.0333,  0.0143],\n",
      "        [ 0.0062, -0.0336, -0.0217,  0.0197,  0.0131],\n",
      "        [ 0.0065,  0.0008,  0.0316, -0.0284,  0.0138],\n",
      "        [ 0.0190, -0.0242, -0.0187,  0.0112, -0.0052],\n",
      "        [ 0.0171,  0.0217,  0.0203,  0.0032,  0.0372],\n",
      "        [-0.0302,  0.0093,  0.0010, -0.0046,  0.0519]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0135, -0.0220, -0.0316,  0.0787,  0.0447],\n",
      "        [-0.0180,  0.0592,  0.0263, -0.0417, -0.0166],\n",
      "        [ 0.0181, -0.0208, -0.0305,  0.0089,  0.0348],\n",
      "        [-0.0099,  0.0238,  0.0131, -0.0074,  0.0336],\n",
      "        [ 0.0035, -0.0412,  0.0016,  0.0079,  0.0132],\n",
      "        [ 0.0500,  0.0250,  0.0693, -0.0391, -0.0059],\n",
      "        [ 0.0017,  0.0189, -0.0055,  0.0278,  0.0335],\n",
      "        [ 0.0488,  0.0149, -0.0503,  0.0118, -0.0045]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0189, grad_fn=<MinBackward1>), tensor(0.9318, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.15490515530109406\n",
      "@sample 541: tensor([[ 0.0034, -0.0585,  0.0195,  0.0208,  0.0141],\n",
      "        [ 0.0051,  0.0314,  0.0426, -0.0617, -0.0033],\n",
      "        [-0.0052, -0.0034, -0.0025, -0.0508,  0.0242],\n",
      "        [ 0.0084, -0.0007,  0.0479, -0.0612,  0.0441],\n",
      "        [ 0.0234, -0.0384, -0.0083, -0.0223, -0.0050],\n",
      "        [ 0.0164,  0.0017, -0.0087, -0.0179,  0.0124],\n",
      "        [ 0.0100, -0.0188,  0.0045, -0.0104,  0.0043],\n",
      "        [-0.0223,  0.0171,  0.0399, -0.0372,  0.0588]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0364,  0.0182,  0.0515,  0.0312,  0.0035],\n",
      "        [-0.0338, -0.0380, -0.0230,  0.0454,  0.0232],\n",
      "        [-0.0620, -0.0074,  0.0299,  0.0710,  0.0167],\n",
      "        [-0.0402, -0.0446, -0.1503,  0.0784, -0.0235],\n",
      "        [-0.0514, -0.0287, -0.0450,  0.0424, -0.0025],\n",
      "        [-0.0279,  0.0011, -0.0347,  0.0025, -0.0077],\n",
      "        [ 0.0138,  0.0415,  0.0263, -0.0067,  0.0227],\n",
      "        [-0.0449,  0.0200, -0.0735,  0.0575,  0.0230]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0177, grad_fn=<MinBackward1>), tensor(0.9295, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.16722466051578522\n",
      "@sample 542: tensor([[-0.0192,  0.0086, -0.0059, -0.0106, -0.0171],\n",
      "        [ 0.0028, -0.0339, -0.0053,  0.0192,  0.0014],\n",
      "        [-0.0118, -0.0074, -0.0082,  0.0133,  0.0257],\n",
      "        [ 0.0109,  0.0119,  0.0234, -0.0042,  0.0284],\n",
      "        [ 0.0248, -0.0430, -0.0165,  0.0153, -0.0079],\n",
      "        [-0.0144,  0.0742,  0.0511, -0.0940,  0.0536],\n",
      "        [-0.0029,  0.0060, -0.0213,  0.0286, -0.0263],\n",
      "        [-0.0165,  0.0193,  0.0172, -0.0128,  0.0030]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0162,  0.0241,  0.0037, -0.0101,  0.0133],\n",
      "        [-0.0006, -0.0002,  0.0100, -0.0149,  0.0032],\n",
      "        [ 0.0617,  0.0405,  0.0237, -0.0352,  0.0099],\n",
      "        [ 0.0002,  0.0081, -0.0223,  0.0415,  0.0188],\n",
      "        [ 0.0174,  0.0162,  0.0200, -0.0251,  0.0038],\n",
      "        [-0.0267, -0.0726, -0.0953,  0.0507, -0.0061],\n",
      "        [ 0.0194, -0.0002,  0.0225, -0.0254,  0.0070],\n",
      "        [-0.0332, -0.0176,  0.0278, -0.0411, -0.0288]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0175, grad_fn=<MinBackward1>), tensor(0.9185, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.15522699058055878\n",
      "@sample 543: tensor([[-0.0351, -0.0147,  0.0123,  0.0240, -0.0397],\n",
      "        [ 0.0128, -0.0233,  0.0023,  0.0413,  0.0314],\n",
      "        [-0.0121,  0.0226,  0.0374, -0.0623,  0.0180],\n",
      "        [ 0.0007,  0.0192, -0.0039, -0.0739,  0.0485],\n",
      "        [-0.0445,  0.0141,  0.0362, -0.0433,  0.0546],\n",
      "        [ 0.0141,  0.0515, -0.0181, -0.0200,  0.0292],\n",
      "        [-0.0103, -0.0028, -0.0052, -0.0152, -0.0013],\n",
      "        [ 0.0200, -0.0281, -0.0307,  0.0168,  0.0002]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-5.9213e-03,  3.8888e-03,  6.6644e-02, -4.5183e-02, -6.8138e-03],\n",
      "        [-2.6059e-03,  2.2804e-02,  3.0404e-02,  1.0512e-02, -2.2778e-02],\n",
      "        [-4.3666e-02, -1.2887e-02, -7.6863e-02,  4.5857e-02, -1.3078e-03],\n",
      "        [-4.0363e-02, -6.6227e-02, -1.0740e-01,  6.6183e-02, -4.5803e-02],\n",
      "        [ 7.7827e-03, -2.5870e-02, -7.1511e-05, -3.1316e-03, -2.6431e-02],\n",
      "        [ 4.0781e-03, -6.3749e-02, -6.6725e-02,  7.4660e-02,  3.7137e-02],\n",
      "        [-1.0878e-02, -7.9739e-03,  3.9288e-02,  8.2049e-03, -1.7177e-02],\n",
      "        [ 7.7885e-03, -5.0511e-02,  1.5457e-02, -4.1637e-02, -4.3833e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0211, grad_fn=<MinBackward1>), tensor(0.9055, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1606585681438446\n",
      "@sample 544: tensor([[ 0.0382,  0.0644, -0.0232, -0.0107, -0.0066],\n",
      "        [-0.0020,  0.0038,  0.0104, -0.0070,  0.0063],\n",
      "        [-0.0019,  0.0341,  0.0125, -0.0101, -0.0019],\n",
      "        [-0.0327, -0.0134, -0.0011,  0.0324,  0.0115],\n",
      "        [-0.0057, -0.0092,  0.0082, -0.0156, -0.0186],\n",
      "        [ 0.0198, -0.0257,  0.0008, -0.0164,  0.0263],\n",
      "        [ 0.0241,  0.0278, -0.0008, -0.0265,  0.0154],\n",
      "        [-0.0188,  0.0175,  0.0106,  0.0210, -0.0157]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0094, -0.0191, -0.0630,  0.0824,  0.0335],\n",
      "        [-0.0200, -0.0042, -0.0440,  0.0097,  0.0049],\n",
      "        [-0.0029,  0.0050, -0.0408,  0.0184, -0.0133],\n",
      "        [ 0.0297,  0.0228, -0.0080, -0.0140,  0.0404],\n",
      "        [-0.0268,  0.0293, -0.0081, -0.0309,  0.0169],\n",
      "        [ 0.0236,  0.0014,  0.0859, -0.0071,  0.0137],\n",
      "        [-0.0186, -0.0208, -0.0432,  0.0467,  0.0172],\n",
      "        [-0.0109,  0.0357,  0.0060,  0.0158,  0.0033]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0227, grad_fn=<MinBackward1>), tensor(0.9262, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1507953554391861\n",
      "@sample 545: tensor([[ 0.0146, -0.0060, -0.0335,  0.0430, -0.0319],\n",
      "        [ 0.0066, -0.0538, -0.0530,  0.0420, -0.0568],\n",
      "        [-0.0280, -0.0153, -0.0016, -0.0078,  0.0163],\n",
      "        [ 0.0015, -0.0301, -0.0291,  0.0539, -0.0160],\n",
      "        [-0.0058, -0.0032, -0.0055,  0.0341,  0.0015],\n",
      "        [ 0.0069,  0.0073,  0.0039, -0.0116, -0.0151],\n",
      "        [ 0.0131, -0.0016, -0.0125,  0.0172,  0.0011],\n",
      "        [-0.0143,  0.0184, -0.0427,  0.0172, -0.0211]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0284, -0.0061,  0.0043, -0.0435,  0.0041],\n",
      "        [ 0.0149,  0.0039,  0.0979, -0.0479, -0.0206],\n",
      "        [ 0.0176, -0.0131,  0.0505, -0.0236,  0.0359],\n",
      "        [-0.0148,  0.0430,  0.0372, -0.0076,  0.0026],\n",
      "        [ 0.0435,  0.0405,  0.0319, -0.0306, -0.0336],\n",
      "        [-0.0285,  0.0145, -0.0476,  0.0073, -0.0212],\n",
      "        [ 0.0035,  0.0246,  0.0025, -0.0038,  0.0105],\n",
      "        [ 0.0063,  0.0133,  0.0167, -0.0281,  0.0288]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0176, grad_fn=<MinBackward1>), tensor(0.9186, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.15505485236644745\n",
      "@sample 546: tensor([[ 0.0117,  0.0422,  0.0398,  0.0059,  0.0132],\n",
      "        [ 0.0088, -0.0256, -0.0240,  0.0149, -0.0007],\n",
      "        [ 0.0050, -0.0070,  0.0294,  0.0517, -0.0098],\n",
      "        [-0.0050, -0.0216, -0.0411,  0.0356, -0.0290],\n",
      "        [ 0.0044,  0.0022, -0.0131, -0.0190,  0.0236],\n",
      "        [-0.0010,  0.0256,  0.0134, -0.0349,  0.0226],\n",
      "        [-0.0130,  0.0116, -0.0154, -0.0175, -0.0025],\n",
      "        [ 0.0146, -0.0289,  0.0044,  0.0280, -0.0026]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0599,  0.0364, -0.0785,  0.0337, -0.0283],\n",
      "        [-0.0066, -0.0068,  0.0009,  0.0152,  0.0301],\n",
      "        [-0.0041,  0.0606,  0.0390, -0.0702, -0.0485],\n",
      "        [ 0.0184,  0.0397, -0.0052, -0.0266, -0.0519],\n",
      "        [ 0.0125,  0.0130,  0.0074,  0.0098, -0.0060],\n",
      "        [-0.0207, -0.0352, -0.0526,  0.0228, -0.0127],\n",
      "        [-0.0086, -0.0266, -0.0095, -0.0272, -0.0030],\n",
      "        [-0.0124,  0.0380,  0.0433, -0.0271,  0.0142]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0254, grad_fn=<MinBackward1>), tensor(0.9178, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.15228284895420074\n",
      "@sample 547: tensor([[ 1.8878e-02,  5.1563e-02, -3.9426e-02, -1.3465e-03, -1.3943e-02],\n",
      "        [-3.6006e-02, -2.2614e-03, -2.3622e-02,  3.7660e-03, -2.5404e-02],\n",
      "        [ 1.4909e-02, -1.0437e-02,  1.1739e-02,  9.8616e-03, -6.1861e-03],\n",
      "        [ 9.4784e-03, -2.2148e-02,  2.9065e-05,  9.9416e-04, -2.9450e-03],\n",
      "        [ 1.2801e-02,  5.5858e-02, -1.0077e-02, -1.5025e-02, -2.1487e-02],\n",
      "        [ 7.9385e-03,  1.7584e-02,  2.0813e-02,  2.1822e-02, -7.7432e-03],\n",
      "        [ 2.5717e-02, -2.1327e-02, -3.0704e-02,  5.3953e-02, -2.9222e-02],\n",
      "        [ 2.6270e-02,  3.6032e-02,  2.1942e-02, -1.6588e-02,  2.3715e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0130, -0.0121, -0.0212, -0.0029,  0.0092],\n",
      "        [-0.0120, -0.0131,  0.0260, -0.0373,  0.0044],\n",
      "        [ 0.0417, -0.0062,  0.0254,  0.0200,  0.0176],\n",
      "        [ 0.0304, -0.0260,  0.0792, -0.0267,  0.0165],\n",
      "        [-0.0202, -0.0061, -0.0382,  0.0038, -0.0084],\n",
      "        [ 0.0253,  0.0190, -0.0566,  0.0040, -0.0037],\n",
      "        [ 0.0164,  0.0417, -0.0426, -0.0428, -0.0671],\n",
      "        [-0.0207, -0.0682, -0.0822,  0.0019, -0.0272]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0213, grad_fn=<MinBackward1>), tensor(0.9356, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.14521524310112\n",
      "@sample 548: tensor([[ 1.0918e-03, -5.8180e-02, -2.0796e-03,  4.9295e-02, -1.5968e-02],\n",
      "        [ 2.2938e-02, -3.6185e-02,  2.6186e-02,  9.8689e-02, -2.9453e-02],\n",
      "        [-2.4846e-02, -3.6584e-02,  2.7016e-03, -3.1968e-03,  2.5669e-03],\n",
      "        [ 2.1315e-02,  3.7730e-02, -1.0642e-03, -2.0790e-02,  1.4115e-02],\n",
      "        [ 1.5901e-02,  4.5790e-02,  2.7483e-02, -1.8770e-02,  2.8833e-02],\n",
      "        [ 2.5878e-02,  3.5668e-02, -3.1777e-02, -2.2799e-06, -2.8904e-03],\n",
      "        [-5.2219e-03,  1.4307e-02,  1.9254e-02, -1.8049e-02,  1.4533e-02],\n",
      "        [-2.8655e-02, -5.7611e-03, -6.0541e-03, -2.3476e-02,  3.3373e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0396,  0.0063,  0.0531, -0.0541, -0.0191],\n",
      "        [ 0.0469,  0.0424, -0.0038,  0.0160, -0.0076],\n",
      "        [ 0.0475,  0.0259,  0.1033,  0.0024,  0.0251],\n",
      "        [-0.0226, -0.0356, -0.0959,  0.0783,  0.0395],\n",
      "        [ 0.0244, -0.0287, -0.0801,  0.0357,  0.0008],\n",
      "        [ 0.0308,  0.0163, -0.0057, -0.0299,  0.0186],\n",
      "        [-0.0197, -0.0140, -0.0584,  0.0263,  0.0031],\n",
      "        [-0.0450, -0.0099,  0.0002, -0.0136,  0.0002]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0178, grad_fn=<MinBackward1>), tensor(0.9117, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.16106808185577393\n",
      "@sample 549: tensor([[ 0.0092,  0.0101, -0.0239,  0.0239, -0.0012],\n",
      "        [-0.0174, -0.0307, -0.0268,  0.0623, -0.0387],\n",
      "        [-0.0061,  0.0203, -0.0126, -0.0246,  0.0058],\n",
      "        [-0.0058, -0.0143,  0.0026, -0.0325, -0.0155],\n",
      "        [-0.0114, -0.0021,  0.0265,  0.0314, -0.0477],\n",
      "        [-0.0345,  0.0175, -0.0060,  0.0204,  0.0220],\n",
      "        [ 0.0111, -0.0022, -0.0328,  0.0420, -0.0175],\n",
      "        [-0.0140,  0.0019,  0.0339, -0.0350, -0.0261]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0274,  0.0072,  0.0197,  0.0042,  0.0133],\n",
      "        [ 0.0236, -0.0125,  0.0551, -0.0827, -0.0177],\n",
      "        [-0.0397, -0.0019,  0.0009,  0.0087, -0.0132],\n",
      "        [-0.0244, -0.0154,  0.0099, -0.0176,  0.0295],\n",
      "        [ 0.0257, -0.0067,  0.0795, -0.0614, -0.0079],\n",
      "        [ 0.0123, -0.0126, -0.0900,  0.0233,  0.0547],\n",
      "        [ 0.0397,  0.0224, -0.0120, -0.0158,  0.0176],\n",
      "        [ 0.0002, -0.0362, -0.0386,  0.0202,  0.0171]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0204, grad_fn=<MinBackward1>), tensor(0.9485, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1545432209968567\n",
      "@sample 550: tensor([[-0.0202,  0.0232, -0.0437,  0.0117,  0.0213],\n",
      "        [ 0.0112, -0.0042,  0.0065,  0.0266, -0.0392],\n",
      "        [ 0.0136, -0.0045, -0.0230, -0.0305,  0.0096],\n",
      "        [ 0.0251,  0.0471,  0.0075,  0.0188,  0.0209],\n",
      "        [ 0.0010, -0.0188, -0.0023, -0.0207, -0.0180],\n",
      "        [ 0.0103, -0.0184,  0.0067,  0.0288, -0.0094],\n",
      "        [ 0.0135, -0.0407, -0.0228,  0.0582,  0.0088],\n",
      "        [ 0.0123, -0.0005, -0.0062,  0.0343,  0.0023]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0177,  0.0085,  0.0131, -0.0833, -0.0049],\n",
      "        [-0.0021,  0.0538,  0.0121, -0.0076, -0.0124],\n",
      "        [-0.0077, -0.0256, -0.0220, -0.0185, -0.0427],\n",
      "        [ 0.0021,  0.0059, -0.0711,  0.0541, -0.0143],\n",
      "        [-0.0001,  0.0710,  0.0141,  0.0077,  0.0241],\n",
      "        [ 0.0233,  0.0172,  0.0200, -0.0154,  0.0067],\n",
      "        [-0.0079,  0.0241, -0.0328, -0.0195, -0.0509],\n",
      "        [-0.0087,  0.0050,  0.0231, -0.0122,  0.0132]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0170, grad_fn=<MinBackward1>), tensor(0.9020, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1524704396724701\n",
      "@sample 551: tensor([[ 7.0394e-03, -8.4845e-03,  3.8298e-02, -1.3768e-02, -4.9282e-03],\n",
      "        [ 1.2204e-02,  5.8084e-02, -5.9139e-04, -7.7013e-02,  3.3685e-02],\n",
      "        [ 1.2630e-02, -2.6384e-02, -3.3408e-05,  1.8589e-02,  1.4544e-02],\n",
      "        [ 3.9926e-02,  3.3380e-02,  4.2840e-02, -1.3654e-02,  4.7366e-02],\n",
      "        [ 1.1860e-02, -3.2469e-03,  2.5693e-02,  2.1506e-02, -1.2776e-02],\n",
      "        [-2.4224e-03, -2.7690e-03,  3.6353e-03, -8.4140e-03, -1.8477e-02],\n",
      "        [ 1.1913e-03,  1.4182e-02,  2.7337e-02,  1.2384e-02, -1.5480e-02],\n",
      "        [-3.3693e-03,  1.6453e-02, -2.1923e-03, -3.3717e-02,  3.1145e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0244,  0.0158,  0.0062, -0.0295, -0.0172],\n",
      "        [-0.0445, -0.0661, -0.0630,  0.0401, -0.0229],\n",
      "        [-0.0104, -0.0147,  0.0265, -0.0035, -0.0038],\n",
      "        [-0.0258,  0.0047, -0.1187,  0.0283, -0.0098],\n",
      "        [ 0.0027, -0.0300,  0.0725,  0.0045, -0.0114],\n",
      "        [ 0.0023, -0.0076, -0.0565,  0.0158, -0.0182],\n",
      "        [-0.0276,  0.0334, -0.0046,  0.0238, -0.0005],\n",
      "        [-0.0105, -0.0324, -0.0464,  0.0287, -0.0188]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0187, grad_fn=<MinBackward1>), tensor(0.9211, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.15364858508110046\n",
      "@sample 552: tensor([[ 0.0160,  0.0554, -0.0156, -0.0861,  0.0868],\n",
      "        [ 0.0293,  0.0045, -0.0179,  0.0092,  0.0052],\n",
      "        [-0.0135,  0.0288,  0.0389, -0.0327, -0.0249],\n",
      "        [-0.0146, -0.0178,  0.0073, -0.0271,  0.0318],\n",
      "        [-0.0458, -0.0423,  0.0192, -0.0162,  0.0379],\n",
      "        [ 0.0128, -0.0142,  0.0235, -0.0261,  0.0102],\n",
      "        [-0.0226,  0.0073, -0.0179,  0.0087, -0.0022],\n",
      "        [-0.0135,  0.0188,  0.0217,  0.0021,  0.0331]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0399, -0.0379, -0.0793,  0.0572,  0.0037],\n",
      "        [-0.0226, -0.0299, -0.0556,  0.0423,  0.0247],\n",
      "        [-0.0141, -0.0436, -0.0704,  0.0498, -0.0287],\n",
      "        [-0.0109, -0.0208, -0.0314,  0.0147, -0.0309],\n",
      "        [ 0.0401, -0.0379,  0.0584, -0.0239, -0.0563],\n",
      "        [-0.0114, -0.0321, -0.0237,  0.0289, -0.0207],\n",
      "        [ 0.0041, -0.0312,  0.0459, -0.0214,  0.0111],\n",
      "        [-0.0049,  0.0091, -0.0258,  0.0014, -0.0096]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0205, grad_fn=<MinBackward1>), tensor(0.8968, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.17055568099021912\n",
      "@sample 553: tensor([[ 0.0002, -0.0285, -0.0402,  0.0301, -0.0469],\n",
      "        [-0.0182,  0.0751,  0.0428, -0.0841, -0.0118],\n",
      "        [ 0.0032, -0.0021,  0.0091, -0.0044,  0.0280],\n",
      "        [-0.0311, -0.0021,  0.0219, -0.0304,  0.0126],\n",
      "        [-0.0120, -0.0172,  0.0093, -0.0095, -0.0230],\n",
      "        [-0.0193, -0.0032,  0.0139, -0.0365,  0.0252],\n",
      "        [ 0.0083,  0.0440, -0.0210, -0.0197, -0.0140],\n",
      "        [ 0.0164,  0.0035,  0.0372,  0.0065, -0.0169]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0010, -0.0202,  0.0457, -0.0347, -0.0088],\n",
      "        [-0.0590, -0.0504, -0.0435,  0.0572, -0.0312],\n",
      "        [-0.0293,  0.0145, -0.0085, -0.0158, -0.0256],\n",
      "        [-0.0361, -0.0550, -0.0066,  0.0171,  0.0014],\n",
      "        [-0.0008, -0.0391, -0.0162, -0.0032,  0.0181],\n",
      "        [-0.0155, -0.0253, -0.0247,  0.0133, -0.0189],\n",
      "        [-0.0016,  0.0181, -0.0001,  0.0155,  0.0051],\n",
      "        [ 0.0069, -0.0200, -0.0134,  0.0295, -0.0023]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0141, grad_fn=<MinBackward1>), tensor(0.9287, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1490364819765091\n",
      "@sample 554: tensor([[-6.1412e-03,  2.8122e-02,  3.6740e-02, -6.3567e-03,  3.2852e-02],\n",
      "        [-3.8058e-03,  7.6426e-02,  5.3623e-02, -4.3854e-02, -4.3925e-03],\n",
      "        [-2.8149e-02,  9.9860e-03,  2.3134e-02, -1.0193e-02,  1.7438e-02],\n",
      "        [-3.5942e-03,  2.2011e-02,  4.7240e-02, -2.0846e-02,  1.5306e-02],\n",
      "        [-6.9693e-03, -4.7516e-02,  1.1001e-02,  2.4856e-03, -1.6826e-02],\n",
      "        [ 1.7494e-02,  3.5703e-02,  2.3669e-02, -4.6566e-02,  3.2267e-02],\n",
      "        [ 3.9547e-02, -6.5734e-03,  9.0296e-04, -2.0129e-02,  1.2528e-02],\n",
      "        [-3.9771e-03,  6.2478e-03, -9.1203e-05,  9.0982e-03, -1.2360e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0416, -0.0056, -0.0523,  0.0082,  0.0285],\n",
      "        [-0.0471,  0.0038, -0.0600,  0.0136,  0.0288],\n",
      "        [ 0.0089,  0.0169,  0.0164, -0.0087, -0.0106],\n",
      "        [-0.0178,  0.0109, -0.0282, -0.0117, -0.0333],\n",
      "        [-0.0170, -0.0225,  0.0498, -0.0279, -0.0336],\n",
      "        [-0.0034,  0.0023, -0.0453, -0.0058, -0.0168],\n",
      "        [-0.0046, -0.0387,  0.0379,  0.0049,  0.0372],\n",
      "        [ 0.0163,  0.0137,  0.0117,  0.0101,  0.0183]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0212, grad_fn=<MinBackward1>), tensor(0.9280, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.15082383155822754\n",
      "@sample 555: tensor([[-0.0445, -0.0339,  0.0118, -0.0161, -0.0005],\n",
      "        [ 0.0110, -0.0747, -0.0375,  0.0096,  0.0180],\n",
      "        [ 0.0135,  0.0060,  0.0242, -0.0309, -0.0158],\n",
      "        [-0.0190, -0.0038,  0.0030, -0.0431, -0.0210],\n",
      "        [ 0.0036,  0.0345,  0.0522, -0.0236, -0.0177],\n",
      "        [-0.0428,  0.0225,  0.0175, -0.0457,  0.0159],\n",
      "        [-0.0382,  0.0289,  0.0214,  0.0021, -0.0078],\n",
      "        [-0.0213, -0.0164,  0.0246, -0.0362,  0.0154]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0070, -0.0585,  0.0413, -0.0419,  0.0241],\n",
      "        [ 0.0123, -0.0326,  0.0266,  0.0181, -0.0120],\n",
      "        [ 0.0079,  0.0001, -0.0276,  0.0100, -0.0174],\n",
      "        [-0.0566, -0.0542,  0.0149,  0.0152, -0.0023],\n",
      "        [-0.0364, -0.0196,  0.0066,  0.0264,  0.0020],\n",
      "        [-0.0440, -0.0700, -0.0798,  0.0184, -0.0313],\n",
      "        [ 0.0127, -0.0150,  0.0373, -0.0330, -0.0211],\n",
      "        [ 0.0042, -0.0470,  0.0637, -0.0008, -0.0113]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0194, grad_fn=<MinBackward1>), tensor(0.9141, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.15320664644241333\n",
      "@sample 556: tensor([[-0.0042, -0.0416, -0.0043,  0.0088,  0.0038],\n",
      "        [-0.0362, -0.0201,  0.0079, -0.0041, -0.0233],\n",
      "        [ 0.0072, -0.0202,  0.0563, -0.0145,  0.0046],\n",
      "        [-0.0170,  0.0015,  0.0175, -0.0004,  0.0070],\n",
      "        [ 0.0122, -0.0144,  0.0133,  0.0255, -0.0012],\n",
      "        [ 0.0223, -0.0064, -0.0101,  0.0228, -0.0221],\n",
      "        [-0.0151, -0.0387,  0.0025,  0.0100,  0.0022],\n",
      "        [-0.0021, -0.0323, -0.0359,  0.0294, -0.0153]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0144, -0.0076,  0.0146,  0.0222, -0.0034],\n",
      "        [ 0.0057, -0.0278,  0.1061, -0.0709, -0.0203],\n",
      "        [ 0.0218, -0.0315, -0.0339, -0.0079, -0.0460],\n",
      "        [-0.0147, -0.0051, -0.0046, -0.0102, -0.0133],\n",
      "        [-0.0089,  0.0496,  0.0131, -0.0209,  0.0039],\n",
      "        [-0.0131,  0.0016,  0.0088, -0.0014,  0.0159],\n",
      "        [-0.0019, -0.0171, -0.0077,  0.0146, -0.0069],\n",
      "        [-0.0089,  0.0063, -0.0162, -0.0134,  0.0102]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0151, grad_fn=<MinBackward1>), tensor(0.8970, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1450432986021042\n",
      "@sample 557: tensor([[-0.0225, -0.0408, -0.0240,  0.0121, -0.0167],\n",
      "        [-0.0185,  0.0712,  0.0373, -0.0141,  0.0279],\n",
      "        [-0.0258,  0.0455,  0.0027,  0.0089, -0.0018],\n",
      "        [-0.0088,  0.0228,  0.0058, -0.0314,  0.0440],\n",
      "        [ 0.0411,  0.0083,  0.0157,  0.0301, -0.0456],\n",
      "        [ 0.0039, -0.0167, -0.0151,  0.0178, -0.0093],\n",
      "        [-0.0351, -0.0047, -0.0154,  0.0096, -0.0157],\n",
      "        [ 0.0348, -0.0245, -0.0205, -0.0264,  0.0101]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-1.5631e-02, -4.7253e-03,  4.8196e-02,  4.0440e-03,  1.4266e-02],\n",
      "        [-5.3962e-03, -5.5572e-03, -5.8155e-02,  1.1728e-02, -1.8642e-02],\n",
      "        [ 2.2052e-02,  3.6886e-02,  1.3221e-03,  7.2964e-03, -2.0766e-03],\n",
      "        [ 2.9582e-02, -4.6079e-02, -1.1229e-02, -1.6652e-02, -2.4808e-03],\n",
      "        [-3.0844e-02,  1.1741e-02,  2.3470e-02, -4.8381e-03, -3.6952e-02],\n",
      "        [ 1.7308e-02, -2.0640e-02, -2.8342e-02, -1.4484e-02,  2.2457e-02],\n",
      "        [-9.6858e-06, -7.0722e-03,  8.2628e-02, -7.5453e-02, -1.1767e-02],\n",
      "        [-2.3162e-02, -6.0449e-02,  5.6357e-03, -7.2245e-03, -1.1116e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0224, grad_fn=<MinBackward1>), tensor(0.9059, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.14985309541225433\n",
      "@sample 558: tensor([[-0.0297,  0.0199, -0.0296, -0.0415,  0.0199],\n",
      "        [-0.0002,  0.0053, -0.0147,  0.0049,  0.0077],\n",
      "        [ 0.0070,  0.0062, -0.0024,  0.0051, -0.0297],\n",
      "        [ 0.0138, -0.0057, -0.0136,  0.0331, -0.0220],\n",
      "        [-0.0093, -0.0045, -0.0026,  0.0176, -0.0146],\n",
      "        [-0.0126,  0.0158,  0.0069,  0.0101,  0.0147],\n",
      "        [-0.0160, -0.0469, -0.0256,  0.0068, -0.0087],\n",
      "        [-0.0138,  0.0732,  0.0331, -0.0561,  0.0037]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0120, -0.0722,  0.1157, -0.1202, -0.0524],\n",
      "        [ 0.0030, -0.0064, -0.0730,  0.0232,  0.0031],\n",
      "        [ 0.0398,  0.0183,  0.0362, -0.0327,  0.0011],\n",
      "        [ 0.0022, -0.0050,  0.0018, -0.0168,  0.0285],\n",
      "        [-0.0289,  0.0119,  0.0658,  0.0138,  0.0145],\n",
      "        [ 0.0406, -0.0145, -0.0528, -0.0004,  0.0016],\n",
      "        [-0.0160, -0.0462,  0.0572, -0.0330, -0.0251],\n",
      "        [-0.0294, -0.0148, -0.0931, -0.0358, -0.0265]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0189, grad_fn=<MinBackward1>), tensor(0.9199, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1608477234840393\n",
      "@sample 559: tensor([[ 0.0132,  0.0255, -0.0076,  0.0479, -0.0283],\n",
      "        [-0.0153, -0.0084, -0.0372,  0.0098,  0.0105],\n",
      "        [-0.0333, -0.0143, -0.0155,  0.0210, -0.0147],\n",
      "        [-0.0295,  0.0085,  0.0127,  0.0009,  0.0321],\n",
      "        [-0.0577,  0.0505,  0.0524, -0.0344,  0.0572],\n",
      "        [-0.0144, -0.0317, -0.0146,  0.0292, -0.0029],\n",
      "        [-0.0191, -0.0057,  0.0086, -0.0053,  0.0684],\n",
      "        [-0.0064,  0.0965,  0.0292, -0.0189,  0.0428]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0424, -0.0008,  0.0034,  0.0120,  0.0095],\n",
      "        [-0.0099, -0.0168,  0.0089,  0.0026, -0.0387],\n",
      "        [-0.0101, -0.0346,  0.0596, -0.0306, -0.0126],\n",
      "        [ 0.0225, -0.0048, -0.0196,  0.0134, -0.0099],\n",
      "        [-0.0116, -0.0569, -0.0320,  0.0208, -0.0150],\n",
      "        [ 0.0035,  0.0398,  0.0158, -0.0244, -0.0069],\n",
      "        [ 0.0245, -0.0021, -0.0255,  0.0314, -0.0050],\n",
      "        [ 0.0300, -0.0397, -0.0793,  0.0223, -0.0253]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0218, grad_fn=<MinBackward1>), tensor(0.8924, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.153912752866745\n",
      "@sample 560: tensor([[-0.0152, -0.0271,  0.0037, -0.0293,  0.0478],\n",
      "        [-0.0075, -0.0584, -0.0247,  0.0132, -0.0102],\n",
      "        [ 0.0017, -0.0066, -0.0037,  0.0080,  0.0237],\n",
      "        [-0.0044, -0.0066,  0.0098,  0.0510, -0.0074],\n",
      "        [-0.0043, -0.0007,  0.0003,  0.0018, -0.0146],\n",
      "        [ 0.0006,  0.0195,  0.0195, -0.0219,  0.0123],\n",
      "        [-0.0032, -0.0341, -0.0072, -0.0105, -0.0194],\n",
      "        [ 0.0240, -0.0033,  0.0012,  0.0374, -0.0075]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0003, -0.0243,  0.0257, -0.0353, -0.0178],\n",
      "        [-0.0039,  0.0025,  0.0150, -0.0326,  0.0318],\n",
      "        [ 0.0283,  0.0076, -0.0086, -0.0115, -0.0340],\n",
      "        [-0.0051,  0.0405,  0.0025, -0.0049,  0.0199],\n",
      "        [-0.0051,  0.0288,  0.0276, -0.0183, -0.0030],\n",
      "        [-0.0247, -0.0153, -0.0578,  0.0170, -0.0090],\n",
      "        [-0.0358, -0.0127,  0.0642, -0.0473,  0.0232],\n",
      "        [ 0.0095,  0.0086, -0.0328,  0.0433,  0.0241]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0133, grad_fn=<MinBackward1>), tensor(0.9391, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13997718691825867\n",
      "@sample 561: tensor([[ 0.0044,  0.0116, -0.0051, -0.0047,  0.0138],\n",
      "        [-0.0070,  0.0331, -0.0086, -0.0227,  0.0086],\n",
      "        [-0.0107,  0.0402, -0.0136, -0.0314,  0.0127],\n",
      "        [ 0.0281,  0.0017, -0.0079,  0.0206, -0.0138],\n",
      "        [ 0.0253, -0.0507, -0.0038,  0.0339,  0.0001],\n",
      "        [-0.0222,  0.0108,  0.0101, -0.0076, -0.0236],\n",
      "        [ 0.0028,  0.0350, -0.0076, -0.0378, -0.0247],\n",
      "        [ 0.0240, -0.0472, -0.0348,  0.0528, -0.0134]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0168,  0.0117, -0.0207, -0.0246,  0.0278],\n",
      "        [-0.0385, -0.0019, -0.0820,  0.0063,  0.0099],\n",
      "        [-0.0048,  0.0050, -0.0601,  0.0219,  0.0095],\n",
      "        [-0.0048, -0.0063, -0.0456,  0.0424,  0.0053],\n",
      "        [ 0.0204, -0.0083,  0.0129,  0.0330,  0.0151],\n",
      "        [ 0.0275, -0.0145,  0.0235, -0.0456, -0.0141],\n",
      "        [-0.0580, -0.0255, -0.1016,  0.0660, -0.0238],\n",
      "        [ 0.0087,  0.0227, -0.0142, -0.0124,  0.0064]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0131, grad_fn=<MinBackward1>), tensor(0.9105, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.14232298731803894\n",
      "@sample 562: tensor([[ 0.0318,  0.0138,  0.0251,  0.0058,  0.0115],\n",
      "        [-0.0167,  0.0003, -0.0221, -0.0592,  0.0214],\n",
      "        [ 0.0387,  0.0024, -0.0304,  0.0188, -0.0123],\n",
      "        [-0.0139, -0.0022, -0.0019,  0.0313, -0.0032],\n",
      "        [ 0.0326,  0.0030, -0.0186, -0.0180,  0.0270],\n",
      "        [-0.0002,  0.0316,  0.0182, -0.0382,  0.0258],\n",
      "        [-0.0135,  0.0450, -0.0441, -0.0062, -0.0045],\n",
      "        [ 0.0072, -0.0541, -0.0362,  0.0423,  0.0067]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0064, -0.0233, -0.0314,  0.0265, -0.0210],\n",
      "        [-0.0343, -0.0514, -0.0361,  0.0582,  0.0019],\n",
      "        [ 0.0091,  0.0283, -0.0075,  0.0034,  0.0221],\n",
      "        [ 0.0172,  0.0241,  0.0223, -0.0044,  0.0024],\n",
      "        [-0.0058, -0.0214, -0.0663,  0.0239, -0.0171],\n",
      "        [-0.0065, -0.0363, -0.0382,  0.0244, -0.0034],\n",
      "        [ 0.0035, -0.0016, -0.0006,  0.0157,  0.0024],\n",
      "        [ 0.0217,  0.0155,  0.0615, -0.0083,  0.0119]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0174, grad_fn=<MinBackward1>), tensor(0.9304, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.15379217267036438\n",
      "@sample 563: tensor([[-0.0084,  0.1044,  0.0189, -0.0301,  0.0349],\n",
      "        [ 0.0078,  0.0163,  0.0187,  0.0114, -0.0403],\n",
      "        [ 0.0273, -0.0203, -0.0316,  0.0504, -0.0178],\n",
      "        [-0.0219, -0.0068, -0.0309,  0.0530, -0.0252],\n",
      "        [-0.0172,  0.0646,  0.0695, -0.0472,  0.0237],\n",
      "        [ 0.0064,  0.0232,  0.0212, -0.0580,  0.0321],\n",
      "        [-0.0028, -0.0276, -0.0168,  0.0337, -0.0212],\n",
      "        [-0.0158, -0.0174, -0.0397,  0.0021,  0.0172]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0163, -0.0389, -0.1070,  0.0407, -0.0041],\n",
      "        [-0.0251,  0.0302,  0.0129, -0.0203, -0.0290],\n",
      "        [ 0.0514, -0.0053, -0.0495,  0.0370,  0.0204],\n",
      "        [ 0.0267,  0.0341,  0.0215, -0.0043, -0.0332],\n",
      "        [ 0.0209,  0.0177, -0.0135,  0.0636,  0.0094],\n",
      "        [-0.0206, -0.0115, -0.0262,  0.0296, -0.0077],\n",
      "        [-0.0108,  0.0149,  0.0406, -0.0525, -0.0192],\n",
      "        [-0.0243,  0.0090, -0.0306,  0.0194, -0.0140]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0137, grad_fn=<MinBackward1>), tensor(0.8900, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1657087206840515\n",
      "@sample 564: tensor([[ 0.0063,  0.0545,  0.0183, -0.0143,  0.0138],\n",
      "        [-0.0163,  0.0181,  0.0262,  0.0040, -0.0060],\n",
      "        [ 0.0048,  0.0143,  0.0141,  0.0001,  0.0091],\n",
      "        [-0.0072, -0.0008, -0.0031,  0.0040,  0.0191],\n",
      "        [-0.0311,  0.0034,  0.0142, -0.0006, -0.0244],\n",
      "        [-0.0351,  0.0243,  0.0100,  0.0056, -0.0129],\n",
      "        [-0.0018, -0.0289, -0.0233, -0.0018, -0.0030],\n",
      "        [-0.0063,  0.0405,  0.0435, -0.0668,  0.0158]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0110,  0.0168, -0.0874,  0.0091,  0.0012],\n",
      "        [ 0.0297,  0.0167, -0.0057, -0.0187,  0.0155],\n",
      "        [-0.0197, -0.0047, -0.0469,  0.0315,  0.0016],\n",
      "        [-0.0084,  0.0104,  0.0163,  0.0126,  0.0258],\n",
      "        [ 0.0174,  0.0029,  0.0062,  0.0209,  0.0529],\n",
      "        [-0.0362, -0.0379, -0.0445, -0.0658, -0.0178],\n",
      "        [ 0.0486, -0.0088,  0.0302, -0.0559,  0.0376],\n",
      "        [-0.0349, -0.0422, -0.0444,  0.0089, -0.0531]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0190, grad_fn=<MinBackward1>), tensor(0.9273, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.14431321620941162\n",
      "@sample 565: tensor([[-0.0222,  0.0021, -0.0015, -0.0105,  0.0281],\n",
      "        [ 0.0104,  0.0462,  0.0066, -0.0228,  0.0266],\n",
      "        [ 0.0186, -0.0302, -0.0048,  0.0588, -0.0171],\n",
      "        [ 0.0534,  0.0076, -0.0089,  0.0092,  0.0066],\n",
      "        [-0.0229,  0.0074, -0.0434,  0.0327,  0.0038],\n",
      "        [ 0.0182,  0.0217,  0.0120, -0.0248,  0.0300],\n",
      "        [-0.0069, -0.0039, -0.0120,  0.0066, -0.0039],\n",
      "        [-0.0027, -0.0181, -0.0219, -0.0084,  0.0274]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0393,  0.0164, -0.0099,  0.0021, -0.0089],\n",
      "        [ 0.0365,  0.0026, -0.0645,  0.0245,  0.0064],\n",
      "        [-0.0087,  0.0442,  0.0288, -0.0108, -0.0191],\n",
      "        [-0.0130,  0.0031, -0.0204,  0.0133, -0.0151],\n",
      "        [-0.0064,  0.0029, -0.0217, -0.0141,  0.0014],\n",
      "        [-0.0267, -0.0400, -0.0139, -0.0014, -0.0147],\n",
      "        [ 0.0212, -0.0021, -0.0128,  0.0143,  0.0441],\n",
      "        [ 0.0419,  0.0160, -0.0244,  0.0643, -0.0092]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0198, grad_fn=<MinBackward1>), tensor(0.8837, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.14609843492507935\n",
      "@sample 566: tensor([[-0.0248,  0.0389, -0.0228, -0.0437,  0.0265],\n",
      "        [-0.0105, -0.0240,  0.0084,  0.0256,  0.0025],\n",
      "        [-0.0118,  0.0482,  0.0173, -0.0697,  0.0226],\n",
      "        [-0.0250,  0.0075, -0.0099,  0.0345, -0.0323],\n",
      "        [-0.0124,  0.0644,  0.0208, -0.0255,  0.0155],\n",
      "        [ 0.0111, -0.0229, -0.0132,  0.0218, -0.0351],\n",
      "        [-0.0141,  0.0183, -0.0169, -0.0548, -0.0057],\n",
      "        [-0.0109,  0.0497,  0.0035,  0.0082, -0.0041]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0342, -0.0118, -0.0004,  0.0063,  0.0006],\n",
      "        [ 0.0131,  0.0201,  0.0137, -0.0062,  0.0145],\n",
      "        [-0.0367, -0.0285, -0.0893,  0.0078,  0.0025],\n",
      "        [ 0.0243,  0.0107,  0.0280, -0.0114, -0.0097],\n",
      "        [-0.0345, -0.0031, -0.0637,  0.0351, -0.0044],\n",
      "        [-0.0261, -0.0026, -0.0482, -0.0021,  0.0093],\n",
      "        [ 0.0005, -0.0204, -0.0152,  0.0295,  0.0231],\n",
      "        [-0.0176,  0.0192, -0.1154,  0.0014, -0.0063]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0253, grad_fn=<MinBackward1>), tensor(0.9168, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.15301363170146942\n",
      "@sample 567: tensor([[ 0.0064, -0.0417,  0.0003,  0.0325,  0.0331],\n",
      "        [ 0.0145, -0.0229,  0.0139,  0.0095, -0.0034],\n",
      "        [ 0.0223,  0.0253,  0.0058, -0.0211, -0.0026],\n",
      "        [-0.0187,  0.0288, -0.0003, -0.0289,  0.0150],\n",
      "        [-0.0070, -0.0161, -0.0038,  0.0075, -0.0325],\n",
      "        [ 0.0277,  0.0183, -0.0181,  0.0179, -0.0064],\n",
      "        [ 0.0563,  0.0140,  0.0158, -0.0154,  0.0098],\n",
      "        [-0.0050,  0.0276, -0.0095, -0.0105,  0.0277]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0019,  0.0388,  0.0314, -0.0193,  0.0205],\n",
      "        [-0.0056,  0.0174,  0.0352,  0.0106, -0.0228],\n",
      "        [ 0.0042,  0.0087, -0.0286,  0.0220,  0.0131],\n",
      "        [ 0.0351,  0.0184,  0.0007, -0.0102,  0.0512],\n",
      "        [-0.0094,  0.0068,  0.0352, -0.0778,  0.0085],\n",
      "        [-0.0078,  0.0047, -0.0698,  0.0149,  0.0081],\n",
      "        [-0.0677, -0.0637, -0.0813,  0.0242,  0.0882],\n",
      "        [-0.0239,  0.0154, -0.0447,  0.0694,  0.0258]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0194, grad_fn=<MinBackward1>), tensor(0.9743, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.15746480226516724\n",
      "@sample 568: tensor([[-0.0114, -0.0097,  0.0540, -0.0177,  0.0397],\n",
      "        [-0.0034,  0.0158, -0.0038,  0.0098, -0.0270],\n",
      "        [ 0.0308,  0.0036,  0.0012,  0.0209, -0.0002],\n",
      "        [ 0.0006,  0.0020,  0.0040,  0.0068,  0.0123],\n",
      "        [ 0.0125,  0.0457, -0.0225, -0.0508, -0.0087],\n",
      "        [ 0.0039,  0.0194,  0.0202, -0.0308,  0.0243],\n",
      "        [ 0.0136,  0.0113,  0.0033,  0.0002, -0.0254],\n",
      "        [ 0.0030, -0.0180, -0.0185, -0.0157,  0.0086]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0416, -0.0328, -0.1142,  0.0141,  0.0269],\n",
      "        [-0.0121,  0.0285,  0.0148, -0.0045, -0.0027],\n",
      "        [-0.0303,  0.0002, -0.0137, -0.0020,  0.0159],\n",
      "        [ 0.0060, -0.0354, -0.0197,  0.0132,  0.0215],\n",
      "        [-0.0042, -0.0232, -0.0479,  0.0643,  0.0289],\n",
      "        [ 0.0316, -0.0295, -0.0358,  0.0384,  0.0021],\n",
      "        [ 0.0260, -0.0148, -0.0236,  0.0343,  0.0253],\n",
      "        [-0.0306,  0.0038, -0.0883,  0.0484,  0.0410]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0132, grad_fn=<MinBackward1>), tensor(0.9299, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1467011272907257\n",
      "@sample 569: tensor([[-0.0050, -0.0214,  0.0030,  0.0006, -0.0280],\n",
      "        [ 0.0152, -0.0844, -0.0405,  0.0643, -0.0397],\n",
      "        [-0.0025,  0.0148,  0.0383, -0.0185,  0.0230],\n",
      "        [ 0.0126, -0.0843, -0.0187,  0.0408, -0.0195],\n",
      "        [ 0.0216,  0.0130,  0.0204, -0.0410,  0.0373],\n",
      "        [-0.0182,  0.0267, -0.0342, -0.0091, -0.0043],\n",
      "        [-0.0215, -0.0162,  0.0171,  0.0064, -0.0008],\n",
      "        [ 0.0039, -0.0126, -0.0443,  0.0194, -0.0273]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-2.5495e-02,  1.4331e-04,  4.5787e-03, -1.6048e-03,  1.0891e-02],\n",
      "        [ 5.9366e-02,  2.4118e-02,  6.7138e-02, -3.7177e-02,  2.4967e-02],\n",
      "        [-3.3208e-02, -4.6805e-02, -6.5838e-02,  9.1483e-02,  4.2024e-02],\n",
      "        [-1.8881e-03, -1.1534e-02,  5.1829e-02, -2.3351e-02,  7.3059e-02],\n",
      "        [-7.0248e-03, -2.4684e-02, -5.7071e-02,  2.7857e-02, -8.2175e-03],\n",
      "        [ 4.1097e-02, -2.0713e-02, -2.1777e-02,  7.4869e-03,  8.1977e-03],\n",
      "        [ 2.3358e-02,  5.6068e-02, -3.4567e-02, -1.3101e-02,  7.0508e-03],\n",
      "        [-4.8844e-03,  1.5322e-02,  9.5754e-03, -4.8861e-05,  2.0295e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0180, grad_fn=<MinBackward1>), tensor(0.9038, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.16054947674274445\n",
      "@sample 570: tensor([[-3.1002e-02,  1.7204e-02,  2.5768e-03, -6.9869e-02,  1.6777e-03],\n",
      "        [ 1.7739e-02,  3.4978e-02,  1.2216e-02, -1.3032e-02,  2.6839e-02],\n",
      "        [-7.3326e-04, -3.4333e-02, -1.7035e-02, -1.8008e-02, -8.3927e-03],\n",
      "        [-1.2653e-02, -8.7835e-04, -3.7462e-02,  1.6743e-04, -3.8889e-02],\n",
      "        [ 1.2739e-02, -7.1626e-03, -3.0737e-04, -3.3783e-02,  5.5283e-05],\n",
      "        [ 9.6076e-03,  5.7423e-03,  7.9550e-03, -2.5962e-02,  3.5183e-02],\n",
      "        [ 1.9116e-02, -2.4571e-02,  1.4885e-02,  2.5235e-02, -3.2107e-02],\n",
      "        [-9.2461e-03,  4.2541e-02, -4.1366e-03, -1.3476e-02, -2.2593e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0534, -0.0272,  0.0590, -0.0200,  0.0062],\n",
      "        [-0.0343, -0.0046, -0.0774,  0.0393, -0.0187],\n",
      "        [-0.0293, -0.0058,  0.0363, -0.0191, -0.0026],\n",
      "        [-0.0199, -0.0268,  0.0341, -0.0057,  0.0333],\n",
      "        [-0.0535, -0.0574, -0.0002,  0.0150, -0.0104],\n",
      "        [-0.0134, -0.0084, -0.0511,  0.0258, -0.0253],\n",
      "        [-0.0073, -0.0159, -0.0152, -0.0632, -0.0180],\n",
      "        [-0.0343, -0.0405, -0.0503,  0.0093,  0.0055]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0144, grad_fn=<MinBackward1>), tensor(0.8785, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.14405566453933716\n",
      "@sample 571: tensor([[ 0.0243,  0.0200, -0.0083,  0.0072,  0.0079],\n",
      "        [ 0.0186,  0.0342,  0.0205, -0.0223, -0.0334],\n",
      "        [-0.0020,  0.0038,  0.0151,  0.0180,  0.0039],\n",
      "        [ 0.0033,  0.0132,  0.0124, -0.0485,  0.0581],\n",
      "        [-0.0065, -0.0199,  0.0016,  0.0350, -0.0452],\n",
      "        [-0.0090,  0.0374,  0.0213, -0.0493, -0.0448],\n",
      "        [-0.0275, -0.0013, -0.0448,  0.0446, -0.0244],\n",
      "        [-0.0044,  0.0112,  0.0129, -0.0022,  0.0091]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0098, -0.0084,  0.0012,  0.0333, -0.0155],\n",
      "        [-0.0434, -0.0030, -0.0309, -0.0054, -0.0153],\n",
      "        [-0.0193,  0.0244,  0.0168,  0.0146, -0.0118],\n",
      "        [ 0.0020, -0.0463, -0.0112,  0.0146, -0.0069],\n",
      "        [ 0.0143,  0.0161,  0.0856, -0.0245, -0.0542],\n",
      "        [-0.0321,  0.0188,  0.0011,  0.0315,  0.0066],\n",
      "        [-0.0335,  0.0672,  0.0354, -0.0446, -0.0504],\n",
      "        [-0.0138,  0.0189, -0.0151,  0.0162, -0.0170]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0196, grad_fn=<MinBackward1>), tensor(0.9059, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1478734165430069\n",
      "@sample 572: tensor([[-5.7429e-05, -7.4961e-03,  4.4371e-02, -3.2376e-02, -1.5499e-02],\n",
      "        [ 2.3511e-02, -1.1336e-02, -4.9375e-03,  2.7614e-02, -2.3282e-03],\n",
      "        [ 4.8167e-03,  2.8450e-02,  2.0332e-03,  1.2005e-03,  3.5205e-02],\n",
      "        [ 1.1836e-02,  2.4857e-02,  3.6428e-02,  2.5547e-02, -9.8195e-03],\n",
      "        [ 1.1728e-02, -3.1349e-02, -2.8523e-03,  2.4945e-02, -1.6525e-03],\n",
      "        [ 2.4098e-02, -6.1903e-03,  3.2331e-02, -2.1572e-03, -3.8479e-03],\n",
      "        [-7.9039e-03,  2.6905e-02,  4.6456e-03, -2.2205e-02, -2.6352e-02],\n",
      "        [ 2.1895e-02,  9.8206e-04, -1.7665e-04, -1.7382e-02,  6.2594e-04]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-7.9543e-02,  2.3694e-02, -1.8577e-02,  1.4430e-02,  1.2641e-02],\n",
      "        [-1.0591e-02,  4.3209e-02, -6.4389e-03,  5.5218e-02,  3.3799e-02],\n",
      "        [ 1.7467e-02, -1.3386e-03, -2.5243e-02, -6.6579e-03, -2.8147e-02],\n",
      "        [-1.5028e-02,  5.0654e-02,  3.5416e-02,  9.1571e-03, -1.8909e-02],\n",
      "        [-1.5863e-02,  6.9847e-03,  5.6412e-02, -3.9052e-02, -3.7023e-03],\n",
      "        [-1.8831e-02, -3.0488e-03,  2.4074e-02, -3.3229e-02, -4.7207e-03],\n",
      "        [ 5.3602e-03,  1.9815e-02, -2.0138e-02, -3.2589e-05,  2.1089e-03],\n",
      "        [-1.7708e-02,  1.4560e-03, -1.0447e-02,  8.0954e-03, -1.1643e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0105, grad_fn=<MinBackward1>), tensor(0.9336, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1457425355911255\n",
      "@sample 573: tensor([[-0.0059, -0.0094, -0.0475,  0.0346, -0.0157],\n",
      "        [ 0.0539, -0.0045, -0.0338, -0.0169, -0.0332],\n",
      "        [ 0.0376, -0.0009,  0.0402,  0.0474, -0.0322],\n",
      "        [-0.0284,  0.0371,  0.0452, -0.0196, -0.0329],\n",
      "        [ 0.0419,  0.0327,  0.0063, -0.0076, -0.0363],\n",
      "        [ 0.0020, -0.0048,  0.0025, -0.0322,  0.0142],\n",
      "        [-0.0343,  0.0126,  0.0230, -0.0503,  0.0178],\n",
      "        [ 0.0101, -0.0146, -0.0088,  0.0145, -0.0236]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 3.2379e-02,  1.1936e-02,  2.3781e-02, -1.7397e-02, -4.2305e-02],\n",
      "        [-1.7405e-02, -4.2183e-02,  8.5173e-03,  3.8345e-02, -5.0895e-03],\n",
      "        [ 6.7973e-03,  7.8384e-02, -7.4623e-03,  3.9712e-03, -3.8395e-03],\n",
      "        [-2.2794e-02,  1.1972e-02, -4.0266e-02, -2.3295e-02, -3.0830e-02],\n",
      "        [-2.6606e-02, -2.4787e-02, -4.5223e-03,  6.6974e-02, -2.9717e-02],\n",
      "        [-9.1046e-05, -8.2432e-03, -1.7708e-02,  7.8681e-03, -1.4919e-02],\n",
      "        [-2.7237e-02, -7.1878e-03,  2.7446e-02, -3.9568e-02, -6.8293e-03],\n",
      "        [-1.2786e-02,  2.0403e-02,  3.1819e-02, -1.4784e-02, -2.1430e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0203, grad_fn=<MinBackward1>), tensor(0.9289, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.14872625470161438\n",
      "@sample 574: tensor([[-0.0410,  0.0205,  0.0513, -0.0444,  0.0817],\n",
      "        [-0.0029, -0.0346, -0.0147,  0.0177, -0.0025],\n",
      "        [-0.0123, -0.0040,  0.0189, -0.0080,  0.0069],\n",
      "        [-0.0298,  0.0307, -0.0018, -0.0330, -0.0293],\n",
      "        [-0.0017,  0.0240,  0.0075,  0.0068,  0.0019],\n",
      "        [ 0.0161, -0.0082, -0.0207,  0.0224, -0.0236],\n",
      "        [ 0.0079, -0.0118,  0.0422, -0.0142, -0.0086],\n",
      "        [-0.0078, -0.0170,  0.0051,  0.0243,  0.0103]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0018, -0.0444, -0.0957,  0.0549,  0.0053],\n",
      "        [ 0.0646,  0.0214,  0.0941, -0.0277, -0.0115],\n",
      "        [-0.0049,  0.0089,  0.0051, -0.0008, -0.0056],\n",
      "        [-0.0154,  0.0068,  0.0115, -0.0040,  0.0046],\n",
      "        [ 0.0179,  0.0159, -0.0102,  0.0130,  0.0111],\n",
      "        [-0.0026,  0.0229,  0.0142, -0.0101,  0.0078],\n",
      "        [-0.0317,  0.0039, -0.0036, -0.0294, -0.0218],\n",
      "        [ 0.0209,  0.0123, -0.0595, -0.0182,  0.0075]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0142, grad_fn=<MinBackward1>), tensor(0.9190, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1490129828453064\n",
      "@sample 575: tensor([[ 0.0065, -0.0518,  0.0333,  0.0145,  0.0091],\n",
      "        [-0.0161,  0.0051,  0.0560, -0.0382,  0.0492],\n",
      "        [-0.0026,  0.0127,  0.0101, -0.0320,  0.0079],\n",
      "        [-0.0112,  0.0179,  0.0101, -0.0219,  0.0155],\n",
      "        [ 0.0232,  0.0141,  0.0308, -0.0216,  0.0560],\n",
      "        [ 0.0169, -0.0166,  0.0294, -0.0176,  0.0275],\n",
      "        [-0.0096,  0.0347,  0.0287, -0.0583,  0.0249],\n",
      "        [-0.0014, -0.0513,  0.0019, -0.0309,  0.0629]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0075,  0.0336,  0.0488, -0.0418, -0.0374],\n",
      "        [ 0.0101,  0.0230,  0.0573,  0.0197, -0.0179],\n",
      "        [-0.0199,  0.0280, -0.0059,  0.0027, -0.0116],\n",
      "        [-0.0050,  0.0122,  0.0307, -0.0139, -0.0276],\n",
      "        [ 0.0162,  0.0340, -0.0730,  0.0649,  0.0043],\n",
      "        [-0.0583, -0.0630, -0.0492,  0.0663, -0.0083],\n",
      "        [ 0.0224, -0.0277, -0.0496,  0.0588,  0.0175],\n",
      "        [ 0.0170,  0.0069,  0.0257, -0.0207, -0.0192]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0221, grad_fn=<MinBackward1>), tensor(0.8551, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1495145559310913\n",
      "@sample 576: tensor([[-0.0124, -0.0171, -0.0122, -0.0220, -0.0162],\n",
      "        [ 0.0086,  0.0240,  0.0618, -0.0178,  0.0024],\n",
      "        [-0.0235,  0.0173, -0.0271,  0.0377, -0.0322],\n",
      "        [-0.0007,  0.0562,  0.0253, -0.0364,  0.0427],\n",
      "        [ 0.0206,  0.0109,  0.0122, -0.0089, -0.0329],\n",
      "        [-0.0183, -0.0118, -0.0043, -0.0034,  0.0153],\n",
      "        [ 0.0222,  0.0031,  0.0202,  0.0345, -0.0400],\n",
      "        [ 0.0391, -0.0047, -0.0249,  0.0357,  0.0102]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0323, -0.0553, -0.0299,  0.0077,  0.0083],\n",
      "        [-0.0193, -0.0203, -0.0264,  0.0244,  0.0198],\n",
      "        [ 0.0109,  0.0611,  0.0446, -0.0706, -0.0109],\n",
      "        [-0.0032, -0.0175, -0.0872,  0.0733, -0.0355],\n",
      "        [ 0.0016, -0.0056,  0.0285, -0.0116,  0.0538],\n",
      "        [-0.0134, -0.0394,  0.0343, -0.0083, -0.0337],\n",
      "        [-0.0264,  0.0166,  0.0130,  0.0331,  0.0354],\n",
      "        [ 0.0183, -0.0130, -0.0070, -0.0496, -0.0377]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0131, grad_fn=<MinBackward1>), tensor(0.9412, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.15807636082172394\n",
      "@sample 577: tensor([[-0.0087,  0.0143, -0.0104, -0.0163,  0.0141],\n",
      "        [ 0.0045, -0.0132,  0.0188,  0.0193,  0.0319],\n",
      "        [-0.0291, -0.0567, -0.0620,  0.0293,  0.0024],\n",
      "        [ 0.0056, -0.0448, -0.0169, -0.0026, -0.0002],\n",
      "        [-0.0022,  0.0230, -0.0018, -0.0279,  0.0311],\n",
      "        [-0.0036,  0.0631, -0.0067, -0.0183,  0.0213],\n",
      "        [ 0.0082,  0.0101,  0.0416, -0.0010,  0.0176],\n",
      "        [-0.0063, -0.0067,  0.0113, -0.0015,  0.0212]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0157, -0.0202, -0.0323, -0.0004, -0.0199],\n",
      "        [ 0.0093,  0.0263,  0.0427, -0.0032, -0.0132],\n",
      "        [ 0.0519, -0.0470,  0.0997, -0.0170, -0.0380],\n",
      "        [ 0.0007, -0.0058,  0.0757, -0.0108, -0.0364],\n",
      "        [-0.0149, -0.0322, -0.0481,  0.0203, -0.0350],\n",
      "        [-0.0048, -0.0359, -0.0632,  0.0283, -0.0067],\n",
      "        [ 0.0344,  0.0221,  0.0065,  0.0141, -0.0030],\n",
      "        [-0.0110,  0.0111, -0.0600,  0.0183, -0.0424]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0228, grad_fn=<MinBackward1>), tensor(0.9165, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.15093332529067993\n",
      "@sample 578: tensor([[ 0.0001,  0.0321, -0.0007, -0.0038,  0.0074],\n",
      "        [ 0.0157, -0.0403,  0.0009,  0.0299, -0.0069],\n",
      "        [-0.0066, -0.0187,  0.0317,  0.0139,  0.0203],\n",
      "        [-0.0136, -0.0123,  0.0365,  0.0435, -0.0343],\n",
      "        [-0.0246, -0.0138,  0.0141,  0.0105,  0.0168],\n",
      "        [-0.0063,  0.0256,  0.0092, -0.0477,  0.0135],\n",
      "        [ 0.0200,  0.0730, -0.0270, -0.0382, -0.0182],\n",
      "        [ 0.0247, -0.0044, -0.0037, -0.0191, -0.0081]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0147, -0.0134, -0.0304,  0.0144,  0.0168],\n",
      "        [ 0.0255, -0.0011,  0.0532,  0.0079, -0.0327],\n",
      "        [ 0.0035,  0.0321,  0.0888, -0.0566, -0.0246],\n",
      "        [-0.0116,  0.0548,  0.0081, -0.0537,  0.0108],\n",
      "        [ 0.0156,  0.0183,  0.0458, -0.0016,  0.0025],\n",
      "        [-0.0226,  0.0040, -0.0585,  0.0416, -0.0227],\n",
      "        [ 0.0202, -0.0077, -0.0818,  0.0527,  0.0010],\n",
      "        [ 0.0126, -0.0329, -0.0174,  0.0068, -0.0202]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0125, grad_fn=<MinBackward1>), tensor(0.9190, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.15404869616031647\n",
      "@sample 579: tensor([[ 8.4195e-03, -2.4075e-02,  3.1754e-02, -6.8849e-02,  2.4043e-02],\n",
      "        [-2.7053e-02,  7.1657e-02,  4.1811e-02, -4.9483e-02,  1.9505e-02],\n",
      "        [ 1.1235e-02,  3.9768e-02, -1.1100e-02, -7.1777e-03,  3.4728e-02],\n",
      "        [ 8.9120e-03,  7.4558e-03,  1.1480e-03, -3.1312e-02, -1.8132e-03],\n",
      "        [-1.0720e-02,  3.3464e-02,  5.1392e-02, -1.3902e-02,  3.7514e-02],\n",
      "        [-2.2181e-02, -9.3086e-04,  2.2915e-04, -7.2584e-05, -2.5691e-03],\n",
      "        [-3.8697e-03,  2.6709e-02,  4.0049e-03, -1.0880e-02, -5.8025e-05],\n",
      "        [ 1.6124e-02, -6.7870e-02,  2.3665e-04, -1.2111e-02,  3.7494e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0576, -0.0631,  0.0315,  0.0177,  0.0380],\n",
      "        [-0.0146, -0.0340, -0.0458,  0.0412,  0.0071],\n",
      "        [ 0.0077, -0.0034, -0.0532,  0.0259, -0.0473],\n",
      "        [-0.0252, -0.0431, -0.0476,  0.0333, -0.0238],\n",
      "        [-0.0158,  0.0532, -0.0490,  0.0265,  0.0290],\n",
      "        [ 0.0083,  0.0118,  0.0207, -0.0187, -0.0141],\n",
      "        [-0.0385,  0.0160, -0.0801,  0.0106,  0.0165],\n",
      "        [ 0.0074, -0.0080,  0.0446, -0.0307, -0.0452]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0226, grad_fn=<MinBackward1>), tensor(0.8928, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.16057522594928741\n",
      "@sample 580: tensor([[ 0.0174, -0.0276, -0.0015,  0.0058, -0.0013],\n",
      "        [-0.0124,  0.0117,  0.0211,  0.0065,  0.0072],\n",
      "        [-0.0156, -0.0359, -0.0286,  0.0064,  0.0752],\n",
      "        [-0.0028, -0.0105,  0.0083,  0.0018,  0.0236],\n",
      "        [ 0.0256,  0.0147,  0.0158, -0.0343, -0.0009],\n",
      "        [-0.0035,  0.0368, -0.0084, -0.0286, -0.0107],\n",
      "        [ 0.0029, -0.0156, -0.0104,  0.0261, -0.0129],\n",
      "        [ 0.0095,  0.0384,  0.0063, -0.0231,  0.0210]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0076, -0.0060,  0.0072, -0.0134, -0.0271],\n",
      "        [ 0.0182,  0.0168,  0.0218, -0.0141,  0.0036],\n",
      "        [ 0.0092,  0.0036,  0.0023,  0.0015,  0.0351],\n",
      "        [-0.0223, -0.0080,  0.0212, -0.0067,  0.0271],\n",
      "        [-0.0195, -0.0288, -0.0238, -0.0175, -0.0196],\n",
      "        [ 0.0121, -0.0166, -0.0302, -0.0078,  0.0076],\n",
      "        [ 0.0281,  0.0242,  0.0187,  0.0162, -0.0018],\n",
      "        [-0.0361,  0.0289, -0.0767,  0.0224, -0.0078]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0229, grad_fn=<MinBackward1>), tensor(0.8903, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1474521905183792\n",
      "@sample 581: tensor([[ 0.0390,  0.0470, -0.0195, -0.0199, -0.0353],\n",
      "        [-0.0089,  0.0150, -0.0292, -0.0089,  0.0226],\n",
      "        [-0.0113, -0.0019,  0.0063,  0.0058, -0.0061],\n",
      "        [-0.0125, -0.0430,  0.0056, -0.0034,  0.0054],\n",
      "        [ 0.0030, -0.0370, -0.0156, -0.0067,  0.0309],\n",
      "        [ 0.0200, -0.0070,  0.0136, -0.0060,  0.0085],\n",
      "        [ 0.0042, -0.0379, -0.0115,  0.0100,  0.0059],\n",
      "        [ 0.0061, -0.0155,  0.0320, -0.0009,  0.0234]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0130,  0.0237, -0.0067,  0.0006,  0.0059],\n",
      "        [-0.0023,  0.0034, -0.0404,  0.0035, -0.0046],\n",
      "        [ 0.0187, -0.0073, -0.0316, -0.0023, -0.0096],\n",
      "        [-0.0157, -0.0239,  0.0224, -0.0451, -0.0155],\n",
      "        [-0.0140, -0.0091, -0.0782,  0.0404, -0.0096],\n",
      "        [ 0.0286,  0.0151, -0.0486,  0.0229,  0.0110],\n",
      "        [ 0.0402,  0.0338,  0.0593, -0.0119,  0.0121],\n",
      "        [ 0.0020, -0.0066, -0.0268, -0.0650, -0.0265]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0190, grad_fn=<MinBackward1>), tensor(0.8860, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13772428035736084\n",
      "@sample 582: tensor([[ 0.0035,  0.0097,  0.0058, -0.0027,  0.0514],\n",
      "        [-0.0086, -0.0037,  0.0114,  0.0216, -0.0086],\n",
      "        [-0.0052, -0.0050, -0.0255,  0.0028,  0.0226],\n",
      "        [-0.0268, -0.0171, -0.0256, -0.0091, -0.0022],\n",
      "        [ 0.0057,  0.0240, -0.0059, -0.0519,  0.0151],\n",
      "        [-0.0025,  0.0577,  0.0142, -0.0642,  0.0266],\n",
      "        [ 0.0282,  0.0062,  0.0096,  0.0124,  0.0065],\n",
      "        [ 0.0148,  0.0081, -0.0605,  0.0156,  0.0015]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0176, -0.0078, -0.0190,  0.0320, -0.0460],\n",
      "        [-0.0007, -0.0064, -0.0414,  0.0440,  0.0019],\n",
      "        [ 0.0093, -0.0248,  0.0699, -0.0361, -0.0284],\n",
      "        [-0.0005,  0.0086, -0.0108,  0.0167,  0.0002],\n",
      "        [-0.0090, -0.0442, -0.0143, -0.0157, -0.0106],\n",
      "        [-0.0369, -0.0602, -0.0816,  0.0665,  0.0145],\n",
      "        [-0.0324, -0.0119, -0.0002, -0.0052, -0.0057],\n",
      "        [ 0.0155, -0.0073,  0.0027, -0.0263,  0.0229]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0123, grad_fn=<MinBackward1>), tensor(0.8859, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.14134317636489868\n",
      "@sample 583: tensor([[ 0.0101,  0.0135, -0.0023, -0.0130,  0.0191],\n",
      "        [ 0.0104, -0.0505,  0.0249,  0.0267, -0.0198],\n",
      "        [-0.0272, -0.0103,  0.0091,  0.0017,  0.0228],\n",
      "        [-0.0076, -0.0189, -0.0170,  0.0268, -0.0301],\n",
      "        [-0.0342,  0.0325, -0.0171,  0.0082, -0.0285],\n",
      "        [-0.0245, -0.0242, -0.0077, -0.0185,  0.0189],\n",
      "        [ 0.0021,  0.0220, -0.0184,  0.0092,  0.0009],\n",
      "        [-0.0103, -0.0211, -0.0019,  0.0102, -0.0147]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0125,  0.0117, -0.0568,  0.0185,  0.0018],\n",
      "        [ 0.0262,  0.0373,  0.0148,  0.0041, -0.0034],\n",
      "        [ 0.0105,  0.0159,  0.0289, -0.0019,  0.0100],\n",
      "        [-0.0019,  0.0205,  0.0113, -0.0062, -0.0205],\n",
      "        [-0.0135,  0.0078,  0.0368, -0.0134,  0.0035],\n",
      "        [-0.0304, -0.0169,  0.0078, -0.0127, -0.0073],\n",
      "        [ 0.0010,  0.0032, -0.0241,  0.0226, -0.0073],\n",
      "        [-0.0051,  0.0270,  0.0575, -0.0342,  0.0130]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0163, grad_fn=<MinBackward1>), tensor(0.9117, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13085241615772247\n",
      "@sample 584: tensor([[-0.0115, -0.0646, -0.0260,  0.0438, -0.0095],\n",
      "        [-0.0053,  0.0522, -0.0035, -0.0196,  0.0059],\n",
      "        [-0.0024,  0.0115, -0.0044,  0.0076, -0.0034],\n",
      "        [ 0.0028,  0.0119, -0.0216, -0.0152,  0.0134],\n",
      "        [ 0.0153,  0.0158,  0.0054, -0.0067, -0.0248],\n",
      "        [ 0.0236, -0.0109, -0.0052,  0.0173, -0.0197],\n",
      "        [-0.0333,  0.0450,  0.0092, -0.0733,  0.0314],\n",
      "        [ 0.0126, -0.0070,  0.0102, -0.0108, -0.0413]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0274, -0.0186,  0.0540, -0.0323, -0.0060],\n",
      "        [-0.0173, -0.0352, -0.0652,  0.0240,  0.0040],\n",
      "        [ 0.0090,  0.0323,  0.0218,  0.0334, -0.0132],\n",
      "        [-0.0082, -0.0082, -0.0817,  0.0152, -0.0303],\n",
      "        [-0.0093,  0.0441, -0.0042,  0.0048,  0.0205],\n",
      "        [-0.0168,  0.0210,  0.0031,  0.0225,  0.0128],\n",
      "        [-0.0479, -0.0655, -0.0687,  0.0154, -0.0326],\n",
      "        [-0.0638, -0.0028, -0.0012, -0.0024, -0.0219]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0152, grad_fn=<MinBackward1>), tensor(0.9278, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.14303427934646606\n",
      "@sample 585: tensor([[-0.0052,  0.0071,  0.0091, -0.0173, -0.0170],\n",
      "        [ 0.0045,  0.0206, -0.0044, -0.0183, -0.0033],\n",
      "        [-0.0068, -0.0345, -0.0402,  0.0263, -0.0129],\n",
      "        [-0.0069,  0.0021, -0.0156, -0.0024,  0.0330],\n",
      "        [-0.0175,  0.0119, -0.0076, -0.0290,  0.0018],\n",
      "        [-0.0015,  0.0391,  0.0051, -0.0008,  0.0267],\n",
      "        [ 0.0130, -0.0045, -0.0024, -0.0056, -0.0374],\n",
      "        [-0.0203, -0.0542, -0.0697,  0.0109,  0.0565]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0181,  0.0195, -0.0085, -0.0389,  0.0528],\n",
      "        [ 0.0167,  0.0109, -0.0655,  0.0013,  0.0487],\n",
      "        [ 0.0108,  0.0181,  0.0132, -0.0331, -0.0049],\n",
      "        [ 0.0238,  0.0233,  0.0535, -0.0212, -0.0187],\n",
      "        [ 0.0211,  0.0056,  0.0041, -0.0112,  0.0138],\n",
      "        [ 0.0140, -0.0182, -0.0523,  0.0119, -0.0056],\n",
      "        [-0.0030, -0.0080,  0.0156, -0.0113,  0.0059],\n",
      "        [-0.0163,  0.0229,  0.0168, -0.0202,  0.0022]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0055, grad_fn=<MinBackward1>), tensor(0.9384, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1380346566438675\n",
      "@sample 586: tensor([[-0.0168,  0.0224, -0.0029, -0.0136,  0.0010],\n",
      "        [-0.0215, -0.0233, -0.0316,  0.0230, -0.0241],\n",
      "        [-0.0504, -0.0496,  0.0088, -0.0261,  0.0213],\n",
      "        [-0.0115,  0.0292,  0.0444,  0.0055,  0.0228],\n",
      "        [ 0.0095,  0.0036,  0.0002, -0.0117, -0.0003],\n",
      "        [ 0.0137, -0.0195, -0.0205,  0.0133, -0.0088],\n",
      "        [-0.0077, -0.0228, -0.0554,  0.0296, -0.0078],\n",
      "        [-0.0155,  0.0368,  0.0084, -0.0199,  0.0243]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 1.6908e-02, -7.9689e-03,  5.5003e-02,  9.6234e-03,  5.0648e-03],\n",
      "        [ 1.9187e-02,  2.9287e-02,  4.3344e-02, -2.9080e-02,  7.9420e-03],\n",
      "        [-3.5025e-05, -1.0546e-03, -1.3844e-02,  2.3122e-02,  4.7870e-02],\n",
      "        [-1.7385e-02, -2.2907e-02, -5.8460e-03, -1.6005e-02, -4.0655e-02],\n",
      "        [-1.8720e-02, -3.2535e-03, -4.1492e-02,  1.2561e-02,  2.3294e-02],\n",
      "        [ 3.2087e-02,  5.7232e-03, -4.4136e-02,  1.6756e-02,  5.1366e-02],\n",
      "        [ 3.1409e-02, -2.2188e-02,  2.1918e-02, -2.9868e-02,  5.9700e-03],\n",
      "        [-3.2311e-02, -2.8839e-02, -6.8082e-02,  1.9518e-02,  4.4381e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0179, grad_fn=<MinBackward1>), tensor(0.9227, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1439686268568039\n",
      "@sample 587: tensor([[ 5.9877e-03, -2.0221e-02, -4.7150e-02, -1.7986e-02,  1.3426e-05],\n",
      "        [ 2.1826e-02, -3.1269e-02, -1.3732e-02,  4.1704e-02, -3.1111e-03],\n",
      "        [ 6.8007e-03, -5.0298e-02, -1.5590e-02,  2.9593e-02, -2.3020e-02],\n",
      "        [ 1.7574e-02, -6.7379e-03, -2.0133e-02,  5.4341e-02, -1.8836e-02],\n",
      "        [-5.5939e-03,  5.2481e-03,  4.6889e-03, -1.6066e-03, -3.7887e-02],\n",
      "        [ 8.8773e-03, -3.4383e-03, -3.2684e-02, -1.7250e-02, -9.1199e-03],\n",
      "        [-4.2845e-02, -2.3257e-03,  9.1485e-03,  2.7840e-02,  2.2848e-02],\n",
      "        [-3.1552e-02,  7.7096e-03, -9.0302e-03,  8.5199e-03,  2.5225e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-2.7653e-02, -7.0670e-03,  1.0216e-02, -6.7273e-02,  5.0522e-02],\n",
      "        [ 5.5620e-02,  2.5095e-02,  1.6549e-02, -1.0616e-02,  3.1466e-02],\n",
      "        [ 2.6709e-02, -1.0384e-02, -3.4865e-02, -2.8553e-02, -2.3431e-02],\n",
      "        [ 5.0917e-02,  3.3727e-02,  1.9361e-02,  5.2081e-03,  4.4498e-02],\n",
      "        [ 4.8959e-02,  4.8450e-03,  6.6704e-02, -7.7635e-06,  3.1554e-02],\n",
      "        [-3.1471e-03, -2.0417e-02,  2.4064e-02, -2.7874e-02, -4.0909e-02],\n",
      "        [ 4.0540e-02,  4.8390e-02, -1.2383e-02,  8.7834e-03,  1.5957e-02],\n",
      "        [ 3.4632e-02, -2.4805e-02,  6.0980e-02,  2.4701e-02, -8.9113e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0194, grad_fn=<MinBackward1>), tensor(0.8799, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.14747712016105652\n",
      "@sample 588: tensor([[ 0.0116,  0.0836,  0.0423, -0.0130,  0.0061],\n",
      "        [ 0.0165, -0.0419, -0.0210,  0.0283, -0.0137],\n",
      "        [-0.0028, -0.0521, -0.0535,  0.0331, -0.0215],\n",
      "        [ 0.0118, -0.0394,  0.0213,  0.0247,  0.0227],\n",
      "        [ 0.0018,  0.0041, -0.0369, -0.0117,  0.0143],\n",
      "        [-0.0003, -0.0199, -0.0033,  0.0344, -0.0109],\n",
      "        [-0.0204,  0.0651,  0.0218, -0.0595, -0.0110],\n",
      "        [ 0.0038, -0.0113, -0.0198,  0.0472, -0.0387]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0021, -0.0082, -0.0482,  0.0603, -0.0198],\n",
      "        [ 0.0093,  0.0278,  0.0043, -0.0278,  0.0192],\n",
      "        [ 0.0349,  0.0170,  0.0307, -0.0325,  0.0041],\n",
      "        [ 0.0054, -0.0053,  0.0053, -0.0067,  0.0057],\n",
      "        [-0.0048,  0.0041,  0.0179,  0.0148,  0.0048],\n",
      "        [ 0.0265,  0.0228,  0.0384, -0.0576,  0.0255],\n",
      "        [-0.0355,  0.0092, -0.1053,  0.0523,  0.0287],\n",
      "        [ 0.0041,  0.0206, -0.0028, -0.0072,  0.0178]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0072, grad_fn=<MinBackward1>), tensor(0.9283, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.15680274367332458\n",
      "@sample 589: tensor([[-3.7063e-03,  2.0808e-03, -3.1659e-03,  1.0803e-02, -5.7520e-03],\n",
      "        [-5.7461e-03,  2.4840e-02,  2.3201e-02, -8.6227e-03,  3.4331e-02],\n",
      "        [ 5.8244e-02, -1.4232e-03,  7.8224e-05,  1.8446e-03,  5.6116e-03],\n",
      "        [-3.1720e-02,  2.5835e-02, -1.4499e-03, -3.2342e-02,  2.7404e-02],\n",
      "        [ 9.7882e-03,  1.1745e-02,  2.7358e-02,  8.6220e-03, -4.2362e-02],\n",
      "        [-1.8131e-02, -4.9684e-02,  1.2623e-02, -7.6424e-04, -1.6165e-02],\n",
      "        [ 3.4080e-02,  2.8218e-02, -1.1352e-02,  6.7203e-03,  1.6809e-03],\n",
      "        [ 5.0421e-03, -1.1880e-02, -1.2896e-02,  5.1899e-02,  1.2602e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0124,  0.0021, -0.0108,  0.0063,  0.0101],\n",
      "        [ 0.0008,  0.0028, -0.0329,  0.0034, -0.0089],\n",
      "        [ 0.0637,  0.0053, -0.0338, -0.0041,  0.0138],\n",
      "        [ 0.0051, -0.0418, -0.0952,  0.0155,  0.0048],\n",
      "        [ 0.0075, -0.0195, -0.0087, -0.0211,  0.0034],\n",
      "        [-0.0224,  0.0047,  0.0227, -0.0175,  0.0030],\n",
      "        [-0.0311,  0.0075, -0.0411, -0.0169, -0.0108],\n",
      "        [ 0.0457,  0.0498,  0.0417, -0.0253,  0.0408]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0214, grad_fn=<MinBackward1>), tensor(0.9070, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13732635974884033\n",
      "@sample 590: tensor([[ 0.0131, -0.0310, -0.0299,  0.0310, -0.0228],\n",
      "        [ 0.0090,  0.0508,  0.0132, -0.0557,  0.0033],\n",
      "        [-0.0063,  0.0020,  0.0171,  0.0125,  0.0069],\n",
      "        [ 0.0120, -0.0181, -0.0204,  0.0367, -0.0169],\n",
      "        [-0.0034, -0.0081,  0.0032, -0.0245, -0.0016],\n",
      "        [ 0.0065, -0.0059,  0.0016,  0.0486, -0.0376],\n",
      "        [-0.0140, -0.0346,  0.0114,  0.0164,  0.0118],\n",
      "        [ 0.0039, -0.0141, -0.0101, -0.0347,  0.0072]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0145, -0.0029, -0.0304, -0.0020,  0.0075],\n",
      "        [-0.0250,  0.0180, -0.0834,  0.0292,  0.0420],\n",
      "        [ 0.0004,  0.0250, -0.0171, -0.0304,  0.0227],\n",
      "        [ 0.0021,  0.0074,  0.0415, -0.0606, -0.0061],\n",
      "        [ 0.0426, -0.0013, -0.0018,  0.0377,  0.0275],\n",
      "        [ 0.0014,  0.0361,  0.0489, -0.0409, -0.0296],\n",
      "        [ 0.0094,  0.0019,  0.0624, -0.0159, -0.0204],\n",
      "        [-0.0093, -0.0295, -0.0153,  0.0222, -0.0002]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0142, grad_fn=<MinBackward1>), tensor(0.9104, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.147645503282547\n",
      "@sample 591: tensor([[-0.0434,  0.0371,  0.0296, -0.0404,  0.0068],\n",
      "        [ 0.0183, -0.0261, -0.0021, -0.0139, -0.0263],\n",
      "        [-0.0341, -0.0257,  0.0083, -0.0218,  0.0105],\n",
      "        [-0.0154, -0.0174,  0.0118,  0.0133, -0.0045],\n",
      "        [ 0.0031, -0.0111, -0.0231,  0.0052, -0.0070],\n",
      "        [-0.0071, -0.0124,  0.0226, -0.0081,  0.0012],\n",
      "        [ 0.0079,  0.0252, -0.0086, -0.0111, -0.0092],\n",
      "        [ 0.0004,  0.0305,  0.0073, -0.0260,  0.0186]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0073, -0.0196,  0.0081,  0.0091,  0.0004],\n",
      "        [-0.0913,  0.0043,  0.0010,  0.0028, -0.0057],\n",
      "        [-0.0148,  0.0097, -0.0059,  0.0317, -0.0022],\n",
      "        [ 0.0254,  0.0036,  0.0903,  0.0102, -0.0125],\n",
      "        [ 0.0215, -0.0067,  0.0567, -0.0264, -0.0450],\n",
      "        [-0.0093,  0.0302,  0.0156, -0.0110,  0.0069],\n",
      "        [-0.0203, -0.0060, -0.0154, -0.0112, -0.0166],\n",
      "        [-0.0075, -0.0060,  0.0013,  0.0237, -0.0113]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0173, grad_fn=<MinBackward1>), tensor(0.9154, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13211841881275177\n",
      "@sample 592: tensor([[ 0.0052,  0.0606,  0.0222, -0.0571,  0.0166],\n",
      "        [-0.0148,  0.0087, -0.0071, -0.0244,  0.0079],\n",
      "        [-0.0347,  0.0498,  0.0177, -0.0319, -0.0069],\n",
      "        [ 0.0107, -0.0216, -0.0067,  0.0064, -0.0278],\n",
      "        [-0.0123,  0.0145,  0.0219, -0.0008, -0.0033],\n",
      "        [-0.0151, -0.0085, -0.0083, -0.0321,  0.0285],\n",
      "        [ 0.0115, -0.0217,  0.0350,  0.0055, -0.0107],\n",
      "        [ 0.0145, -0.0261,  0.0039,  0.0008,  0.0015]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0317, -0.0473, -0.0412,  0.0237,  0.0110],\n",
      "        [-0.0155, -0.0021, -0.0158,  0.0126,  0.0216],\n",
      "        [-0.0242, -0.0179, -0.0349,  0.0309,  0.0103],\n",
      "        [-0.0419, -0.0254, -0.0338, -0.0146, -0.0298],\n",
      "        [ 0.0315, -0.0092,  0.0038, -0.0286,  0.0080],\n",
      "        [-0.0165, -0.0342, -0.0060,  0.0159, -0.0058],\n",
      "        [ 0.0007, -0.0175,  0.0699, -0.0152, -0.0025],\n",
      "        [-0.0128,  0.0051,  0.0279, -0.0143,  0.0547]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0148, grad_fn=<MinBackward1>), tensor(0.9481, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.14518950879573822\n",
      "@sample 593: tensor([[ 0.0065,  0.0095, -0.0247,  0.0192,  0.0027],\n",
      "        [ 0.0251,  0.0323,  0.0131, -0.0130,  0.0055],\n",
      "        [-0.0302,  0.0326,  0.0126,  0.0105,  0.0199],\n",
      "        [ 0.0064, -0.0530, -0.0027,  0.0311, -0.0207],\n",
      "        [ 0.0034,  0.0168,  0.0137,  0.0181, -0.0230],\n",
      "        [-0.0390, -0.0295, -0.0135,  0.0243, -0.0036],\n",
      "        [-0.0030, -0.0198, -0.0178,  0.0014, -0.0276],\n",
      "        [ 0.0145,  0.0350, -0.0084, -0.0051, -0.0237]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0029,  0.0492,  0.0545, -0.0083, -0.0368],\n",
      "        [-0.0522, -0.0425, -0.0864,  0.0355,  0.0034],\n",
      "        [ 0.0390, -0.0017, -0.0512, -0.0002,  0.0120],\n",
      "        [-0.0177, -0.0023,  0.0328,  0.0027, -0.0559],\n",
      "        [ 0.0170, -0.0010, -0.0617, -0.0114, -0.0006],\n",
      "        [ 0.0364,  0.0199,  0.0705, -0.0590, -0.0027],\n",
      "        [-0.0218,  0.0373,  0.0338, -0.0168, -0.0072],\n",
      "        [ 0.0077, -0.0160, -0.0206,  0.0106,  0.0545]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0182, grad_fn=<MinBackward1>), tensor(0.9352, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1462724208831787\n",
      "@sample 594: tensor([[ 1.9743e-02,  1.8027e-03, -2.9402e-02,  1.9515e-03, -1.0055e-02],\n",
      "        [ 2.4355e-02,  5.2347e-02,  4.5123e-02, -1.8324e-02, -1.5957e-02],\n",
      "        [-8.4542e-05,  3.1215e-02,  2.2359e-02, -3.4938e-03,  1.4159e-03],\n",
      "        [ 2.3572e-02,  2.4947e-02, -1.3212e-03, -1.3739e-04, -1.4608e-02],\n",
      "        [-1.2349e-02,  1.6220e-02,  1.0392e-02,  1.0200e-02, -1.0227e-02],\n",
      "        [-7.8826e-03,  5.2243e-02, -3.3412e-02,  2.5759e-02, -7.9147e-03],\n",
      "        [-2.8977e-02,  1.2150e-03, -3.5277e-03,  4.9919e-02,  6.4568e-04],\n",
      "        [-2.2187e-02, -5.0121e-02,  2.0149e-03,  4.5123e-02,  4.8632e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0163, -0.0044, -0.0033,  0.0016,  0.0211],\n",
      "        [-0.0321, -0.0055, -0.0134, -0.0129, -0.0197],\n",
      "        [-0.0133,  0.0124, -0.0488,  0.0126,  0.0278],\n",
      "        [-0.0071, -0.0039,  0.0015,  0.0233, -0.0119],\n",
      "        [-0.0128,  0.0021, -0.0338, -0.0065,  0.0104],\n",
      "        [-0.0308,  0.0181, -0.0646,  0.0065, -0.0246],\n",
      "        [ 0.0427,  0.0343,  0.0153, -0.0081,  0.0009],\n",
      "        [ 0.0469,  0.0209,  0.0849, -0.0579, -0.0166]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0213, grad_fn=<MinBackward1>), tensor(0.9230, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.14278516173362732\n",
      "@sample 595: tensor([[-0.0304, -0.0028, -0.0322, -0.0398,  0.0150],\n",
      "        [ 0.0161,  0.0158,  0.0440,  0.0138, -0.0275],\n",
      "        [ 0.0356,  0.0662,  0.0668, -0.0652,  0.0046],\n",
      "        [-0.0035, -0.0222, -0.0052,  0.0036, -0.0168],\n",
      "        [ 0.0248, -0.0349, -0.0390, -0.0087, -0.0346],\n",
      "        [-0.0013, -0.0517, -0.0450,  0.0183, -0.0120],\n",
      "        [ 0.0031, -0.0129,  0.0106, -0.0034, -0.0205],\n",
      "        [-0.0270,  0.0132, -0.0201, -0.0231,  0.0090]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0034, -0.0123,  0.0291, -0.0132,  0.0184],\n",
      "        [ 0.0280, -0.0049, -0.0073,  0.0205,  0.0200],\n",
      "        [-0.0616,  0.0086, -0.0205,  0.0369, -0.0066],\n",
      "        [ 0.0062,  0.0153,  0.0115, -0.0304, -0.0163],\n",
      "        [-0.0437, -0.0065,  0.0293, -0.0192, -0.0062],\n",
      "        [ 0.0091, -0.0028,  0.0626, -0.0172,  0.0073],\n",
      "        [-0.0146, -0.0090,  0.0070, -0.0121, -0.0004],\n",
      "        [-0.0371, -0.0468, -0.0434, -0.0080, -0.0135]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0154, grad_fn=<MinBackward1>), tensor(0.9109, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.14733126759529114\n",
      "@sample 596: tensor([[ 0.0304, -0.0168,  0.0256,  0.0052, -0.0213],\n",
      "        [-0.0264, -0.0052,  0.0068, -0.0025, -0.0079],\n",
      "        [-0.0124,  0.0027, -0.0245, -0.0523,  0.0157],\n",
      "        [-0.0122,  0.0293,  0.0037, -0.0531,  0.0131],\n",
      "        [-0.0195, -0.0091, -0.0078,  0.0022,  0.0150],\n",
      "        [-0.0073,  0.0122,  0.0150, -0.0259,  0.0238],\n",
      "        [-0.0517,  0.0051,  0.0196,  0.0003,  0.0151],\n",
      "        [ 0.0292,  0.0026,  0.0166, -0.0331, -0.0190]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0240,  0.0259,  0.0626, -0.0677, -0.0606],\n",
      "        [-0.0204,  0.0110,  0.0255, -0.0090,  0.0055],\n",
      "        [-0.0138, -0.0303,  0.0205, -0.0076, -0.0125],\n",
      "        [-0.0090, -0.0648,  0.0431,  0.0024,  0.0046],\n",
      "        [-0.0025,  0.0259,  0.0315, -0.0253, -0.0359],\n",
      "        [-0.0184, -0.0325, -0.0326,  0.0111,  0.0133],\n",
      "        [-0.0077, -0.0014, -0.0539,  0.0208,  0.0452],\n",
      "        [-0.0937, -0.0391, -0.0147, -0.0180, -0.0247]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0186, grad_fn=<MinBackward1>), tensor(0.8742, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13684260845184326\n",
      "@sample 597: tensor([[ 0.0136,  0.0060,  0.0071, -0.0399,  0.0322],\n",
      "        [-0.0003,  0.0312, -0.0030, -0.0386,  0.0062],\n",
      "        [-0.0011,  0.0706,  0.0178, -0.0511, -0.0025],\n",
      "        [ 0.0100, -0.0125,  0.0212,  0.0111, -0.0020],\n",
      "        [ 0.0013, -0.0184, -0.0123, -0.0127, -0.0060],\n",
      "        [-0.0355, -0.0046,  0.0021, -0.0154,  0.0072],\n",
      "        [-0.0062, -0.0218, -0.0024, -0.0022, -0.0243],\n",
      "        [ 0.0344,  0.0015,  0.0262, -0.0095, -0.0166]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0495, -0.0383, -0.0287,  0.0255, -0.0282],\n",
      "        [-0.0290, -0.0351, -0.0398,  0.0165,  0.0013],\n",
      "        [-0.0869, -0.0297, -0.1284,  0.0691, -0.0208],\n",
      "        [-0.0203, -0.0432, -0.0636,  0.0272, -0.0119],\n",
      "        [ 0.0481, -0.0159,  0.0183, -0.0002,  0.0259],\n",
      "        [-0.0064,  0.0027,  0.0210,  0.0144, -0.0165],\n",
      "        [-0.0105, -0.0143,  0.0941, -0.0476, -0.0163],\n",
      "        [-0.0507, -0.0117, -0.0081,  0.0414, -0.0461]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0181, grad_fn=<MinBackward1>), tensor(0.8746, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.14881204068660736\n",
      "@sample 598: tensor([[-0.0191, -0.0287, -0.0050,  0.0287,  0.0041],\n",
      "        [-0.0242,  0.0194,  0.0033, -0.0149, -0.0004],\n",
      "        [-0.0361, -0.0033, -0.0062,  0.0213, -0.0220],\n",
      "        [ 0.0217,  0.0265,  0.0459, -0.0348, -0.0029],\n",
      "        [ 0.0155, -0.0135,  0.0313, -0.0211,  0.0347],\n",
      "        [-0.0246, -0.0335, -0.0325,  0.0056,  0.0154],\n",
      "        [-0.0254, -0.0211,  0.0167,  0.0165,  0.0066],\n",
      "        [-0.0330, -0.0089, -0.0134, -0.0286,  0.0157]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0223,  0.0096,  0.0142,  0.0119, -0.0210],\n",
      "        [ 0.0030,  0.0045,  0.0090, -0.0131, -0.0059],\n",
      "        [ 0.0114,  0.0026,  0.0392, -0.0319, -0.0204],\n",
      "        [ 0.0091,  0.0004, -0.0064, -0.0094, -0.0257],\n",
      "        [ 0.0269,  0.0016, -0.0579, -0.0263, -0.0196],\n",
      "        [ 0.0505, -0.0145,  0.0642, -0.0353, -0.0208],\n",
      "        [-0.0235, -0.0152,  0.0521, -0.0289, -0.0261],\n",
      "        [ 0.0016, -0.0322,  0.0146, -0.0462, -0.0476]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0172, grad_fn=<MinBackward1>), tensor(0.9043, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.14132459461688995\n",
      "@sample 599: tensor([[ 0.0012, -0.0288, -0.0333,  0.0382, -0.0167],\n",
      "        [ 0.0058, -0.0336, -0.0114,  0.0133, -0.0065],\n",
      "        [-0.0198,  0.0247,  0.0306, -0.0742,  0.0429],\n",
      "        [-0.0413,  0.0343,  0.0184, -0.0424,  0.0375],\n",
      "        [-0.0067,  0.0157,  0.0391, -0.0203, -0.0102],\n",
      "        [-0.0110, -0.0058, -0.0037,  0.0250,  0.0123],\n",
      "        [ 0.0058,  0.0035,  0.0122, -0.0024,  0.0118],\n",
      "        [-0.0140,  0.0030, -0.0270, -0.0187,  0.0219]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0013,  0.0682,  0.0646, -0.0388,  0.0072],\n",
      "        [ 0.0087, -0.0087,  0.0388,  0.0175,  0.0075],\n",
      "        [-0.0720, -0.0408, -0.0541,  0.0304, -0.0194],\n",
      "        [-0.0355,  0.0034, -0.0209,  0.0050, -0.0220],\n",
      "        [-0.0111, -0.0175, -0.0356,  0.0341,  0.0141],\n",
      "        [ 0.0237,  0.0219, -0.0026, -0.0159, -0.0132],\n",
      "        [ 0.0119, -0.0040, -0.0461,  0.0391,  0.0096],\n",
      "        [ 0.0203,  0.0086,  0.0302, -0.0250,  0.0004]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0225, grad_fn=<MinBackward1>), tensor(0.9386, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12916013598442078\n",
      "@sample 600: tensor([[-0.0118, -0.0146,  0.0260,  0.0011, -0.0157],\n",
      "        [-0.0262,  0.0063, -0.0251, -0.0019,  0.0243],\n",
      "        [-0.0249,  0.0349, -0.0027, -0.0136, -0.0089],\n",
      "        [-0.0232,  0.0031, -0.0134, -0.0086, -0.0206],\n",
      "        [ 0.0016,  0.0200,  0.0161, -0.0251,  0.0105],\n",
      "        [-0.0220, -0.0364,  0.0076,  0.0212,  0.0108],\n",
      "        [-0.0107, -0.0056, -0.0106, -0.0034, -0.0023],\n",
      "        [-0.0084,  0.0380,  0.0228, -0.0180,  0.0092]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0105,  0.0140, -0.0023, -0.0280, -0.0308],\n",
      "        [ 0.0160,  0.0148,  0.0001,  0.0219, -0.0238],\n",
      "        [ 0.0017, -0.0024,  0.0256, -0.0155, -0.0035],\n",
      "        [-0.0092,  0.0021,  0.0564, -0.0513,  0.0183],\n",
      "        [-0.0484, -0.0063, -0.0405,  0.0032,  0.0090],\n",
      "        [-0.0209,  0.0068,  0.0024, -0.0342, -0.0042],\n",
      "        [ 0.0249,  0.0217,  0.0295, -0.0333, -0.0043],\n",
      "        [ 0.0203, -0.0360, -0.0583,  0.0090, -0.0163]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0230, grad_fn=<MinBackward1>), tensor(0.9034, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13420429825782776\n",
      "@sample 601: tensor([[ 0.0078, -0.0189, -0.0009,  0.0143,  0.0006],\n",
      "        [-0.0170,  0.0137,  0.0536, -0.0015,  0.0214],\n",
      "        [ 0.0024,  0.0082,  0.0152, -0.0073, -0.0034],\n",
      "        [ 0.0080, -0.0589, -0.0112,  0.0255,  0.0360],\n",
      "        [ 0.0234, -0.0197, -0.0212,  0.0627, -0.0334],\n",
      "        [-0.0222, -0.0269,  0.0303, -0.0251,  0.0475],\n",
      "        [-0.0138, -0.0333,  0.0051,  0.0594, -0.0171],\n",
      "        [ 0.0176, -0.0523, -0.0085,  0.0177,  0.0080]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0103,  0.0166, -0.0008,  0.0029,  0.0088],\n",
      "        [-0.0074,  0.0199, -0.0461,  0.0221, -0.0224],\n",
      "        [-0.0089, -0.0056,  0.0088,  0.0487,  0.0248],\n",
      "        [-0.0024,  0.0190, -0.0194, -0.0429, -0.0126],\n",
      "        [ 0.0284,  0.0428,  0.0303, -0.0395, -0.0333],\n",
      "        [-0.0416, -0.0124,  0.0427, -0.0116, -0.0473],\n",
      "        [ 0.0108,  0.0387,  0.0618, -0.0612,  0.0119],\n",
      "        [ 0.0076, -0.0396,  0.0722, -0.0398, -0.0085]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0140, grad_fn=<MinBackward1>), tensor(0.8970, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1579647809267044\n",
      "@sample 602: tensor([[-0.0025,  0.0678, -0.0011, -0.0303, -0.0282],\n",
      "        [ 0.0103, -0.0071, -0.0017,  0.0532, -0.0369],\n",
      "        [ 0.0018,  0.0259,  0.0363, -0.0326,  0.0246],\n",
      "        [ 0.0212,  0.0148, -0.0062, -0.0089, -0.0129],\n",
      "        [-0.0146, -0.0342, -0.0065,  0.0157, -0.0223],\n",
      "        [ 0.0053,  0.0436,  0.0317, -0.0274,  0.0293],\n",
      "        [ 0.0058,  0.0183,  0.0173, -0.0079, -0.0047],\n",
      "        [ 0.0098,  0.0067,  0.0019, -0.0339,  0.0319]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0135, -0.0163, -0.0400,  0.0231, -0.0498],\n",
      "        [ 0.0169,  0.0352,  0.0109,  0.0052,  0.0456],\n",
      "        [-0.0170, -0.0604, -0.0523,  0.0219,  0.0209],\n",
      "        [-0.0296,  0.0350, -0.0465, -0.0056,  0.0121],\n",
      "        [-0.0058,  0.0408,  0.0460, -0.0256,  0.0132],\n",
      "        [-0.0240,  0.0146, -0.0456, -0.0032, -0.0027],\n",
      "        [-0.0334,  0.0325, -0.0533, -0.0057, -0.0056],\n",
      "        [-0.0385, -0.0359, -0.0159, -0.0153,  0.0034]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0233, grad_fn=<MinBackward1>), tensor(0.8802, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1407124400138855\n",
      "@sample 603: tensor([[-0.0136,  0.0184, -0.0116, -0.0298,  0.0322],\n",
      "        [-0.0043,  0.0053, -0.0052,  0.0176,  0.0386],\n",
      "        [-0.0133, -0.0067, -0.0021,  0.0185, -0.0068],\n",
      "        [ 0.0269,  0.0337,  0.0091, -0.0231, -0.0406],\n",
      "        [-0.0378,  0.0024,  0.0245,  0.0011,  0.0127],\n",
      "        [ 0.0023,  0.0372,  0.0298, -0.0258,  0.0163],\n",
      "        [ 0.0329, -0.0044,  0.0020, -0.0175,  0.0214],\n",
      "        [-0.0136, -0.0144, -0.0360,  0.0015, -0.0209]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0065,  0.0054, -0.0450, -0.0124, -0.0119],\n",
      "        [ 0.0274, -0.0357, -0.0226, -0.0058,  0.0146],\n",
      "        [-0.0291,  0.0261, -0.0844,  0.0157,  0.0054],\n",
      "        [-0.0368, -0.0096,  0.0485,  0.0054,  0.0025],\n",
      "        [ 0.0090,  0.0192,  0.0085, -0.0056,  0.0099],\n",
      "        [-0.0442,  0.0246, -0.0553,  0.0434, -0.0455],\n",
      "        [ 0.0034,  0.0188, -0.0250,  0.0121, -0.0145],\n",
      "        [ 0.0028, -0.0596,  0.0170, -0.0146, -0.0093]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0129, grad_fn=<MinBackward1>), tensor(0.8964, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1412034034729004\n",
      "@sample 604: tensor([[-0.0246,  0.0187, -0.0145,  0.0158, -0.0087],\n",
      "        [ 0.0028,  0.0068, -0.0124,  0.0410, -0.0284],\n",
      "        [-0.0026, -0.0074,  0.0037,  0.0261, -0.0025],\n",
      "        [ 0.0185,  0.0025, -0.0441,  0.0349, -0.0596],\n",
      "        [ 0.0511, -0.0264, -0.0180,  0.0079, -0.0033],\n",
      "        [-0.0019, -0.0205, -0.0092,  0.0352, -0.0290],\n",
      "        [ 0.0135, -0.0069, -0.0044,  0.0300, -0.0394],\n",
      "        [ 0.0269, -0.0485, -0.0018,  0.0751, -0.0537]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0181,  0.0216,  0.0120, -0.0391, -0.0121],\n",
      "        [ 0.0077,  0.0112, -0.0347, -0.0087,  0.0151],\n",
      "        [ 0.0162,  0.0177,  0.0788, -0.0169, -0.0246],\n",
      "        [ 0.0234,  0.0359,  0.0419, -0.0210,  0.0075],\n",
      "        [-0.0013, -0.0155, -0.0470, -0.0022,  0.0451],\n",
      "        [ 0.0059,  0.0028,  0.0142, -0.0232,  0.0375],\n",
      "        [ 0.0297,  0.0270,  0.0041, -0.0261, -0.0150],\n",
      "        [-0.0371,  0.0208, -0.0262,  0.0011, -0.0018]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0192, grad_fn=<MinBackward1>), tensor(0.9113, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.15766024589538574\n",
      "@sample 605: tensor([[ 1.6052e-02,  4.9508e-02,  6.9286e-03, -5.4616e-03, -2.7032e-02],\n",
      "        [ 3.1686e-02, -1.2396e-02, -4.9511e-03,  1.2317e-02,  2.8007e-03],\n",
      "        [ 1.4789e-02,  1.9044e-02,  2.2919e-02, -4.3695e-02,  1.1618e-02],\n",
      "        [ 1.1641e-02,  7.8914e-03, -1.2148e-03, -1.3997e-02,  4.4182e-05],\n",
      "        [ 9.1180e-03, -5.1380e-03, -1.5751e-02,  2.9620e-02, -2.6230e-02],\n",
      "        [-9.5660e-03, -2.3656e-02,  1.4762e-02,  3.3268e-02, -4.5512e-02],\n",
      "        [-1.1002e-02,  1.9629e-04, -7.3954e-03,  2.0921e-02, -1.2952e-02],\n",
      "        [-1.7591e-02, -5.3053e-03,  1.2551e-02,  3.4036e-02, -4.0723e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0195,  0.0350, -0.0194,  0.0076, -0.0061],\n",
      "        [-0.0215,  0.0042, -0.0202,  0.0082,  0.0021],\n",
      "        [-0.0558, -0.0362, -0.0622,  0.0371, -0.0281],\n",
      "        [-0.0282, -0.0279, -0.0353,  0.0615,  0.0484],\n",
      "        [-0.0075, -0.0067,  0.0185, -0.0085,  0.0168],\n",
      "        [ 0.0138,  0.0269,  0.0875, -0.0052,  0.0085],\n",
      "        [ 0.0174,  0.0005, -0.0216,  0.0256,  0.0053],\n",
      "        [-0.0214,  0.0082, -0.0010, -0.0371, -0.0436]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0198, grad_fn=<MinBackward1>), tensor(0.9076, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.14286795258522034\n",
      "@sample 606: tensor([[-0.0343, -0.0026, -0.0144,  0.0025,  0.0124],\n",
      "        [-0.0146, -0.0126,  0.0277,  0.0281, -0.0210],\n",
      "        [ 0.0031,  0.0134, -0.0065, -0.0249,  0.0029],\n",
      "        [ 0.0097,  0.0328,  0.0535, -0.0089,  0.0143],\n",
      "        [ 0.0434, -0.0132,  0.0003,  0.0273, -0.0317],\n",
      "        [-0.0135, -0.0056, -0.0177,  0.0078,  0.0246],\n",
      "        [ 0.0170,  0.0115, -0.0219,  0.0258, -0.0075],\n",
      "        [-0.0167,  0.0184,  0.0013,  0.0179,  0.0105]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0140,  0.0043,  0.0137, -0.0302, -0.0079],\n",
      "        [ 0.0242, -0.0021,  0.0013,  0.0211,  0.0371],\n",
      "        [-0.0174, -0.0161, -0.0460,  0.0158, -0.0112],\n",
      "        [-0.0077,  0.0048, -0.0373,  0.0538,  0.0258],\n",
      "        [ 0.0124,  0.0389, -0.0066, -0.0045,  0.0105],\n",
      "        [ 0.0104,  0.0138,  0.0353, -0.0255,  0.0145],\n",
      "        [ 0.0079, -0.0190, -0.0189, -0.0091,  0.0205],\n",
      "        [ 0.0267,  0.0121, -0.0181,  0.0190, -0.0080]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0101, grad_fn=<MinBackward1>), tensor(0.9536, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13773173093795776\n",
      "@sample 607: tensor([[ 0.0112, -0.0416,  0.0035,  0.0616,  0.0137],\n",
      "        [ 0.0250,  0.0053, -0.0063,  0.0134, -0.0008],\n",
      "        [-0.0115, -0.0253,  0.0131,  0.0190,  0.0091],\n",
      "        [-0.0089, -0.0106,  0.0192,  0.0029,  0.0054],\n",
      "        [ 0.0040, -0.0100, -0.0061, -0.0056,  0.0176],\n",
      "        [-0.0140,  0.0241,  0.0134,  0.0067,  0.0115],\n",
      "        [ 0.0034,  0.0159, -0.0011, -0.0209,  0.0058],\n",
      "        [ 0.0183, -0.0305, -0.0571,  0.0466, -0.0246]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0066,  0.0828,  0.0279, -0.0113, -0.0273],\n",
      "        [ 0.0158,  0.0069, -0.0282,  0.0204,  0.0064],\n",
      "        [ 0.0525, -0.0360,  0.0084, -0.0195,  0.0238],\n",
      "        [-0.0269,  0.0183,  0.0023,  0.0053, -0.0041],\n",
      "        [-0.0088,  0.0122, -0.0509,  0.0308, -0.0065],\n",
      "        [ 0.0017, -0.0025, -0.0164, -0.0143,  0.0037],\n",
      "        [-0.0033, -0.0276, -0.0552,  0.0332,  0.0255],\n",
      "        [ 0.0352,  0.0178,  0.0735, -0.0403,  0.0290]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0144, grad_fn=<MinBackward1>), tensor(0.8801, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13853594660758972\n",
      "@sample 608: tensor([[ 0.0034, -0.0080, -0.0107, -0.0022, -0.0014],\n",
      "        [ 0.0093,  0.0545,  0.0355, -0.0009, -0.0221],\n",
      "        [ 0.0147, -0.0231, -0.0223, -0.0130,  0.0275],\n",
      "        [ 0.0132, -0.0217, -0.0072,  0.0006, -0.0115],\n",
      "        [-0.0143, -0.0059, -0.0637,  0.0336,  0.0078],\n",
      "        [-0.0194,  0.0026, -0.0074,  0.0051,  0.0183],\n",
      "        [ 0.0114, -0.0244,  0.0054,  0.0688, -0.0300],\n",
      "        [-0.0422, -0.0021,  0.0056, -0.0282,  0.0733]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0085,  0.0436,  0.0110, -0.0168,  0.0095],\n",
      "        [ 0.0082,  0.0077, -0.0797,  0.0153, -0.0053],\n",
      "        [-0.0056, -0.0501, -0.0203, -0.0023,  0.0253],\n",
      "        [-0.0208, -0.0183, -0.0052,  0.0068, -0.0324],\n",
      "        [ 0.0305,  0.0123, -0.0362, -0.0156, -0.0141],\n",
      "        [-0.0054,  0.0029, -0.0308, -0.0208,  0.0098],\n",
      "        [ 0.0254,  0.0334, -0.0389, -0.0098,  0.0331],\n",
      "        [-0.0256, -0.0041, -0.0566,  0.0124,  0.0025]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0208, grad_fn=<MinBackward1>), tensor(0.8924, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1349831223487854\n",
      "@sample 609: tensor([[ 0.0076,  0.0286,  0.0044, -0.0239, -0.0223],\n",
      "        [ 0.0035, -0.0031, -0.0108, -0.0233,  0.0175],\n",
      "        [-0.0360,  0.0248,  0.0266, -0.0407,  0.0168],\n",
      "        [-0.0313, -0.0035, -0.0239,  0.0169,  0.0094],\n",
      "        [ 0.0039, -0.0463,  0.0049,  0.0088, -0.0041],\n",
      "        [-0.0172,  0.0134,  0.0080,  0.0020,  0.0367],\n",
      "        [ 0.0295, -0.0082,  0.0001,  0.0240, -0.0015],\n",
      "        [-0.0233, -0.0126,  0.0358, -0.0310,  0.0330]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-1.3896e-02, -1.6853e-02, -2.2723e-02,  7.7217e-03, -1.3752e-02],\n",
      "        [ 1.8569e-02, -1.2674e-02, -2.3207e-02,  3.2936e-02,  9.9272e-03],\n",
      "        [ 3.1315e-05, -2.8683e-02, -6.2257e-02,  2.0895e-02, -1.1223e-02],\n",
      "        [ 3.8716e-02, -3.3704e-03,  9.5171e-03, -2.8303e-02,  1.9762e-03],\n",
      "        [-3.7806e-02, -7.6448e-02,  6.3101e-02,  9.7527e-04, -1.2959e-02],\n",
      "        [ 3.2111e-02,  2.4622e-02,  2.0310e-02, -3.1686e-02, -4.4055e-03],\n",
      "        [-5.2640e-02,  3.9756e-02,  9.0121e-04, -5.3285e-02,  1.8314e-02],\n",
      "        [-4.5916e-03, -1.4890e-02, -2.6352e-02,  6.2763e-02,  6.9276e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0182, grad_fn=<MinBackward1>), tensor(0.8655, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.14817997813224792\n",
      "@sample 610: tensor([[ 0.0062, -0.0180, -0.0109,  0.0285, -0.0061],\n",
      "        [ 0.0174, -0.0415, -0.0080,  0.0141, -0.0083],\n",
      "        [-0.0104, -0.0168, -0.0215,  0.0037,  0.0121],\n",
      "        [-0.0269, -0.0330, -0.0133,  0.0174,  0.0088],\n",
      "        [-0.0237,  0.0022,  0.0094, -0.0113,  0.0385],\n",
      "        [-0.0170,  0.0213, -0.0119, -0.0015,  0.0110],\n",
      "        [ 0.0034, -0.0384, -0.0013,  0.0050, -0.0008],\n",
      "        [-0.0183,  0.0074,  0.0187,  0.0117, -0.0019]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0402,  0.0050,  0.0261, -0.0237, -0.0108],\n",
      "        [-0.0272,  0.0017, -0.0269,  0.0249,  0.0508],\n",
      "        [ 0.0467, -0.0092, -0.0313,  0.0468,  0.0458],\n",
      "        [ 0.0184, -0.0081,  0.0404, -0.0112, -0.0103],\n",
      "        [ 0.0035, -0.0301,  0.0098,  0.0077,  0.0082],\n",
      "        [ 0.0011,  0.0026, -0.0030, -0.0060, -0.0038],\n",
      "        [ 0.0429,  0.0078,  0.0622, -0.0395,  0.0083],\n",
      "        [ 0.0217,  0.0333,  0.0526, -0.0283,  0.0061]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0148, grad_fn=<MinBackward1>), tensor(0.8927, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.14090391993522644\n",
      "@sample 611: tensor([[-0.0060, -0.0002, -0.0142, -0.0193, -0.0099],\n",
      "        [-0.0126,  0.0625,  0.0054, -0.0329,  0.0114],\n",
      "        [-0.0459,  0.0372,  0.0743, -0.0189,  0.0094],\n",
      "        [ 0.0077, -0.0122,  0.0031,  0.0262, -0.0197],\n",
      "        [-0.0177, -0.0020, -0.0055, -0.0352,  0.0289],\n",
      "        [-0.0118, -0.0211, -0.0130,  0.0039, -0.0190],\n",
      "        [ 0.0256,  0.0140,  0.0153, -0.0464,  0.0391],\n",
      "        [-0.0168,  0.0329,  0.0224, -0.0286,  0.0440]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0071, -0.0235, -0.0041,  0.0027,  0.0330],\n",
      "        [ 0.0015, -0.0132, -0.0845,  0.0678,  0.0128],\n",
      "        [-0.0180, -0.0134, -0.0670,  0.0594,  0.0210],\n",
      "        [-0.0166,  0.0093, -0.0164,  0.0101, -0.0049],\n",
      "        [-0.0019, -0.0112, -0.0362,  0.0251,  0.0212],\n",
      "        [ 0.0381, -0.0230,  0.0429,  0.0053, -0.0084],\n",
      "        [-0.0115, -0.0060, -0.0208, -0.0219, -0.0317],\n",
      "        [-0.0334,  0.0043, -0.0577,  0.0321, -0.0154]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0152, grad_fn=<MinBackward1>), tensor(0.9216, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1372685432434082\n",
      "@sample 612: tensor([[-0.0002,  0.0116, -0.0144,  0.0142,  0.0011],\n",
      "        [-0.0183, -0.0276, -0.0433,  0.0279, -0.0040],\n",
      "        [-0.0362, -0.0027,  0.0069, -0.0061,  0.0173],\n",
      "        [ 0.0101, -0.0373,  0.0187, -0.0004,  0.0013],\n",
      "        [-0.0161,  0.0112,  0.0081, -0.0391, -0.0005],\n",
      "        [ 0.0109, -0.0187, -0.0166, -0.0188, -0.0133],\n",
      "        [ 0.0062, -0.0200, -0.0079, -0.0103, -0.0371],\n",
      "        [-0.0207, -0.0414, -0.0015, -0.0109,  0.0225]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0304,  0.0182,  0.0148, -0.0110,  0.0081],\n",
      "        [ 0.0051, -0.0107, -0.0105,  0.0199,  0.0413],\n",
      "        [ 0.0097,  0.0058,  0.0640,  0.0053,  0.0065],\n",
      "        [-0.0022, -0.0169,  0.0501, -0.0383, -0.0091],\n",
      "        [-0.0080, -0.0257, -0.0003, -0.0340, -0.0013],\n",
      "        [-0.0185, -0.0068,  0.0397, -0.0044,  0.0135],\n",
      "        [ 0.0091,  0.0362,  0.0132,  0.0211,  0.0295],\n",
      "        [-0.0091, -0.0373,  0.0820, -0.0322, -0.0185]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0114, grad_fn=<MinBackward1>), tensor(0.9297, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1391233503818512\n",
      "@sample 613: tensor([[-0.0165,  0.0386,  0.0236, -0.0205,  0.0261],\n",
      "        [ 0.0120, -0.0172, -0.0231,  0.0460, -0.0105],\n",
      "        [-0.0131,  0.0253, -0.0216,  0.0188,  0.0130],\n",
      "        [ 0.0006,  0.0227,  0.0086,  0.0206, -0.0068],\n",
      "        [ 0.0108, -0.0397, -0.0145,  0.0309, -0.0080],\n",
      "        [-0.0290, -0.0245,  0.0177, -0.0209, -0.0132],\n",
      "        [-0.0213, -0.0295, -0.0220,  0.0286,  0.0178],\n",
      "        [-0.0199, -0.0205,  0.0048,  0.0087, -0.0161]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0344,  0.0181, -0.0238,  0.0083, -0.0084],\n",
      "        [ 0.0254,  0.0107, -0.0002,  0.0037, -0.0188],\n",
      "        [ 0.0318,  0.0124, -0.0215, -0.0028, -0.0057],\n",
      "        [-0.0301, -0.0309, -0.0631,  0.0056, -0.0043],\n",
      "        [ 0.0437, -0.0054, -0.0104, -0.0161,  0.0089],\n",
      "        [-0.0326,  0.0071,  0.0052,  0.0162,  0.0075],\n",
      "        [ 0.0424, -0.0209,  0.0668, -0.0331, -0.0035],\n",
      "        [ 0.0011, -0.0189,  0.0128,  0.0042,  0.0044]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0188, grad_fn=<MinBackward1>), tensor(0.8975, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12917913496494293\n",
      "@sample 614: tensor([[-0.0012, -0.0179, -0.0290, -0.0280,  0.0107],\n",
      "        [ 0.0243, -0.0132, -0.0093, -0.0099, -0.0174],\n",
      "        [-0.0182,  0.0210,  0.0180,  0.0058, -0.0355],\n",
      "        [-0.0292, -0.0138, -0.0227,  0.0327, -0.0030],\n",
      "        [-0.0023, -0.0636, -0.0138,  0.0175, -0.0118],\n",
      "        [-0.0027, -0.0054, -0.0083, -0.0028, -0.0041],\n",
      "        [ 0.0011, -0.0184,  0.0140,  0.0110, -0.0026],\n",
      "        [ 0.0185,  0.0079,  0.0042,  0.0183, -0.0019]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 1.7201e-02, -5.3264e-02,  3.1823e-02, -3.0865e-02, -3.0083e-02],\n",
      "        [-2.6042e-02, -6.4166e-03,  4.8260e-03, -1.2765e-02,  9.3793e-04],\n",
      "        [ 2.8303e-02,  5.2562e-02, -1.7688e-02,  1.1057e-02,  8.1568e-03],\n",
      "        [-6.2577e-05,  7.5005e-03,  2.8449e-02, -6.9070e-02, -3.8303e-02],\n",
      "        [ 3.7302e-03, -4.5198e-03,  5.7760e-02, -2.6267e-02, -2.7562e-02],\n",
      "        [ 1.2631e-02, -8.4916e-03,  2.3115e-02, -2.7375e-02, -2.6235e-03],\n",
      "        [ 4.0570e-02,  4.0962e-02,  1.1288e-02,  1.4886e-02,  3.3488e-02],\n",
      "        [ 6.8831e-04, -1.5140e-02, -2.5096e-02, -2.1087e-02,  6.7414e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0162, grad_fn=<MinBackward1>), tensor(0.9384, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13873058557510376\n",
      "@sample 615: tensor([[-1.6208e-02, -6.4757e-02,  2.1305e-02, -1.0403e-02,  1.2245e-02],\n",
      "        [-7.4234e-03,  2.7138e-02,  2.0447e-02, -2.5041e-02,  5.3586e-02],\n",
      "        [-2.2455e-03, -2.9477e-02, -1.9829e-02,  2.6848e-03, -8.2597e-03],\n",
      "        [ 2.8545e-03,  1.5653e-02,  1.9311e-03,  4.2375e-02, -2.6903e-02],\n",
      "        [ 1.3173e-02,  2.4641e-02, -3.4265e-05,  3.7123e-03,  1.5161e-03],\n",
      "        [ 9.7739e-03,  5.9004e-02,  7.4481e-02, -2.4251e-02,  5.1531e-03],\n",
      "        [ 7.1998e-03, -5.4580e-02, -2.4824e-02,  5.2808e-03,  2.0654e-02],\n",
      "        [ 2.0754e-03,  1.0133e-04,  1.0526e-02,  2.8941e-02,  4.9135e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-4.3470e-02,  9.5462e-03,  8.5422e-03,  1.3755e-02, -8.1444e-03],\n",
      "        [-8.7701e-04,  2.8850e-02, -4.7347e-02,  4.1303e-02, -8.7646e-03],\n",
      "        [-6.6720e-03, -2.0145e-02, -3.6435e-02,  6.2074e-02,  1.1734e-02],\n",
      "        [-4.9786e-03,  1.0977e-02, -1.8178e-02,  2.0944e-02,  7.3643e-03],\n",
      "        [-2.0866e-02,  8.1182e-05, -3.4031e-02,  3.7042e-02,  1.8538e-02],\n",
      "        [-3.1881e-02, -1.1551e-02, -2.9329e-02,  3.2242e-02, -1.0477e-03],\n",
      "        [ 3.9783e-02,  1.8745e-02, -6.4680e-02,  1.2459e-02,  5.1709e-04],\n",
      "        [ 5.1936e-03,  5.0492e-02, -1.7890e-02, -5.2642e-02,  2.9955e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0186, grad_fn=<MinBackward1>), tensor(0.8689, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1467231810092926\n",
      "@sample 616: tensor([[-0.0115,  0.0137,  0.0037, -0.0144,  0.0091],\n",
      "        [ 0.0323, -0.0041,  0.0421, -0.0039,  0.0199],\n",
      "        [-0.0041,  0.0241,  0.0236, -0.0191,  0.0361],\n",
      "        [-0.0071,  0.0677,  0.0124, -0.0835,  0.0171],\n",
      "        [-0.0016,  0.0089,  0.0150, -0.0185, -0.0025],\n",
      "        [-0.0018, -0.0161,  0.0052,  0.0290, -0.0435],\n",
      "        [-0.0109,  0.0050,  0.0021,  0.0023,  0.0187],\n",
      "        [ 0.0135,  0.0205,  0.0138, -0.0226, -0.0162]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0129,  0.0173,  0.0040,  0.0059,  0.0449],\n",
      "        [-0.0224, -0.0160, -0.0222,  0.0123, -0.0442],\n",
      "        [ 0.0013, -0.0100, -0.0368,  0.0570,  0.0155],\n",
      "        [-0.0207, -0.0456,  0.0425, -0.0027, -0.0329],\n",
      "        [-0.0009, -0.0231, -0.0022, -0.0061, -0.0269],\n",
      "        [ 0.0039,  0.0545, -0.0062, -0.0221,  0.0126],\n",
      "        [ 0.0012,  0.0194,  0.0163,  0.0124,  0.0117],\n",
      "        [-0.0368, -0.0087,  0.0030,  0.0538,  0.0075]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0125, grad_fn=<MinBackward1>), tensor(0.9319, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13371258974075317\n",
      "@sample 617: tensor([[ 0.0268,  0.0488,  0.0073, -0.0249, -0.0080],\n",
      "        [-0.0318,  0.0058,  0.0238, -0.0273, -0.0042],\n",
      "        [-0.0128,  0.0141,  0.0013,  0.0012, -0.0229],\n",
      "        [-0.0109,  0.0284,  0.0219, -0.0210, -0.0091],\n",
      "        [ 0.0211, -0.0125,  0.0156,  0.0121,  0.0106],\n",
      "        [ 0.0004, -0.0015,  0.0003, -0.0092,  0.0291],\n",
      "        [ 0.0022, -0.0044,  0.0064,  0.0158,  0.0081],\n",
      "        [ 0.0007, -0.0040,  0.0246, -0.0072,  0.0257]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0402, -0.0480, -0.1222,  0.0507,  0.0141],\n",
      "        [-0.0043, -0.0145,  0.0727, -0.0376, -0.0353],\n",
      "        [-0.0046,  0.0152,  0.0341,  0.0361,  0.0101],\n",
      "        [-0.0019, -0.0124, -0.0183,  0.0107,  0.0050],\n",
      "        [-0.0082,  0.0307, -0.0439,  0.0245,  0.0012],\n",
      "        [ 0.0124, -0.0143, -0.0227, -0.0279, -0.0299],\n",
      "        [-0.0058,  0.0295,  0.0016,  0.0255,  0.0333],\n",
      "        [-0.0092, -0.0013, -0.0008, -0.0020, -0.0005]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0256, grad_fn=<MinBackward1>), tensor(0.9195, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13757701218128204\n",
      "@sample 618: tensor([[-0.0282,  0.0763, -0.0319, -0.0555,  0.0083],\n",
      "        [-0.0122,  0.0006,  0.0085, -0.0432,  0.0243],\n",
      "        [ 0.0289, -0.0070,  0.0178, -0.0024,  0.0060],\n",
      "        [-0.0016, -0.0005,  0.0215, -0.0124,  0.0254],\n",
      "        [ 0.0031, -0.0204, -0.0556,  0.0250,  0.0132],\n",
      "        [ 0.0166,  0.0012,  0.0110, -0.0300,  0.0557],\n",
      "        [-0.0225,  0.0280, -0.0070, -0.0399,  0.0049],\n",
      "        [-0.0007,  0.0181, -0.0104, -0.0143,  0.0215]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0228, -0.0278, -0.0818, -0.0072, -0.0046],\n",
      "        [-0.0280, -0.0325,  0.0409,  0.0176, -0.0447],\n",
      "        [ 0.0261,  0.0092, -0.0137,  0.0023, -0.0458],\n",
      "        [ 0.0187, -0.0081, -0.0086,  0.0002, -0.0266],\n",
      "        [ 0.0588, -0.0106,  0.0654,  0.0210, -0.0180],\n",
      "        [-0.0274,  0.0121, -0.0588,  0.0238, -0.0138],\n",
      "        [-0.0070, -0.0039, -0.0241, -0.0079, -0.0046],\n",
      "        [ 0.0023, -0.0105, -0.0197,  0.0449,  0.0281]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0146, grad_fn=<MinBackward1>), tensor(0.8860, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13811272382736206\n",
      "@sample 619: tensor([[-7.5655e-03, -1.5620e-02, -1.7416e-02,  2.0998e-02,  2.2598e-02],\n",
      "        [-4.8857e-04,  3.7966e-02, -2.7152e-03, -1.3236e-02, -2.6986e-02],\n",
      "        [ 1.8275e-02, -9.8528e-03, -4.4354e-03, -2.8390e-04,  2.5372e-03],\n",
      "        [ 4.2896e-04, -1.9807e-02,  2.3097e-07,  9.1975e-03, -4.0629e-02],\n",
      "        [-1.1359e-02, -3.1954e-02, -4.4660e-02,  8.7832e-03,  8.5835e-03],\n",
      "        [ 2.1367e-02,  9.1388e-03, -1.5589e-02, -1.8208e-02, -1.0071e-03],\n",
      "        [ 1.6608e-02,  7.5402e-02,  9.2749e-03, -5.2528e-02,  9.4274e-03],\n",
      "        [-9.0105e-03, -3.2992e-02, -1.1941e-02,  1.0066e-02,  2.2276e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-1.4037e-05,  1.9888e-02, -3.8355e-02, -2.3457e-02,  3.3997e-02],\n",
      "        [ 1.9636e-02,  8.4218e-03,  1.5961e-02, -6.1124e-02,  1.8406e-03],\n",
      "        [ 7.5597e-03,  1.3206e-02,  2.8823e-02,  1.3176e-02,  2.3668e-02],\n",
      "        [ 3.7085e-03, -4.0537e-02,  4.5718e-02, -2.0531e-03,  1.2502e-02],\n",
      "        [ 1.5029e-02, -6.1571e-02,  7.0440e-02, -3.4817e-02, -2.9403e-02],\n",
      "        [-2.8104e-02, -9.0628e-03, -5.8657e-02,  3.9602e-02,  2.1803e-02],\n",
      "        [-8.0655e-02,  1.5871e-02, -7.2664e-02,  7.4659e-02,  2.1095e-02],\n",
      "        [-1.4252e-03,  4.6947e-03, -1.7407e-02, -9.3150e-03, -1.2041e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0156, grad_fn=<MinBackward1>), tensor(0.8978, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.15060152113437653\n",
      "@sample 620: tensor([[ 0.0051,  0.0200,  0.0176,  0.0052, -0.0191],\n",
      "        [-0.0070,  0.0150, -0.0179, -0.0106, -0.0166],\n",
      "        [ 0.0017,  0.0664,  0.0231, -0.0410,  0.0067],\n",
      "        [-0.0276, -0.0364, -0.0005, -0.0022, -0.0040],\n",
      "        [ 0.0136, -0.0050, -0.0241,  0.0182,  0.0062],\n",
      "        [ 0.0155,  0.0118, -0.0528,  0.0294, -0.0075],\n",
      "        [-0.0259, -0.0054, -0.0006, -0.0117,  0.0095],\n",
      "        [ 0.0054,  0.0128,  0.0249, -0.0118,  0.0146]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0033,  0.0065, -0.0074,  0.0102,  0.0111],\n",
      "        [-0.0071,  0.0035, -0.0147,  0.0015,  0.0051],\n",
      "        [-0.0283,  0.0057, -0.0472, -0.0113,  0.0222],\n",
      "        [-0.0018, -0.0235,  0.0458, -0.0245,  0.0125],\n",
      "        [ 0.0321,  0.0238,  0.0232, -0.0026, -0.0157],\n",
      "        [ 0.0025,  0.0332, -0.0451,  0.0494,  0.0195],\n",
      "        [ 0.0058,  0.0070, -0.0179,  0.0056,  0.0051],\n",
      "        [-0.0050, -0.0275, -0.0263,  0.0132, -0.0208]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0243, grad_fn=<MinBackward1>), tensor(0.9062, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12911546230316162\n",
      "@sample 621: tensor([[-0.0356, -0.0096,  0.0127,  0.0098,  0.0090],\n",
      "        [ 0.0029, -0.0264, -0.0170,  0.0115, -0.0084],\n",
      "        [ 0.0091,  0.0393, -0.0141, -0.0101, -0.0100],\n",
      "        [-0.0113,  0.0205,  0.0105, -0.0182, -0.0161],\n",
      "        [ 0.0149, -0.0162,  0.0103, -0.0157,  0.0125],\n",
      "        [-0.0081,  0.0324,  0.0138, -0.0125, -0.0182],\n",
      "        [-0.0017, -0.0028, -0.0326, -0.0130, -0.0042],\n",
      "        [ 0.0038,  0.0135, -0.0215, -0.0632, -0.0025]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0096,  0.0392,  0.0213,  0.0076,  0.0140],\n",
      "        [ 0.0224, -0.0208,  0.0893, -0.0009, -0.0021],\n",
      "        [-0.0200,  0.0012,  0.0169,  0.0248,  0.0210],\n",
      "        [-0.0138, -0.0086,  0.0222, -0.0092, -0.0224],\n",
      "        [ 0.0028, -0.0015, -0.0194,  0.0098, -0.0006],\n",
      "        [ 0.0023,  0.0020,  0.0046,  0.0033,  0.0043],\n",
      "        [-0.0039,  0.0209,  0.0879, -0.0223,  0.0340],\n",
      "        [ 0.0123, -0.0450, -0.0429,  0.0123, -0.0181]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0134, grad_fn=<MinBackward1>), tensor(0.9269, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13947997987270355\n",
      "@sample 622: tensor([[ 0.0160, -0.0243,  0.0138,  0.0103,  0.0313],\n",
      "        [ 0.0112,  0.0103,  0.0223, -0.0339,  0.0167],\n",
      "        [-0.0039,  0.0711,  0.0306, -0.0483,  0.0719],\n",
      "        [-0.0139, -0.0243, -0.0171, -0.0163,  0.0121],\n",
      "        [-0.0009, -0.0081,  0.0487,  0.0118, -0.0052],\n",
      "        [-0.0091,  0.0041, -0.0092,  0.0017, -0.0215],\n",
      "        [ 0.0262, -0.0200, -0.0275,  0.0292, -0.0246],\n",
      "        [-0.0322, -0.0184,  0.0144, -0.0273, -0.0018]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0325, -0.0337,  0.0466, -0.0179, -0.0041],\n",
      "        [-0.0289, -0.0489,  0.0306, -0.0459, -0.0407],\n",
      "        [-0.0311, -0.0177, -0.1640,  0.0776,  0.0041],\n",
      "        [ 0.0231, -0.0368,  0.0008, -0.0210,  0.0332],\n",
      "        [-0.0357, -0.0050,  0.0122, -0.0159, -0.0298],\n",
      "        [-0.0087, -0.0013,  0.0368, -0.0316, -0.0055],\n",
      "        [ 0.0188, -0.0319,  0.0613, -0.0129, -0.0477],\n",
      "        [-0.0395, -0.0451, -0.0166,  0.0318, -0.0259]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0210, grad_fn=<MinBackward1>), tensor(0.9402, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.16435976326465607\n",
      "@sample 623: tensor([[-6.8023e-03,  1.4447e-02, -1.2553e-02, -2.7431e-02,  6.9579e-03],\n",
      "        [ 2.0090e-02, -2.5909e-03,  8.6861e-03,  5.4881e-05, -3.5513e-02],\n",
      "        [-7.6430e-03,  4.1054e-02,  3.2511e-02, -4.8404e-02,  2.2103e-02],\n",
      "        [ 2.5954e-02,  3.7286e-02, -9.9640e-03, -4.4727e-02,  2.3418e-02],\n",
      "        [-1.3899e-02,  1.0798e-02, -9.2652e-03,  8.6824e-03, -3.2318e-02],\n",
      "        [ 2.7211e-03,  5.8666e-02,  2.9544e-02, -3.8234e-02, -4.6252e-03],\n",
      "        [-4.5499e-03, -1.5148e-02, -1.0519e-03, -1.8312e-03,  3.6810e-02],\n",
      "        [ 1.3380e-02,  4.1322e-02,  1.0051e-02, -2.2400e-02,  9.6569e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0274, -0.0219,  0.0137, -0.0007, -0.0252],\n",
      "        [-0.0106, -0.0281, -0.0237,  0.0118,  0.0366],\n",
      "        [-0.0124,  0.0181, -0.0475,  0.0028,  0.0030],\n",
      "        [-0.0552, -0.0524, -0.1130,  0.0583, -0.0402],\n",
      "        [ 0.0084, -0.0057, -0.0012, -0.0507,  0.0012],\n",
      "        [-0.0242, -0.0461, -0.0509,  0.0415, -0.0113],\n",
      "        [ 0.0179, -0.0049,  0.0048, -0.0148,  0.0405],\n",
      "        [ 0.0033, -0.0142, -0.0228,  0.0224,  0.0208]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0211, grad_fn=<MinBackward1>), tensor(0.9289, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13737547397613525\n",
      "@sample 624: tensor([[-0.0059,  0.0581, -0.0204, -0.0078,  0.0075],\n",
      "        [-0.0111,  0.0332,  0.0008,  0.0007,  0.0030],\n",
      "        [ 0.0009,  0.0188, -0.0006, -0.0040, -0.0110],\n",
      "        [ 0.0183, -0.0118, -0.0154,  0.0650, -0.0443],\n",
      "        [ 0.0113, -0.0088,  0.0081, -0.0347, -0.0047],\n",
      "        [-0.0214,  0.0286,  0.0083, -0.0156, -0.0273],\n",
      "        [-0.0093,  0.0070,  0.0024,  0.0274, -0.0132],\n",
      "        [ 0.0160, -0.0137,  0.0539, -0.0320,  0.0115]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-2.5150e-02, -1.2270e-02, -8.5886e-02,  3.5328e-02, -1.0865e-02],\n",
      "        [-1.1524e-04,  1.1723e-02,  9.5422e-03, -1.5231e-02, -1.5824e-02],\n",
      "        [-2.5500e-03, -5.6069e-03,  6.3335e-03,  4.4324e-03, -3.1051e-02],\n",
      "        [ 3.2691e-02,  4.5021e-02,  1.5001e-02, -1.7682e-02, -2.6374e-03],\n",
      "        [ 2.6679e-03,  7.6968e-03,  1.2277e-02, -5.8117e-02, -1.1561e-02],\n",
      "        [-4.3973e-02,  1.6772e-02,  5.7502e-02, -7.4328e-02, -5.2881e-02],\n",
      "        [ 2.0235e-02,  5.6137e-02,  4.6486e-03, -6.0155e-02, -2.1386e-03],\n",
      "        [ 1.1466e-05, -2.1721e-02,  7.8714e-03,  5.7944e-02, -6.3188e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0166, grad_fn=<MinBackward1>), tensor(0.9217, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.14648288488388062\n",
      "@sample 625: tensor([[ 0.0182, -0.0242,  0.0147, -0.0065,  0.0096],\n",
      "        [ 0.0040,  0.0429,  0.0302, -0.0030, -0.0088],\n",
      "        [-0.0104,  0.0417, -0.0011, -0.0176,  0.0076],\n",
      "        [ 0.0075, -0.0024,  0.0109, -0.0240, -0.0188],\n",
      "        [ 0.0234, -0.0111,  0.0187,  0.0288,  0.0044],\n",
      "        [-0.0113,  0.0037, -0.0216,  0.0368, -0.0400],\n",
      "        [-0.0107,  0.0063,  0.0377,  0.0124, -0.0153],\n",
      "        [ 0.0237,  0.0047,  0.0114,  0.0584, -0.0368]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-5.6820e-02,  6.0188e-02, -4.9617e-02, -8.7411e-03,  1.3942e-02],\n",
      "        [-2.5363e-02,  5.1640e-02, -2.6754e-02,  1.4020e-02,  1.6626e-02],\n",
      "        [-1.9493e-02,  6.2591e-03, -6.3360e-02,  5.3171e-02, -1.7131e-03],\n",
      "        [-4.6097e-02, -6.5050e-04,  3.2101e-02,  3.3983e-03, -1.4875e-03],\n",
      "        [ 2.1638e-02, -4.5252e-03,  5.1606e-02,  2.0193e-02,  2.5851e-02],\n",
      "        [-1.6421e-02, -6.6297e-03,  5.0935e-02,  3.5028e-03,  2.7544e-02],\n",
      "        [-2.5444e-02, -2.5984e-02, -1.9216e-02, -8.6494e-03, -2.0313e-02],\n",
      "        [ 9.6560e-06,  2.7157e-02,  3.1981e-03, -1.9902e-02,  3.2960e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0179, grad_fn=<MinBackward1>), tensor(0.9456, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.14703699946403503\n",
      "@sample 626: tensor([[-0.0023, -0.0166, -0.0202,  0.0091, -0.0050],\n",
      "        [ 0.0215,  0.0176, -0.0395,  0.0487, -0.0363],\n",
      "        [ 0.0159, -0.0084, -0.0158, -0.0027, -0.0305],\n",
      "        [-0.0212,  0.0275, -0.0198, -0.0072,  0.0031],\n",
      "        [ 0.0130,  0.0600,  0.0100,  0.0036, -0.0096],\n",
      "        [-0.0109,  0.0200,  0.0062, -0.0167, -0.0139],\n",
      "        [ 0.0076,  0.0230, -0.0178,  0.0161, -0.0194],\n",
      "        [-0.0301,  0.0043,  0.0091, -0.0024,  0.0176]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0155,  0.0119, -0.0274, -0.0030,  0.0050],\n",
      "        [ 0.0112,  0.0030, -0.0257,  0.0052,  0.0115],\n",
      "        [-0.0261,  0.0049, -0.0136, -0.0251, -0.0149],\n",
      "        [-0.0029, -0.0338, -0.0238,  0.0221,  0.0159],\n",
      "        [ 0.0001,  0.0160,  0.0059, -0.0121, -0.0262],\n",
      "        [-0.0102, -0.0066, -0.0571,  0.0269, -0.0193],\n",
      "        [ 0.0340,  0.0242,  0.0126, -0.0294,  0.0225],\n",
      "        [ 0.0045,  0.0179,  0.0045,  0.0032, -0.0277]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0251, grad_fn=<MinBackward1>), tensor(0.9127, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13263505697250366\n",
      "@sample 627: tensor([[ 0.0070,  0.0444,  0.0324, -0.0301,  0.0174],\n",
      "        [-0.0009, -0.0234,  0.0136, -0.0168, -0.0138],\n",
      "        [ 0.0264, -0.0092,  0.0143,  0.0294, -0.0202],\n",
      "        [-0.0037, -0.0151,  0.0180, -0.0009, -0.0051],\n",
      "        [-0.0007,  0.0094, -0.0093,  0.0093, -0.0246],\n",
      "        [-0.0270,  0.0236,  0.0110,  0.0039,  0.0043],\n",
      "        [ 0.0217,  0.0065, -0.0057, -0.0089, -0.0171],\n",
      "        [-0.0502,  0.0872, -0.0027, -0.0172,  0.0032]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0074, -0.0148, -0.0336,  0.0401,  0.0188],\n",
      "        [-0.0341, -0.0221,  0.0044, -0.0506, -0.0209],\n",
      "        [-0.0129,  0.0364,  0.0304, -0.0527, -0.0264],\n",
      "        [-0.0087, -0.0213,  0.0181, -0.0102, -0.0445],\n",
      "        [ 0.0043, -0.0150,  0.0411, -0.0387, -0.0054],\n",
      "        [-0.0030,  0.0155, -0.0303,  0.0237, -0.0259],\n",
      "        [-0.0151,  0.0015,  0.0041,  0.0241,  0.0107],\n",
      "        [-0.0253,  0.0009, -0.0679, -0.0107,  0.0060]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0149, grad_fn=<MinBackward1>), tensor(0.8694, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12891247868537903\n",
      "@sample 628: tensor([[-0.0040, -0.0035, -0.0007, -0.0272,  0.0129],\n",
      "        [-0.0019, -0.0170, -0.0045,  0.0258, -0.0300],\n",
      "        [ 0.0156,  0.0409,  0.0095, -0.0069,  0.0075],\n",
      "        [-0.0034,  0.0377,  0.0528, -0.0276,  0.0206],\n",
      "        [-0.0082, -0.0298, -0.0086,  0.0339,  0.0314],\n",
      "        [ 0.0101,  0.0293, -0.0297, -0.0235, -0.0066],\n",
      "        [-0.0102, -0.0046, -0.0693, -0.0109, -0.0189],\n",
      "        [ 0.0330,  0.0065,  0.0304, -0.0247,  0.0295]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0306, -0.0273, -0.0487,  0.0587, -0.0078],\n",
      "        [ 0.0060, -0.0082,  0.0021,  0.0195,  0.0056],\n",
      "        [-0.0298,  0.0071, -0.0564,  0.0009,  0.0247],\n",
      "        [-0.0357, -0.0555, -0.0295,  0.0419, -0.0121],\n",
      "        [ 0.0009, -0.0148,  0.0090, -0.0220, -0.0468],\n",
      "        [-0.0498, -0.0343, -0.0203,  0.0439, -0.0413],\n",
      "        [-0.0309, -0.0596, -0.0459,  0.0575, -0.0141],\n",
      "        [-0.0112,  0.0532, -0.0344,  0.0187, -0.0221]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0166, grad_fn=<MinBackward1>), tensor(0.9260, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.15030574798583984\n",
      "@sample 629: tensor([[ 0.0033,  0.0153,  0.0074, -0.0005, -0.0060],\n",
      "        [ 0.0161, -0.0193,  0.0068,  0.0210, -0.0385],\n",
      "        [ 0.0327,  0.0133, -0.0327,  0.0238,  0.0025],\n",
      "        [-0.0057,  0.0305,  0.0381, -0.0119,  0.0058],\n",
      "        [ 0.0242,  0.0126, -0.0043, -0.0324, -0.0033],\n",
      "        [ 0.0544,  0.0188,  0.0281, -0.0001, -0.0411],\n",
      "        [ 0.0206, -0.0217,  0.0096,  0.0239,  0.0041],\n",
      "        [ 0.0154,  0.0562,  0.0148, -0.0251,  0.0136]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0033,  0.0071,  0.0063,  0.0153, -0.0265],\n",
      "        [ 0.0112,  0.0067,  0.0019,  0.0160,  0.0663],\n",
      "        [ 0.0330,  0.0260,  0.0229, -0.0052,  0.0067],\n",
      "        [-0.0301, -0.0180, -0.0557, -0.0079, -0.0164],\n",
      "        [-0.0275, -0.0353,  0.0029, -0.0239, -0.0288],\n",
      "        [-0.0432,  0.0314, -0.0102,  0.0196, -0.0039],\n",
      "        [ 0.0169,  0.0135, -0.0180, -0.0016,  0.0034],\n",
      "        [ 0.0063, -0.0262, -0.0442,  0.0318,  0.0248]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0174, grad_fn=<MinBackward1>), tensor(0.8826, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.126870796084404\n",
      "@sample 630: tensor([[ 0.0258, -0.0157,  0.0008,  0.0277, -0.0161],\n",
      "        [ 0.0318, -0.0320, -0.0272,  0.0614, -0.0510],\n",
      "        [ 0.0124,  0.0346,  0.0222, -0.0153, -0.0176],\n",
      "        [ 0.0186,  0.0078,  0.0010, -0.0157,  0.0008],\n",
      "        [ 0.0397,  0.0231,  0.0078,  0.0206,  0.0081],\n",
      "        [-0.0036,  0.0393,  0.0277, -0.0344,  0.0147],\n",
      "        [ 0.0116,  0.0192,  0.0145, -0.0077,  0.0124],\n",
      "        [ 0.0291, -0.0211, -0.0163,  0.0285,  0.0161]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0480,  0.0420,  0.0051, -0.0161, -0.0134],\n",
      "        [ 0.0041,  0.0560, -0.0132, -0.0165, -0.0043],\n",
      "        [-0.0482, -0.0157, -0.0329, -0.0115, -0.0419],\n",
      "        [ 0.0002, -0.0161, -0.0551,  0.0129,  0.0010],\n",
      "        [-0.0130, -0.0008, -0.0332,  0.0320,  0.0379],\n",
      "        [-0.0388,  0.0112,  0.0015,  0.0256, -0.0157],\n",
      "        [-0.0112, -0.0165, -0.0624,  0.0220, -0.0489],\n",
      "        [-0.0042,  0.0129,  0.0264, -0.0263, -0.0296]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0175, grad_fn=<MinBackward1>), tensor(0.8864, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.14110559225082397\n",
      "@sample 631: tensor([[ 0.0226,  0.0182,  0.0204, -0.0151,  0.0643],\n",
      "        [-0.0179, -0.0020, -0.0495,  0.0084,  0.0120],\n",
      "        [ 0.0108,  0.0094, -0.0061,  0.0234, -0.0084],\n",
      "        [ 0.0298,  0.0157,  0.0024, -0.0258,  0.0700],\n",
      "        [-0.0064,  0.0048,  0.0154, -0.0006,  0.0298],\n",
      "        [ 0.0137, -0.0159, -0.0055,  0.0306, -0.0130],\n",
      "        [-0.0066, -0.0314, -0.0267,  0.0251, -0.0004],\n",
      "        [ 0.0411, -0.0336, -0.0156,  0.0273, -0.0063]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0400, -0.0101, -0.0067, -0.0020, -0.0617],\n",
      "        [ 0.0127,  0.0068, -0.0099,  0.0027,  0.0299],\n",
      "        [ 0.0161, -0.0035, -0.0848,  0.0285, -0.0221],\n",
      "        [-0.0318, -0.0204, -0.0374, -0.0160,  0.0059],\n",
      "        [ 0.0003,  0.0326,  0.0063, -0.0040,  0.0168],\n",
      "        [ 0.0283,  0.0328,  0.0017, -0.0246,  0.0060],\n",
      "        [ 0.0049, -0.0273, -0.0214, -0.0156, -0.0329],\n",
      "        [ 0.0271, -0.0157, -0.0106,  0.0263,  0.0086]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0173, grad_fn=<MinBackward1>), tensor(0.8796, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1326516568660736\n",
      "@sample 632: tensor([[-7.5373e-03, -6.3671e-02,  1.6250e-03,  5.8906e-02,  1.8597e-04],\n",
      "        [-9.4933e-03, -4.5520e-02,  1.5903e-02,  2.6520e-02,  1.3901e-02],\n",
      "        [ 2.1782e-02, -8.4966e-03,  7.4511e-03,  9.3648e-03, -9.1322e-04],\n",
      "        [ 1.1074e-02, -7.1591e-02, -8.6382e-03,  3.4929e-02,  1.6457e-02],\n",
      "        [ 3.6567e-02, -1.0771e-02,  5.5751e-03,  2.7714e-02, -5.7248e-03],\n",
      "        [ 6.8271e-03, -2.5517e-02, -2.1517e-05,  9.5646e-03, -4.9351e-03],\n",
      "        [ 2.1519e-03, -7.2721e-03,  7.7076e-03,  9.5247e-04,  2.7241e-02],\n",
      "        [-1.3889e-02,  4.6259e-03,  1.5158e-02,  6.5180e-03, -1.0071e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 6.0785e-02,  3.2964e-02,  8.1476e-02, -6.7248e-02, -5.2362e-02],\n",
      "        [ 1.8479e-02, -5.1861e-03,  3.2893e-02, -1.4625e-02, -1.9597e-02],\n",
      "        [ 3.0707e-02, -1.5022e-02, -7.8829e-03,  1.4830e-02,  2.8777e-02],\n",
      "        [ 2.6395e-02, -1.5265e-02,  3.8009e-02, -5.1312e-03,  2.8472e-02],\n",
      "        [ 1.3723e-02, -1.7162e-02, -4.8303e-02,  7.1577e-02,  2.1781e-02],\n",
      "        [-7.5843e-03, -1.4165e-02, -2.1589e-02,  8.4950e-03, -3.4310e-02],\n",
      "        [-3.0107e-02, -2.0774e-02, -3.5399e-02,  9.6111e-03, -9.5896e-03],\n",
      "        [ 2.8480e-03,  3.7158e-02, -3.6666e-02,  4.3213e-07, -1.4672e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0216, grad_fn=<MinBackward1>), tensor(0.8824, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1342582106590271\n",
      "@sample 633: tensor([[-1.0121e-02, -4.4704e-02, -1.8745e-02,  1.3351e-02,  1.2663e-02],\n",
      "        [-8.6051e-03, -4.0067e-02,  9.3789e-03,  3.2921e-02,  8.0412e-03],\n",
      "        [-2.8140e-05, -2.1352e-02,  1.7809e-02,  2.6741e-02,  2.2066e-03],\n",
      "        [ 6.2819e-03,  3.1047e-03,  2.3708e-02, -3.2684e-02,  8.8133e-03],\n",
      "        [-1.7359e-03,  3.3432e-02,  7.8543e-03, -4.5633e-03, -6.6888e-03],\n",
      "        [-2.7992e-02, -1.9895e-02,  2.4500e-02,  1.7409e-02, -6.1072e-03],\n",
      "        [-8.1237e-03, -1.4300e-02,  4.0735e-02,  2.9903e-02,  7.0677e-03],\n",
      "        [ 6.2823e-03, -8.8673e-03, -1.2407e-02,  1.2547e-02,  1.0165e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0084, -0.0302,  0.0192, -0.0076, -0.0119],\n",
      "        [ 0.0246,  0.0037,  0.0233, -0.0227, -0.0079],\n",
      "        [ 0.0262,  0.0392,  0.0062, -0.0168, -0.0022],\n",
      "        [-0.0211,  0.0023, -0.0402,  0.0122, -0.0380],\n",
      "        [ 0.0040,  0.0302, -0.0651,  0.0533,  0.0264],\n",
      "        [ 0.0005,  0.0043,  0.0081, -0.0252,  0.0218],\n",
      "        [ 0.0430,  0.0037, -0.0387,  0.0106,  0.0108],\n",
      "        [ 0.0119,  0.0170, -0.0386, -0.0188, -0.0122]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0219, grad_fn=<MinBackward1>), tensor(0.9393, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12849393486976624\n",
      "@sample 634: tensor([[ 0.0045,  0.0341,  0.0321, -0.0100, -0.0043],\n",
      "        [ 0.0203,  0.0288,  0.0169, -0.0219,  0.0058],\n",
      "        [-0.0113, -0.0646, -0.0399,  0.0397,  0.0151],\n",
      "        [-0.0117, -0.0596,  0.0069, -0.0063,  0.0474],\n",
      "        [ 0.0067,  0.0117, -0.0133, -0.0266,  0.0214],\n",
      "        [ 0.0435, -0.0025,  0.0127,  0.0185,  0.0329],\n",
      "        [-0.0056, -0.0065, -0.0022, -0.0056, -0.0195],\n",
      "        [-0.0013, -0.0789, -0.0194,  0.0196,  0.0257]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-5.5529e-02, -3.1242e-02, -5.7767e-02, -9.2500e-03, -3.0931e-04],\n",
      "        [-7.3458e-03, -1.7355e-02, -5.6086e-02,  3.4038e-03,  1.3776e-05],\n",
      "        [ 3.0512e-02, -1.3401e-02,  8.6188e-02, -1.0603e-02, -9.6481e-05],\n",
      "        [-3.3835e-02, -8.9917e-03, -9.5081e-03, -2.9967e-02, -5.1422e-02],\n",
      "        [-3.3223e-02, -2.6223e-02, -5.4695e-02, -8.2464e-03,  9.3913e-04],\n",
      "        [-2.3363e-02,  4.9799e-03, -1.0116e-01,  2.9525e-02, -2.1002e-02],\n",
      "        [-2.0551e-02, -1.3105e-02, -1.2690e-02, -1.9415e-02,  1.8927e-02],\n",
      "        [ 1.9396e-02,  1.8747e-02, -2.8157e-02, -3.0751e-02, -2.9611e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0190, grad_fn=<MinBackward1>), tensor(0.9163, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.14047196507453918\n",
      "@sample 635: tensor([[ 0.0068, -0.0492, -0.0297,  0.0352,  0.0106],\n",
      "        [ 0.0066,  0.0127,  0.0167, -0.0311,  0.0257],\n",
      "        [-0.0083,  0.0246, -0.0096, -0.0407,  0.0026],\n",
      "        [ 0.0210, -0.0092, -0.0067,  0.0154, -0.0075],\n",
      "        [-0.0051, -0.0316,  0.0186, -0.0051, -0.0161],\n",
      "        [ 0.0153, -0.0119, -0.0225,  0.0096,  0.0031],\n",
      "        [ 0.0342, -0.0158,  0.0062,  0.0292, -0.0244],\n",
      "        [ 0.0112, -0.0372, -0.0620,  0.0009,  0.0405]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0201, -0.0010, -0.0048,  0.0047,  0.0076],\n",
      "        [-0.0303, -0.0381, -0.0507,  0.0319, -0.0035],\n",
      "        [-0.0242, -0.0100, -0.0655,  0.0023, -0.0070],\n",
      "        [ 0.0329,  0.0127, -0.0267, -0.0163, -0.0066],\n",
      "        [-0.0303, -0.0082,  0.0105, -0.0146, -0.0550],\n",
      "        [-0.0171, -0.0228, -0.0091,  0.0166, -0.0138],\n",
      "        [ 0.0011,  0.0288,  0.0053,  0.0038,  0.0237],\n",
      "        [ 0.0103,  0.0022, -0.0645,  0.0434,  0.0115]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0155, grad_fn=<MinBackward1>), tensor(0.8854, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1272340714931488\n",
      "@sample 636: tensor([[ 0.0205, -0.0119,  0.0079,  0.0085,  0.0212],\n",
      "        [-0.0048,  0.0121, -0.0082, -0.0409, -0.0002],\n",
      "        [ 0.0078,  0.0147, -0.0234, -0.0126, -0.0001],\n",
      "        [-0.0047, -0.0541,  0.0061,  0.0030,  0.0272],\n",
      "        [ 0.0152,  0.0383,  0.0067, -0.0280,  0.0216],\n",
      "        [ 0.0198, -0.0061, -0.0050,  0.0213, -0.0283],\n",
      "        [ 0.0003,  0.0048,  0.0160, -0.0056,  0.0206],\n",
      "        [-0.0406, -0.0099,  0.0336, -0.0389,  0.0332]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0063,  0.0300, -0.0413,  0.0061, -0.0055],\n",
      "        [-0.0235, -0.0174, -0.0499, -0.0087,  0.0143],\n",
      "        [-0.0118, -0.0467, -0.0234,  0.0202,  0.0235],\n",
      "        [ 0.0101,  0.0291,  0.0264, -0.0128,  0.0213],\n",
      "        [-0.0242, -0.0210, -0.0601,  0.0150,  0.0158],\n",
      "        [ 0.0108, -0.0105,  0.0082,  0.0144,  0.0090],\n",
      "        [ 0.0014, -0.0039, -0.0055, -0.0076, -0.0067],\n",
      "        [-0.0467, -0.0382,  0.0087, -0.0269, -0.0239]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0212, grad_fn=<MinBackward1>), tensor(0.9021, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12997139990329742\n",
      "@sample 637: tensor([[ 0.0149, -0.0048,  0.0073, -0.0299,  0.0293],\n",
      "        [-0.0213, -0.0131,  0.0008, -0.0197,  0.0018],\n",
      "        [-0.0391,  0.0010,  0.0019,  0.0296,  0.0054],\n",
      "        [ 0.0030,  0.0409,  0.0093, -0.0526,  0.0287],\n",
      "        [-0.0150, -0.0071,  0.0214,  0.0071,  0.0115],\n",
      "        [-0.0088,  0.1113,  0.0071, -0.0308,  0.0244],\n",
      "        [-0.0103,  0.0176, -0.0222, -0.0072, -0.0050],\n",
      "        [-0.0075,  0.0177,  0.0207, -0.0377, -0.0040]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0106,  0.0293, -0.0731,  0.0105,  0.0230],\n",
      "        [-0.0073, -0.0018,  0.0475, -0.0118, -0.0047],\n",
      "        [ 0.0256,  0.0095,  0.0599, -0.0378, -0.0417],\n",
      "        [-0.0192,  0.0038, -0.0471,  0.0157,  0.0225],\n",
      "        [ 0.0055,  0.0003, -0.0037,  0.0286,  0.0278],\n",
      "        [ 0.0121, -0.0440, -0.0896,  0.0337, -0.0113],\n",
      "        [ 0.0080,  0.0084, -0.0316,  0.0012,  0.0191],\n",
      "        [-0.0021,  0.0020,  0.0287,  0.0359, -0.0196]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0260, grad_fn=<MinBackward1>), tensor(0.9248, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1431327760219574\n",
      "@sample 638: tensor([[ 1.2795e-03,  9.5778e-03,  2.7185e-02, -1.2390e-02, -2.5512e-02],\n",
      "        [-1.1546e-02, -3.7479e-02,  7.2240e-03, -3.3225e-04,  8.3686e-03],\n",
      "        [-3.8580e-02, -1.6305e-02,  4.5348e-03,  5.4373e-03, -1.2746e-02],\n",
      "        [-1.7353e-02, -2.0445e-02, -4.8484e-02,  2.7152e-02, -1.4622e-02],\n",
      "        [ 9.2016e-03, -1.1407e-02,  2.1163e-02,  4.0035e-03, -9.3380e-03],\n",
      "        [-1.1202e-02, -4.5961e-02, -2.2891e-02, -9.3632e-03,  7.1760e-03],\n",
      "        [-1.1870e-02, -2.6010e-02, -3.1058e-02, -5.1780e-03,  5.9798e-05],\n",
      "        [ 7.6680e-03,  1.9968e-02, -1.7705e-02, -2.1305e-02,  3.9127e-04]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0346, -0.0092,  0.0561, -0.0074,  0.0038],\n",
      "        [ 0.0393, -0.0012,  0.0389, -0.0263,  0.0060],\n",
      "        [ 0.0044, -0.0372,  0.0814, -0.0273, -0.0283],\n",
      "        [ 0.0456, -0.0155,  0.0382, -0.0042,  0.0162],\n",
      "        [-0.0020,  0.0134, -0.0446,  0.0073, -0.0266],\n",
      "        [-0.0214, -0.0188,  0.0075,  0.0197,  0.0045],\n",
      "        [ 0.0151, -0.0088,  0.0680, -0.0346, -0.0293],\n",
      "        [-0.0049,  0.0395, -0.0299,  0.0250,  0.0395]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0150, grad_fn=<MinBackward1>), tensor(0.8897, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13292431831359863\n",
      "@sample 639: tensor([[ 0.0021,  0.0141,  0.0078, -0.0891,  0.0268],\n",
      "        [ 0.0002, -0.0428, -0.0159,  0.0461,  0.0007],\n",
      "        [-0.0468,  0.0261, -0.0063, -0.0148, -0.0139],\n",
      "        [-0.0055,  0.0016,  0.0158, -0.0195,  0.0100],\n",
      "        [ 0.0347, -0.0163, -0.0308,  0.0148,  0.0156],\n",
      "        [ 0.0244,  0.0233,  0.0270, -0.0373,  0.0291],\n",
      "        [-0.0152, -0.0025, -0.0143, -0.0377,  0.0336],\n",
      "        [-0.0231,  0.0010,  0.0067, -0.0260,  0.0110]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-4.8001e-02, -2.5922e-02, -4.7398e-02,  1.9229e-03, -3.3681e-02],\n",
      "        [ 4.8211e-02,  4.4478e-02,  7.9263e-02, -4.3403e-02, -1.8493e-02],\n",
      "        [ 4.0109e-03,  3.3389e-02,  1.7349e-02, -3.6174e-02, -2.8830e-05],\n",
      "        [ 1.0350e-02, -3.0001e-02, -2.1509e-02,  3.3450e-02,  4.1529e-02],\n",
      "        [ 3.8933e-02,  5.1533e-02,  5.3346e-06,  4.9393e-03,  5.1138e-03],\n",
      "        [ 5.7381e-03, -5.8973e-03, -5.3033e-02,  5.0148e-02,  2.4895e-02],\n",
      "        [-1.1628e-02, -2.1475e-02, -3.0011e-02, -2.1665e-03, -9.5708e-03],\n",
      "        [-1.8029e-02, -4.2313e-02, -1.4440e-02, -1.0590e-02,  1.9900e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0193, grad_fn=<MinBackward1>), tensor(0.8644, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.14396555721759796\n",
      "@sample 640: tensor([[-0.0020,  0.0138,  0.0409, -0.0287, -0.0095],\n",
      "        [ 0.0178,  0.0237,  0.0093,  0.0023, -0.0090],\n",
      "        [-0.0081,  0.0313,  0.0348, -0.0400,  0.0202],\n",
      "        [-0.0024,  0.0160, -0.0087, -0.0285,  0.0069],\n",
      "        [ 0.0191, -0.0190, -0.0446,  0.0252, -0.0103],\n",
      "        [ 0.0091, -0.0450, -0.0601,  0.0355, -0.0027],\n",
      "        [-0.0113,  0.0155,  0.0126, -0.0130,  0.0014],\n",
      "        [-0.0323,  0.0279, -0.0306,  0.0019, -0.0124]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0096,  0.0280,  0.0227,  0.0059,  0.0273],\n",
      "        [-0.0144,  0.0198,  0.0018, -0.0094, -0.0003],\n",
      "        [ 0.0363,  0.0157, -0.0847,  0.0174,  0.0323],\n",
      "        [-0.0323, -0.0395, -0.0527,  0.0117, -0.0403],\n",
      "        [ 0.0246,  0.0074, -0.0038, -0.0019, -0.0011],\n",
      "        [ 0.0418,  0.0085,  0.0233, -0.0104,  0.0036],\n",
      "        [-0.0124, -0.0008, -0.0192, -0.0067, -0.0182],\n",
      "        [ 0.0072, -0.0154,  0.0014,  0.0112,  0.0172]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0159, grad_fn=<MinBackward1>), tensor(0.9056, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.130232572555542\n",
      "@sample 641: tensor([[-0.0016,  0.0050,  0.0227, -0.0254,  0.0134],\n",
      "        [-0.0165, -0.0232, -0.0331, -0.0273,  0.0246],\n",
      "        [-0.0288,  0.0122, -0.0193, -0.0165,  0.0047],\n",
      "        [ 0.0031,  0.0222, -0.0053, -0.0277,  0.0067],\n",
      "        [ 0.0145,  0.0281, -0.0005, -0.0253,  0.0127],\n",
      "        [ 0.0144,  0.0010,  0.0032, -0.0035,  0.0079],\n",
      "        [-0.0261,  0.0104, -0.0061, -0.0277, -0.0056],\n",
      "        [-0.0258,  0.0734,  0.0173, -0.0391,  0.0155]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0069, -0.0492, -0.0268,  0.0184,  0.0186],\n",
      "        [ 0.0101, -0.0317,  0.0410, -0.0558,  0.0081],\n",
      "        [ 0.0180, -0.0168,  0.0262, -0.0284, -0.0060],\n",
      "        [-0.0114,  0.0148, -0.0383,  0.0108,  0.0185],\n",
      "        [-0.0109, -0.0115, -0.0685,  0.0127, -0.0156],\n",
      "        [ 0.0167,  0.0015,  0.0077, -0.0110, -0.0036],\n",
      "        [ 0.0068, -0.0177,  0.0007, -0.0057, -0.0322],\n",
      "        [-0.0199, -0.0049, -0.0996,  0.0155, -0.0283]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0217, grad_fn=<MinBackward1>), tensor(0.8786, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.131709024310112\n",
      "@sample 642: tensor([[-0.0362,  0.0024, -0.0005,  0.0010, -0.0196],\n",
      "        [-0.0266,  0.0074,  0.0251, -0.0346, -0.0151],\n",
      "        [ 0.0145,  0.0521,  0.0222, -0.0358,  0.0138],\n",
      "        [-0.0175,  0.0356,  0.0259, -0.0108, -0.0096],\n",
      "        [ 0.0200,  0.0008, -0.0158,  0.0102, -0.0049],\n",
      "        [-0.0232,  0.0042,  0.0150, -0.0195, -0.0016],\n",
      "        [ 0.0433,  0.0379, -0.0366, -0.0042, -0.0240],\n",
      "        [-0.0240,  0.0523,  0.0397, -0.0633,  0.0070]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0042, -0.0250,  0.0530, -0.0006,  0.0098],\n",
      "        [-0.0634, -0.0172, -0.0335, -0.0030,  0.0218],\n",
      "        [-0.0459, -0.0086, -0.0330,  0.0249, -0.0050],\n",
      "        [ 0.0119,  0.0084, -0.0096,  0.0200, -0.0190],\n",
      "        [-0.0009, -0.0037, -0.0258,  0.0163, -0.0035],\n",
      "        [-0.0147, -0.0125,  0.0315, -0.0171, -0.0140],\n",
      "        [ 0.0166,  0.0474, -0.0283, -0.0033,  0.0170],\n",
      "        [-0.0783, -0.0346, -0.0472,  0.0690,  0.0331]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0121, grad_fn=<MinBackward1>), tensor(0.8997, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13483062386512756\n",
      "@sample 643: tensor([[-0.0094,  0.0086,  0.0178, -0.0032, -0.0207],\n",
      "        [ 0.0017,  0.0445,  0.0155, -0.0245,  0.0025],\n",
      "        [-0.0044,  0.0447,  0.0275, -0.0420,  0.0215],\n",
      "        [ 0.0044,  0.0218,  0.0153, -0.0341,  0.0003],\n",
      "        [ 0.0443,  0.0336,  0.0309, -0.0455,  0.0611],\n",
      "        [ 0.0156, -0.0018, -0.0377,  0.0292, -0.0471],\n",
      "        [-0.0570,  0.0492,  0.0488, -0.0877,  0.0446],\n",
      "        [-0.0180, -0.0262,  0.0222, -0.0074,  0.0283]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0081,  0.0034,  0.0544, -0.0321, -0.0027],\n",
      "        [-0.0289, -0.0397, -0.0289,  0.0255,  0.0055],\n",
      "        [-0.0166, -0.0025, -0.0312,  0.0353, -0.0033],\n",
      "        [ 0.0381, -0.0242, -0.0284, -0.0020, -0.0166],\n",
      "        [-0.0402,  0.0034, -0.0587,  0.0436,  0.0044],\n",
      "        [ 0.0111, -0.0099,  0.0740, -0.0542,  0.0261],\n",
      "        [-0.0347, -0.0322, -0.0577,  0.0007, -0.0034],\n",
      "        [ 0.0170,  0.0030, -0.0081, -0.0108, -0.0079]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0173, grad_fn=<MinBackward1>), tensor(0.9219, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.14658223092556\n",
      "@sample 644: tensor([[ 0.0060, -0.0075, -0.0004,  0.0231, -0.0025],\n",
      "        [ 0.0101,  0.0024,  0.0445, -0.0021, -0.0488],\n",
      "        [ 0.0269,  0.0072,  0.0172,  0.0030,  0.0224],\n",
      "        [-0.0306,  0.0350,  0.0222,  0.0030, -0.0089],\n",
      "        [ 0.0047,  0.0169, -0.0032,  0.0116, -0.0097],\n",
      "        [-0.0125,  0.0307,  0.0322,  0.0069, -0.0174],\n",
      "        [ 0.0104,  0.0108,  0.0024,  0.0398, -0.0083],\n",
      "        [ 0.0250, -0.0302, -0.0038, -0.0003, -0.0215]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0054,  0.0351,  0.0253, -0.0288,  0.0062],\n",
      "        [-0.0531,  0.0088,  0.0031, -0.0065,  0.0083],\n",
      "        [ 0.0149,  0.0082, -0.0386, -0.0428, -0.0615],\n",
      "        [ 0.0215,  0.0120,  0.0042, -0.0082,  0.0304],\n",
      "        [-0.0041, -0.0018, -0.0166, -0.0217,  0.0369],\n",
      "        [ 0.0275,  0.0621,  0.0534, -0.0074, -0.0088],\n",
      "        [ 0.0074,  0.0306, -0.0695, -0.0147,  0.0022],\n",
      "        [-0.0250,  0.0108,  0.0437, -0.0220,  0.0397]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0195, grad_fn=<MinBackward1>), tensor(0.8917, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13321520388126373\n",
      "@sample 645: tensor([[-0.0119,  0.0127,  0.0415, -0.0318,  0.0398],\n",
      "        [ 0.0191,  0.0018,  0.0150, -0.0072, -0.0234],\n",
      "        [ 0.0027,  0.0186, -0.0130,  0.0326, -0.0237],\n",
      "        [ 0.0101, -0.0131,  0.0133,  0.0075,  0.0016],\n",
      "        [-0.0108,  0.0239, -0.0285, -0.0051, -0.0172],\n",
      "        [ 0.0058, -0.0037, -0.0166,  0.0301, -0.0043],\n",
      "        [ 0.0246, -0.0024, -0.0079, -0.0220, -0.0079],\n",
      "        [-0.0226,  0.0305,  0.0268, -0.0090,  0.0096]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0173, -0.0039, -0.0287,  0.0260,  0.0089],\n",
      "        [-0.0295,  0.0241, -0.0028,  0.0103,  0.0272],\n",
      "        [ 0.0036,  0.0217,  0.0268, -0.0071,  0.0056],\n",
      "        [-0.0086,  0.0092, -0.0113, -0.0012,  0.0017],\n",
      "        [-0.0051, -0.0023,  0.0169, -0.0073, -0.0043],\n",
      "        [ 0.0186,  0.0220,  0.0165, -0.0295, -0.0094],\n",
      "        [-0.0301, -0.0277, -0.0215, -0.0015,  0.0493],\n",
      "        [ 0.0210,  0.0131, -0.0038, -0.0006,  0.0356]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0106, grad_fn=<MinBackward1>), tensor(0.9179, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13527651131153107\n",
      "@sample 646: tensor([[ 0.0069, -0.0520, -0.0224,  0.0280,  0.0090],\n",
      "        [ 0.0137, -0.0274, -0.0010,  0.0117, -0.0291],\n",
      "        [-0.0455,  0.0583,  0.0304, -0.0455,  0.0341],\n",
      "        [ 0.0123, -0.0118, -0.0057,  0.0305, -0.0073],\n",
      "        [ 0.0045, -0.0200, -0.0154,  0.0040,  0.0203],\n",
      "        [-0.0239,  0.0229, -0.0028, -0.0066, -0.0252],\n",
      "        [ 0.0149,  0.0154, -0.0039, -0.0197,  0.0029],\n",
      "        [-0.0105, -0.0023, -0.0082, -0.0060, -0.0106]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 4.9364e-02,  7.1216e-03,  6.4891e-02, -1.5049e-02, -2.6976e-02],\n",
      "        [ 2.6864e-02,  9.1579e-03,  4.0561e-05, -1.8105e-02, -9.4446e-03],\n",
      "        [-5.1877e-03, -5.3288e-02, -1.8332e-02,  1.4482e-02, -7.1066e-03],\n",
      "        [ 3.4159e-02,  9.8477e-04,  3.4532e-02, -2.0740e-02,  5.5262e-02],\n",
      "        [ 1.4785e-02, -3.6535e-02, -1.8338e-02,  2.8549e-02,  1.5731e-02],\n",
      "        [ 3.5912e-03,  2.1845e-02,  3.3723e-02,  1.3418e-02,  8.3648e-03],\n",
      "        [-1.0816e-02, -1.8376e-02, -4.5226e-02,  2.8123e-02, -7.9889e-03],\n",
      "        [ 1.7009e-02, -6.8442e-03,  2.2542e-02, -1.8279e-02, -2.0063e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0185, grad_fn=<MinBackward1>), tensor(0.9647, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1289389431476593\n",
      "@sample 647: tensor([[-2.9315e-02,  6.6887e-03, -2.3778e-03,  1.9492e-02,  3.1478e-02],\n",
      "        [-2.0405e-02,  2.5012e-02, -2.7880e-03, -3.8666e-02,  3.2978e-02],\n",
      "        [-3.9371e-03,  8.6026e-02, -1.3021e-02, -4.7643e-02,  2.5595e-02],\n",
      "        [ 7.6607e-03, -9.9368e-03, -1.8710e-02,  1.6651e-02, -1.4579e-02],\n",
      "        [-7.2514e-03,  3.0612e-02, -2.1350e-02, -2.9045e-03,  2.5377e-02],\n",
      "        [ 2.5740e-03,  4.6328e-02,  1.8758e-02, -4.2586e-02,  3.2077e-02],\n",
      "        [ 4.4965e-02,  6.2111e-03, -1.8753e-05,  2.9411e-02, -5.5720e-02],\n",
      "        [ 1.4252e-03, -8.8853e-03,  2.7341e-02,  7.6022e-03, -9.2998e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 4.9893e-02,  5.5669e-02, -1.0156e-02,  1.8185e-02,  5.1096e-02],\n",
      "        [-1.5236e-02, -1.7063e-02, -2.8636e-02,  2.7027e-02,  7.5173e-02],\n",
      "        [-3.9765e-02, -2.3105e-02, -4.3289e-02,  4.3752e-02, -6.2937e-03],\n",
      "        [ 9.1731e-03,  3.1944e-02, -6.5669e-05, -1.3915e-02,  1.1010e-03],\n",
      "        [-7.3457e-03, -3.2495e-02,  3.3622e-02, -1.1706e-02, -1.3497e-02],\n",
      "        [-2.6254e-02, -4.5067e-02, -8.8872e-02,  8.5245e-02,  1.0991e-02],\n",
      "        [-2.9600e-02,  2.6572e-02,  4.0438e-02, -6.8506e-03, -4.4600e-02],\n",
      "        [ 6.6098e-03,  9.0375e-03,  3.7913e-02, -1.4737e-02,  3.8260e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0231, grad_fn=<MinBackward1>), tensor(0.9177, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.147901713848114\n",
      "@sample 648: tensor([[ 0.0210,  0.0278, -0.0111, -0.0173,  0.0111],\n",
      "        [-0.0103, -0.0291,  0.0264,  0.0229, -0.0051],\n",
      "        [ 0.0478, -0.0166,  0.0040,  0.0377, -0.0022],\n",
      "        [-0.0147, -0.0088,  0.0249,  0.0051,  0.0228],\n",
      "        [ 0.0047,  0.0066,  0.0028,  0.0242,  0.0054],\n",
      "        [ 0.0153,  0.0055, -0.0009, -0.0326,  0.0027],\n",
      "        [-0.0114, -0.0001, -0.0119,  0.0034, -0.0342],\n",
      "        [-0.0078, -0.0246,  0.0094,  0.0067, -0.0006]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0110, -0.0236, -0.0117,  0.0220, -0.0043],\n",
      "        [ 0.0278,  0.0175,  0.0040,  0.0248,  0.0261],\n",
      "        [-0.0139,  0.0450,  0.0118, -0.0424,  0.0203],\n",
      "        [-0.0037, -0.0233,  0.0556, -0.0211, -0.0375],\n",
      "        [ 0.0147,  0.0385, -0.0083,  0.0409,  0.0246],\n",
      "        [ 0.0033, -0.0116,  0.0344,  0.0428,  0.0172],\n",
      "        [ 0.0167, -0.0050,  0.0533, -0.0186, -0.0035],\n",
      "        [ 0.0324, -0.0331,  0.0543,  0.0170, -0.0180]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0195, grad_fn=<MinBackward1>), tensor(0.8937, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13139905035495758\n",
      "@sample 649: tensor([[ 0.0104, -0.0268,  0.0163, -0.0277,  0.0052],\n",
      "        [ 0.0190, -0.0119, -0.0228,  0.0168, -0.0205],\n",
      "        [-0.0381,  0.0565,  0.0198, -0.0293, -0.0125],\n",
      "        [-0.0133, -0.0172, -0.0416,  0.0229, -0.0231],\n",
      "        [-0.0112,  0.0036,  0.0015, -0.0075,  0.0177],\n",
      "        [ 0.0105, -0.0240, -0.0084,  0.0107, -0.0192],\n",
      "        [-0.0023,  0.0176, -0.0069, -0.0105,  0.0104],\n",
      "        [-0.0035,  0.0487, -0.0569, -0.0254, -0.0046]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0155, -0.0431,  0.0103,  0.0241,  0.0257],\n",
      "        [ 0.0024,  0.0201,  0.0209, -0.0112, -0.0139],\n",
      "        [-0.0272,  0.0123, -0.0848,  0.0692,  0.0401],\n",
      "        [ 0.0178, -0.0409,  0.0435, -0.0547, -0.0210],\n",
      "        [ 0.0114,  0.0096,  0.0103, -0.0147, -0.0178],\n",
      "        [-0.0167,  0.0346,  0.0061,  0.0328,  0.0032],\n",
      "        [-0.0164, -0.0291,  0.0143, -0.0093, -0.0171],\n",
      "        [-0.0142,  0.0082, -0.0450,  0.0099, -0.0088]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0132, grad_fn=<MinBackward1>), tensor(0.8840, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13406874239444733\n",
      "@sample 650: tensor([[ 0.0190,  0.0377, -0.0090, -0.0284,  0.0099],\n",
      "        [ 0.0001, -0.0060,  0.0032, -0.0060,  0.0292],\n",
      "        [ 0.0122,  0.0034, -0.0020, -0.0042, -0.0516],\n",
      "        [-0.0037, -0.0054,  0.0133,  0.0016,  0.0158],\n",
      "        [-0.0164, -0.0054,  0.0026,  0.0123,  0.0010],\n",
      "        [ 0.0030, -0.0413, -0.0207,  0.0104, -0.0169],\n",
      "        [ 0.0016,  0.0137,  0.0171,  0.0239,  0.0053],\n",
      "        [ 0.0124, -0.0104, -0.0001,  0.0274, -0.0360]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0261, -0.0417, -0.0788,  0.0608, -0.0140],\n",
      "        [ 0.0242, -0.0056, -0.0173,  0.0100,  0.0334],\n",
      "        [-0.0318, -0.0058, -0.0138,  0.0027,  0.0240],\n",
      "        [ 0.0249,  0.0015,  0.0287, -0.0449,  0.0128],\n",
      "        [-0.0122, -0.0138, -0.0390,  0.0448,  0.0267],\n",
      "        [ 0.0053, -0.0229,  0.0452, -0.0365, -0.0031],\n",
      "        [ 0.0035,  0.0395,  0.0012, -0.0021,  0.0067],\n",
      "        [ 0.0116,  0.0238, -0.0086, -0.0098, -0.0118]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0208, grad_fn=<MinBackward1>), tensor(0.8751, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12451447546482086\n",
      "@sample 651: tensor([[-0.0182,  0.0101, -0.0111,  0.0006,  0.0353],\n",
      "        [-0.0170, -0.0242, -0.0084,  0.0314,  0.0022],\n",
      "        [ 0.0112,  0.0071, -0.0017,  0.0357, -0.0446],\n",
      "        [-0.0403, -0.0044,  0.0200, -0.0102, -0.0289],\n",
      "        [ 0.0130, -0.0018,  0.0105, -0.0301,  0.0282],\n",
      "        [-0.0251, -0.0023, -0.0403,  0.0399, -0.0130],\n",
      "        [-0.0069,  0.0347,  0.0314, -0.0192,  0.0172],\n",
      "        [-0.0219, -0.0192, -0.0203,  0.0423, -0.0461]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0128, -0.0038, -0.0200, -0.0259, -0.0260],\n",
      "        [ 0.0324,  0.0160,  0.0205, -0.0235, -0.0040],\n",
      "        [ 0.0302,  0.0522,  0.0195, -0.0204,  0.0042],\n",
      "        [-0.0181, -0.0327,  0.0780, -0.0347, -0.0720],\n",
      "        [-0.0542, -0.0285, -0.0612,  0.0491,  0.0109],\n",
      "        [ 0.0379,  0.0549,  0.0104,  0.0026,  0.0137],\n",
      "        [-0.0240, -0.0188, -0.0417,  0.0442, -0.0002],\n",
      "        [ 0.0439,  0.0160,  0.0515, -0.0493, -0.0124]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0195, grad_fn=<MinBackward1>), tensor(0.8882, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13923689723014832\n",
      "@sample 652: tensor([[ 0.0075, -0.0159,  0.0054,  0.0424, -0.0161],\n",
      "        [ 0.0207,  0.0160,  0.0186, -0.0007, -0.0270],\n",
      "        [-0.0055,  0.0339,  0.0184, -0.0219,  0.0197],\n",
      "        [ 0.0004, -0.0007,  0.0087, -0.0282,  0.0070],\n",
      "        [ 0.0122, -0.0049,  0.0008, -0.0064, -0.0200],\n",
      "        [-0.0420,  0.0033, -0.0335,  0.0487, -0.0218],\n",
      "        [-0.0117,  0.0151,  0.0056,  0.0265, -0.0487],\n",
      "        [ 0.0170,  0.0169, -0.0444,  0.0110, -0.0035]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0250,  0.0485,  0.0260,  0.0241,  0.0226],\n",
      "        [-0.0270, -0.0024, -0.0460,  0.0195, -0.0024],\n",
      "        [-0.0171,  0.0354,  0.0119, -0.0037, -0.0293],\n",
      "        [-0.0010, -0.0029,  0.0076, -0.0141, -0.0261],\n",
      "        [ 0.0117, -0.0118,  0.0203,  0.0185,  0.0153],\n",
      "        [ 0.0014,  0.0035,  0.0566, -0.0467, -0.0329],\n",
      "        [ 0.0212,  0.0266,  0.0112, -0.0053, -0.0190],\n",
      "        [ 0.0164,  0.0224, -0.0037, -0.0255, -0.0043]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0133, grad_fn=<MinBackward1>), tensor(0.8999, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.14001184701919556\n",
      "@sample 653: tensor([[-0.0114,  0.0092,  0.0272, -0.0030,  0.0292],\n",
      "        [-0.0002, -0.0291, -0.0359,  0.0497,  0.0093],\n",
      "        [ 0.0276,  0.0116, -0.0432,  0.0232,  0.0114],\n",
      "        [-0.0173, -0.0237, -0.0092,  0.0314, -0.0175],\n",
      "        [-0.0274, -0.0104, -0.0148,  0.0123, -0.0175],\n",
      "        [ 0.0202,  0.0112, -0.0288, -0.0321,  0.0074],\n",
      "        [ 0.0352, -0.0236, -0.0227,  0.0415, -0.0133],\n",
      "        [-0.0295, -0.0047, -0.0167,  0.0105,  0.0076]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0263,  0.0017,  0.0344, -0.0166, -0.0051],\n",
      "        [ 0.0443,  0.0118,  0.0071, -0.0033, -0.0419],\n",
      "        [ 0.0159, -0.0255, -0.0491, -0.0256, -0.0034],\n",
      "        [ 0.0426,  0.0012,  0.0456, -0.0195,  0.0368],\n",
      "        [-0.0171,  0.0067, -0.0332, -0.0180, -0.0224],\n",
      "        [-0.0068,  0.0029,  0.0037,  0.0090,  0.0274],\n",
      "        [ 0.0215,  0.0101, -0.0185,  0.0221,  0.0325],\n",
      "        [ 0.0241, -0.0188,  0.0612, -0.0079, -0.0286]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0143, grad_fn=<MinBackward1>), tensor(0.9206, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1371421217918396\n",
      "@sample 654: tensor([[ 0.0050,  0.0215,  0.0005, -0.0215,  0.0194],\n",
      "        [-0.0103, -0.0117,  0.0239,  0.0049,  0.0029],\n",
      "        [-0.0286,  0.0031,  0.0217, -0.0143,  0.0115],\n",
      "        [-0.0030,  0.0054,  0.0185,  0.0001,  0.0050],\n",
      "        [ 0.0375, -0.0215, -0.0052,  0.0059, -0.0353],\n",
      "        [-0.0101, -0.0080, -0.0010,  0.0105, -0.0022],\n",
      "        [-0.0170, -0.0499,  0.0202, -0.0189,  0.0268],\n",
      "        [-0.0086, -0.0387, -0.0186,  0.0310, -0.0124]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0205, -0.0434, -0.0550,  0.0393,  0.0254],\n",
      "        [-0.0168,  0.0294, -0.0321,  0.0063,  0.0056],\n",
      "        [ 0.0088, -0.0339, -0.0226, -0.0173,  0.0002],\n",
      "        [-0.0083,  0.0091,  0.0263,  0.0018, -0.0125],\n",
      "        [-0.0335,  0.0234,  0.0348, -0.0536, -0.0185],\n",
      "        [-0.0083,  0.0102,  0.0040,  0.0017, -0.0191],\n",
      "        [ 0.0083, -0.0107,  0.0384,  0.0042, -0.0173],\n",
      "        [ 0.0061,  0.0013,  0.0303, -0.0080,  0.0185]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0165, grad_fn=<MinBackward1>), tensor(0.8754, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12667453289031982\n",
      "@sample 655: tensor([[ 0.0361, -0.0216,  0.0077,  0.0270, -0.0002],\n",
      "        [-0.0041, -0.0079,  0.0353,  0.0148,  0.0128],\n",
      "        [-0.0187,  0.0679,  0.0156, -0.0087,  0.0031],\n",
      "        [ 0.0136,  0.0544,  0.0400, -0.0812,  0.0251],\n",
      "        [ 0.0096,  0.0019, -0.0361, -0.0142,  0.0059],\n",
      "        [-0.0093,  0.0100,  0.0150, -0.0111, -0.0070],\n",
      "        [-0.0032,  0.0022, -0.0274,  0.0299, -0.0256],\n",
      "        [-0.0020, -0.0214,  0.0140,  0.0173,  0.0057]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0351,  0.0224, -0.0142,  0.0021, -0.0041],\n",
      "        [-0.0466,  0.0144, -0.0396,  0.0264,  0.0204],\n",
      "        [-0.0164,  0.0158, -0.0606, -0.0082, -0.0525],\n",
      "        [-0.0683, -0.0137, -0.0625,  0.0381, -0.0184],\n",
      "        [-0.0255,  0.0046, -0.0392,  0.0313,  0.0135],\n",
      "        [ 0.0114,  0.0149,  0.0287, -0.0288, -0.0234],\n",
      "        [ 0.0019,  0.0193,  0.0587, -0.0002,  0.0073],\n",
      "        [ 0.0323,  0.0177, -0.0114, -0.0177, -0.0080]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0234, grad_fn=<MinBackward1>), tensor(0.9261, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1266503781080246\n",
      "@sample 656: tensor([[-0.0034,  0.0509,  0.0215, -0.0538,  0.0173],\n",
      "        [-0.0135, -0.0100,  0.0110, -0.0174,  0.0207],\n",
      "        [ 0.0367, -0.0107,  0.0068, -0.0053, -0.0028],\n",
      "        [ 0.0164, -0.0412,  0.0360,  0.0012,  0.0169],\n",
      "        [ 0.0056, -0.0679, -0.0281,  0.0254, -0.0282],\n",
      "        [-0.0112, -0.0254, -0.0134,  0.0312, -0.0273],\n",
      "        [-0.0030,  0.0111,  0.0401, -0.0176,  0.0068],\n",
      "        [-0.0487,  0.0128, -0.0043,  0.0009,  0.0095]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0142, -0.0097, -0.0480,  0.0294, -0.0020],\n",
      "        [-0.0030, -0.0257,  0.0092, -0.0113, -0.0591],\n",
      "        [-0.0146, -0.0152, -0.0369,  0.0063,  0.0053],\n",
      "        [ 0.0161, -0.0129,  0.0040,  0.0301,  0.0296],\n",
      "        [-0.0036, -0.0138,  0.0387,  0.0120,  0.0006],\n",
      "        [ 0.0132,  0.0047,  0.0355,  0.0243,  0.0152],\n",
      "        [-0.0013,  0.0072, -0.0626,  0.0536, -0.0204],\n",
      "        [-0.0114, -0.0366, -0.0055, -0.0202, -0.0335]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0227, grad_fn=<MinBackward1>), tensor(0.9222, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13164907693862915\n",
      "@sample 657: tensor([[ 0.0358,  0.0281, -0.0026,  0.0091, -0.0121],\n",
      "        [-0.0067,  0.0047,  0.0134, -0.0404, -0.0009],\n",
      "        [ 0.0333,  0.0228,  0.0009, -0.0119,  0.0197],\n",
      "        [-0.0156, -0.0150, -0.0021, -0.0122, -0.0021],\n",
      "        [-0.0074,  0.0645, -0.0220, -0.0511,  0.0229],\n",
      "        [-0.0371, -0.0485, -0.0310,  0.0104,  0.0009],\n",
      "        [-0.0017, -0.0259, -0.0276, -0.0195,  0.0548],\n",
      "        [ 0.0068, -0.0351,  0.0061, -0.0106, -0.0139]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0011,  0.0653,  0.0474, -0.0299,  0.0167],\n",
      "        [ 0.0057, -0.0185,  0.0191,  0.0357,  0.0113],\n",
      "        [-0.0481, -0.0573, -0.0867,  0.0591, -0.0117],\n",
      "        [-0.0027, -0.0226,  0.1006, -0.0353, -0.0042],\n",
      "        [-0.0566, -0.0252, -0.1059,  0.0863,  0.0163],\n",
      "        [ 0.0056, -0.0463,  0.0834, -0.0161, -0.0445],\n",
      "        [ 0.0195, -0.0495,  0.0267, -0.0122, -0.0109],\n",
      "        [ 0.0082,  0.0012,  0.0639, -0.0060,  0.0073]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0161, grad_fn=<MinBackward1>), tensor(0.8902, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.14312663674354553\n",
      "@sample 658: tensor([[-0.0447,  0.0062,  0.0437, -0.0179,  0.0167],\n",
      "        [-0.0019, -0.0003, -0.0050, -0.0182, -0.0148],\n",
      "        [-0.0066, -0.0007,  0.0213, -0.0160,  0.0072],\n",
      "        [ 0.0152,  0.0039,  0.0341, -0.0138,  0.0149],\n",
      "        [-0.0200, -0.0542, -0.0226,  0.0361,  0.0141],\n",
      "        [-0.0278,  0.0481,  0.0069, -0.0642, -0.0026],\n",
      "        [-0.0111, -0.0346,  0.0090,  0.0139, -0.0113],\n",
      "        [ 0.0042,  0.0125,  0.0008, -0.0020,  0.0153]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0203, -0.0081, -0.0079, -0.0234,  0.0333],\n",
      "        [-0.0084,  0.0156, -0.0903,  0.0518,  0.0396],\n",
      "        [-0.0251,  0.0217,  0.0307, -0.0221, -0.0220],\n",
      "        [-0.0022,  0.0018, -0.0605,  0.0240,  0.0070],\n",
      "        [ 0.0014,  0.0035,  0.0309,  0.0001, -0.0269],\n",
      "        [-0.0115, -0.0233, -0.0302,  0.0017, -0.0202],\n",
      "        [ 0.0136,  0.0366,  0.0186, -0.0089, -0.0009],\n",
      "        [-0.0206, -0.0056,  0.0060, -0.0382, -0.0258]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0114, grad_fn=<MinBackward1>), tensor(0.9019, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12352444976568222\n",
      "@sample 659: tensor([[ 0.0177, -0.0042, -0.0119,  0.0298, -0.0157],\n",
      "        [-0.0139,  0.0214,  0.0038,  0.0091, -0.0190],\n",
      "        [-0.0239, -0.0288,  0.0050,  0.0031,  0.0141],\n",
      "        [-0.0176, -0.0014, -0.0319, -0.0177, -0.0036],\n",
      "        [-0.0508, -0.0399,  0.0248, -0.0064,  0.0158],\n",
      "        [-0.0217, -0.0215,  0.0072,  0.0194,  0.0088],\n",
      "        [ 0.0160, -0.0164,  0.0317, -0.0200, -0.0023],\n",
      "        [-0.0198, -0.0124,  0.0266,  0.0113, -0.0127]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 3.8522e-02,  1.1798e-02,  2.0822e-02, -5.9438e-02,  1.5813e-02],\n",
      "        [-8.3677e-03,  2.5242e-02, -1.7729e-02, -2.9277e-02, -2.5027e-02],\n",
      "        [-3.6241e-03,  2.7354e-02,  1.2198e-02, -1.3228e-02,  2.1975e-03],\n",
      "        [-2.4817e-02, -2.2674e-02,  8.1096e-02, -5.2845e-02, -5.5227e-02],\n",
      "        [ 5.3170e-03,  4.0736e-02,  5.7464e-02, -1.3517e-02,  3.1661e-03],\n",
      "        [ 2.3070e-03, -1.0677e-02, -7.3479e-03,  9.0978e-03, -2.7532e-02],\n",
      "        [-9.4893e-03,  2.2470e-02,  3.4823e-02,  4.4910e-03,  4.4238e-05],\n",
      "        [ 4.2987e-03, -6.8969e-03,  2.3018e-02,  2.9320e-02, -1.7081e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0078, grad_fn=<MinBackward1>), tensor(0.8843, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13715341687202454\n",
      "@sample 660: tensor([[-0.0094, -0.0197, -0.0107,  0.0134, -0.0053],\n",
      "        [-0.0058,  0.0168,  0.0320, -0.0173,  0.0182],\n",
      "        [ 0.0020, -0.0204,  0.0103, -0.0042,  0.0013],\n",
      "        [-0.0106,  0.0304,  0.0055, -0.0227, -0.0110],\n",
      "        [-0.0030,  0.0168,  0.0281, -0.0660,  0.0676],\n",
      "        [ 0.0163,  0.0062,  0.0014,  0.0171,  0.0045],\n",
      "        [-0.0031, -0.0148, -0.0242,  0.0201,  0.0023],\n",
      "        [-0.0113,  0.0226,  0.0234, -0.0150,  0.0248]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0096, -0.0277,  0.0248,  0.0121,  0.0485],\n",
      "        [-0.0164, -0.0164, -0.0368,  0.0171, -0.0033],\n",
      "        [-0.0219, -0.0371, -0.0197,  0.0122,  0.0195],\n",
      "        [-0.0142,  0.0179, -0.0342,  0.0052, -0.0025],\n",
      "        [-0.0634, -0.0034, -0.0473,  0.0229, -0.0258],\n",
      "        [ 0.0198,  0.0195,  0.0119, -0.0120,  0.0016],\n",
      "        [-0.0036, -0.0298,  0.0101, -0.0044, -0.0310],\n",
      "        [-0.0090, -0.0116, -0.0364,  0.0040, -0.0136]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0187, grad_fn=<MinBackward1>), tensor(0.9106, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1340550184249878\n",
      "@sample 661: tensor([[ 0.0030, -0.0208, -0.0208,  0.0104, -0.0028],\n",
      "        [ 0.0032, -0.0036,  0.0097,  0.0123, -0.0023],\n",
      "        [-0.0316,  0.0006,  0.0076, -0.0195,  0.0099],\n",
      "        [-0.0235, -0.0027,  0.0075, -0.0407, -0.0136],\n",
      "        [-0.0351,  0.0060,  0.0393,  0.0188, -0.0074],\n",
      "        [-0.0017,  0.0082, -0.0152, -0.0126, -0.0115],\n",
      "        [ 0.0171,  0.0072,  0.0226, -0.0223,  0.0184],\n",
      "        [-0.0281,  0.0028,  0.0158,  0.0406, -0.0193]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0229,  0.0299, -0.0205,  0.0273,  0.0319],\n",
      "        [-0.0034,  0.0346,  0.0182, -0.0109,  0.0174],\n",
      "        [-0.0120, -0.0118, -0.0234, -0.0048,  0.0193],\n",
      "        [-0.0519, -0.0194, -0.0271,  0.0149, -0.0112],\n",
      "        [ 0.0098,  0.0023,  0.0500, -0.0230, -0.0245],\n",
      "        [ 0.0353, -0.0333, -0.0039, -0.0332, -0.0294],\n",
      "        [-0.0126, -0.0132, -0.0260,  0.0179,  0.0234],\n",
      "        [-0.0013, -0.0167,  0.0063,  0.0046,  0.0301]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0218, grad_fn=<MinBackward1>), tensor(0.9265, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1339879035949707\n",
      "@sample 662: tensor([[-0.0053, -0.0097, -0.0036,  0.0148,  0.0091],\n",
      "        [-0.0122, -0.0203, -0.0150, -0.0203,  0.0044],\n",
      "        [ 0.0266, -0.0299, -0.0310,  0.0074, -0.0194],\n",
      "        [-0.0275, -0.0294, -0.0244, -0.0008, -0.0094],\n",
      "        [ 0.0201, -0.0099, -0.0281, -0.0303,  0.0358],\n",
      "        [-0.0124,  0.0253,  0.0404, -0.0038, -0.0128],\n",
      "        [-0.0079,  0.0241,  0.0312, -0.0121,  0.0354],\n",
      "        [ 0.0199,  0.0180,  0.0149, -0.0233,  0.0192]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0201,  0.0162, -0.0084,  0.0001,  0.0174],\n",
      "        [-0.0063, -0.0086, -0.0244,  0.0126,  0.0002],\n",
      "        [-0.0035, -0.0099, -0.0128,  0.0178, -0.0006],\n",
      "        [ 0.0132, -0.0415,  0.0086, -0.0553, -0.0349],\n",
      "        [-0.0111, -0.0105, -0.0488,  0.0358,  0.0109],\n",
      "        [-0.0163,  0.0189, -0.0387,  0.0587,  0.0110],\n",
      "        [-0.0235,  0.0057, -0.0457,  0.0438,  0.0015],\n",
      "        [-0.0036, -0.0032, -0.0363,  0.0566,  0.0195]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0179, grad_fn=<MinBackward1>), tensor(0.9135, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11816467344760895\n",
      "@sample 663: tensor([[-0.0183,  0.0065,  0.0020, -0.0251, -0.0130],\n",
      "        [ 0.0252, -0.0189, -0.0047,  0.0264, -0.0112],\n",
      "        [-0.0196, -0.0063, -0.0138, -0.0092, -0.0100],\n",
      "        [-0.0110, -0.0193, -0.0058,  0.0097,  0.0277],\n",
      "        [-0.0012,  0.0055,  0.0283, -0.0329, -0.0063],\n",
      "        [-0.0160,  0.0210,  0.0010,  0.0126, -0.0083],\n",
      "        [-0.0327,  0.0330,  0.0045, -0.0322,  0.0047],\n",
      "        [-0.0114, -0.0579,  0.0032,  0.0143,  0.0023]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0314, -0.0276,  0.0099,  0.0009,  0.0420],\n",
      "        [-0.0073,  0.0316, -0.0168, -0.0232, -0.0149],\n",
      "        [-0.0251, -0.0265, -0.0223, -0.0244,  0.0019],\n",
      "        [ 0.0109,  0.0097,  0.0091, -0.0032,  0.0089],\n",
      "        [-0.0331, -0.0036, -0.0215,  0.0071,  0.0430],\n",
      "        [-0.0014,  0.0459, -0.0315,  0.0091,  0.0379],\n",
      "        [-0.0454, -0.0143, -0.0299, -0.0063,  0.0169],\n",
      "        [-0.0051,  0.0169, -0.0217,  0.0382,  0.0109]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0110, grad_fn=<MinBackward1>), tensor(0.9336, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1370871365070343\n",
      "@sample 664: tensor([[-0.0067,  0.0193,  0.0335, -0.0068,  0.0252],\n",
      "        [ 0.0015,  0.0480, -0.0215, -0.0336, -0.0123],\n",
      "        [-0.0141,  0.0002, -0.0052,  0.0181,  0.0022],\n",
      "        [ 0.0180,  0.0431, -0.0162,  0.0069, -0.0038],\n",
      "        [ 0.0172, -0.0086,  0.0046, -0.0344,  0.0314],\n",
      "        [ 0.0171,  0.0291, -0.0283, -0.0462,  0.0323],\n",
      "        [-0.0073,  0.0258, -0.0177, -0.0016, -0.0171],\n",
      "        [-0.0167, -0.0256, -0.0250,  0.0159, -0.0080]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0293, -0.0120, -0.0386, -0.0382, -0.0363],\n",
      "        [-0.0235, -0.0497, -0.0636,  0.0036, -0.0133],\n",
      "        [ 0.0228,  0.0321, -0.0036,  0.0215, -0.0122],\n",
      "        [-0.0076,  0.0310, -0.0368, -0.0063,  0.0199],\n",
      "        [-0.0403, -0.0225, -0.1120,  0.0601, -0.0085],\n",
      "        [-0.0185, -0.0348, -0.0767,  0.0426, -0.0106],\n",
      "        [-0.0057, -0.0108, -0.0239,  0.0030, -0.0097],\n",
      "        [ 0.0220,  0.0035,  0.0365, -0.0271,  0.0011]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0206, grad_fn=<MinBackward1>), tensor(0.8983, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13113999366760254\n",
      "@sample 665: tensor([[ 2.1234e-02,  2.4433e-02, -2.9821e-03, -4.6989e-03,  3.3865e-02],\n",
      "        [ 6.2771e-03, -1.7554e-05,  7.2576e-03,  1.0300e-02, -1.4304e-02],\n",
      "        [ 4.6955e-03,  8.3523e-03,  1.9905e-02,  1.2376e-02, -1.1394e-02],\n",
      "        [-2.7979e-03,  1.0059e-02,  2.6713e-02, -2.4985e-02,  3.0770e-02],\n",
      "        [ 3.1722e-02,  4.1976e-02, -3.6364e-02, -3.4294e-02,  1.1890e-02],\n",
      "        [ 9.9813e-04,  2.4402e-02,  1.8537e-02, -2.3171e-02,  1.1322e-02],\n",
      "        [-1.8580e-02,  3.8648e-02,  4.8523e-02, -3.7215e-02,  1.6443e-02],\n",
      "        [ 4.2597e-02,  4.6953e-03, -2.0666e-02,  5.0211e-02, -3.9385e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0446, -0.0465, -0.0556,  0.0078,  0.0082],\n",
      "        [ 0.0126,  0.0196,  0.0165, -0.0170,  0.0094],\n",
      "        [-0.0097,  0.0261, -0.0380,  0.0230,  0.0525],\n",
      "        [-0.0003,  0.0202,  0.0166,  0.0012, -0.0177],\n",
      "        [-0.0601, -0.0367, -0.1503,  0.0857, -0.0011],\n",
      "        [-0.0104, -0.0449, -0.0375,  0.0391,  0.0291],\n",
      "        [-0.0493, -0.0366,  0.0200,  0.0102, -0.0695],\n",
      "        [ 0.0423,  0.0727,  0.0142,  0.0343, -0.0258]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0177, grad_fn=<MinBackward1>), tensor(0.9100, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13803896307945251\n",
      "@sample 666: tensor([[-0.0329, -0.0127,  0.0061,  0.0263, -0.0148],\n",
      "        [ 0.0092,  0.0373, -0.0427,  0.0294,  0.0120],\n",
      "        [-0.0128,  0.0092, -0.0423, -0.0152, -0.0139],\n",
      "        [-0.0018,  0.0040, -0.0157, -0.0042,  0.0129],\n",
      "        [ 0.0048, -0.0060,  0.0091,  0.0141, -0.0143],\n",
      "        [ 0.0129,  0.0405, -0.0111, -0.0419,  0.0254],\n",
      "        [-0.0096, -0.0017,  0.0011, -0.0071,  0.0450],\n",
      "        [ 0.0309,  0.0123, -0.0007, -0.0020, -0.0103]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0388,  0.0029,  0.0426, -0.0425, -0.0147],\n",
      "        [ 0.0610,  0.0541, -0.0039, -0.0009,  0.0168],\n",
      "        [ 0.0046,  0.0186,  0.0463, -0.0348,  0.0184],\n",
      "        [ 0.0395, -0.0015,  0.0152, -0.0146, -0.0307],\n",
      "        [ 0.0159,  0.0079,  0.0402, -0.0400, -0.0172],\n",
      "        [ 0.0034, -0.0079, -0.1282,  0.0506, -0.0096],\n",
      "        [-0.0030,  0.0136, -0.0391, -0.0157,  0.0081],\n",
      "        [-0.0104,  0.0027, -0.0230,  0.0367,  0.0147]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0169, grad_fn=<MinBackward1>), tensor(0.8755, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12942349910736084\n",
      "@sample 667: tensor([[-1.7575e-02,  1.4944e-02,  1.7972e-02,  3.9534e-02,  1.0477e-02],\n",
      "        [ 3.3968e-02,  1.4962e-02, -1.0784e-03,  1.0038e-02, -4.2709e-02],\n",
      "        [-3.2333e-05, -4.6960e-03, -4.0332e-03,  1.8449e-02,  3.2227e-03],\n",
      "        [-8.5731e-03, -1.0756e-02,  2.6941e-02,  3.6966e-03, -1.8557e-02],\n",
      "        [ 9.5305e-03, -3.2021e-02, -9.3991e-03,  2.7663e-02,  3.5220e-03],\n",
      "        [ 6.5077e-03,  2.6487e-02, -1.7916e-02,  5.8300e-03, -1.1716e-02],\n",
      "        [-2.6738e-03, -1.5513e-03, -1.0425e-02, -7.3226e-03,  1.1244e-03],\n",
      "        [ 5.2329e-02,  1.3689e-02,  8.5464e-03, -1.6403e-02, -7.5178e-04]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0059,  0.0234, -0.0293, -0.0282, -0.0254],\n",
      "        [-0.0384,  0.0246, -0.0172,  0.0332,  0.0741],\n",
      "        [ 0.0248,  0.0241,  0.0480, -0.0252,  0.0020],\n",
      "        [ 0.0089, -0.0089,  0.0220, -0.0333, -0.0051],\n",
      "        [ 0.0230,  0.0133,  0.0594, -0.0259, -0.0114],\n",
      "        [ 0.0070,  0.0024, -0.0558,  0.0150, -0.0152],\n",
      "        [-0.0184,  0.0065, -0.0233,  0.0033, -0.0134],\n",
      "        [-0.0214, -0.0101, -0.0625,  0.0422, -0.0138]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0197, grad_fn=<MinBackward1>), tensor(0.8833, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12499173730611801\n",
      "@sample 668: tensor([[ 0.0033, -0.0047, -0.0180,  0.0111, -0.0068],\n",
      "        [ 0.0174,  0.0289,  0.0047, -0.0032, -0.0028],\n",
      "        [-0.0032, -0.0218, -0.0068, -0.0069, -0.0156],\n",
      "        [ 0.0161, -0.0360, -0.0183,  0.0532, -0.0153],\n",
      "        [ 0.0150, -0.0278, -0.0270,  0.0063,  0.0080],\n",
      "        [ 0.0349,  0.0006, -0.0334,  0.0138, -0.0319],\n",
      "        [ 0.0131, -0.0204, -0.0451,  0.0307, -0.0029],\n",
      "        [ 0.0386, -0.0089,  0.0135,  0.0019, -0.0386]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0118, -0.0088, -0.0242,  0.0146,  0.0144],\n",
      "        [-0.0320,  0.0188,  0.0075, -0.0066, -0.0320],\n",
      "        [-0.0548, -0.0261, -0.0031,  0.0115, -0.0216],\n",
      "        [ 0.0234,  0.0217,  0.0665, -0.0195,  0.0108],\n",
      "        [ 0.0102, -0.0131, -0.0289, -0.0008, -0.0028],\n",
      "        [ 0.0203,  0.0273, -0.0127, -0.0254, -0.0120],\n",
      "        [ 0.0386,  0.0012,  0.0352, -0.0061, -0.0517],\n",
      "        [-0.0455, -0.0102,  0.0099,  0.0280,  0.0220]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0166, grad_fn=<MinBackward1>), tensor(0.8990, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12511733174324036\n",
      "@sample 669: tensor([[ 0.0273,  0.0058,  0.0188,  0.0509, -0.0242],\n",
      "        [ 0.0005, -0.0309, -0.0114,  0.0222, -0.0198],\n",
      "        [ 0.0180,  0.0351, -0.0041,  0.0068,  0.0175],\n",
      "        [ 0.0109,  0.0031,  0.0007, -0.0056,  0.0135],\n",
      "        [ 0.0241,  0.0307, -0.0097,  0.0071, -0.0159],\n",
      "        [ 0.0097,  0.0175, -0.0058, -0.0089, -0.0223],\n",
      "        [ 0.0004,  0.0132,  0.0004, -0.0105,  0.0074],\n",
      "        [ 0.0069, -0.0072, -0.0147,  0.0414, -0.0103]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0052,  0.0368,  0.0260, -0.0083, -0.0096],\n",
      "        [-0.0021, -0.0033,  0.0309, -0.0330,  0.0090],\n",
      "        [-0.0089,  0.0348, -0.0405, -0.0325, -0.0099],\n",
      "        [-0.0148, -0.0358, -0.0544, -0.0241,  0.0159],\n",
      "        [-0.0272,  0.0113, -0.0134, -0.0704, -0.0538],\n",
      "        [ 0.0005, -0.0266, -0.0233, -0.0005,  0.0013],\n",
      "        [-0.0505, -0.0690, -0.0282, -0.0112, -0.0211],\n",
      "        [ 0.0213, -0.0072,  0.0318, -0.0263, -0.0226]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0168, grad_fn=<MinBackward1>), tensor(0.8689, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12678492069244385\n",
      "@sample 670: tensor([[ 0.0306,  0.0162,  0.0314, -0.0203, -0.0049],\n",
      "        [-0.0016, -0.0433, -0.0098,  0.0035, -0.0012],\n",
      "        [-0.0050, -0.0047,  0.0164, -0.0015, -0.0346],\n",
      "        [-0.0363, -0.0161, -0.0188, -0.0068,  0.0084],\n",
      "        [-0.0060, -0.0466, -0.0210,  0.0322,  0.0311],\n",
      "        [ 0.0375,  0.0170, -0.0055, -0.0289,  0.0278],\n",
      "        [-0.0159,  0.0165,  0.0251, -0.0238,  0.0143],\n",
      "        [ 0.0002, -0.0084, -0.0048, -0.0062,  0.0104]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0388,  0.0261, -0.0295,  0.0405,  0.0028],\n",
      "        [-0.0349, -0.0439,  0.0459, -0.0240, -0.0610],\n",
      "        [-0.0059, -0.0091, -0.0240, -0.0008, -0.0142],\n",
      "        [ 0.0074, -0.0155,  0.1013, -0.0410, -0.0331],\n",
      "        [ 0.0517, -0.0240,  0.0049, -0.0082,  0.0129],\n",
      "        [-0.0145, -0.0006, -0.0205,  0.0287, -0.0105],\n",
      "        [-0.0135,  0.0315, -0.0400,  0.0105, -0.0169],\n",
      "        [ 0.0015,  0.0169, -0.0098, -0.0184, -0.0031]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0206, grad_fn=<MinBackward1>), tensor(0.9488, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13963019847869873\n",
      "@sample 671: tensor([[ 0.0051, -0.0016,  0.0312, -0.0007, -0.0207],\n",
      "        [-0.0148, -0.0295, -0.0139, -0.0130, -0.0051],\n",
      "        [ 0.0072,  0.0062,  0.0091,  0.0031, -0.0244],\n",
      "        [ 0.0127, -0.0257,  0.0171, -0.0068, -0.0294],\n",
      "        [ 0.0105, -0.0043,  0.0122,  0.0083, -0.0096],\n",
      "        [-0.0012, -0.0439, -0.0004,  0.0216,  0.0051],\n",
      "        [ 0.0099,  0.0154, -0.0350, -0.0253,  0.0095],\n",
      "        [ 0.0222, -0.0271,  0.0004,  0.0197, -0.0225]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0006,  0.0383,  0.0195, -0.0005,  0.0150],\n",
      "        [-0.0187, -0.0326,  0.0124, -0.0023, -0.0098],\n",
      "        [ 0.0031,  0.0077, -0.0201,  0.0058,  0.0019],\n",
      "        [-0.0379, -0.0217, -0.0318, -0.0114, -0.0093],\n",
      "        [-0.0188, -0.0126,  0.0402, -0.0219, -0.0085],\n",
      "        [ 0.0142,  0.0017,  0.0111, -0.0030,  0.0026],\n",
      "        [-0.0373,  0.0144, -0.0458,  0.0445,  0.0283],\n",
      "        [ 0.0286, -0.0275, -0.0052,  0.0040,  0.0355]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0196, grad_fn=<MinBackward1>), tensor(0.9071, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12986139953136444\n",
      "@sample 672: tensor([[-0.0030,  0.0057,  0.0210, -0.0357,  0.0209],\n",
      "        [ 0.0112, -0.0213,  0.0126, -0.0192,  0.0136],\n",
      "        [ 0.0249, -0.0416, -0.0134,  0.0208,  0.0131],\n",
      "        [ 0.0027,  0.0059,  0.0112, -0.0267,  0.0107],\n",
      "        [ 0.0111,  0.0242, -0.0090, -0.0146, -0.0033],\n",
      "        [ 0.0305, -0.0023,  0.0181,  0.0186, -0.0193],\n",
      "        [ 0.0218,  0.0059,  0.0216, -0.0233, -0.0110],\n",
      "        [-0.0041, -0.0214, -0.0142,  0.0314, -0.0162]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0059, -0.0225,  0.0048,  0.0311,  0.0131],\n",
      "        [-0.0199, -0.0104,  0.0015,  0.0091, -0.0211],\n",
      "        [ 0.0358,  0.0361,  0.0350, -0.0169, -0.0099],\n",
      "        [-0.0164,  0.0050, -0.0097, -0.0022,  0.0059],\n",
      "        [ 0.0031,  0.0257, -0.0381, -0.0054,  0.0146],\n",
      "        [-0.0163,  0.0038, -0.0013,  0.0474,  0.0100],\n",
      "        [-0.0073,  0.0117,  0.0002,  0.0430,  0.0076],\n",
      "        [ 0.0112,  0.0170, -0.0071, -0.0287, -0.0020]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0179, grad_fn=<MinBackward1>), tensor(0.8665, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11953996121883392\n",
      "@sample 673: tensor([[ 3.1977e-02, -5.3799e-02, -3.0386e-02,  3.5844e-02, -1.1353e-02],\n",
      "        [ 2.1762e-02,  1.1298e-02,  3.6249e-02,  3.1398e-03,  2.9764e-03],\n",
      "        [-7.8364e-04,  1.2250e-02,  2.4576e-02,  2.4882e-02, -3.8795e-03],\n",
      "        [-5.7182e-05,  9.8681e-03,  2.4496e-03, -2.5442e-03, -1.3504e-02],\n",
      "        [-1.2801e-02,  1.4777e-02,  2.0575e-03,  2.6827e-02, -2.0625e-03],\n",
      "        [ 2.4093e-02,  1.0157e-02,  2.7789e-02, -8.8511e-03,  2.3245e-02],\n",
      "        [-3.2199e-02, -1.5403e-02,  1.2354e-02, -1.5558e-02,  4.7603e-02],\n",
      "        [ 5.7815e-03,  3.3828e-02,  1.4980e-02, -4.9022e-02,  3.6342e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0133, -0.0165,  0.0244, -0.0538, -0.0369],\n",
      "        [-0.0014,  0.0084, -0.0299,  0.0067,  0.0020],\n",
      "        [ 0.0256,  0.0229, -0.0118,  0.0143,  0.0175],\n",
      "        [ 0.0030, -0.0014, -0.0290,  0.0102,  0.0097],\n",
      "        [-0.0205,  0.0019, -0.0080, -0.0265, -0.0221],\n",
      "        [ 0.0214,  0.0179, -0.0068, -0.0276, -0.0578],\n",
      "        [ 0.0230, -0.0055, -0.1311,  0.0255,  0.0357],\n",
      "        [-0.0369, -0.0545, -0.0636,  0.0651,  0.0027]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0178, grad_fn=<MinBackward1>), tensor(0.8622, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13170963525772095\n",
      "@sample 674: tensor([[ 0.0110, -0.0196, -0.0129,  0.0255, -0.0016],\n",
      "        [-0.0003,  0.0374,  0.0046, -0.0185,  0.0198],\n",
      "        [ 0.0177, -0.0285, -0.0395,  0.0004, -0.0033],\n",
      "        [ 0.0213,  0.0154,  0.0106,  0.0090, -0.0014],\n",
      "        [-0.0621,  0.0275,  0.0053, -0.0583,  0.0222],\n",
      "        [ 0.0091,  0.0075,  0.0214, -0.0072, -0.0075],\n",
      "        [-0.0205,  0.0230,  0.0422, -0.0297,  0.0114],\n",
      "        [ 0.0203, -0.0130, -0.0120, -0.0179,  0.0039]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0023,  0.0110, -0.0494, -0.0243, -0.0009],\n",
      "        [-0.0003, -0.0326, -0.0674,  0.0224, -0.0008],\n",
      "        [-0.0298, -0.0103, -0.0647,  0.0126,  0.0057],\n",
      "        [-0.0597,  0.0337, -0.0950,  0.0053,  0.0337],\n",
      "        [-0.0543, -0.0340,  0.0071, -0.0405, -0.0370],\n",
      "        [-0.0186,  0.0165,  0.0326, -0.0149, -0.0308],\n",
      "        [-0.0015, -0.0191, -0.0272,  0.0180, -0.0031],\n",
      "        [ 0.0345, -0.0575,  0.0037,  0.0203,  0.0323]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0101, grad_fn=<MinBackward1>), tensor(0.8673, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1325187385082245\n",
      "@sample 675: tensor([[-0.0140, -0.0208, -0.0194, -0.0101,  0.0143],\n",
      "        [ 0.0049, -0.0163,  0.0086, -0.0465,  0.0208],\n",
      "        [ 0.0029,  0.0549,  0.0198, -0.0326,  0.0161],\n",
      "        [-0.0130,  0.0316,  0.0275, -0.0372,  0.0216],\n",
      "        [-0.0323,  0.0277,  0.0212, -0.0443,  0.0270],\n",
      "        [ 0.0100, -0.0427,  0.0234,  0.0219,  0.0058],\n",
      "        [-0.0376, -0.0176,  0.0178, -0.0065,  0.0097],\n",
      "        [-0.0103, -0.0161,  0.0254, -0.0131,  0.0080]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0365, -0.0089,  0.0537, -0.0318,  0.0061],\n",
      "        [ 0.0051, -0.0364,  0.0253, -0.0043, -0.0006],\n",
      "        [-0.0391,  0.0165, -0.0947,  0.0660,  0.0116],\n",
      "        [-0.0190, -0.0467, -0.0377,  0.0430,  0.0063],\n",
      "        [-0.0003, -0.0183, -0.0114, -0.0228, -0.0048],\n",
      "        [ 0.0494,  0.0060,  0.0360,  0.0457,  0.0278],\n",
      "        [-0.0104, -0.0396,  0.0594, -0.0314, -0.0345],\n",
      "        [-0.0087, -0.0065, -0.0113,  0.0020,  0.0099]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0097, grad_fn=<MinBackward1>), tensor(0.8804, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13186857104301453\n",
      "@sample 676: tensor([[-0.0229,  0.0029, -0.0127, -0.0003,  0.0178],\n",
      "        [ 0.0013,  0.0561, -0.0109, -0.0150,  0.0441],\n",
      "        [ 0.0183,  0.0125, -0.0176, -0.0176, -0.0050],\n",
      "        [-0.0252,  0.0199,  0.0365, -0.0112,  0.0040],\n",
      "        [ 0.0100, -0.0241, -0.0145,  0.0008, -0.0131],\n",
      "        [ 0.0149, -0.0011, -0.0206, -0.0142,  0.0088],\n",
      "        [-0.0127, -0.0294, -0.0595,  0.0317, -0.0113],\n",
      "        [ 0.0323, -0.0222, -0.0204,  0.0429, -0.0245]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0050, -0.0155, -0.0417, -0.0281,  0.0042],\n",
      "        [ 0.0293, -0.0119, -0.0879,  0.0459,  0.0208],\n",
      "        [ 0.0018, -0.0108,  0.0544, -0.0231, -0.0165],\n",
      "        [ 0.0092, -0.0093, -0.0336,  0.0506,  0.0137],\n",
      "        [ 0.0286,  0.0175, -0.0028,  0.0016,  0.0130],\n",
      "        [ 0.0299, -0.0098, -0.0026, -0.0503, -0.0132],\n",
      "        [ 0.0081,  0.0090,  0.0562, -0.0509, -0.0275],\n",
      "        [-0.0175,  0.0415, -0.0474, -0.0305,  0.0395]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0156, grad_fn=<MinBackward1>), tensor(0.9032, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12788903713226318\n",
      "@sample 677: tensor([[-0.0100,  0.0263, -0.0018, -0.0363,  0.0295],\n",
      "        [-0.0118, -0.0122,  0.0205,  0.0208,  0.0072],\n",
      "        [-0.0143,  0.0292,  0.0325, -0.0429,  0.0136],\n",
      "        [-0.0073,  0.0493,  0.0008, -0.0212, -0.0136],\n",
      "        [-0.0108,  0.0493,  0.0528, -0.0587,  0.0197],\n",
      "        [-0.0287, -0.0211, -0.0181,  0.0077,  0.0041],\n",
      "        [ 0.0031, -0.0061, -0.0022, -0.0110,  0.0160],\n",
      "        [ 0.0003,  0.0108, -0.0534,  0.0625, -0.0217]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-2.3132e-02, -2.6845e-02, -5.1681e-02, -3.2006e-02,  2.9556e-02],\n",
      "        [ 8.8623e-03,  2.9331e-02, -1.7859e-02, -2.5478e-02, -1.7008e-02],\n",
      "        [-2.8556e-02, -1.1167e-02, -1.3212e-02,  1.8337e-02,  2.2825e-02],\n",
      "        [ 1.1783e-02,  1.0232e-02, -6.9806e-03,  9.0704e-03,  1.5829e-03],\n",
      "        [-2.6675e-02, -2.7322e-02, -7.5614e-02,  1.7690e-02,  6.8293e-03],\n",
      "        [ 3.5975e-02, -4.3958e-07,  6.2884e-02, -4.1051e-02,  4.7042e-03],\n",
      "        [-5.8548e-03,  1.2983e-02,  3.0773e-02,  1.6147e-03,  1.9955e-02],\n",
      "        [ 3.9915e-02,  3.2906e-02, -6.5099e-03,  2.4028e-03,  3.0827e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0164, grad_fn=<MinBackward1>), tensor(0.8857, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13002121448516846\n",
      "@sample 678: tensor([[-0.0241, -0.0197,  0.0191, -0.0097,  0.0391],\n",
      "        [-0.0043,  0.0112, -0.0093, -0.0194,  0.0310],\n",
      "        [-0.0077, -0.0017, -0.0170, -0.0065,  0.0297],\n",
      "        [ 0.0084, -0.0165,  0.0039, -0.0195,  0.0261],\n",
      "        [-0.0346,  0.0206, -0.0202, -0.0120,  0.0068],\n",
      "        [-0.0020,  0.0438,  0.0075, -0.0426,  0.0236],\n",
      "        [-0.0239, -0.0262, -0.0128,  0.0102, -0.0234],\n",
      "        [ 0.0079, -0.0003,  0.0067,  0.0130, -0.0045]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0027, -0.0091,  0.0505, -0.0208, -0.0114],\n",
      "        [-0.0141, -0.0059, -0.0331, -0.0200, -0.0320],\n",
      "        [ 0.0117, -0.0242, -0.0365, -0.0060, -0.0490],\n",
      "        [ 0.0113, -0.0054, -0.0113,  0.0335, -0.0121],\n",
      "        [ 0.0127,  0.0254,  0.0590, -0.0059,  0.0105],\n",
      "        [-0.0429,  0.0122, -0.0499,  0.0702,  0.0262],\n",
      "        [-0.0135, -0.0165,  0.0082, -0.0074,  0.0107],\n",
      "        [-0.0049,  0.0025, -0.0181,  0.0241,  0.0096]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0178, grad_fn=<MinBackward1>), tensor(0.8701, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11776972562074661\n",
      "@sample 679: tensor([[-0.0192, -0.0072,  0.0062,  0.0015, -0.0028],\n",
      "        [ 0.0109, -0.0093,  0.0044, -0.0036,  0.0064],\n",
      "        [-0.0095,  0.0075,  0.0111, -0.0194,  0.0181],\n",
      "        [ 0.0021, -0.0188,  0.0088,  0.0154,  0.0069],\n",
      "        [ 0.0478, -0.0025, -0.0131, -0.0069, -0.0001],\n",
      "        [-0.0089,  0.0160,  0.0246, -0.0087,  0.0106],\n",
      "        [ 0.0176,  0.0061, -0.0018, -0.0315,  0.0335],\n",
      "        [-0.0237,  0.0171, -0.0263,  0.0084, -0.0100]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0096,  0.0031,  0.0160, -0.0138,  0.0128],\n",
      "        [-0.0222, -0.0134,  0.0225, -0.0086, -0.0255],\n",
      "        [ 0.0041,  0.0029, -0.0115, -0.0211, -0.0099],\n",
      "        [ 0.0289,  0.0029, -0.0018, -0.0213,  0.0359],\n",
      "        [-0.0136, -0.0381, -0.0527,  0.0500, -0.0095],\n",
      "        [ 0.0047,  0.0125,  0.0242, -0.0152,  0.0061],\n",
      "        [-0.0140, -0.0051, -0.0324,  0.0234,  0.0184],\n",
      "        [ 0.0096,  0.0146, -0.0043, -0.0330,  0.0223]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0208, grad_fn=<MinBackward1>), tensor(0.8816, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11482220888137817\n",
      "@sample 680: tensor([[-0.0272, -0.0367, -0.0150,  0.0245,  0.0260],\n",
      "        [-0.0119,  0.0476,  0.0101, -0.0434,  0.0152],\n",
      "        [ 0.0148,  0.0254,  0.0094, -0.0070,  0.0012],\n",
      "        [-0.0187, -0.0320, -0.0310,  0.0078, -0.0331],\n",
      "        [-0.0089,  0.0100, -0.0096, -0.0174,  0.0279],\n",
      "        [ 0.0219, -0.0326, -0.0147,  0.0492, -0.0168],\n",
      "        [-0.0271,  0.0145,  0.0007, -0.0063, -0.0012],\n",
      "        [ 0.0148, -0.0017, -0.0182,  0.0094, -0.0036]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0415, -0.0129,  0.0937, -0.0055,  0.0106],\n",
      "        [-0.0045, -0.0354, -0.0522, -0.0017,  0.0061],\n",
      "        [-0.0203,  0.0018,  0.0127,  0.0059,  0.0073],\n",
      "        [-0.0366, -0.0217,  0.0268, -0.0088, -0.0471],\n",
      "        [ 0.0065, -0.0365, -0.0954,  0.0258, -0.0011],\n",
      "        [ 0.0149, -0.0064,  0.0404, -0.0133, -0.0101],\n",
      "        [-0.0223,  0.0198,  0.0298, -0.0386, -0.0366],\n",
      "        [ 0.0363, -0.0076,  0.0492, -0.0403, -0.0084]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0180, grad_fn=<MinBackward1>), tensor(0.9090, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13053908944129944\n",
      "@sample 681: tensor([[ 0.0115,  0.0037, -0.0055,  0.0173,  0.0012],\n",
      "        [ 0.0043,  0.0192, -0.0241,  0.0007,  0.0190],\n",
      "        [ 0.0193, -0.0275,  0.0138,  0.0319, -0.0174],\n",
      "        [-0.0008, -0.0403, -0.0525,  0.0396, -0.0317],\n",
      "        [ 0.0163,  0.0164,  0.0104,  0.0224,  0.0041],\n",
      "        [-0.0176,  0.0500,  0.0164, -0.0107,  0.0566],\n",
      "        [ 0.0122,  0.0524, -0.0273,  0.0169, -0.0354],\n",
      "        [ 0.0213, -0.0035,  0.0257, -0.0233, -0.0119]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0048,  0.0138,  0.0249, -0.0099,  0.0115],\n",
      "        [ 0.0093, -0.0116, -0.0007, -0.0165, -0.0045],\n",
      "        [ 0.0498, -0.0106,  0.0006, -0.0205,  0.0047],\n",
      "        [ 0.0294,  0.0096,  0.0378, -0.0522, -0.0254],\n",
      "        [ 0.0180,  0.0217, -0.0450,  0.0046, -0.0110],\n",
      "        [ 0.0322,  0.0077, -0.0463,  0.0253, -0.0266],\n",
      "        [-0.0002,  0.0197, -0.0337,  0.0505, -0.0411],\n",
      "        [-0.0198,  0.0135, -0.0123,  0.0646,  0.0003]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0171, grad_fn=<MinBackward1>), tensor(0.9587, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.14278292655944824\n",
      "@sample 682: tensor([[ 0.0288, -0.0065, -0.0221,  0.0015, -0.0161],\n",
      "        [ 0.0151,  0.0399,  0.0271, -0.0251,  0.0039],\n",
      "        [ 0.0134, -0.0036,  0.0135,  0.0128, -0.0250],\n",
      "        [ 0.0201,  0.0276,  0.0014, -0.0423, -0.0199],\n",
      "        [ 0.0172,  0.0166, -0.0069, -0.0042,  0.0158],\n",
      "        [-0.0270, -0.0140, -0.0037,  0.0330, -0.0166],\n",
      "        [ 0.0191, -0.0062, -0.0039,  0.0119, -0.0066],\n",
      "        [-0.0047,  0.0509,  0.0193, -0.0038, -0.0337]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0255, -0.0006,  0.0284, -0.0042, -0.0006],\n",
      "        [-0.0295, -0.0170, -0.0730,  0.0111, -0.0425],\n",
      "        [-0.0137,  0.0009,  0.0041, -0.0082, -0.0062],\n",
      "        [-0.0168, -0.0305, -0.0058, -0.0415, -0.0106],\n",
      "        [ 0.0224,  0.0141,  0.0173, -0.0253,  0.0293],\n",
      "        [-0.0035,  0.0204,  0.0217,  0.0167,  0.0034],\n",
      "        [ 0.0222, -0.0013,  0.0298, -0.0520, -0.0253],\n",
      "        [-0.0415,  0.0449,  0.0134,  0.0128,  0.0515]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0187, grad_fn=<MinBackward1>), tensor(0.8932, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12238655984401703\n",
      "@sample 683: tensor([[ 0.0175, -0.0026, -0.0262,  0.0394, -0.0251],\n",
      "        [-0.0264,  0.0005,  0.0005,  0.0075,  0.0228],\n",
      "        [ 0.0179, -0.0170,  0.0099, -0.0076,  0.0208],\n",
      "        [-0.0182,  0.0186,  0.0016, -0.0200,  0.0281],\n",
      "        [-0.0015,  0.0006,  0.0241, -0.0141,  0.0184],\n",
      "        [-0.0037,  0.0751,  0.0242, -0.0809,  0.0270],\n",
      "        [ 0.0319,  0.0082, -0.0379, -0.0146,  0.0234],\n",
      "        [-0.0218, -0.0192, -0.0130,  0.0251, -0.0063]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0059, -0.0017, -0.0017, -0.0334, -0.0117],\n",
      "        [ 0.0372, -0.0025,  0.0119, -0.0127,  0.0095],\n",
      "        [ 0.0007, -0.0274, -0.0071,  0.0159, -0.0063],\n",
      "        [-0.0068,  0.0246, -0.0205,  0.0099,  0.0303],\n",
      "        [ 0.0091,  0.0140, -0.0119,  0.0124,  0.0386],\n",
      "        [-0.0883, -0.0563, -0.1308,  0.0558, -0.0002],\n",
      "        [ 0.0019, -0.0161, -0.0998,  0.0155,  0.0038],\n",
      "        [ 0.0211,  0.0109,  0.0063, -0.0232,  0.0266]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0092, grad_fn=<MinBackward1>), tensor(0.8712, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.14204302430152893\n",
      "@sample 684: tensor([[-0.0121,  0.0092, -0.0063,  0.0229, -0.0011],\n",
      "        [-0.0087, -0.0182, -0.0004,  0.0066, -0.0125],\n",
      "        [ 0.0160,  0.0325,  0.0188, -0.0011,  0.0014],\n",
      "        [-0.0243,  0.0473,  0.0065, -0.0234, -0.0088],\n",
      "        [ 0.0015,  0.0002,  0.0326, -0.0092, -0.0001],\n",
      "        [ 0.0003,  0.0058, -0.0052, -0.0398,  0.0186],\n",
      "        [ 0.0157,  0.0674,  0.0241, -0.0228, -0.0097],\n",
      "        [ 0.0001,  0.0126,  0.0053, -0.0188, -0.0109]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0150,  0.0404,  0.0382, -0.0137,  0.0281],\n",
      "        [ 0.0094,  0.0196,  0.0112, -0.0221,  0.0368],\n",
      "        [-0.0697, -0.0436, -0.0688,  0.0303, -0.0158],\n",
      "        [-0.0244,  0.0070,  0.0329,  0.0255,  0.0236],\n",
      "        [-0.0128,  0.0233, -0.0345,  0.0051,  0.0011],\n",
      "        [-0.0365,  0.0314, -0.0399,  0.0244, -0.0024],\n",
      "        [-0.0010, -0.0144, -0.0447, -0.0203,  0.0129],\n",
      "        [-0.0152, -0.0174, -0.0194,  0.0182, -0.0166]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0138, grad_fn=<MinBackward1>), tensor(0.9028, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12144240736961365\n",
      "@sample 685: tensor([[ 0.0212, -0.0035, -0.0013,  0.0225, -0.0418],\n",
      "        [ 0.0107, -0.0177,  0.0111,  0.0180, -0.0236],\n",
      "        [-0.0152, -0.0201, -0.0372,  0.0146, -0.0052],\n",
      "        [ 0.0058,  0.0291,  0.0259, -0.0247, -0.0079],\n",
      "        [ 0.0048, -0.0440, -0.0377, -0.0113, -0.0040],\n",
      "        [ 0.0346, -0.0025, -0.0151, -0.0035,  0.0185],\n",
      "        [ 0.0170, -0.0159,  0.0088, -0.0015, -0.0155],\n",
      "        [ 0.0167,  0.0100,  0.0247,  0.0051, -0.0263]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0513,  0.0129,  0.0238, -0.0035, -0.0145],\n",
      "        [ 0.0163, -0.0185,  0.0537, -0.0238, -0.0405],\n",
      "        [ 0.0107,  0.0112,  0.0296, -0.0313, -0.0251],\n",
      "        [-0.0100, -0.0201, -0.0168,  0.0254, -0.0178],\n",
      "        [ 0.0145, -0.0357,  0.0441, -0.0392,  0.0061],\n",
      "        [ 0.0087, -0.0200,  0.0012, -0.0156,  0.0123],\n",
      "        [-0.0229, -0.0249, -0.0011,  0.0142,  0.0242],\n",
      "        [-0.0163, -0.0007,  0.0208, -0.0135,  0.0063]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0173, grad_fn=<MinBackward1>), tensor(0.9039, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12852106988430023\n",
      "@sample 686: tensor([[-0.0305,  0.0099,  0.0076, -0.0144,  0.0063],\n",
      "        [ 0.0117,  0.0150, -0.0037, -0.0306,  0.0162],\n",
      "        [ 0.0413,  0.0291,  0.0087,  0.0234, -0.0401],\n",
      "        [-0.0263, -0.0476,  0.0374, -0.0091, -0.0112],\n",
      "        [-0.0125,  0.0435,  0.0064, -0.0518,  0.0230],\n",
      "        [ 0.0135,  0.0295,  0.0507, -0.0350,  0.0337],\n",
      "        [-0.0054, -0.0024, -0.0031, -0.0050, -0.0039],\n",
      "        [ 0.0101, -0.0197, -0.0511, -0.0119,  0.0145]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0004,  0.0050,  0.0142, -0.0346, -0.0010],\n",
      "        [-0.0287, -0.0364, -0.0410,  0.0417, -0.0211],\n",
      "        [ 0.0492,  0.0415,  0.0145,  0.0089,  0.0327],\n",
      "        [ 0.0127,  0.0009,  0.0379, -0.0069,  0.0086],\n",
      "        [-0.0046, -0.0119, -0.0291,  0.0300,  0.0064],\n",
      "        [-0.0405,  0.0220, -0.0567,  0.0058, -0.0199],\n",
      "        [ 0.0122,  0.0006,  0.0223, -0.0553, -0.0082],\n",
      "        [-0.0001, -0.0008,  0.0134, -0.0108, -0.0068]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0123, grad_fn=<MinBackward1>), tensor(0.8879, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13443784415721893\n",
      "@sample 687: tensor([[ 0.0114,  0.0342,  0.0066, -0.0231,  0.0169],\n",
      "        [-0.0088, -0.0111,  0.0311,  0.0072,  0.0044],\n",
      "        [-0.0225, -0.0184, -0.0393,  0.0250, -0.0247],\n",
      "        [-0.0130, -0.0234, -0.0056, -0.0064,  0.0124],\n",
      "        [-0.0078,  0.0096, -0.0374, -0.0004,  0.0134],\n",
      "        [-0.0108, -0.0101, -0.0096,  0.0453, -0.0115],\n",
      "        [-0.0037,  0.0096, -0.0004, -0.0206,  0.0004],\n",
      "        [ 0.0280,  0.0432,  0.0383,  0.0183,  0.0096]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-4.9714e-02, -3.0060e-03, -6.6493e-02, -5.4021e-03, -1.1479e-03],\n",
      "        [-9.9750e-03,  2.3377e-02,  3.2113e-02, -1.3257e-03, -6.8189e-03],\n",
      "        [ 8.0655e-03, -2.4579e-02, -2.4180e-04, -1.7597e-02, -1.6257e-02],\n",
      "        [ 1.8830e-02, -1.1814e-02,  4.1638e-02, -8.7082e-03, -1.5492e-02],\n",
      "        [-2.1950e-04,  3.4048e-03,  1.4028e-02, -2.4481e-02,  2.1556e-03],\n",
      "        [ 1.7102e-02,  1.7152e-02,  2.2597e-02, -3.3577e-02,  1.6610e-03],\n",
      "        [ 8.9541e-05,  2.2797e-02,  1.3934e-02, -1.7095e-02, -9.9050e-04],\n",
      "        [-1.7938e-02,  5.5139e-03, -4.9320e-02,  7.1047e-03, -2.7699e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0167, grad_fn=<MinBackward1>), tensor(0.8891, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12220803648233414\n",
      "@sample 688: tensor([[-0.0187, -0.0176, -0.0039,  0.0104, -0.0110],\n",
      "        [ 0.0042, -0.0238, -0.0394,  0.0314, -0.0421],\n",
      "        [ 0.0193,  0.0256,  0.0040, -0.0326,  0.0103],\n",
      "        [-0.0112, -0.0076, -0.0039, -0.0060, -0.0255],\n",
      "        [ 0.0177, -0.0269, -0.0050,  0.0116, -0.0256],\n",
      "        [-0.0175,  0.0001,  0.0100, -0.0273,  0.0398],\n",
      "        [-0.0003, -0.0145,  0.0068,  0.0200, -0.0122],\n",
      "        [-0.0077, -0.0193,  0.0045,  0.0289,  0.0025]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0103, -0.0055,  0.0288, -0.0118,  0.0074],\n",
      "        [-0.0052, -0.0010,  0.0599, -0.0453, -0.0041],\n",
      "        [ 0.0060,  0.0234, -0.0297,  0.0474,  0.0427],\n",
      "        [-0.0140, -0.0192,  0.0651, -0.0207, -0.0215],\n",
      "        [ 0.0056,  0.0179,  0.0156, -0.0024, -0.0146],\n",
      "        [-0.0108, -0.0116,  0.0298, -0.0354,  0.0214],\n",
      "        [ 0.0228, -0.0124,  0.0007, -0.0031,  0.0032],\n",
      "        [ 0.0140, -0.0143,  0.0015, -0.0010,  0.0265]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0245, grad_fn=<MinBackward1>), tensor(0.9452, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13351763784885406\n",
      "@sample 689: tensor([[-0.0181,  0.0105, -0.0122,  0.0034,  0.0233],\n",
      "        [ 0.0026, -0.0133,  0.0279, -0.0016, -0.0155],\n",
      "        [-0.0011, -0.0007,  0.0326,  0.0190, -0.0028],\n",
      "        [-0.0365,  0.0183,  0.0462, -0.0043, -0.0282],\n",
      "        [-0.0013,  0.0037,  0.0221, -0.0184,  0.0078],\n",
      "        [-0.0026,  0.0020,  0.0312,  0.0023, -0.0214],\n",
      "        [ 0.0096,  0.0077,  0.0378, -0.0251,  0.0279],\n",
      "        [-0.0102, -0.0236, -0.0137,  0.0340, -0.0192]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0231,  0.0158, -0.0586, -0.0213,  0.0157],\n",
      "        [-0.0097, -0.0116,  0.0137,  0.0009, -0.0079],\n",
      "        [ 0.0260,  0.0054,  0.0351, -0.0167,  0.0127],\n",
      "        [-0.0227,  0.0013, -0.0106,  0.0630,  0.0045],\n",
      "        [-0.0113, -0.0358, -0.0206,  0.0206, -0.0229],\n",
      "        [-0.0223,  0.0638,  0.0519, -0.0273, -0.0415],\n",
      "        [-0.0433, -0.0059, -0.0423,  0.0435,  0.0129],\n",
      "        [ 0.0098,  0.0069,  0.0417,  0.0037,  0.0066]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0180, grad_fn=<MinBackward1>), tensor(0.8825, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11709603667259216\n",
      "@sample 690: tensor([[ 0.0034, -0.0017, -0.0310,  0.0065,  0.0116],\n",
      "        [-0.0098, -0.0385, -0.0088,  0.0032, -0.0074],\n",
      "        [ 0.0103,  0.0263,  0.0007, -0.0192,  0.0052],\n",
      "        [ 0.0102,  0.0019,  0.0265,  0.0314, -0.0456],\n",
      "        [-0.0095,  0.0086,  0.0155, -0.0399,  0.0255],\n",
      "        [ 0.0124,  0.0030, -0.0100,  0.0074, -0.0086],\n",
      "        [-0.0097, -0.0011, -0.0074,  0.0155,  0.0018],\n",
      "        [-0.0249, -0.0209, -0.0065, -0.0009, -0.0231]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0173,  0.0133, -0.0341, -0.0132, -0.0167],\n",
      "        [-0.0044,  0.0175,  0.0275,  0.0054,  0.0033],\n",
      "        [-0.0243, -0.0485, -0.0365,  0.0394,  0.0077],\n",
      "        [ 0.0309,  0.0407,  0.0232, -0.0069,  0.0070],\n",
      "        [-0.0234,  0.0087, -0.0281,  0.0106, -0.0277],\n",
      "        [-0.0029,  0.0443, -0.0024,  0.0379,  0.0318],\n",
      "        [-0.0020,  0.0233, -0.0405, -0.0062, -0.0171],\n",
      "        [-0.0283, -0.0519,  0.0668, -0.0116, -0.0271]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0233, grad_fn=<MinBackward1>), tensor(0.8771, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12308023124933243\n",
      "@sample 691: tensor([[-0.0215,  0.0055,  0.0021,  0.0065,  0.0004],\n",
      "        [ 0.0077,  0.0359,  0.0455, -0.0323, -0.0083],\n",
      "        [-0.0181,  0.0510,  0.0234, -0.0267, -0.0215],\n",
      "        [-0.0037,  0.0078,  0.0068, -0.0075,  0.0071],\n",
      "        [-0.0297,  0.0208,  0.0055, -0.0073,  0.0211],\n",
      "        [-0.0069,  0.0092,  0.0161, -0.0306,  0.0130],\n",
      "        [-0.0175,  0.0023, -0.0237,  0.0233, -0.0134],\n",
      "        [-0.0326, -0.0042,  0.0088, -0.0097,  0.0016]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0114,  0.0231, -0.0559, -0.0096,  0.0010],\n",
      "        [-0.0358, -0.0252, -0.0351, -0.0062, -0.0482],\n",
      "        [-0.0197, -0.0437,  0.0002,  0.0201,  0.0208],\n",
      "        [-0.0136, -0.0212, -0.0249,  0.0232, -0.0321],\n",
      "        [-0.0113,  0.0042, -0.0295, -0.0183, -0.0002],\n",
      "        [-0.0258, -0.0312, -0.0379,  0.0374,  0.0012],\n",
      "        [ 0.0173,  0.0098,  0.0205, -0.0187,  0.0030],\n",
      "        [-0.0394, -0.0387,  0.0183,  0.0123, -0.0023]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0180, grad_fn=<MinBackward1>), tensor(0.9054, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12509384751319885\n",
      "@sample 692: tensor([[-0.0002, -0.0034, -0.0051, -0.0213,  0.0205],\n",
      "        [-0.0081, -0.0640,  0.0043,  0.0082,  0.0279],\n",
      "        [ 0.0071, -0.0052, -0.0077,  0.0507,  0.0032],\n",
      "        [-0.0124, -0.0260, -0.0397,  0.0055, -0.0090],\n",
      "        [-0.0033, -0.0016,  0.0059, -0.0006,  0.0087],\n",
      "        [-0.0276, -0.0046,  0.0015, -0.0104,  0.0078],\n",
      "        [ 0.0044,  0.0281,  0.0256, -0.0302,  0.0065],\n",
      "        [-0.0342,  0.0380,  0.0376, -0.0613,  0.0548]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 2.6413e-02,  4.4412e-03,  1.8266e-02,  4.5627e-03,  9.2638e-03],\n",
      "        [-3.4697e-05,  7.1784e-03,  4.2258e-02, -3.4442e-02,  5.6911e-03],\n",
      "        [ 3.4163e-02,  1.5065e-02, -1.2420e-02, -1.1671e-02, -2.6918e-02],\n",
      "        [-6.6138e-03, -6.3462e-02,  9.9487e-03,  1.0472e-02,  7.5377e-04],\n",
      "        [-1.1956e-03,  3.2465e-02,  3.4936e-02,  1.0117e-02, -5.3733e-03],\n",
      "        [-2.7896e-02, -1.6192e-02,  1.1542e-02,  7.9444e-03, -2.2776e-02],\n",
      "        [-6.6647e-02, -5.5331e-02, -6.1304e-02,  2.8343e-02,  4.0865e-02],\n",
      "        [-6.4468e-03, -5.0012e-02, -5.9687e-02,  1.2498e-02, -3.1242e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0179, grad_fn=<MinBackward1>), tensor(0.8992, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13363592326641083\n",
      "@sample 693: tensor([[ 0.0288, -0.0048,  0.0038,  0.0031, -0.0011],\n",
      "        [-0.0212,  0.0141, -0.0136,  0.0002,  0.0203],\n",
      "        [ 0.0146,  0.0247,  0.0166, -0.0214,  0.0144],\n",
      "        [ 0.0052, -0.0413, -0.0176,  0.0231, -0.0038],\n",
      "        [-0.0065,  0.0099,  0.0165, -0.0128,  0.0174],\n",
      "        [ 0.0239,  0.0666,  0.0430, -0.0312,  0.0010],\n",
      "        [ 0.0035, -0.0201,  0.0143,  0.0033, -0.0141],\n",
      "        [-0.0180,  0.0697,  0.0426, -0.0754,  0.0195]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0041,  0.0266,  0.0116,  0.0062, -0.0024],\n",
      "        [-0.0012,  0.0216,  0.0057, -0.0303, -0.0042],\n",
      "        [ 0.0263,  0.0579,  0.0136,  0.0336,  0.0227],\n",
      "        [ 0.0320,  0.0165,  0.0042,  0.0183,  0.0216],\n",
      "        [-0.0085,  0.0031, -0.0011,  0.0106,  0.0202],\n",
      "        [-0.0616,  0.0048, -0.1215,  0.0287, -0.0277],\n",
      "        [-0.0076, -0.0185,  0.0158, -0.0074,  0.0360],\n",
      "        [-0.0853, -0.0445, -0.0519,  0.0400, -0.0375]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0169, grad_fn=<MinBackward1>), tensor(0.8707, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13453593850135803\n",
      "@sample 694: tensor([[ 0.0193,  0.0183,  0.0044, -0.0342,  0.0223],\n",
      "        [-0.0258,  0.0181, -0.0050, -0.0010, -0.0360],\n",
      "        [-0.0034, -0.0244, -0.0212,  0.0552, -0.0130],\n",
      "        [-0.0299, -0.0250, -0.0281,  0.0004,  0.0331],\n",
      "        [-0.0042, -0.0511,  0.0100, -0.0105, -0.0121],\n",
      "        [ 0.0199, -0.0160, -0.0385,  0.0133,  0.0308],\n",
      "        [ 0.0064, -0.0099, -0.0121, -0.0237,  0.0027],\n",
      "        [-0.0011,  0.0414, -0.0025, -0.0207, -0.0178]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-2.7863e-02, -4.2740e-02, -7.0120e-02,  7.8748e-02, -8.3771e-05],\n",
      "        [ 7.4401e-03,  1.3866e-02, -1.8409e-02, -4.1926e-02,  3.9717e-03],\n",
      "        [ 5.7445e-02,  3.6716e-02, -3.1619e-02,  3.8690e-02,  9.8196e-03],\n",
      "        [-5.0550e-03, -3.1179e-02,  2.5518e-02, -3.7816e-02, -8.4498e-03],\n",
      "        [-3.1580e-02, -1.3033e-02,  8.0970e-03,  3.4324e-02,  2.0752e-02],\n",
      "        [-7.5656e-03, -5.4375e-02, -9.0706e-02,  1.3646e-02, -7.8121e-03],\n",
      "        [-1.9460e-02, -1.7601e-03, -1.6103e-02, -2.8828e-03, -1.7036e-02],\n",
      "        [ 2.7294e-03, -2.5655e-02,  3.5425e-03, -6.4787e-03,  1.8329e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0152, grad_fn=<MinBackward1>), tensor(0.9306, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1347198188304901\n",
      "@sample 695: tensor([[-0.0023,  0.0114, -0.0243, -0.0111,  0.0027],\n",
      "        [-0.0136, -0.0151, -0.0188, -0.0013,  0.0127],\n",
      "        [-0.0482,  0.0273,  0.0228, -0.0521,  0.0600],\n",
      "        [-0.0284, -0.0104,  0.0055,  0.0226,  0.0187],\n",
      "        [ 0.0068, -0.0051,  0.0173, -0.0233,  0.0294],\n",
      "        [ 0.0042,  0.0126,  0.0225, -0.0378, -0.0035],\n",
      "        [ 0.0246, -0.0010,  0.0415, -0.0180, -0.0010],\n",
      "        [ 0.0325, -0.0099,  0.0206, -0.0199,  0.0098]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0114, -0.0496, -0.0175,  0.0160,  0.0049],\n",
      "        [-0.0003, -0.0260,  0.0358, -0.0159, -0.0090],\n",
      "        [ 0.0177, -0.0521, -0.0858,  0.0368, -0.0126],\n",
      "        [ 0.0316,  0.0172,  0.0597, -0.0198, -0.0221],\n",
      "        [-0.0068, -0.0354, -0.0423,  0.0269,  0.0011],\n",
      "        [-0.0211, -0.0683, -0.0596,  0.0511,  0.0145],\n",
      "        [-0.0239,  0.0053, -0.0173,  0.0021, -0.0023],\n",
      "        [-0.0291,  0.0054, -0.0362,  0.0363,  0.0049]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0127, grad_fn=<MinBackward1>), tensor(0.8629, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12111262232065201\n",
      "@sample 696: tensor([[ 0.0093,  0.0177,  0.0339, -0.0302,  0.0197],\n",
      "        [-0.0011, -0.0010, -0.0302,  0.0409, -0.0446],\n",
      "        [ 0.0044,  0.0135,  0.0225, -0.0162,  0.0036],\n",
      "        [-0.0181, -0.0104, -0.0025,  0.0158, -0.0030],\n",
      "        [-0.0033, -0.0100, -0.0106, -0.0154,  0.0356],\n",
      "        [ 0.0260,  0.0104,  0.0315,  0.0259, -0.0386],\n",
      "        [ 0.0019, -0.0230, -0.0496,  0.0517, -0.0234],\n",
      "        [ 0.0248, -0.0194, -0.0415, -0.0183, -0.0037]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0288, -0.0262, -0.0571,  0.0216, -0.0066],\n",
      "        [ 0.0359, -0.0063,  0.0383, -0.0271,  0.0175],\n",
      "        [ 0.0021,  0.0107,  0.0007, -0.0200, -0.0254],\n",
      "        [ 0.0054, -0.0111,  0.0269,  0.0256, -0.0029],\n",
      "        [ 0.0367,  0.0125,  0.0229, -0.0203, -0.0124],\n",
      "        [-0.0233,  0.0153, -0.0459,  0.0380,  0.0129],\n",
      "        [ 0.0240,  0.0125,  0.0047, -0.0128,  0.0071],\n",
      "        [ 0.0240, -0.0227, -0.0266, -0.0099, -0.0332]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0216, grad_fn=<MinBackward1>), tensor(0.9305, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11789152026176453\n",
      "@sample 697: tensor([[ 0.0324, -0.0113, -0.0241,  0.0061, -0.0208],\n",
      "        [-0.0003, -0.0056,  0.0306, -0.0142,  0.0307],\n",
      "        [ 0.0079, -0.0238,  0.0042,  0.0069,  0.0316],\n",
      "        [ 0.0129, -0.0362, -0.0237,  0.0416, -0.0319],\n",
      "        [-0.0181,  0.0142,  0.0238,  0.0002,  0.0400],\n",
      "        [ 0.0019, -0.0105,  0.0196,  0.0203, -0.0165],\n",
      "        [-0.0344, -0.0135,  0.0331, -0.0100,  0.0072],\n",
      "        [ 0.0110,  0.0074,  0.0047, -0.0038, -0.0002]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0108,  0.0247, -0.0288,  0.0353,  0.0262],\n",
      "        [-0.0091, -0.0173, -0.0075,  0.0448, -0.0019],\n",
      "        [ 0.0116,  0.0051,  0.0237,  0.0009, -0.0326],\n",
      "        [ 0.0452,  0.0197,  0.0593,  0.0057, -0.0005],\n",
      "        [-0.0133, -0.0103,  0.0006,  0.0206, -0.0033],\n",
      "        [-0.0022,  0.0235,  0.0082,  0.0134,  0.0220],\n",
      "        [-0.0089,  0.0236, -0.0261,  0.0130, -0.0084],\n",
      "        [ 0.0284,  0.0173, -0.0333,  0.0458,  0.0112]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0148, grad_fn=<MinBackward1>), tensor(0.8934, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1270567774772644\n",
      "@sample 698: tensor([[-0.0035, -0.0136, -0.0109, -0.0066, -0.0071],\n",
      "        [ 0.0053, -0.0297, -0.0257,  0.0210,  0.0011],\n",
      "        [ 0.0092, -0.0072,  0.0026, -0.0037, -0.0033],\n",
      "        [ 0.0056,  0.0144,  0.0073, -0.0320,  0.0177],\n",
      "        [-0.0225, -0.0549, -0.0229,  0.0170,  0.0208],\n",
      "        [ 0.0050,  0.0343,  0.0366, -0.0337,  0.0121],\n",
      "        [-0.0017,  0.0546,  0.0145, -0.0377,  0.0183],\n",
      "        [-0.0138, -0.0239, -0.0106,  0.0169,  0.0091]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0173, -0.0335, -0.0158,  0.0032, -0.0098],\n",
      "        [ 0.0222, -0.0222, -0.0101, -0.0145, -0.0192],\n",
      "        [ 0.0209, -0.0091,  0.0288, -0.0405, -0.0522],\n",
      "        [-0.0111, -0.0035,  0.0156,  0.0302, -0.0019],\n",
      "        [ 0.0108, -0.0208,  0.0400, -0.0162, -0.0181],\n",
      "        [-0.0152,  0.0274,  0.0131, -0.0013,  0.0115],\n",
      "        [ 0.0204, -0.0380, -0.0554,  0.0242,  0.0052],\n",
      "        [ 0.0296,  0.0244,  0.0069, -0.0008, -0.0219]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0143, grad_fn=<MinBackward1>), tensor(0.9224, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12217461317777634\n",
      "@sample 699: tensor([[-0.0079,  0.0024,  0.0166, -0.0318,  0.0268],\n",
      "        [-0.0063, -0.0050,  0.0002,  0.0041, -0.0227],\n",
      "        [-0.0018, -0.0039, -0.0154,  0.0082, -0.0246],\n",
      "        [-0.0156, -0.0242, -0.0302,  0.0400, -0.0239],\n",
      "        [ 0.0318, -0.0017,  0.0091,  0.0481, -0.0212],\n",
      "        [ 0.0184, -0.0296, -0.0007,  0.0254,  0.0105],\n",
      "        [-0.0264, -0.0435,  0.0236,  0.0213, -0.0220],\n",
      "        [ 0.0080, -0.0270, -0.0245, -0.0023,  0.0143]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0077, -0.0449, -0.0178,  0.0291, -0.0012],\n",
      "        [ 0.0037,  0.0246,  0.0104, -0.0216,  0.0057],\n",
      "        [-0.0011, -0.0014, -0.0301,  0.0008,  0.0216],\n",
      "        [ 0.0408,  0.0183,  0.0270, -0.0034,  0.0373],\n",
      "        [ 0.0207,  0.0336,  0.0575, -0.0200,  0.0145],\n",
      "        [ 0.0118,  0.0064,  0.0207, -0.0164, -0.0043],\n",
      "        [ 0.0235,  0.0005,  0.0141,  0.0087,  0.0293],\n",
      "        [-0.0203,  0.0086,  0.0244, -0.0139, -0.0056]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0139, grad_fn=<MinBackward1>), tensor(0.9115, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12398457527160645\n",
      "@sample 700: tensor([[ 0.0147, -0.0637,  0.0208,  0.0483, -0.0229],\n",
      "        [-0.0063, -0.0248, -0.0194,  0.0231, -0.0153],\n",
      "        [ 0.0273,  0.0063,  0.0002,  0.0293,  0.0027],\n",
      "        [-0.0377,  0.0025,  0.0145,  0.0034, -0.0043],\n",
      "        [ 0.0190,  0.0226,  0.0075, -0.0320,  0.0086],\n",
      "        [ 0.0112,  0.0258, -0.0025, -0.0100,  0.0161],\n",
      "        [ 0.0175,  0.0342, -0.0118, -0.0282,  0.0124],\n",
      "        [ 0.0022, -0.0043, -0.0157,  0.0009,  0.0025]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0147,  0.0491, -0.0174, -0.0207,  0.0220],\n",
      "        [ 0.0433,  0.0260,  0.0194, -0.0355, -0.0103],\n",
      "        [ 0.0258,  0.0548,  0.0248,  0.0131, -0.0349],\n",
      "        [ 0.0077,  0.0188,  0.0186, -0.0169,  0.0039],\n",
      "        [-0.0162,  0.0116, -0.0561,  0.0432, -0.0343],\n",
      "        [ 0.0006,  0.0474,  0.0041, -0.0291, -0.0335],\n",
      "        [-0.0278, -0.0061, -0.0454,  0.0322,  0.0157],\n",
      "        [ 0.0019, -0.0002,  0.0304,  0.0229, -0.0192]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0149, grad_fn=<MinBackward1>), tensor(0.8855, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13881778717041016\n",
      "@sample 701: tensor([[-0.0083,  0.0050, -0.0044,  0.0429,  0.0130],\n",
      "        [-0.0131, -0.0285,  0.0111,  0.0231, -0.0293],\n",
      "        [ 0.0010, -0.0300, -0.0378,  0.0288, -0.0356],\n",
      "        [-0.0049,  0.0266,  0.0111, -0.0271,  0.0153],\n",
      "        [ 0.0019,  0.0055, -0.0016, -0.0266, -0.0098],\n",
      "        [-0.0399,  0.0453,  0.0194, -0.0892,  0.0739],\n",
      "        [-0.0216, -0.0299, -0.0167, -0.0373,  0.0396],\n",
      "        [ 0.0093, -0.0304, -0.0254,  0.0314, -0.0401]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0052,  0.0473, -0.0402,  0.0078,  0.0217],\n",
      "        [-0.0006,  0.0085, -0.0439,  0.0034,  0.0109],\n",
      "        [ 0.0132,  0.0357,  0.0156, -0.0004,  0.0204],\n",
      "        [ 0.0122, -0.0175,  0.0016, -0.0012,  0.0170],\n",
      "        [-0.0003,  0.0160, -0.0523,  0.0294, -0.0108],\n",
      "        [-0.0234, -0.0679, -0.0872,  0.0088, -0.0406],\n",
      "        [-0.0663, -0.0161, -0.0236,  0.0008,  0.0212],\n",
      "        [-0.0327,  0.0234,  0.0397, -0.0230,  0.0083]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0155, grad_fn=<MinBackward1>), tensor(0.9110, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13177375495433807\n",
      "@sample 702: tensor([[ 0.0029,  0.0268, -0.0210,  0.0065,  0.0039],\n",
      "        [ 0.0112,  0.0048, -0.0336, -0.0039, -0.0226],\n",
      "        [ 0.0155,  0.0402,  0.0271, -0.0241,  0.0352],\n",
      "        [-0.0209,  0.0099,  0.0139,  0.0011,  0.0552],\n",
      "        [-0.0416, -0.0133, -0.0024,  0.0190, -0.0018],\n",
      "        [-0.0186,  0.0203,  0.0155,  0.0264, -0.0347],\n",
      "        [-0.0207, -0.0018, -0.0058, -0.0147,  0.0223],\n",
      "        [ 0.0068,  0.0336,  0.0659, -0.0248,  0.0213]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0042,  0.0321, -0.0305, -0.0050, -0.0127],\n",
      "        [-0.0350, -0.0359, -0.0317, -0.0061, -0.0215],\n",
      "        [ 0.0034, -0.0092, -0.0525,  0.0359,  0.0061],\n",
      "        [ 0.0039, -0.0423, -0.0576,  0.0146,  0.0186],\n",
      "        [ 0.0245,  0.0150,  0.0372, -0.0117, -0.0005],\n",
      "        [ 0.0424,  0.0509, -0.0435, -0.0205,  0.0224],\n",
      "        [-0.0052, -0.0070, -0.0222,  0.0351,  0.0077],\n",
      "        [-0.0708,  0.0017, -0.1151,  0.0156, -0.0119]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0182, grad_fn=<MinBackward1>), tensor(0.8597, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1225380077958107\n",
      "@sample 703: tensor([[ 0.0140,  0.0384, -0.0016,  0.0170, -0.0021],\n",
      "        [-0.0066,  0.0030,  0.0005,  0.0088,  0.0082],\n",
      "        [ 0.0241,  0.0071,  0.0007, -0.0045,  0.0246],\n",
      "        [-0.0167, -0.0024, -0.0290, -0.0109,  0.0229],\n",
      "        [-0.0353, -0.0042,  0.0258, -0.0119,  0.0294],\n",
      "        [ 0.0173, -0.0143,  0.0247,  0.0008, -0.0233],\n",
      "        [ 0.0246,  0.0319,  0.0186, -0.0452, -0.0180],\n",
      "        [-0.0340, -0.0097, -0.0089, -0.0010,  0.0116]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0069,  0.0173, -0.0353,  0.0212,  0.0271],\n",
      "        [ 0.0266,  0.0412,  0.0108,  0.0150,  0.0099],\n",
      "        [ 0.0028,  0.0028, -0.0245,  0.0019, -0.0003],\n",
      "        [ 0.0001,  0.0040,  0.0168, -0.0351, -0.0400],\n",
      "        [-0.0063,  0.0036,  0.0567, -0.0422, -0.0107],\n",
      "        [-0.0002,  0.0198,  0.0164, -0.0007, -0.0118],\n",
      "        [-0.0204, -0.0088, -0.0550,  0.0641,  0.0149],\n",
      "        [ 0.0384, -0.0097,  0.0592, -0.0084, -0.0218]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0114, grad_fn=<MinBackward1>), tensor(0.8970, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12363360077142715\n",
      "@sample 704: tensor([[-0.0224,  0.0196,  0.0079, -0.0025,  0.0061],\n",
      "        [-0.0193, -0.0407,  0.0153, -0.0141,  0.0255],\n",
      "        [ 0.0206,  0.0317,  0.0323, -0.0203,  0.0181],\n",
      "        [-0.0076, -0.0164, -0.0172,  0.0348,  0.0039],\n",
      "        [-0.0099, -0.0034,  0.0304,  0.0098,  0.0117],\n",
      "        [ 0.0046, -0.0164, -0.0086,  0.0153, -0.0319],\n",
      "        [ 0.0102, -0.0017, -0.0043,  0.0094,  0.0135],\n",
      "        [ 0.0260,  0.0516,  0.0094,  0.0094, -0.0012]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0245,  0.0171, -0.0441,  0.0218, -0.0125],\n",
      "        [-0.0013, -0.0464,  0.0438, -0.0311, -0.0311],\n",
      "        [ 0.0016, -0.0117, -0.0570, -0.0063, -0.0018],\n",
      "        [ 0.0135,  0.0332,  0.0464, -0.0170,  0.0445],\n",
      "        [ 0.0122,  0.0158, -0.0522,  0.0143, -0.0108],\n",
      "        [ 0.0005, -0.0004,  0.0548, -0.0499, -0.0137],\n",
      "        [-0.0099,  0.0187,  0.0104, -0.0112, -0.0184],\n",
      "        [-0.0140,  0.0023, -0.0509,  0.0485,  0.0023]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0233, grad_fn=<MinBackward1>), tensor(0.8836, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1256420612335205\n",
      "@sample 705: tensor([[-0.0179, -0.0041, -0.0165, -0.0073, -0.0117],\n",
      "        [-0.0205,  0.0127, -0.0338, -0.0298,  0.0072],\n",
      "        [ 0.0169,  0.0174, -0.0151,  0.0009, -0.0067],\n",
      "        [-0.0082, -0.0015, -0.0078, -0.0050,  0.0031],\n",
      "        [ 0.0036, -0.0058,  0.0126, -0.0057, -0.0032],\n",
      "        [-0.0039,  0.0053, -0.0104,  0.0180, -0.0188],\n",
      "        [-0.0234, -0.0075,  0.0467,  0.0260, -0.0122],\n",
      "        [-0.0068, -0.0145, -0.0539, -0.0122, -0.0004]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0035, -0.0051,  0.0420, -0.0220, -0.0158],\n",
      "        [ 0.0241, -0.0443,  0.0182, -0.0122,  0.0047],\n",
      "        [-0.0200, -0.0096, -0.0379,  0.0261,  0.0321],\n",
      "        [-0.0240, -0.0198,  0.0051, -0.0075, -0.0115],\n",
      "        [ 0.0135, -0.0011, -0.0094,  0.0078, -0.0210],\n",
      "        [ 0.0217, -0.0079, -0.0256,  0.0139,  0.0494],\n",
      "        [ 0.0046,  0.0219, -0.0105,  0.0216,  0.0145],\n",
      "        [-0.0137, -0.0095,  0.0783, -0.0413, -0.0032]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0171, grad_fn=<MinBackward1>), tensor(0.8754, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12123594433069229\n",
      "@sample 706: tensor([[-0.0176, -0.0351, -0.0361,  0.0165,  0.0107],\n",
      "        [-0.0402,  0.0258, -0.0151,  0.0237, -0.0206],\n",
      "        [-0.0120,  0.0110, -0.0215,  0.0064,  0.0068],\n",
      "        [-0.0100, -0.0328,  0.0299,  0.0066,  0.0006],\n",
      "        [ 0.0033,  0.0129, -0.0282, -0.0110,  0.0166],\n",
      "        [ 0.0042,  0.0436,  0.0571, -0.0423,  0.0049],\n",
      "        [ 0.0210, -0.0014, -0.0018,  0.0033,  0.0020],\n",
      "        [-0.0382,  0.0323,  0.0009, -0.0010, -0.0299]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0176, -0.0322,  0.0531, -0.0452, -0.0250],\n",
      "        [ 0.0081, -0.0099,  0.0224, -0.0334, -0.0154],\n",
      "        [ 0.0300,  0.0032,  0.0403, -0.0016,  0.0091],\n",
      "        [-0.0523, -0.0383,  0.0281, -0.0251,  0.0033],\n",
      "        [-0.0096,  0.0030, -0.0063, -0.0148,  0.0262],\n",
      "        [-0.0914, -0.0216, -0.0600,  0.0316, -0.0203],\n",
      "        [ 0.0133, -0.0058,  0.0286, -0.0022,  0.0026],\n",
      "        [-0.0039, -0.0384,  0.0107, -0.0282,  0.0061]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0166, grad_fn=<MinBackward1>), tensor(0.8859, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12810517847537994\n",
      "@sample 707: tensor([[-0.0011, -0.0164, -0.0236,  0.0285, -0.0273],\n",
      "        [ 0.0010,  0.0107, -0.0133,  0.0048, -0.0575],\n",
      "        [ 0.0423, -0.0082,  0.0092,  0.0347, -0.0032],\n",
      "        [ 0.0076,  0.0286,  0.0117, -0.0054, -0.0210],\n",
      "        [ 0.0055, -0.0020, -0.0441, -0.0164, -0.0179],\n",
      "        [-0.0051,  0.0038, -0.0125,  0.0064, -0.0118],\n",
      "        [ 0.0038, -0.0171, -0.0145,  0.0250, -0.0043],\n",
      "        [ 0.0147,  0.0607,  0.0170, -0.0473,  0.0364]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0302,  0.0110,  0.0182, -0.0246,  0.0136],\n",
      "        [ 0.0233, -0.0344,  0.0766, -0.0541, -0.0617],\n",
      "        [-0.0117, -0.0139, -0.0166, -0.0133, -0.0244],\n",
      "        [-0.0062, -0.0092, -0.0127,  0.0247, -0.0048],\n",
      "        [-0.0165, -0.0316,  0.0452, -0.0662, -0.0487],\n",
      "        [ 0.0014,  0.0038, -0.0123,  0.0211,  0.0228],\n",
      "        [ 0.0084,  0.0192,  0.0081, -0.0413, -0.0144],\n",
      "        [-0.0289,  0.0099, -0.0760,  0.0210, -0.0347]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0160, grad_fn=<MinBackward1>), tensor(0.8790, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1296636164188385\n",
      "@sample 708: tensor([[-0.0142,  0.0139, -0.0122, -0.0214,  0.0008],\n",
      "        [-0.0087,  0.0235,  0.0025, -0.0031,  0.0015],\n",
      "        [ 0.0093,  0.0101,  0.0136, -0.0291, -0.0189],\n",
      "        [-0.0013, -0.0120, -0.0024,  0.0031, -0.0284],\n",
      "        [ 0.0246, -0.0223,  0.0161, -0.0405, -0.0086],\n",
      "        [ 0.0011, -0.0036,  0.0026,  0.0169, -0.0045],\n",
      "        [-0.0165,  0.0487,  0.0153, -0.0309, -0.0203],\n",
      "        [ 0.0135,  0.0713,  0.0104, -0.0191,  0.0008]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0173, -0.0295,  0.0357, -0.0239,  0.0092],\n",
      "        [-0.0018,  0.0425,  0.0054, -0.0094,  0.0089],\n",
      "        [-0.0222, -0.0492, -0.0268,  0.0290,  0.0151],\n",
      "        [-0.0437, -0.0050,  0.0367, -0.0480, -0.0155],\n",
      "        [-0.0202, -0.0194, -0.0272, -0.0257, -0.0019],\n",
      "        [ 0.0085, -0.0023,  0.0187, -0.0013,  0.0169],\n",
      "        [-0.0176,  0.0531, -0.0345,  0.0233, -0.0212],\n",
      "        [-0.0192, -0.0038, -0.0594,  0.0009,  0.0271]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0200, grad_fn=<MinBackward1>), tensor(0.9034, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13114815950393677\n",
      "@sample 709: tensor([[-0.0210,  0.0184,  0.0359,  0.0101, -0.0125],\n",
      "        [-0.0138,  0.0068, -0.0084, -0.0041, -0.0066],\n",
      "        [ 0.0087, -0.0078, -0.0245,  0.0058, -0.0078],\n",
      "        [ 0.0095,  0.0405, -0.0028, -0.0096,  0.0070],\n",
      "        [-0.0273, -0.0080, -0.0259, -0.0031,  0.0029],\n",
      "        [ 0.0234,  0.0095,  0.0254, -0.0163,  0.0320],\n",
      "        [ 0.0234, -0.0109, -0.0153,  0.0380, -0.0393],\n",
      "        [-0.0287,  0.0416, -0.0284, -0.0174, -0.0264]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0020,  0.0078,  0.0135, -0.0136,  0.0179],\n",
      "        [-0.0033, -0.0024,  0.0115, -0.0202, -0.0096],\n",
      "        [-0.0216,  0.0183,  0.0162,  0.0176,  0.0194],\n",
      "        [ 0.0016,  0.0284, -0.0273, -0.0110,  0.0017],\n",
      "        [-0.0230, -0.0177, -0.0138, -0.0138, -0.0079],\n",
      "        [-0.0011,  0.0047,  0.0155,  0.0070,  0.0297],\n",
      "        [ 0.0310,  0.0114,  0.0755, -0.0410,  0.0058],\n",
      "        [-0.0040, -0.0433, -0.0135, -0.0373,  0.0118]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0157, grad_fn=<MinBackward1>), tensor(0.8545, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11193831264972687\n",
      "@sample 710: tensor([[-1.7520e-02,  2.8162e-02,  2.1905e-02, -3.5122e-03, -1.4415e-02],\n",
      "        [-3.8023e-03, -3.0110e-02,  8.9573e-03,  2.3501e-02, -1.4488e-03],\n",
      "        [ 2.7787e-02,  1.1662e-02, -4.6955e-02,  6.3334e-02, -5.7704e-02],\n",
      "        [-1.0537e-02, -2.3388e-03,  2.5059e-03,  1.8707e-02, -1.3575e-02],\n",
      "        [ 3.3273e-02,  3.1739e-02, -1.7981e-02,  6.5866e-03, -5.5745e-03],\n",
      "        [-2.0068e-02,  1.9796e-02,  1.5680e-02, -1.1369e-02,  3.2461e-02],\n",
      "        [-1.3270e-02,  2.7412e-02,  1.2642e-02,  2.2245e-02, -4.8645e-03],\n",
      "        [-5.0076e-05,  1.6619e-02, -1.3219e-02, -2.6771e-02,  4.2107e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0272,  0.0455,  0.0109,  0.0261,  0.0209],\n",
      "        [ 0.0203,  0.0208,  0.0340, -0.0253, -0.0116],\n",
      "        [ 0.0367,  0.0153,  0.0026, -0.0468,  0.0035],\n",
      "        [ 0.0026,  0.0290,  0.0385, -0.0280,  0.0303],\n",
      "        [-0.0096,  0.0076, -0.0213, -0.0076, -0.0257],\n",
      "        [-0.0008, -0.0150, -0.0715,  0.0404,  0.0201],\n",
      "        [ 0.0301,  0.0214, -0.0321,  0.0297,  0.0399],\n",
      "        [-0.0072, -0.0151, -0.0315, -0.0119,  0.0167]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0215, grad_fn=<MinBackward1>), tensor(0.8572, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11571092903614044\n",
      "@sample 711: tensor([[-0.0056,  0.0345,  0.0136, -0.0149, -0.0043],\n",
      "        [ 0.0116, -0.0199, -0.0106,  0.0266, -0.0166],\n",
      "        [ 0.0129,  0.0018,  0.0253, -0.0106, -0.0025],\n",
      "        [-0.0045,  0.0136, -0.0008,  0.0049, -0.0012],\n",
      "        [ 0.0003,  0.0638,  0.0196, -0.0381,  0.0074],\n",
      "        [ 0.0222,  0.0028, -0.0297, -0.0038,  0.0122],\n",
      "        [-0.0312,  0.0055, -0.0024, -0.0109, -0.0063],\n",
      "        [ 0.0036,  0.0106, -0.0267,  0.0286, -0.0504]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0087,  0.0235, -0.0264, -0.0164, -0.0103],\n",
      "        [ 0.0082,  0.0264, -0.0388,  0.0014,  0.0277],\n",
      "        [ 0.0201, -0.0047, -0.0397,  0.0213,  0.0097],\n",
      "        [ 0.0123,  0.0181, -0.0170, -0.0212, -0.0437],\n",
      "        [-0.0238, -0.0362, -0.0479,  0.0379,  0.0092],\n",
      "        [ 0.0402,  0.0122, -0.0326,  0.0074, -0.0045],\n",
      "        [-0.0336,  0.0031, -0.0034,  0.0288,  0.0362],\n",
      "        [ 0.0167,  0.0016,  0.0323, -0.0467, -0.0139]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0239, grad_fn=<MinBackward1>), tensor(0.8783, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12259738147258759\n",
      "@sample 712: tensor([[-1.5504e-02,  2.3047e-02,  3.0606e-02, -5.1917e-02,  2.9120e-02],\n",
      "        [ 1.5137e-03,  4.6544e-02,  3.2263e-03, -2.1554e-02,  1.4794e-02],\n",
      "        [-8.0483e-03,  2.7412e-02,  1.6179e-02, -1.9217e-02, -9.5292e-03],\n",
      "        [-3.1331e-02, -2.0592e-02, -2.0218e-02,  2.6923e-02, -2.0734e-02],\n",
      "        [ 2.3743e-04, -4.5375e-03, -2.2114e-02,  2.3707e-02, -5.7140e-04],\n",
      "        [-2.0285e-02, -9.0439e-03,  1.5191e-03, -2.3854e-02,  2.8999e-02],\n",
      "        [ 1.8511e-02, -1.3489e-03,  5.2561e-03, -3.7104e-05,  7.8020e-03],\n",
      "        [ 1.7662e-03,  1.4236e-03,  1.4022e-02, -6.3263e-03,  1.9735e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0003, -0.0319, -0.0188,  0.0427,  0.0040],\n",
      "        [-0.0092, -0.0009, -0.0470,  0.0409,  0.0272],\n",
      "        [-0.0186,  0.0214,  0.0429, -0.0321, -0.0181],\n",
      "        [ 0.0053,  0.0048,  0.0113, -0.0549, -0.0163],\n",
      "        [ 0.0045,  0.0255,  0.0257, -0.0311, -0.0252],\n",
      "        [-0.0112, -0.0005, -0.0026,  0.0229, -0.0063],\n",
      "        [-0.0341, -0.0054, -0.0215, -0.0040, -0.0123],\n",
      "        [ 0.0241,  0.0109, -0.0240,  0.0072,  0.0257]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0188, grad_fn=<MinBackward1>), tensor(0.9190, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11512383073568344\n",
      "@sample 713: tensor([[-0.0124,  0.0275,  0.0093,  0.0024, -0.0155],\n",
      "        [ 0.0210, -0.0087, -0.0289,  0.0286,  0.0218],\n",
      "        [ 0.0225, -0.0286,  0.0051,  0.0221, -0.0159],\n",
      "        [ 0.0003, -0.0028,  0.0298, -0.0107,  0.0176],\n",
      "        [-0.0181,  0.0322,  0.0316, -0.0279,  0.0414],\n",
      "        [ 0.0084,  0.0048,  0.0234, -0.0182,  0.0269],\n",
      "        [-0.0521,  0.0103,  0.0286, -0.0104,  0.0118],\n",
      "        [ 0.0147, -0.0061,  0.0082, -0.0136, -0.0097]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0089,  0.0008,  0.0196, -0.0310, -0.0319],\n",
      "        [ 0.0274,  0.0189, -0.0300,  0.0112,  0.0060],\n",
      "        [ 0.0327,  0.0270, -0.0024, -0.0144, -0.0102],\n",
      "        [-0.0159, -0.0313, -0.0115,  0.0275,  0.0158],\n",
      "        [-0.0189, -0.0418, -0.0229,  0.0022, -0.0017],\n",
      "        [ 0.0304, -0.0275, -0.0224,  0.0420,  0.0173],\n",
      "        [-0.0032, -0.0027, -0.0131,  0.0158,  0.0028],\n",
      "        [-0.0040,  0.0109,  0.0354, -0.0091, -0.0014]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0243, grad_fn=<MinBackward1>), tensor(0.8921, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1156664788722992\n",
      "@sample 714: tensor([[ 0.0060, -0.0206, -0.0007,  0.0414, -0.0315],\n",
      "        [ 0.0265, -0.0290,  0.0114, -0.0136, -0.0454],\n",
      "        [ 0.0161,  0.0350,  0.0127, -0.0350,  0.0069],\n",
      "        [-0.0078, -0.0068, -0.0137, -0.0017,  0.0107],\n",
      "        [-0.0292, -0.0303,  0.0053,  0.0235, -0.0066],\n",
      "        [-0.0275, -0.0083, -0.0044, -0.0123,  0.0115],\n",
      "        [-0.0055, -0.0403,  0.0026, -0.0150,  0.0042],\n",
      "        [ 0.0254,  0.0042,  0.0003,  0.0039, -0.0063]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0180, -0.0106,  0.0126,  0.0161, -0.0101],\n",
      "        [-0.0532,  0.0083, -0.0290, -0.0305, -0.0196],\n",
      "        [-0.0184, -0.0162, -0.0117,  0.0539,  0.0084],\n",
      "        [-0.0118, -0.0224, -0.0139, -0.0116, -0.0086],\n",
      "        [ 0.0047,  0.0023,  0.0476, -0.0099,  0.0030],\n",
      "        [-0.0114,  0.0203,  0.0229, -0.0045, -0.0076],\n",
      "        [ 0.0041, -0.0578,  0.0703, -0.0230,  0.0033],\n",
      "        [ 0.0023,  0.0215,  0.0121, -0.0183, -0.0180]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0139, grad_fn=<MinBackward1>), tensor(0.8932, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11763963848352432\n",
      "@sample 715: tensor([[ 1.0866e-02, -3.2074e-02,  3.2708e-03,  1.0111e-02,  2.0025e-02],\n",
      "        [ 1.6500e-02, -1.0610e-02, -1.0983e-03,  1.7988e-02,  6.6494e-03],\n",
      "        [-9.4905e-05, -3.4495e-02, -5.7141e-03, -3.6260e-02, -5.2377e-03],\n",
      "        [ 2.1039e-03,  3.3446e-02,  5.5928e-02, -2.6664e-02,  2.3561e-02],\n",
      "        [-2.4963e-03,  1.8240e-02,  3.5234e-02, -2.3719e-02,  3.6155e-02],\n",
      "        [-1.2915e-02,  2.7437e-02,  1.8451e-02, -2.6701e-02,  2.2720e-02],\n",
      "        [ 4.4990e-02, -3.1710e-02,  2.9716e-02, -9.6840e-03,  2.5000e-02],\n",
      "        [ 8.2312e-03,  2.1862e-02,  1.1645e-02, -1.0450e-02,  1.0304e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0011, -0.0011, -0.0241, -0.0001,  0.0066],\n",
      "        [-0.0144,  0.0116, -0.0180,  0.0094,  0.0014],\n",
      "        [-0.0481, -0.0140,  0.0192, -0.0049, -0.0009],\n",
      "        [ 0.0010, -0.0257,  0.0034,  0.0395,  0.0216],\n",
      "        [-0.0514,  0.0133, -0.0819,  0.0078,  0.0050],\n",
      "        [-0.0351, -0.0217, -0.0567,  0.0351,  0.0005],\n",
      "        [-0.0046, -0.0124, -0.0362,  0.0493,  0.0011],\n",
      "        [-0.0178, -0.0166, -0.0433, -0.0022, -0.0266]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0209, grad_fn=<MinBackward1>), tensor(0.9276, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11951196193695068\n",
      "@sample 716: tensor([[ 0.0051,  0.0196,  0.0442, -0.0207,  0.0063],\n",
      "        [ 0.0046, -0.0114, -0.0035, -0.0108,  0.0068],\n",
      "        [-0.0133,  0.0073, -0.0058, -0.0085,  0.0012],\n",
      "        [-0.0196, -0.0060,  0.0197, -0.0171,  0.0045],\n",
      "        [ 0.0026, -0.0496,  0.0461, -0.0229, -0.0060],\n",
      "        [-0.0042,  0.0012,  0.0006,  0.0069,  0.0017],\n",
      "        [-0.0184, -0.0309,  0.0006,  0.0026,  0.0151],\n",
      "        [ 0.0103,  0.0063,  0.0014, -0.0036,  0.0245]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0242,  0.0170, -0.0534,  0.0534,  0.0259],\n",
      "        [ 0.0185,  0.0179,  0.0449,  0.0109,  0.0009],\n",
      "        [-0.0103, -0.0017,  0.0027, -0.0195, -0.0145],\n",
      "        [-0.0064, -0.0132,  0.0292, -0.0015,  0.0103],\n",
      "        [-0.0523, -0.0499,  0.0287, -0.0104, -0.0398],\n",
      "        [ 0.0554,  0.0004,  0.0143,  0.0008,  0.0120],\n",
      "        [-0.0002, -0.0097,  0.0047,  0.0007,  0.0155],\n",
      "        [-0.0257,  0.0191, -0.0229, -0.0007, -0.0045]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0188, grad_fn=<MinBackward1>), tensor(0.8817, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11586342006921768\n",
      "@sample 717: tensor([[ 0.0023,  0.0542,  0.0099, -0.0319,  0.0408],\n",
      "        [ 0.0182, -0.0069, -0.0045,  0.0180, -0.0185],\n",
      "        [ 0.0350,  0.0100, -0.0076,  0.0167, -0.0193],\n",
      "        [ 0.0281, -0.0027, -0.0109,  0.0259, -0.0065],\n",
      "        [ 0.0313, -0.0091, -0.0098,  0.0295, -0.0182],\n",
      "        [-0.0048, -0.0157,  0.0011, -0.0128,  0.0051],\n",
      "        [-0.0038, -0.0558,  0.0014,  0.0185, -0.0007],\n",
      "        [-0.0017, -0.0227, -0.0121,  0.0072, -0.0127]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0003, -0.0158, -0.0619,  0.0614,  0.0003],\n",
      "        [ 0.0011,  0.0015, -0.0161,  0.0392,  0.0255],\n",
      "        [-0.0157, -0.0078, -0.0143,  0.0058, -0.0171],\n",
      "        [ 0.0030, -0.0060, -0.0333, -0.0119, -0.0099],\n",
      "        [-0.0135, -0.0170,  0.0012,  0.0008, -0.0134],\n",
      "        [-0.0170, -0.0078,  0.0279, -0.0372, -0.0289],\n",
      "        [-0.0076,  0.0123,  0.0272, -0.0238, -0.0304],\n",
      "        [-0.0285, -0.0039,  0.0121, -0.0328, -0.0207]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0187, grad_fn=<MinBackward1>), tensor(0.9219, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12204897403717041\n",
      "@sample 718: tensor([[-0.0070, -0.0190, -0.0027,  0.0231,  0.0017],\n",
      "        [-0.0152,  0.0147,  0.0086,  0.0045,  0.0047],\n",
      "        [-0.0240,  0.0081,  0.0191, -0.0101,  0.0146],\n",
      "        [ 0.0108, -0.0147,  0.0120,  0.0064,  0.0157],\n",
      "        [ 0.0032, -0.0083,  0.0161, -0.0422,  0.0414],\n",
      "        [ 0.0095, -0.0377, -0.0078,  0.0218,  0.0315],\n",
      "        [-0.0336, -0.0009, -0.0029,  0.0357, -0.0041],\n",
      "        [-0.0009,  0.0241,  0.0158, -0.0021,  0.0025]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0313, -0.0291,  0.0234, -0.0042, -0.0298],\n",
      "        [-0.0040,  0.0248,  0.0005,  0.0051, -0.0154],\n",
      "        [-0.0084,  0.0070, -0.0565,  0.0143,  0.0073],\n",
      "        [-0.0097,  0.0109,  0.0001,  0.0020,  0.0029],\n",
      "        [-0.0264,  0.0021, -0.0485,  0.0298,  0.0219],\n",
      "        [ 0.0157, -0.0023,  0.0082, -0.0144,  0.0211],\n",
      "        [ 0.0061,  0.0028,  0.0096,  0.0105, -0.0217],\n",
      "        [-0.0083, -0.0083, -0.0053,  0.0081, -0.0132]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0193, grad_fn=<MinBackward1>), tensor(0.8825, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12091287225484848\n",
      "@sample 719: tensor([[ 0.0006, -0.0083,  0.0200,  0.0048,  0.0113],\n",
      "        [-0.0017, -0.0150,  0.0087,  0.0226, -0.0156],\n",
      "        [-0.0003, -0.0140, -0.0314,  0.0130,  0.0050],\n",
      "        [-0.0013, -0.0148,  0.0031,  0.0164, -0.0138],\n",
      "        [-0.0237,  0.0238,  0.0189,  0.0065,  0.0225],\n",
      "        [-0.0152,  0.0099,  0.0082,  0.0083,  0.0161],\n",
      "        [-0.0063, -0.0391, -0.0085,  0.0291,  0.0023],\n",
      "        [-0.0218, -0.0247, -0.0405,  0.0271, -0.0199]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0019,  0.0148,  0.0088,  0.0228, -0.0118],\n",
      "        [ 0.0115, -0.0126,  0.0338,  0.0053, -0.0305],\n",
      "        [ 0.0310, -0.0155,  0.0284, -0.0059,  0.0151],\n",
      "        [ 0.0183,  0.0345, -0.0341, -0.0059, -0.0075],\n",
      "        [ 0.0032,  0.0393, -0.0227, -0.0486,  0.0043],\n",
      "        [ 0.0180,  0.0260, -0.0459,  0.0009, -0.0011],\n",
      "        [ 0.0314,  0.0316,  0.0586, -0.0312, -0.0128],\n",
      "        [ 0.0276, -0.0192,  0.0514, -0.0416,  0.0158]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0096, grad_fn=<MinBackward1>), tensor(0.9156, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1252373307943344\n",
      "@sample 720: tensor([[ 0.0047, -0.0050, -0.0507, -0.0038, -0.0024],\n",
      "        [ 0.0172, -0.0090, -0.0026,  0.0014, -0.0052],\n",
      "        [-0.0087,  0.0259, -0.0138, -0.0182,  0.0084],\n",
      "        [-0.0028,  0.0136,  0.0203, -0.0417,  0.0212],\n",
      "        [ 0.0076,  0.0042,  0.0043, -0.0142,  0.0206],\n",
      "        [ 0.0269,  0.0208, -0.0112, -0.0167,  0.0202],\n",
      "        [ 0.0106,  0.0154, -0.0239, -0.0405,  0.0460],\n",
      "        [-0.0024, -0.0095,  0.0194,  0.0094,  0.0165]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-7.3828e-05, -1.6724e-02, -3.2275e-02,  3.1445e-02,  2.9681e-02],\n",
      "        [ 1.6342e-03, -1.6741e-02, -4.2425e-02,  3.8368e-03, -2.9360e-02],\n",
      "        [-2.9158e-02, -5.1280e-02, -8.7348e-02,  5.1235e-02, -1.4394e-02],\n",
      "        [-2.8222e-02, -3.2859e-02, -2.4391e-02,  2.5377e-02,  1.9003e-02],\n",
      "        [ 2.0014e-02,  1.4778e-02, -5.1016e-03,  4.6750e-02, -9.4411e-03],\n",
      "        [-4.4918e-02, -4.0578e-02, -7.8290e-02,  5.9768e-02,  4.4513e-03],\n",
      "        [ 1.1149e-03, -2.2912e-02, -1.1178e-01,  5.8396e-02, -3.4331e-03],\n",
      "        [-4.4938e-03,  1.2479e-02, -2.4760e-03, -6.3552e-03,  8.4602e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0153, grad_fn=<MinBackward1>), tensor(0.8701, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12658822536468506\n",
      "@sample 721: tensor([[-0.0146,  0.0261,  0.0164, -0.0133,  0.0191],\n",
      "        [ 0.0086, -0.0054, -0.0250, -0.0289, -0.0256],\n",
      "        [-0.0184, -0.0039,  0.0046,  0.0118,  0.0008],\n",
      "        [ 0.0193, -0.0114,  0.0075,  0.0285, -0.0126],\n",
      "        [ 0.0179,  0.0096,  0.0109,  0.0227, -0.0037],\n",
      "        [-0.0328, -0.0122,  0.0111,  0.0075,  0.0340],\n",
      "        [ 0.0058, -0.0140, -0.0289,  0.0032,  0.0011],\n",
      "        [-0.0059, -0.0215, -0.0044,  0.0161, -0.0117]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0023,  0.0016, -0.0427, -0.0071, -0.0450],\n",
      "        [-0.0003, -0.0411, -0.0095, -0.0080,  0.0114],\n",
      "        [-0.0071,  0.0068,  0.0324,  0.0126, -0.0090],\n",
      "        [-0.0214,  0.0180, -0.0256,  0.0152,  0.0066],\n",
      "        [-0.0475,  0.0298,  0.0005,  0.0133,  0.0247],\n",
      "        [ 0.0077, -0.0280, -0.0197,  0.0572,  0.0566],\n",
      "        [ 0.0291, -0.0328,  0.0004, -0.0200,  0.0202],\n",
      "        [-0.0097,  0.0067,  0.0119,  0.0207, -0.0149]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0188, grad_fn=<MinBackward1>), tensor(0.8423, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11919845640659332\n",
      "@sample 722: tensor([[-0.0060,  0.0200,  0.0048, -0.0396,  0.0308],\n",
      "        [ 0.0199,  0.0103, -0.0101, -0.0220, -0.0173],\n",
      "        [ 0.0048,  0.0278, -0.0034, -0.0468,  0.0255],\n",
      "        [-0.0075, -0.0006,  0.0115,  0.0328,  0.0010],\n",
      "        [-0.0032, -0.0122,  0.0158,  0.0152,  0.0200],\n",
      "        [ 0.0195,  0.0597,  0.0304, -0.0376,  0.0400],\n",
      "        [ 0.0020, -0.0375, -0.0265,  0.0420, -0.0039],\n",
      "        [ 0.0062,  0.0070, -0.0220,  0.0008,  0.0279]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0457, -0.0492,  0.0145,  0.0117,  0.0273],\n",
      "        [-0.0335,  0.0189, -0.0401,  0.0275,  0.0167],\n",
      "        [-0.0524, -0.0414, -0.0336,  0.0107, -0.0084],\n",
      "        [ 0.0174,  0.0292,  0.0088,  0.0198, -0.0480],\n",
      "        [-0.0121,  0.0134, -0.0065,  0.0166, -0.0049],\n",
      "        [-0.0366,  0.0099, -0.0739,  0.0360, -0.0086],\n",
      "        [ 0.0747,  0.0152,  0.0602, -0.0010,  0.0222],\n",
      "        [ 0.0065, -0.0309, -0.0764,  0.0394,  0.0119]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0170, grad_fn=<MinBackward1>), tensor(0.9241, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.133596271276474\n",
      "@sample 723: tensor([[ 0.0546,  0.0408, -0.0155, -0.0177, -0.0009],\n",
      "        [-0.0248, -0.0168,  0.0202, -0.0110, -0.0141],\n",
      "        [-0.0004,  0.0298, -0.0170, -0.0114, -0.0059],\n",
      "        [-0.0194, -0.0162,  0.0025, -0.0170, -0.0016],\n",
      "        [ 0.0052,  0.0260,  0.0281, -0.0455,  0.0293],\n",
      "        [ 0.0221,  0.0082,  0.0124, -0.0046,  0.0158],\n",
      "        [-0.0014, -0.0068,  0.0101, -0.0002,  0.0132],\n",
      "        [-0.0014,  0.0138,  0.0091, -0.0265, -0.0156]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0012, -0.0228, -0.0652,  0.0687, -0.0210],\n",
      "        [ 0.0152, -0.0240, -0.0452,  0.0166,  0.0347],\n",
      "        [-0.0018, -0.0101, -0.0362,  0.0188, -0.0008],\n",
      "        [-0.0455, -0.0467,  0.0539, -0.0201, -0.0454],\n",
      "        [-0.0130, -0.0282, -0.0694,  0.0186, -0.0058],\n",
      "        [ 0.0044, -0.0154, -0.0263,  0.0220, -0.0061],\n",
      "        [ 0.0158,  0.0114,  0.0178, -0.0118, -0.0267],\n",
      "        [-0.0058, -0.0058, -0.0444,  0.0189, -0.0132]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0168, grad_fn=<MinBackward1>), tensor(0.8892, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12440983206033707\n",
      "@sample 724: tensor([[ 0.0220,  0.0087, -0.0157,  0.0064,  0.0062],\n",
      "        [ 0.0230, -0.0255, -0.0281,  0.0118, -0.0288],\n",
      "        [-0.0178,  0.0663,  0.0145, -0.0357,  0.0196],\n",
      "        [ 0.0079,  0.0801,  0.0315, -0.0088,  0.0232],\n",
      "        [ 0.0252,  0.0048, -0.0034, -0.0028,  0.0008],\n",
      "        [ 0.0102, -0.0138, -0.0330,  0.0379, -0.0347],\n",
      "        [ 0.0092, -0.0234, -0.0233, -0.0065,  0.0009],\n",
      "        [-0.0192,  0.0151, -0.0143, -0.0292, -0.0185]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-2.0052e-02,  2.1218e-02, -4.8273e-02, -1.4232e-02,  8.1629e-03],\n",
      "        [-1.2771e-02,  2.2588e-02, -1.4825e-02,  9.2891e-03,  3.6704e-02],\n",
      "        [-6.3330e-06, -9.4467e-03, -5.7114e-02,  1.2129e-02,  1.3548e-02],\n",
      "        [ 2.4178e-02, -3.2989e-02, -6.2893e-02,  1.0592e-02, -1.5243e-02],\n",
      "        [-5.3291e-03, -1.4255e-02, -1.6240e-02, -2.8133e-02, -1.7288e-02],\n",
      "        [ 2.6701e-02,  4.1033e-03,  9.6568e-03, -1.8133e-02, -2.3743e-02],\n",
      "        [-1.3164e-02, -2.4000e-03, -3.2854e-02,  2.9964e-02, -6.5290e-03],\n",
      "        [ 2.5533e-05, -2.4663e-02,  1.3955e-03, -3.1426e-02, -3.1308e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0157, grad_fn=<MinBackward1>), tensor(0.8532, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11745450645685196\n",
      "@sample 725: tensor([[ 0.0109, -0.0103, -0.0300,  0.0247, -0.0036],\n",
      "        [-0.0222,  0.0112,  0.0044,  0.0166,  0.0096],\n",
      "        [ 0.0082,  0.0018, -0.0066,  0.0399, -0.0178],\n",
      "        [-0.0075,  0.0086, -0.0434, -0.0048, -0.0238],\n",
      "        [ 0.0238,  0.0172, -0.0091, -0.0157, -0.0047],\n",
      "        [ 0.0055,  0.0087, -0.0167,  0.0088, -0.0172],\n",
      "        [ 0.0227, -0.0001, -0.0002, -0.0021,  0.0270],\n",
      "        [ 0.0184,  0.0233,  0.0114, -0.0181,  0.0344]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0426,  0.0345,  0.0553, -0.0137, -0.0211],\n",
      "        [ 0.0077, -0.0103,  0.0070, -0.0280, -0.0034],\n",
      "        [ 0.0121,  0.0140,  0.0073, -0.0085, -0.0168],\n",
      "        [ 0.0173, -0.0026,  0.0210, -0.0399, -0.0327],\n",
      "        [-0.0067,  0.0075, -0.0731,  0.0156,  0.0192],\n",
      "        [-0.0047,  0.0235,  0.0018, -0.0200, -0.0136],\n",
      "        [ 0.0176,  0.0129, -0.0652,  0.0493,  0.0136],\n",
      "        [-0.0054, -0.0030, -0.0375,  0.0572,  0.0083]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0202, grad_fn=<MinBackward1>), tensor(0.8773, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11623416841030121\n",
      "@sample 726: tensor([[ 9.2502e-04, -5.3723e-03,  6.9546e-03,  1.3867e-02,  1.4464e-02],\n",
      "        [ 9.7150e-03,  3.5852e-02, -2.3188e-02, -4.3476e-02,  3.4447e-02],\n",
      "        [ 6.8003e-03,  1.1684e-02,  1.8140e-02, -6.3912e-03,  1.9950e-02],\n",
      "        [ 1.9115e-02,  1.9269e-02,  5.5420e-03, -2.6328e-03,  3.7726e-02],\n",
      "        [-1.3819e-02, -4.2014e-03, -3.7819e-03, -4.9950e-03, -1.4430e-02],\n",
      "        [ 2.9924e-02, -6.3353e-03, -4.4501e-04, -1.5601e-02,  1.6035e-02],\n",
      "        [-2.9690e-02,  5.4583e-05, -1.0903e-02, -3.7661e-03, -1.0470e-02],\n",
      "        [ 2.0825e-02, -1.1639e-02,  1.7603e-02, -1.4363e-02,  1.1342e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0128,  0.0095,  0.0121, -0.0197, -0.0207],\n",
      "        [ 0.0228, -0.0023, -0.0634,  0.0281, -0.0204],\n",
      "        [-0.0069, -0.0133, -0.0242, -0.0136,  0.0017],\n",
      "        [ 0.0176, -0.0160, -0.0518, -0.0072,  0.0119],\n",
      "        [ 0.0101, -0.0004,  0.0230, -0.0383, -0.0101],\n",
      "        [ 0.0057, -0.0243, -0.0540,  0.0481,  0.0538],\n",
      "        [-0.0152,  0.0018,  0.0037, -0.0478,  0.0022],\n",
      "        [-0.0271, -0.0011, -0.0464,  0.0442, -0.0335]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0121, grad_fn=<MinBackward1>), tensor(0.9034, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1191929429769516\n",
      "@sample 727: tensor([[ 0.0387,  0.0584, -0.0068, -0.0305,  0.0075],\n",
      "        [ 0.0050, -0.0122,  0.0126,  0.0066,  0.0253],\n",
      "        [ 0.0011,  0.0393, -0.0020, -0.0318,  0.0078],\n",
      "        [ 0.0069, -0.0600,  0.0122,  0.0088, -0.0015],\n",
      "        [ 0.0191, -0.0274,  0.0254,  0.0217, -0.0074],\n",
      "        [-0.0031, -0.0018, -0.0277,  0.0145,  0.0130],\n",
      "        [-0.0014, -0.0043,  0.0328,  0.0104, -0.0170],\n",
      "        [ 0.0144, -0.0647, -0.0097,  0.0175,  0.0303]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0262, -0.0100, -0.0878,  0.0476,  0.0390],\n",
      "        [ 0.0007, -0.0076, -0.0040,  0.0252, -0.0354],\n",
      "        [-0.0249, -0.0205, -0.0774, -0.0149,  0.0218],\n",
      "        [-0.0094, -0.0122,  0.0366, -0.0159, -0.0323],\n",
      "        [-0.0057,  0.0468,  0.0061,  0.0054, -0.0373],\n",
      "        [ 0.0552,  0.0204,  0.0593, -0.0632, -0.0328],\n",
      "        [-0.0416,  0.0247, -0.0410,  0.0466, -0.0051],\n",
      "        [ 0.0234, -0.0034,  0.0871, -0.0409, -0.0352]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0125, grad_fn=<MinBackward1>), tensor(0.8636, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11848762631416321\n",
      "@sample 728: tensor([[ 0.0039, -0.0183, -0.0030,  0.0250, -0.0264],\n",
      "        [ 0.0044, -0.0034, -0.0185,  0.0416, -0.0175],\n",
      "        [ 0.0316,  0.0135, -0.0327,  0.0085, -0.0296],\n",
      "        [ 0.0067, -0.0010, -0.0012, -0.0180,  0.0326],\n",
      "        [-0.0185, -0.0122, -0.0107,  0.0026, -0.0001],\n",
      "        [-0.0067,  0.0014,  0.0084,  0.0197, -0.0306],\n",
      "        [-0.0002, -0.0358, -0.0026,  0.0077, -0.0021],\n",
      "        [ 0.0219,  0.0742,  0.0595, -0.0289,  0.0131]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0373, -0.0008,  0.0268,  0.0472,  0.0231],\n",
      "        [ 0.0135,  0.0322,  0.0146,  0.0022,  0.0108],\n",
      "        [-0.0106, -0.0064, -0.0572, -0.0158,  0.0432],\n",
      "        [ 0.0086, -0.0089,  0.0359,  0.0205,  0.0158],\n",
      "        [-0.0206, -0.0074,  0.0253, -0.0115,  0.0134],\n",
      "        [ 0.0141, -0.0204, -0.0018,  0.0013, -0.0465],\n",
      "        [-0.0033,  0.0238,  0.0063,  0.0051,  0.0091],\n",
      "        [ 0.0238, -0.0159, -0.0166, -0.0110, -0.0431]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0165, grad_fn=<MinBackward1>), tensor(0.9057, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12678159773349762\n",
      "@sample 729: tensor([[-0.0117, -0.0194,  0.0072,  0.0220, -0.0130],\n",
      "        [-0.0474,  0.0160, -0.0056, -0.0447, -0.0029],\n",
      "        [ 0.0116,  0.0073, -0.0101, -0.0088,  0.0086],\n",
      "        [ 0.0056,  0.0048, -0.0044, -0.0081,  0.0143],\n",
      "        [ 0.0016,  0.0189, -0.0158,  0.0082,  0.0012],\n",
      "        [-0.0018,  0.0083, -0.0196,  0.0182, -0.0089],\n",
      "        [-0.0171,  0.0186,  0.0011, -0.0071, -0.0125],\n",
      "        [ 0.0112,  0.0164,  0.0165, -0.0263,  0.0092]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 2.0515e-02, -2.9671e-03,  2.6992e-02, -9.6322e-03,  5.6526e-03],\n",
      "        [-3.4381e-02, -2.1883e-02, -5.6121e-02,  2.4513e-02, -7.9065e-03],\n",
      "        [-5.1117e-03,  5.9040e-04,  2.5608e-02,  9.4596e-03, -9.1792e-03],\n",
      "        [ 1.1325e-06, -6.7115e-03, -3.3553e-02,  1.4357e-02, -1.6104e-03],\n",
      "        [ 1.7929e-02,  3.3332e-03,  1.8687e-02, -2.2890e-02, -5.5103e-03],\n",
      "        [-1.1566e-03,  3.3108e-02,  1.1792e-02, -2.0655e-02,  5.5793e-03],\n",
      "        [-5.1542e-02, -9.0534e-03,  6.8482e-03, -3.2421e-02, -1.4565e-02],\n",
      "        [-4.0454e-03, -2.3046e-02, -2.3057e-02,  2.6945e-02,  2.9492e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0127, grad_fn=<MinBackward1>), tensor(0.8934, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11189953982830048\n",
      "@sample 730: tensor([[ 0.0125, -0.0318, -0.0076,  0.0188, -0.0128],\n",
      "        [ 0.0217,  0.0246,  0.0082, -0.0194,  0.0017],\n",
      "        [ 0.0182,  0.0220,  0.0097, -0.0152,  0.0220],\n",
      "        [ 0.0035, -0.0126, -0.0008,  0.0262, -0.0367],\n",
      "        [-0.0341, -0.0117,  0.0097, -0.0081, -0.0031],\n",
      "        [-0.0053,  0.0132, -0.0435,  0.0346, -0.0345],\n",
      "        [ 0.0209,  0.0252,  0.0023, -0.0231, -0.0514],\n",
      "        [-0.0148,  0.0106,  0.0229,  0.0045,  0.0130]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0417, -0.0060,  0.0544,  0.0039,  0.0192],\n",
      "        [-0.0152,  0.0125,  0.0105,  0.0100, -0.0068],\n",
      "        [-0.0281, -0.0122, -0.0544,  0.0059, -0.0393],\n",
      "        [ 0.0191,  0.0195, -0.0013,  0.0089,  0.0115],\n",
      "        [-0.0264, -0.0031, -0.0123, -0.0136, -0.0011],\n",
      "        [ 0.0211,  0.0138,  0.0173, -0.0163, -0.0044],\n",
      "        [-0.0409, -0.0334, -0.0007,  0.0254,  0.0189],\n",
      "        [ 0.0142,  0.0271, -0.0364, -0.0336, -0.0185]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0163, grad_fn=<MinBackward1>), tensor(0.8598, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11349665373563766\n",
      "@sample 731: tensor([[-0.0031, -0.0019, -0.0145,  0.0246, -0.0068],\n",
      "        [ 0.0118,  0.0006,  0.0084, -0.0339,  0.0089],\n",
      "        [ 0.0176, -0.0061, -0.0066,  0.0197, -0.0137],\n",
      "        [ 0.0279,  0.0357,  0.0478, -0.0709,  0.0420],\n",
      "        [ 0.0096,  0.0144, -0.0029, -0.0054, -0.0119],\n",
      "        [-0.0199, -0.0242,  0.0039,  0.0042,  0.0042],\n",
      "        [-0.0121,  0.0115, -0.0003, -0.0433,  0.0222],\n",
      "        [-0.0092,  0.0215, -0.0065, -0.0136,  0.0098]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0145,  0.0170,  0.0325,  0.0103, -0.0118],\n",
      "        [ 0.0068, -0.0126, -0.0411,  0.0558,  0.0044],\n",
      "        [ 0.0220,  0.0202,  0.0221, -0.0003, -0.0042],\n",
      "        [-0.0355, -0.0433, -0.0677,  0.0516, -0.0293],\n",
      "        [ 0.0096, -0.0082,  0.0103, -0.0518, -0.0303],\n",
      "        [-0.0046, -0.0434,  0.0682, -0.0208, -0.0167],\n",
      "        [-0.0328,  0.0099, -0.0580,  0.0270, -0.0074],\n",
      "        [-0.0217,  0.0105, -0.0335, -0.0226, -0.0222]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0205, grad_fn=<MinBackward1>), tensor(0.8884, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12891528010368347\n",
      "@sample 732: tensor([[-0.0250, -0.0081,  0.0066,  0.0275, -0.0109],\n",
      "        [ 0.0010,  0.0823,  0.0146, -0.0974,  0.0025],\n",
      "        [-0.0134, -0.0218, -0.0204,  0.0050,  0.0259],\n",
      "        [ 0.0212, -0.0023, -0.0107, -0.0070,  0.0034],\n",
      "        [-0.0154,  0.0263,  0.0427, -0.0192, -0.0201],\n",
      "        [ 0.0431,  0.0097,  0.0236, -0.0321, -0.0087],\n",
      "        [ 0.0029,  0.0208, -0.0197, -0.0163, -0.0055],\n",
      "        [ 0.0078, -0.0002, -0.0045,  0.0060, -0.0198]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0100,  0.0006,  0.0786, -0.0314, -0.0283],\n",
      "        [-0.0134, -0.0324,  0.0177,  0.0058,  0.0252],\n",
      "        [ 0.0339, -0.0148,  0.0414, -0.0047, -0.0031],\n",
      "        [-0.0205,  0.0182,  0.0014, -0.0051,  0.0229],\n",
      "        [-0.0339,  0.0280, -0.0320, -0.0385, -0.0051],\n",
      "        [-0.0031,  0.0121,  0.0279,  0.0658,  0.0113],\n",
      "        [ 0.0259,  0.0176, -0.0235, -0.0040,  0.0113],\n",
      "        [-0.0209,  0.0182,  0.0030, -0.0101,  0.0268]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0132, grad_fn=<MinBackward1>), tensor(0.8785, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12623800337314606\n",
      "@sample 733: tensor([[ 0.0263,  0.0203, -0.0097, -0.0246, -0.0211],\n",
      "        [ 0.0145,  0.0410,  0.0208, -0.0244,  0.0245],\n",
      "        [-0.0059,  0.0409,  0.0297, -0.0532,  0.0384],\n",
      "        [-0.0167, -0.0319, -0.0163,  0.0110, -0.0114],\n",
      "        [ 0.0367,  0.0114,  0.0001,  0.0181, -0.0088],\n",
      "        [ 0.0297, -0.0149, -0.0262, -0.0221,  0.0290],\n",
      "        [-0.0038, -0.0206, -0.0010, -0.0084,  0.0128],\n",
      "        [ 0.0058,  0.0389,  0.0199, -0.0122,  0.0083]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0338,  0.0343, -0.0458,  0.0341,  0.0031],\n",
      "        [-0.0175,  0.0064, -0.0293,  0.0010,  0.0349],\n",
      "        [-0.0078, -0.0097, -0.0527,  0.0120, -0.0185],\n",
      "        [ 0.0113,  0.0030,  0.0419, -0.0322,  0.0237],\n",
      "        [ 0.0376,  0.0081,  0.0094, -0.0047, -0.0043],\n",
      "        [-0.0017, -0.0353, -0.0415,  0.0275,  0.0362],\n",
      "        [ 0.0166,  0.0240,  0.0097, -0.0175, -0.0020],\n",
      "        [ 0.0130, -0.0197, -0.0426,  0.0041, -0.0049]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0120, grad_fn=<MinBackward1>), tensor(0.8691, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11150635778903961\n",
      "@sample 734: tensor([[ 0.0019,  0.0121,  0.0093, -0.0156,  0.0068],\n",
      "        [-0.0141,  0.0056, -0.0134,  0.0316,  0.0096],\n",
      "        [ 0.0058,  0.0206,  0.0030,  0.0011,  0.0009],\n",
      "        [-0.0040,  0.0241,  0.0155,  0.0063, -0.0096],\n",
      "        [-0.0092, -0.0115, -0.0037,  0.0048,  0.0011],\n",
      "        [-0.0072, -0.0062,  0.0214, -0.0235,  0.0013],\n",
      "        [-0.0265,  0.0225, -0.0010, -0.0203, -0.0047],\n",
      "        [-0.0159, -0.0072,  0.0195, -0.0387,  0.0239]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0191,  0.0038, -0.0409,  0.0131,  0.0228],\n",
      "        [ 0.0037,  0.0290,  0.0118, -0.0578, -0.0158],\n",
      "        [-0.0250, -0.0224, -0.0293,  0.0099, -0.0120],\n",
      "        [ 0.0045,  0.0242, -0.0318, -0.0109, -0.0053],\n",
      "        [-0.0042, -0.0039,  0.0593, -0.0463, -0.0084],\n",
      "        [-0.0238, -0.0012,  0.0180,  0.0042,  0.0010],\n",
      "        [-0.0354, -0.0222, -0.0289, -0.0054,  0.0147],\n",
      "        [-0.0239, -0.0175,  0.0311, -0.0075, -0.0118]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0163, grad_fn=<MinBackward1>), tensor(0.8872, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1079719215631485\n",
      "@sample 735: tensor([[ 0.0409, -0.0118,  0.0295, -0.0450,  0.0099],\n",
      "        [ 0.0051,  0.0006,  0.0202,  0.0121,  0.0107],\n",
      "        [-0.0032, -0.0119,  0.0110, -0.0107,  0.0031],\n",
      "        [-0.0072, -0.0175,  0.0180,  0.0104, -0.0249],\n",
      "        [-0.0041, -0.0134,  0.0076, -0.0035, -0.0123],\n",
      "        [ 0.0108, -0.0382,  0.0064,  0.0265, -0.0246],\n",
      "        [ 0.0033, -0.0177,  0.0170, -0.0305,  0.0571],\n",
      "        [-0.0133, -0.0033,  0.0126, -0.0153,  0.0072]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0160, -0.0197, -0.0430, -0.0275, -0.0137],\n",
      "        [-0.0037,  0.0200,  0.0163, -0.0110, -0.0014],\n",
      "        [-0.0029, -0.0048,  0.0581, -0.0058,  0.0242],\n",
      "        [-0.0152,  0.0098,  0.0641,  0.0025,  0.0156],\n",
      "        [ 0.0202, -0.0135,  0.0212,  0.0018,  0.0190],\n",
      "        [-0.0205, -0.0176, -0.0056, -0.0424,  0.0102],\n",
      "        [ 0.0384, -0.0025,  0.0149, -0.0012, -0.0270],\n",
      "        [-0.0160, -0.0211,  0.0252, -0.0230, -0.0194]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0184, grad_fn=<MinBackward1>), tensor(0.8965, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11710745841264725\n",
      "@sample 736: tensor([[-0.0278, -0.0044,  0.0069, -0.0393,  0.0221],\n",
      "        [-0.0137, -0.0230,  0.0214,  0.0183, -0.0085],\n",
      "        [ 0.0261,  0.0204,  0.0200, -0.0009, -0.0101],\n",
      "        [ 0.0483,  0.0192, -0.0343, -0.0100, -0.0392],\n",
      "        [-0.0123, -0.0061,  0.0204, -0.0168, -0.0009],\n",
      "        [-0.0333,  0.0241,  0.0155, -0.0545,  0.0077],\n",
      "        [ 0.0035,  0.0025, -0.0250,  0.0031, -0.0184],\n",
      "        [ 0.0186,  0.0262, -0.0081, -0.0252,  0.0159]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0224, -0.0499,  0.0476,  0.0170, -0.0022],\n",
      "        [ 0.0274,  0.0062,  0.0857, -0.0269,  0.0055],\n",
      "        [-0.0051,  0.0280, -0.0184,  0.0340, -0.0215],\n",
      "        [-0.0146,  0.0096, -0.0175,  0.0112,  0.0236],\n",
      "        [ 0.0106, -0.0033,  0.0454, -0.0126,  0.0173],\n",
      "        [ 0.0141, -0.0288, -0.0253,  0.0243,  0.0068],\n",
      "        [ 0.0234,  0.0053, -0.0195,  0.0279,  0.0239],\n",
      "        [ 0.0154, -0.0512, -0.0481, -0.0045,  0.0139]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0156, grad_fn=<MinBackward1>), tensor(0.8885, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11831749230623245\n",
      "@sample 737: tensor([[ 0.0063, -0.0189, -0.0071,  0.0174, -0.0002],\n",
      "        [ 0.0013, -0.0060,  0.0099,  0.0044,  0.0047],\n",
      "        [-0.0144,  0.0124, -0.0031,  0.0125, -0.0013],\n",
      "        [-0.0059, -0.0110,  0.0184,  0.0011, -0.0228],\n",
      "        [-0.0156,  0.0170, -0.0097,  0.0224, -0.0289],\n",
      "        [-0.0244,  0.0105, -0.0080, -0.0194,  0.0045],\n",
      "        [-0.0157,  0.0103,  0.0046,  0.0216, -0.0211],\n",
      "        [-0.0047,  0.0242, -0.0036, -0.0392,  0.0060]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0229, -0.0353, -0.0192, -0.0091, -0.0094],\n",
      "        [ 0.0081,  0.0047, -0.0479,  0.0425, -0.0035],\n",
      "        [-0.0024,  0.0061, -0.0460, -0.0165,  0.0174],\n",
      "        [-0.0083,  0.0115,  0.0422, -0.0494, -0.0523],\n",
      "        [-0.0167,  0.0024, -0.0041,  0.0449,  0.0033],\n",
      "        [ 0.0064, -0.0335, -0.0487,  0.0072,  0.0303],\n",
      "        [-0.0162,  0.0443, -0.0088, -0.0041,  0.0154],\n",
      "        [-0.0016, -0.0275, -0.0791,  0.0435,  0.0009]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0141, grad_fn=<MinBackward1>), tensor(0.9119, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10795798152685165\n",
      "@sample 738: tensor([[ 0.0098, -0.0380,  0.0215,  0.0075,  0.0131],\n",
      "        [-0.0093,  0.0270,  0.0730, -0.0038,  0.0056],\n",
      "        [ 0.0038,  0.0545,  0.0663, -0.0001,  0.0321],\n",
      "        [-0.0277,  0.0188, -0.0227,  0.0036, -0.0155],\n",
      "        [ 0.0268,  0.0047,  0.0673, -0.0052, -0.0271],\n",
      "        [-0.0041, -0.0056, -0.0004,  0.0024, -0.0213],\n",
      "        [ 0.0072,  0.0032, -0.0007, -0.0263, -0.0088],\n",
      "        [ 0.0063, -0.0356,  0.0270,  0.0372, -0.0176]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0147,  0.0176,  0.0114,  0.0090,  0.0108],\n",
      "        [-0.0143,  0.0264, -0.0950,  0.0396,  0.0130],\n",
      "        [-0.0028,  0.0160, -0.0902,  0.0025, -0.0425],\n",
      "        [-0.0036,  0.0029, -0.0056, -0.0023,  0.0206],\n",
      "        [-0.0710,  0.0077, -0.0548,  0.0729,  0.0251],\n",
      "        [ 0.0017,  0.0333,  0.0051, -0.0045,  0.0209],\n",
      "        [ 0.0189, -0.0186,  0.0310, -0.0268, -0.0078],\n",
      "        [-0.0423, -0.0070, -0.0672,  0.0661,  0.0243]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0147, grad_fn=<MinBackward1>), tensor(0.9069, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1416902244091034\n",
      "@sample 739: tensor([[ 0.0154,  0.0154,  0.0021, -0.0351, -0.0051],\n",
      "        [ 0.0043, -0.0137, -0.0216, -0.0001, -0.0016],\n",
      "        [ 0.0108,  0.0009,  0.0474, -0.0399,  0.0059],\n",
      "        [-0.0081,  0.0221,  0.0311,  0.0099, -0.0092],\n",
      "        [ 0.0281, -0.0005,  0.0106,  0.0046, -0.0192],\n",
      "        [-0.0021,  0.0129,  0.0130, -0.0336,  0.0183],\n",
      "        [ 0.0067, -0.0172,  0.0312,  0.0179, -0.0153],\n",
      "        [ 0.0023, -0.0336, -0.0119,  0.0277, -0.0128]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0445, -0.0384, -0.0296,  0.0147,  0.0143],\n",
      "        [-0.0006, -0.0126, -0.0217,  0.0191, -0.0227],\n",
      "        [-0.0277,  0.0039, -0.0766,  0.0329,  0.0340],\n",
      "        [ 0.0092,  0.0011, -0.0108,  0.0105,  0.0169],\n",
      "        [-0.0104,  0.0116, -0.0192,  0.0089,  0.0377],\n",
      "        [-0.0273,  0.0012, -0.0662,  0.0333,  0.0126],\n",
      "        [-0.0299,  0.0162, -0.0196,  0.0391,  0.0243],\n",
      "        [ 0.0072,  0.0292,  0.0446, -0.0073,  0.0196]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0137, grad_fn=<MinBackward1>), tensor(0.9007, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1197492852807045\n",
      "@sample 740: tensor([[-0.0187, -0.0220,  0.0002,  0.0179,  0.0123],\n",
      "        [ 0.0215, -0.0011,  0.0035, -0.0049,  0.0045],\n",
      "        [ 0.0014, -0.0144, -0.0102,  0.0281, -0.0233],\n",
      "        [ 0.0041, -0.0152, -0.0093,  0.0291, -0.0097],\n",
      "        [-0.0175, -0.0268, -0.0308,  0.0145,  0.0085],\n",
      "        [-0.0118,  0.0224,  0.0290, -0.0482, -0.0079],\n",
      "        [-0.0079, -0.0063,  0.0040,  0.0171, -0.0075],\n",
      "        [-0.0009, -0.0127, -0.0513,  0.0020,  0.0270]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0108, -0.0202,  0.0255, -0.0465, -0.0188],\n",
      "        [-0.0315,  0.0059, -0.0288,  0.0218,  0.0059],\n",
      "        [ 0.0277,  0.0316,  0.0156, -0.0257,  0.0208],\n",
      "        [ 0.0513,  0.0179,  0.0292, -0.0263, -0.0265],\n",
      "        [ 0.0237, -0.0292, -0.0012, -0.0291,  0.0026],\n",
      "        [-0.0398, -0.0397,  0.0479,  0.0190, -0.0202],\n",
      "        [-0.0341,  0.0471, -0.0010,  0.0105,  0.0164],\n",
      "        [ 0.0075, -0.0018,  0.0332, -0.0061,  0.0071]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0138, grad_fn=<MinBackward1>), tensor(0.9221, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12002719938755035\n",
      "@sample 741: tensor([[ 0.0042,  0.0079,  0.0080, -0.0144,  0.0008],\n",
      "        [ 0.0207,  0.0117, -0.0025, -0.0182,  0.0100],\n",
      "        [-0.0124, -0.0335, -0.0391,  0.0251,  0.0008],\n",
      "        [-0.0153, -0.0246, -0.0088,  0.0130, -0.0140],\n",
      "        [-0.0161,  0.0331, -0.0050, -0.0029,  0.0009],\n",
      "        [-0.0194, -0.0246,  0.0194, -0.0084, -0.0142],\n",
      "        [-0.0158,  0.0322, -0.0101, -0.0445,  0.0479],\n",
      "        [ 0.0061,  0.0010, -0.0031,  0.0218, -0.0391]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0149,  0.0002, -0.0262,  0.0099,  0.0288],\n",
      "        [-0.0298,  0.0021, -0.0393,  0.0097, -0.0022],\n",
      "        [ 0.0059,  0.0002, -0.0271, -0.0014,  0.0126],\n",
      "        [ 0.0073, -0.0393, -0.0145, -0.0257,  0.0111],\n",
      "        [-0.0099, -0.0025, -0.0006, -0.0281, -0.0276],\n",
      "        [-0.0041,  0.0367,  0.0356, -0.0089, -0.0135],\n",
      "        [-0.0289, -0.0366, -0.0812,  0.0822, -0.0370],\n",
      "        [-0.0046,  0.0029,  0.0162,  0.0151,  0.0192]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0215, grad_fn=<MinBackward1>), tensor(0.8806, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11757955700159073\n",
      "@sample 742: tensor([[-5.9626e-04,  3.9031e-02, -1.5937e-03, -3.9104e-03,  1.1987e-02],\n",
      "        [-2.7917e-02, -2.4160e-03,  3.9337e-02, -4.1069e-02,  3.2368e-03],\n",
      "        [-4.5161e-02, -1.4231e-03,  6.1393e-06,  1.3934e-02,  9.4928e-03],\n",
      "        [ 4.1555e-02, -4.7794e-03, -3.0654e-03,  4.2942e-04,  1.0377e-02],\n",
      "        [ 2.4726e-02, -1.2436e-02, -2.5188e-02,  4.1430e-02, -3.5980e-02],\n",
      "        [-2.0216e-02,  2.8780e-03,  2.5457e-03, -4.6743e-03, -1.0242e-02],\n",
      "        [ 2.3825e-03,  3.8108e-02,  2.2339e-02, -1.9912e-02,  3.5293e-02],\n",
      "        [ 1.2208e-02,  1.4807e-02, -3.5264e-02,  8.4287e-03,  7.3396e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0013, -0.0205, -0.0818,  0.0401, -0.0201],\n",
      "        [ 0.0175, -0.0005,  0.0911, -0.0190, -0.0367],\n",
      "        [ 0.0038,  0.0030, -0.0159, -0.0095, -0.0077],\n",
      "        [-0.0054,  0.0385, -0.0011,  0.0036, -0.0275],\n",
      "        [ 0.0251,  0.0416,  0.0293, -0.0412,  0.0007],\n",
      "        [ 0.0010, -0.0035,  0.0229,  0.0167,  0.0099],\n",
      "        [ 0.0029, -0.0026, -0.0335,  0.0263, -0.0009],\n",
      "        [-0.0031,  0.0192,  0.0109,  0.0135,  0.0218]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0176, grad_fn=<MinBackward1>), tensor(0.8982, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11637143045663834\n",
      "@sample 743: tensor([[ 0.0095,  0.0008,  0.0535, -0.0452,  0.0228],\n",
      "        [-0.0010,  0.0433,  0.0109,  0.0121,  0.0049],\n",
      "        [ 0.0029,  0.0355, -0.0126, -0.0413,  0.0383],\n",
      "        [ 0.0185,  0.0509,  0.0009, -0.0307, -0.0195],\n",
      "        [-0.0251,  0.0484,  0.0263, -0.0722,  0.0094],\n",
      "        [-0.0043,  0.0261,  0.0116, -0.0284,  0.0024],\n",
      "        [-0.0021,  0.0492,  0.0322, -0.0513,  0.0006],\n",
      "        [-0.0197, -0.0208, -0.0152,  0.0228,  0.0134]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0126, -0.0131, -0.0352,  0.0078, -0.0321],\n",
      "        [-0.0057,  0.0018,  0.0186, -0.0207, -0.0190],\n",
      "        [ 0.0055,  0.0118, -0.0339, -0.0034,  0.0088],\n",
      "        [-0.0287, -0.0223, -0.0406,  0.0372,  0.0046],\n",
      "        [-0.0085, -0.0246, -0.0583,  0.0558, -0.0085],\n",
      "        [-0.0238, -0.0248, -0.0199,  0.0312,  0.0152],\n",
      "        [-0.0537, -0.0196, -0.0382,  0.0479,  0.0188],\n",
      "        [ 0.0073,  0.0095, -0.0036, -0.0363, -0.0156]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0153, grad_fn=<MinBackward1>), tensor(0.9322, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1211715117096901\n",
      "@sample 744: tensor([[ 0.0013, -0.0071,  0.0021,  0.0171,  0.0040],\n",
      "        [ 0.0040,  0.0076,  0.0026, -0.0078, -0.0022],\n",
      "        [ 0.0180, -0.0223, -0.0179,  0.0468, -0.0196],\n",
      "        [ 0.0173, -0.0142, -0.0031, -0.0072,  0.0047],\n",
      "        [ 0.0248,  0.0084, -0.0145, -0.0098,  0.0045],\n",
      "        [-0.0312, -0.0387, -0.0434,  0.0047,  0.0317],\n",
      "        [ 0.0041,  0.0458,  0.0262, -0.0118, -0.0112],\n",
      "        [ 0.0084, -0.0647, -0.0379,  0.0246,  0.0230]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0087,  0.0089, -0.0356,  0.0232,  0.0074],\n",
      "        [ 0.0182, -0.0158, -0.0050,  0.0027,  0.0249],\n",
      "        [ 0.0548,  0.0102,  0.0208,  0.0113, -0.0087],\n",
      "        [-0.0076, -0.0198,  0.0263,  0.0200,  0.0121],\n",
      "        [ 0.0065, -0.0229, -0.0214,  0.0314, -0.0044],\n",
      "        [ 0.0225, -0.0023,  0.0265, -0.0012,  0.0018],\n",
      "        [-0.0016, -0.0175, -0.0262,  0.0062,  0.0099],\n",
      "        [ 0.0229, -0.0209,  0.0691,  0.0177, -0.0279]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0177, grad_fn=<MinBackward1>), tensor(0.8351, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11985541135072708\n",
      "@sample 745: tensor([[ 0.0053,  0.0368,  0.0329, -0.0087,  0.0042],\n",
      "        [-0.0099,  0.0382, -0.0060, -0.0085, -0.0289],\n",
      "        [-0.0091,  0.0032, -0.0060,  0.0196, -0.0005],\n",
      "        [-0.0277, -0.0218,  0.0022,  0.0368, -0.0361],\n",
      "        [ 0.0161,  0.0147,  0.0389,  0.0177, -0.0032],\n",
      "        [-0.0333, -0.0135, -0.0081, -0.0210,  0.0012],\n",
      "        [ 0.0190,  0.0343, -0.0095,  0.0074, -0.0155],\n",
      "        [-0.0299, -0.0190, -0.0003,  0.0270, -0.0225]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-1.4161e-02,  3.2429e-02, -7.9798e-02,  1.5401e-02, -2.0991e-02],\n",
      "        [-9.6835e-05,  3.9811e-02,  4.1820e-02,  4.5446e-03, -6.5322e-03],\n",
      "        [-2.6625e-04, -8.0029e-04,  2.1592e-02,  8.9064e-03, -9.3162e-03],\n",
      "        [ 4.4114e-02,  1.8732e-02,  9.6457e-02, -4.3091e-02,  1.1734e-02],\n",
      "        [-4.3572e-02, -6.6670e-03, -2.1044e-02, -1.9782e-02, -6.2542e-02],\n",
      "        [-1.2061e-02, -1.5414e-02,  3.7879e-02, -2.4706e-02,  5.1590e-02],\n",
      "        [-1.5310e-02,  9.5043e-03, -4.4335e-02,  7.0956e-03,  2.1390e-02],\n",
      "        [ 8.6974e-03,  9.1869e-03,  5.1664e-03, -1.8363e-02, -7.8060e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0140, grad_fn=<MinBackward1>), tensor(0.8924, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12909556925296783\n",
      "@sample 746: tensor([[ 0.0297,  0.0053,  0.0002, -0.0108,  0.0258],\n",
      "        [ 0.0039,  0.0437,  0.0243, -0.0132,  0.0003],\n",
      "        [-0.0140, -0.0013, -0.0210,  0.0142, -0.0011],\n",
      "        [ 0.0145,  0.0443,  0.0131, -0.0262, -0.0064],\n",
      "        [ 0.0301, -0.0223, -0.0084,  0.0021, -0.0154],\n",
      "        [-0.0340, -0.0188, -0.0457,  0.0449, -0.0500],\n",
      "        [ 0.0177, -0.0248, -0.0442, -0.0203, -0.0031],\n",
      "        [-0.0013, -0.0489, -0.0029, -0.0004,  0.0091]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0306,  0.0120, -0.0485,  0.0263,  0.0133],\n",
      "        [ 0.0054, -0.0068, -0.0398,  0.0085, -0.0062],\n",
      "        [ 0.0146, -0.0153,  0.0362, -0.0386,  0.0049],\n",
      "        [-0.0285, -0.0111, -0.0276,  0.0144, -0.0195],\n",
      "        [-0.0083,  0.0485,  0.0470, -0.0072, -0.0157],\n",
      "        [-0.0011, -0.0080, -0.0034, -0.0537,  0.0092],\n",
      "        [ 0.0044, -0.0320,  0.0424, -0.0321,  0.0141],\n",
      "        [ 0.0280, -0.0010,  0.0196, -0.0095, -0.0407]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0237, grad_fn=<MinBackward1>), tensor(0.8931, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1304720640182495\n",
      "@sample 747: tensor([[ 0.0015, -0.0050, -0.0291, -0.0088,  0.0296],\n",
      "        [ 0.0230, -0.0399, -0.0248,  0.0501, -0.0002],\n",
      "        [-0.0023, -0.0338, -0.0095,  0.0322,  0.0162],\n",
      "        [-0.0061, -0.0144, -0.0202,  0.0140, -0.0086],\n",
      "        [-0.0186,  0.0011, -0.0006,  0.0285, -0.0148],\n",
      "        [ 0.0111, -0.0357, -0.0288,  0.0189, -0.0125],\n",
      "        [ 0.0076, -0.0351, -0.0189,  0.0281,  0.0132],\n",
      "        [-0.0090, -0.0231, -0.0175, -0.0064,  0.0015]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-2.6587e-02, -6.1669e-03, -2.1446e-02,  1.2365e-02, -3.2381e-02],\n",
      "        [-5.0395e-03,  3.8073e-02,  1.2303e-02, -2.5727e-02,  8.5507e-03],\n",
      "        [ 8.6732e-03,  9.0032e-03,  3.1698e-02, -4.7466e-02, -6.5722e-05],\n",
      "        [ 3.7259e-02,  1.2429e-02,  1.2649e-02, -1.5576e-02, -3.3981e-02],\n",
      "        [ 2.6018e-02,  5.5595e-02,  2.4877e-02, -9.3812e-03, -1.5037e-02],\n",
      "        [ 1.4488e-02, -1.1238e-02, -5.6473e-03, -9.9011e-03,  2.0345e-02],\n",
      "        [ 3.6806e-02, -1.9190e-03,  2.0759e-02, -6.4293e-04,  2.1459e-02],\n",
      "        [ 3.0328e-02, -1.5664e-02,  6.8421e-02, -3.5767e-02, -8.7491e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0107, grad_fn=<MinBackward1>), tensor(0.9121, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11729465425014496\n",
      "@sample 748: tensor([[-0.0011,  0.0160,  0.0172,  0.0007,  0.0312],\n",
      "        [-0.0082,  0.0270, -0.0100, -0.0034, -0.0109],\n",
      "        [-0.0022, -0.0254, -0.0024,  0.0037, -0.0238],\n",
      "        [ 0.0159,  0.0464, -0.0102, -0.0066,  0.0008],\n",
      "        [-0.0037,  0.0352,  0.0079, -0.0124, -0.0094],\n",
      "        [-0.0081, -0.0131, -0.0062,  0.0044,  0.0042],\n",
      "        [-0.0323, -0.0111, -0.0103, -0.0061,  0.0205],\n",
      "        [ 0.0396,  0.0725,  0.0244, -0.0292,  0.0204]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0276,  0.0062, -0.0247, -0.0239, -0.0164],\n",
      "        [-0.0176,  0.0089, -0.0552, -0.0122,  0.0026],\n",
      "        [ 0.0051, -0.0174,  0.0776, -0.0088,  0.0041],\n",
      "        [ 0.0077, -0.0226, -0.0678,  0.0399,  0.0232],\n",
      "        [ 0.0201,  0.0154, -0.0174,  0.0338,  0.0114],\n",
      "        [-0.0120, -0.0371, -0.0090, -0.0005,  0.0075],\n",
      "        [-0.0061,  0.0095,  0.0143,  0.0057, -0.0257],\n",
      "        [-0.0616, -0.0026,  0.0067,  0.0314, -0.0236]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0181, grad_fn=<MinBackward1>), tensor(0.8868, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11307922005653381\n",
      "@sample 749: tensor([[ 0.0059, -0.0450, -0.0007,  0.0419, -0.0234],\n",
      "        [-0.0156, -0.0489, -0.0239,  0.0552,  0.0026],\n",
      "        [-0.0217, -0.0266, -0.0313,  0.0328, -0.0178],\n",
      "        [-0.0086, -0.0114, -0.0256,  0.0164,  0.0074],\n",
      "        [-0.0220, -0.0205, -0.0101,  0.0054, -0.0115],\n",
      "        [-0.0015,  0.0150,  0.0148, -0.0145, -0.0259],\n",
      "        [-0.0060,  0.0260,  0.0244, -0.0391,  0.0396],\n",
      "        [ 0.0221, -0.0318, -0.0162,  0.0511, -0.0272]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0088,  0.0118,  0.0429, -0.0320,  0.0070],\n",
      "        [ 0.0514,  0.0356,  0.0626, -0.0227,  0.0175],\n",
      "        [ 0.0161,  0.0020,  0.0455,  0.0134,  0.0263],\n",
      "        [ 0.0060,  0.0059, -0.0098,  0.0001,  0.0174],\n",
      "        [ 0.0103,  0.0047,  0.0161, -0.0186,  0.0097],\n",
      "        [ 0.0030,  0.0017, -0.0403, -0.0079,  0.0161],\n",
      "        [-0.0065, -0.0373, -0.0330,  0.0536,  0.0088],\n",
      "        [ 0.0121,  0.0430, -0.0250, -0.0236, -0.0081]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0167, grad_fn=<MinBackward1>), tensor(0.8944, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1227126270532608\n",
      "@sample 750: tensor([[ 0.0081,  0.0023, -0.0111,  0.0148, -0.0180],\n",
      "        [-0.0112, -0.0073,  0.0047, -0.0202,  0.0194],\n",
      "        [-0.0213,  0.0103,  0.0223, -0.0177,  0.0043],\n",
      "        [ 0.0138,  0.0309,  0.0011, -0.0095, -0.0031],\n",
      "        [-0.0019, -0.0474, -0.0078,  0.0079, -0.0065],\n",
      "        [-0.0136,  0.0083,  0.0034, -0.0040,  0.0345],\n",
      "        [ 0.0012,  0.0458,  0.0019, -0.0205,  0.0617],\n",
      "        [ 0.0318,  0.0503,  0.0256, -0.0199,  0.0091]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0151,  0.0165,  0.0323,  0.0008,  0.0106],\n",
      "        [-0.0026,  0.0086,  0.0125,  0.0076, -0.0195],\n",
      "        [ 0.0061,  0.0118, -0.0085, -0.0021,  0.0037],\n",
      "        [ 0.0113, -0.0226, -0.0379,  0.0217,  0.0124],\n",
      "        [ 0.0028, -0.0120,  0.0261, -0.0341,  0.0283],\n",
      "        [ 0.0101, -0.0034,  0.0048, -0.0135, -0.0570],\n",
      "        [-0.0040, -0.0056, -0.0975, -0.0222, -0.0278],\n",
      "        [-0.0559, -0.0290, -0.0796,  0.0836,  0.0037]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0211, grad_fn=<MinBackward1>), tensor(0.9046, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11795030534267426\n",
      "@sample 751: tensor([[-0.0279,  0.0376,  0.0027, -0.0083,  0.0233],\n",
      "        [ 0.0030, -0.0195, -0.0120,  0.0408, -0.0235],\n",
      "        [-0.0173,  0.0014, -0.0170,  0.0028,  0.0123],\n",
      "        [-0.0250,  0.0126, -0.0068, -0.0094, -0.0104],\n",
      "        [ 0.0365,  0.0017,  0.0293,  0.0105, -0.0204],\n",
      "        [ 0.0004,  0.0194,  0.0543, -0.0070,  0.0317],\n",
      "        [ 0.0220, -0.0099, -0.0060,  0.0241,  0.0145],\n",
      "        [-0.0135, -0.0296,  0.0118,  0.0141,  0.0321]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0014,  0.0166, -0.0383,  0.0461, -0.0004],\n",
      "        [ 0.0482,  0.0439,  0.0024, -0.0209,  0.0243],\n",
      "        [ 0.0406,  0.0095,  0.0277, -0.0109,  0.0200],\n",
      "        [-0.0047, -0.0062, -0.0295, -0.0141,  0.0137],\n",
      "        [-0.0052,  0.0339, -0.0361,  0.0227,  0.0194],\n",
      "        [-0.0022, -0.0051, -0.0316,  0.0392, -0.0641],\n",
      "        [ 0.0330,  0.0153,  0.0197,  0.0012,  0.0309],\n",
      "        [ 0.0020,  0.0209, -0.0049,  0.0084, -0.0054]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0198, grad_fn=<MinBackward1>), tensor(0.8938, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12444630265235901\n",
      "@sample 752: tensor([[ 0.0074, -0.0236,  0.0117,  0.0164, -0.0021],\n",
      "        [-0.0002, -0.0558, -0.0469, -0.0396,  0.0126],\n",
      "        [ 0.0012,  0.0113,  0.0004, -0.0104,  0.0121],\n",
      "        [ 0.0003,  0.0004, -0.0016, -0.0375,  0.0267],\n",
      "        [ 0.0031,  0.0070,  0.0160, -0.0139,  0.0194],\n",
      "        [ 0.0152, -0.0404, -0.0158,  0.0397, -0.0380],\n",
      "        [-0.0223, -0.0039, -0.0016, -0.0126,  0.0106],\n",
      "        [-0.0172, -0.0132, -0.0088, -0.0264,  0.0421]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0380,  0.0009,  0.0531, -0.0267, -0.0156],\n",
      "        [-0.0566, -0.0701,  0.0276, -0.0142,  0.0059],\n",
      "        [ 0.0109,  0.0215, -0.0289,  0.0307,  0.0204],\n",
      "        [-0.0203, -0.0233, -0.0427,  0.0050, -0.0064],\n",
      "        [ 0.0067,  0.0224, -0.0600,  0.0421,  0.0451],\n",
      "        [ 0.0226,  0.0397, -0.0035, -0.0040,  0.0034],\n",
      "        [ 0.0109,  0.0029,  0.0018, -0.0313, -0.0451],\n",
      "        [-0.0290, -0.0320,  0.0022,  0.0180,  0.0036]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0119, grad_fn=<MinBackward1>), tensor(0.8474, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11550860106945038\n",
      "@sample 753: tensor([[ 0.0143,  0.0245,  0.0132, -0.0129,  0.0069],\n",
      "        [ 0.0092, -0.0293, -0.0364,  0.0278, -0.0059],\n",
      "        [-0.0096,  0.0302,  0.0108, -0.0359,  0.0407],\n",
      "        [ 0.0200, -0.0063,  0.0174, -0.0104,  0.0201],\n",
      "        [-0.0032, -0.0060, -0.0037, -0.0098,  0.0221],\n",
      "        [ 0.0096, -0.0287,  0.0112,  0.0025, -0.0047],\n",
      "        [-0.0068,  0.0104,  0.0270, -0.0102,  0.0071],\n",
      "        [ 0.0246, -0.0014,  0.0199, -0.0069,  0.0037]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0049,  0.0005,  0.0155,  0.0304, -0.0305],\n",
      "        [ 0.0084,  0.0374,  0.0037, -0.0595,  0.0095],\n",
      "        [ 0.0219, -0.0235, -0.0884,  0.0613,  0.0136],\n",
      "        [-0.0054, -0.0286, -0.0532,  0.0697, -0.0087],\n",
      "        [ 0.0170, -0.0001, -0.0098,  0.0149,  0.0282],\n",
      "        [-0.0091, -0.0129,  0.0240, -0.0140, -0.0100],\n",
      "        [ 0.0248,  0.0032,  0.0021, -0.0139,  0.0366],\n",
      "        [-0.0169,  0.0008, -0.0298,  0.0123,  0.0004]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0125, grad_fn=<MinBackward1>), tensor(0.8527, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11025747656822205\n",
      "@sample 754: tensor([[ 1.8158e-02,  9.4563e-04,  4.3149e-02, -2.6276e-02, -2.4040e-02],\n",
      "        [-5.0702e-03,  1.6299e-02,  2.7936e-02, -2.7290e-03, -2.9636e-03],\n",
      "        [-1.9126e-03, -3.1978e-02, -3.4365e-03, -9.2815e-03,  1.6314e-02],\n",
      "        [ 5.1859e-03,  4.6930e-02,  2.9882e-02, -4.2926e-02,  1.5759e-02],\n",
      "        [-1.5045e-02,  6.7250e-04,  1.6187e-03, -6.3951e-03,  1.0526e-03],\n",
      "        [ 1.8516e-02,  1.7470e-03, -1.8480e-03,  1.0523e-02, -2.4754e-02],\n",
      "        [ 4.0219e-03, -6.7726e-05, -3.0497e-02,  3.3853e-02, -7.2860e-03],\n",
      "        [-1.0045e-02, -7.2254e-03, -2.6260e-02,  1.2881e-02, -9.2841e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0411,  0.0045,  0.0005, -0.0106, -0.0281],\n",
      "        [-0.0165, -0.0050, -0.0308,  0.0532,  0.0232],\n",
      "        [ 0.0056, -0.0459,  0.0427, -0.0159, -0.0632],\n",
      "        [-0.0538, -0.0190, -0.0538,  0.0486, -0.0179],\n",
      "        [-0.0263, -0.0098, -0.0029,  0.0040,  0.0108],\n",
      "        [-0.0029,  0.0318, -0.0313, -0.0381, -0.0020],\n",
      "        [ 0.0524,  0.0214,  0.0192, -0.0622, -0.0474],\n",
      "        [ 0.0069,  0.0206,  0.0201, -0.0312,  0.0143]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0158, grad_fn=<MinBackward1>), tensor(0.8539, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11806220561265945\n",
      "@sample 755: tensor([[-0.0178, -0.0207, -0.0409,  0.0093, -0.0033],\n",
      "        [-0.0127,  0.0322,  0.0171,  0.0007,  0.0005],\n",
      "        [-0.0141, -0.0112, -0.0013, -0.0168,  0.0023],\n",
      "        [ 0.0114,  0.0018, -0.0144,  0.0046, -0.0101],\n",
      "        [-0.0206,  0.0275,  0.0097, -0.0281,  0.0054],\n",
      "        [-0.0110, -0.0049, -0.0105,  0.0256, -0.0155],\n",
      "        [-0.0023,  0.0036,  0.0215, -0.0172, -0.0020],\n",
      "        [-0.0155,  0.0097,  0.0055, -0.0137,  0.0098]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0228, -0.0304,  0.0462, -0.0513, -0.0009],\n",
      "        [ 0.0109,  0.0145, -0.0100,  0.0043, -0.0285],\n",
      "        [ 0.0368, -0.0033,  0.0323, -0.0290, -0.0283],\n",
      "        [ 0.0030, -0.0067, -0.0256, -0.0184, -0.0332],\n",
      "        [ 0.0067, -0.0100,  0.0081, -0.0072, -0.0095],\n",
      "        [ 0.0109,  0.0049,  0.0518, -0.0516, -0.0090],\n",
      "        [-0.0539, -0.0012, -0.0224,  0.0314, -0.0104],\n",
      "        [ 0.0131, -0.0022,  0.0441, -0.0063,  0.0431]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0108, grad_fn=<MinBackward1>), tensor(0.8812, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12104889750480652\n",
      "@sample 756: tensor([[ 0.0246, -0.0433, -0.0424,  0.0415,  0.0058],\n",
      "        [-0.0130, -0.0164,  0.0002,  0.0085, -0.0015],\n",
      "        [-0.0107,  0.0401,  0.0147,  0.0036, -0.0135],\n",
      "        [ 0.0356,  0.0121,  0.0098, -0.0244,  0.0197],\n",
      "        [ 0.0246,  0.0158,  0.0203, -0.0156,  0.0271],\n",
      "        [-0.0152,  0.0326,  0.0272, -0.0294,  0.0292],\n",
      "        [-0.0325, -0.0297, -0.0031, -0.0023,  0.0136],\n",
      "        [-0.0362, -0.0216,  0.0228,  0.0192, -0.0114]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0380,  0.0086,  0.0176,  0.0072,  0.0396],\n",
      "        [-0.0202, -0.0170, -0.0063, -0.0108,  0.0046],\n",
      "        [-0.0303, -0.0136, -0.0811, -0.0195, -0.0022],\n",
      "        [-0.0376,  0.0002, -0.0295,  0.0369, -0.0134],\n",
      "        [-0.0066,  0.0149, -0.0515,  0.0272,  0.0167],\n",
      "        [-0.0199, -0.0135, -0.0623,  0.0392, -0.0059],\n",
      "        [-0.0136, -0.0644,  0.0433, -0.0293, -0.0223],\n",
      "        [ 0.0278, -0.0019,  0.0458, -0.0211, -0.0080]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0181, grad_fn=<MinBackward1>), tensor(0.8850, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1271446943283081\n",
      "@sample 757: tensor([[-0.0128, -0.0054, -0.0416,  0.0322, -0.0216],\n",
      "        [-0.0041,  0.0471,  0.0422, -0.0506,  0.0031],\n",
      "        [-0.0118, -0.0291,  0.0109,  0.0076,  0.0017],\n",
      "        [-0.0065,  0.0066, -0.0151,  0.0204,  0.0028],\n",
      "        [ 0.0342,  0.0375,  0.0004, -0.0237, -0.0011],\n",
      "        [-0.0099, -0.0016,  0.0069, -0.0241, -0.0141],\n",
      "        [ 0.0162,  0.0047, -0.0050, -0.0061, -0.0022],\n",
      "        [ 0.0163,  0.0199,  0.0015, -0.0146,  0.0004]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0015, -0.0292,  0.0805, -0.0452, -0.0031],\n",
      "        [-0.0280, -0.0053, -0.0868,  0.0796, -0.0143],\n",
      "        [ 0.0084,  0.0090,  0.0279, -0.0087,  0.0053],\n",
      "        [ 0.0021,  0.0026, -0.0337, -0.0293,  0.0052],\n",
      "        [-0.0264, -0.0194, -0.0272,  0.0529, -0.0378],\n",
      "        [ 0.0268, -0.0250, -0.0388, -0.0255,  0.0030],\n",
      "        [-0.0102, -0.0113, -0.0159,  0.0140,  0.0161],\n",
      "        [-0.0211, -0.0196, -0.0220,  0.0230,  0.0107]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0113, grad_fn=<MinBackward1>), tensor(0.8235, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11293195933103561\n",
      "@sample 758: tensor([[-0.0090,  0.0025,  0.0428, -0.0200,  0.0080],\n",
      "        [ 0.0333,  0.0070, -0.0146, -0.0078, -0.0029],\n",
      "        [ 0.0366,  0.0497, -0.0132, -0.0490, -0.0361],\n",
      "        [ 0.0055,  0.0239,  0.0199, -0.0302, -0.0011],\n",
      "        [-0.0115,  0.0263,  0.0038, -0.0116, -0.0071],\n",
      "        [-0.0041,  0.0133,  0.0165, -0.0242, -0.0348],\n",
      "        [-0.0079,  0.0084, -0.0202,  0.0246, -0.0369],\n",
      "        [-0.0051,  0.0065,  0.0106, -0.0126,  0.0113]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-2.4645e-02, -2.0934e-02,  1.2372e-02, -8.9123e-03, -1.5874e-02],\n",
      "        [-2.5242e-02, -2.2989e-02, -3.4480e-02,  7.2043e-03,  1.4037e-02],\n",
      "        [-6.3470e-04, -3.9318e-02, -2.8498e-02,  4.9652e-02,  4.2532e-03],\n",
      "        [-2.3737e-02, -1.7715e-02, -4.0031e-02,  8.0050e-03, -1.9619e-02],\n",
      "        [-1.9727e-02, -1.0014e-02, -5.2181e-02, -5.1767e-05,  2.7586e-02],\n",
      "        [-2.3385e-02, -4.8536e-03,  5.1130e-02, -1.0767e-02, -4.6447e-03],\n",
      "        [ 4.9343e-02,  9.6371e-03,  1.7484e-03,  3.7108e-02,  1.4492e-02],\n",
      "        [-2.0218e-02, -1.5716e-02,  9.9552e-03,  6.5313e-03,  8.4100e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0183, grad_fn=<MinBackward1>), tensor(0.8760, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11880247294902802\n",
      "@sample 759: tensor([[-0.0083,  0.0453, -0.0041, -0.0461,  0.0195],\n",
      "        [-0.0054,  0.0099, -0.0044, -0.0009,  0.0095],\n",
      "        [-0.0393, -0.0022, -0.0145,  0.0386, -0.0359],\n",
      "        [-0.0220, -0.0315,  0.0135, -0.0202,  0.0363],\n",
      "        [-0.0092, -0.0008, -0.0003, -0.0052,  0.0139],\n",
      "        [ 0.0223,  0.0441, -0.0168, -0.0099, -0.0109],\n",
      "        [-0.0227, -0.0086, -0.0383,  0.0186,  0.0180],\n",
      "        [-0.0273, -0.0334,  0.0298,  0.0295,  0.0110]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0773, -0.0327, -0.0510, -0.0037,  0.0159],\n",
      "        [ 0.0099, -0.0077, -0.0220,  0.0137, -0.0002],\n",
      "        [-0.0089, -0.0075,  0.0119, -0.0050, -0.0013],\n",
      "        [-0.0275, -0.0257,  0.0296, -0.0368, -0.0014],\n",
      "        [-0.0009,  0.0282, -0.0233, -0.0296, -0.0159],\n",
      "        [-0.0155, -0.0219, -0.0080,  0.0371,  0.0010],\n",
      "        [ 0.0175, -0.0066,  0.0272, -0.0297, -0.0155],\n",
      "        [ 0.0262,  0.0074,  0.0987, -0.0344, -0.0425]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0111, grad_fn=<MinBackward1>), tensor(0.8602, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12628090381622314\n",
      "@sample 760: tensor([[ 0.0170, -0.0309, -0.0068,  0.0411, -0.0038],\n",
      "        [-0.0155, -0.0054, -0.0150, -0.0039, -0.0092],\n",
      "        [ 0.0013,  0.0015,  0.0366,  0.0032,  0.0503],\n",
      "        [-0.0078, -0.0150,  0.0357, -0.0022, -0.0159],\n",
      "        [-0.0043,  0.0190, -0.0150,  0.0339, -0.0142],\n",
      "        [-0.0181, -0.0189, -0.0343, -0.0249,  0.0280],\n",
      "        [-0.0348, -0.0091,  0.0046, -0.0381, -0.0275],\n",
      "        [ 0.0023,  0.0295,  0.0129, -0.0173,  0.0097]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0056,  0.0198, -0.0026, -0.0077, -0.0143],\n",
      "        [-0.0010, -0.0251,  0.0547, -0.0428, -0.0418],\n",
      "        [ 0.0490, -0.0222, -0.0287,  0.0428,  0.0147],\n",
      "        [-0.0216,  0.0215,  0.0051,  0.0049, -0.0146],\n",
      "        [ 0.0335,  0.0308, -0.0411, -0.0010, -0.0324],\n",
      "        [-0.0052, -0.0606, -0.0072, -0.0153, -0.0214],\n",
      "        [-0.0471, -0.0605,  0.0627, -0.0392, -0.0534],\n",
      "        [ 0.0024, -0.0012, -0.1190,  0.0498,  0.0287]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0152, grad_fn=<MinBackward1>), tensor(0.8881, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1275879144668579\n",
      "@sample 761: tensor([[-0.0048, -0.0013,  0.0428, -0.0040,  0.0029],\n",
      "        [-0.0074,  0.0207, -0.0052,  0.0017,  0.0020],\n",
      "        [ 0.0019, -0.0321, -0.0314, -0.0012,  0.0109],\n",
      "        [ 0.0128,  0.0030, -0.0094,  0.0077, -0.0080],\n",
      "        [-0.0271, -0.0154, -0.0045, -0.0064, -0.0092],\n",
      "        [-0.0251,  0.0227, -0.0159, -0.0141, -0.0156],\n",
      "        [-0.0199, -0.0125, -0.0230,  0.0112,  0.0109],\n",
      "        [-0.0028, -0.0311,  0.0217, -0.0033,  0.0189]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0402,  0.0515,  0.0241,  0.0186,  0.0228],\n",
      "        [-0.0239,  0.0295, -0.0374, -0.0164,  0.0065],\n",
      "        [-0.0109, -0.0089,  0.0100, -0.0228,  0.0199],\n",
      "        [-0.0123,  0.0040, -0.0168,  0.0021,  0.0091],\n",
      "        [-0.0023, -0.0141,  0.0140, -0.0659, -0.0092],\n",
      "        [-0.0023, -0.0304,  0.0429, -0.0344, -0.0448],\n",
      "        [ 0.0244,  0.0240,  0.0466,  0.0054,  0.0141],\n",
      "        [-0.0266, -0.0053, -0.0131, -0.0010, -0.0147]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0182, grad_fn=<MinBackward1>), tensor(0.9027, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11747302860021591\n",
      "@sample 762: tensor([[ 1.1496e-02,  2.1433e-03,  2.0643e-02, -3.6855e-04, -4.3725e-03],\n",
      "        [ 3.1335e-03,  7.2410e-02,  3.1557e-02, -3.2045e-02,  1.9048e-02],\n",
      "        [ 4.9627e-03, -1.4281e-02,  2.1057e-02,  1.7947e-02,  4.0031e-03],\n",
      "        [ 1.3332e-02, -1.4375e-04, -3.0350e-02,  2.0194e-02, -6.4989e-03],\n",
      "        [ 6.0873e-03,  1.5906e-03,  9.9880e-03,  5.0136e-02, -4.1180e-02],\n",
      "        [-5.1200e-03,  4.8399e-05, -1.8345e-02,  1.8000e-02, -6.5325e-03],\n",
      "        [ 7.5800e-03, -7.3000e-03, -1.7453e-02,  3.2730e-02, -1.2099e-02],\n",
      "        [ 4.1013e-03, -1.9596e-02,  1.9630e-02, -4.0577e-03,  1.9838e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 8.3652e-03, -1.8142e-05,  9.6984e-03,  5.9429e-03,  9.3092e-03],\n",
      "        [-4.5231e-02, -9.4238e-03, -1.0591e-01,  5.0720e-02, -3.4735e-03],\n",
      "        [ 7.6012e-03,  7.6166e-03,  1.9609e-02, -4.6095e-03, -1.6357e-02],\n",
      "        [ 1.5156e-02,  3.0535e-02, -2.8287e-02, -5.2992e-03,  1.9690e-02],\n",
      "        [ 2.7970e-02,  5.7623e-02,  3.1984e-02, -6.9174e-02, -1.4686e-02],\n",
      "        [ 1.7135e-02, -7.9521e-03,  1.3727e-02, -7.4014e-04, -2.6937e-02],\n",
      "        [ 2.6407e-02,  5.0142e-02,  3.5810e-02, -2.8557e-02, -2.2841e-02],\n",
      "        [-1.3764e-02, -5.2074e-03,  6.7272e-02, -2.6571e-02, -1.2868e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0165, grad_fn=<MinBackward1>), tensor(0.9245, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12301112711429596\n",
      "@sample 763: tensor([[ 0.0509,  0.0316,  0.0683,  0.0098, -0.0045],\n",
      "        [-0.0164,  0.0192, -0.0061, -0.0167,  0.0075],\n",
      "        [ 0.0030,  0.0009,  0.0021,  0.0295, -0.0024],\n",
      "        [-0.0036, -0.0113,  0.0074,  0.0346, -0.0026],\n",
      "        [ 0.0202,  0.0520,  0.0409, -0.0712,  0.0383],\n",
      "        [-0.0413,  0.0225,  0.0409, -0.0176,  0.0194],\n",
      "        [-0.0038,  0.0186,  0.0196,  0.0130, -0.0136],\n",
      "        [ 0.0065, -0.0175, -0.0031,  0.0014,  0.0229]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0272,  0.0387, -0.0458,  0.0324, -0.0162],\n",
      "        [-0.0141, -0.0160, -0.0353,  0.0002, -0.0357],\n",
      "        [ 0.0294,  0.0286, -0.0250, -0.0255,  0.0046],\n",
      "        [ 0.0193, -0.0017, -0.0195,  0.0061, -0.0064],\n",
      "        [-0.0348, -0.0217, -0.0873,  0.0440, -0.0456],\n",
      "        [-0.0076, -0.0490, -0.0488,  0.0143,  0.0021],\n",
      "        [-0.0174,  0.0352,  0.0121,  0.0467, -0.0268],\n",
      "        [ 0.0187,  0.0203, -0.0047,  0.0076, -0.0298]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0121, grad_fn=<MinBackward1>), tensor(0.8904, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11449922621250153\n",
      "@sample 764: tensor([[ 0.0166,  0.0199,  0.0173, -0.0228, -0.0159],\n",
      "        [-0.0235, -0.0091, -0.0308,  0.0462, -0.0288],\n",
      "        [-0.0113,  0.0542, -0.0019, -0.0350, -0.0040],\n",
      "        [ 0.0035, -0.0200, -0.0072,  0.0310, -0.0114],\n",
      "        [-0.0188, -0.0085, -0.0179,  0.0322,  0.0035],\n",
      "        [ 0.0024,  0.0121, -0.0031, -0.0068, -0.0096],\n",
      "        [ 0.0398, -0.0128, -0.0220,  0.0094, -0.0004],\n",
      "        [-0.0086,  0.0143, -0.0020,  0.0037,  0.0142]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0018, -0.0058,  0.0694, -0.0081, -0.0126],\n",
      "        [ 0.0307,  0.0105,  0.0346, -0.0436, -0.0013],\n",
      "        [ 0.0067, -0.0062, -0.0463, -0.0349,  0.0224],\n",
      "        [ 0.0096,  0.0438, -0.0099, -0.0109,  0.0072],\n",
      "        [ 0.0356,  0.0162,  0.0181, -0.0399,  0.0061],\n",
      "        [-0.0048, -0.0021, -0.0199,  0.0103, -0.0116],\n",
      "        [ 0.0218, -0.0081, -0.0496,  0.0325,  0.0156],\n",
      "        [-0.0166,  0.0128,  0.0342, -0.0139, -0.0416]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0154, grad_fn=<MinBackward1>), tensor(0.9125, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11317546665668488\n",
      "@sample 765: tensor([[ 0.0216, -0.0090, -0.0106, -0.0043, -0.0013],\n",
      "        [-0.0153, -0.0040, -0.0140,  0.0270, -0.0040],\n",
      "        [-0.0011,  0.0082, -0.0201, -0.0353,  0.0158],\n",
      "        [-0.0057,  0.0109,  0.0325, -0.0077, -0.0066],\n",
      "        [-0.0114, -0.0493, -0.0175,  0.0321, -0.0280],\n",
      "        [ 0.0319,  0.0322, -0.0002, -0.0613,  0.0344],\n",
      "        [-0.0125,  0.0076, -0.0038,  0.0429, -0.0398],\n",
      "        [-0.0049, -0.0309, -0.0079, -0.0045,  0.0089]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0283, -0.0300, -0.0275, -0.0318, -0.0299],\n",
      "        [ 0.0174,  0.0159,  0.0426, -0.0161, -0.0419],\n",
      "        [-0.0334, -0.0094, -0.0277, -0.0244, -0.0375],\n",
      "        [-0.0245,  0.0420, -0.0462,  0.0200, -0.0087],\n",
      "        [ 0.0101,  0.0194,  0.0381, -0.0265, -0.0093],\n",
      "        [-0.0269, -0.0316, -0.0982,  0.0780,  0.0146],\n",
      "        [ 0.0005,  0.0389, -0.0286,  0.0029,  0.0336],\n",
      "        [-0.0537, -0.0126,  0.0148, -0.0409, -0.0207]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0118, grad_fn=<MinBackward1>), tensor(0.9128, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12449456751346588\n",
      "@sample 766: tensor([[-0.0155, -0.0080, -0.0081,  0.0161, -0.0078],\n",
      "        [-0.0122, -0.0104,  0.0030, -0.0148,  0.0074],\n",
      "        [-0.0227,  0.0172,  0.0037, -0.0082,  0.0288],\n",
      "        [-0.0121, -0.0213, -0.0109,  0.0060, -0.0276],\n",
      "        [ 0.0114,  0.0027,  0.0069,  0.0138,  0.0002],\n",
      "        [ 0.0183, -0.0372, -0.0068,  0.0517,  0.0076],\n",
      "        [ 0.0162, -0.0230, -0.0130,  0.0198,  0.0060],\n",
      "        [-0.0122, -0.0222,  0.0028,  0.0346, -0.0245]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0172,  0.0119,  0.0279, -0.0298,  0.0199],\n",
      "        [-0.0338, -0.0171,  0.0208, -0.0096,  0.0233],\n",
      "        [ 0.0184,  0.0038, -0.0436,  0.0141,  0.0167],\n",
      "        [ 0.0044,  0.0146,  0.0259, -0.0339,  0.0071],\n",
      "        [ 0.0076,  0.0406, -0.0263,  0.0265,  0.0238],\n",
      "        [ 0.0440, -0.0026,  0.0130, -0.0165, -0.0336],\n",
      "        [ 0.0175,  0.0547, -0.0192,  0.0180, -0.0107],\n",
      "        [-0.0008,  0.0168,  0.0515,  0.0001,  0.0122]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0143, grad_fn=<MinBackward1>), tensor(0.9374, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12321802228689194\n",
      "@sample 767: tensor([[-0.0054, -0.0064,  0.0151,  0.0057,  0.0071],\n",
      "        [-0.0096, -0.0393, -0.0444,  0.0613, -0.0295],\n",
      "        [ 0.0113, -0.0070, -0.0240,  0.0210, -0.0025],\n",
      "        [-0.0233, -0.0097, -0.0037,  0.0093, -0.0004],\n",
      "        [ 0.0003, -0.0149, -0.0051,  0.0316, -0.0035],\n",
      "        [ 0.0123,  0.0198,  0.0066, -0.0135,  0.0295],\n",
      "        [ 0.0221,  0.0297, -0.0006,  0.0079,  0.0264],\n",
      "        [-0.0165,  0.0157,  0.0038, -0.0126, -0.0007]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0020,  0.0044, -0.0077,  0.0057,  0.0023],\n",
      "        [ 0.0181,  0.0038,  0.0284, -0.0208,  0.0013],\n",
      "        [ 0.0134,  0.0126, -0.0119,  0.0064,  0.0034],\n",
      "        [-0.0012,  0.0157, -0.0033,  0.0029,  0.0013],\n",
      "        [ 0.0438,  0.0344, -0.0077,  0.0018,  0.0494],\n",
      "        [ 0.0029, -0.0254,  0.0178,  0.0178,  0.0035],\n",
      "        [-0.0174, -0.0115, -0.1058,  0.0452,  0.0425],\n",
      "        [ 0.0178, -0.0075,  0.0110, -0.0077, -0.0176]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0163, grad_fn=<MinBackward1>), tensor(0.8868, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12207852303981781\n",
      "@sample 768: tensor([[ 0.0183,  0.0324,  0.0012, -0.0244,  0.0062],\n",
      "        [ 0.0058,  0.0752,  0.0205, -0.0410, -0.0045],\n",
      "        [ 0.0131, -0.0235, -0.0317,  0.0304, -0.0536],\n",
      "        [ 0.0023, -0.0466,  0.0007,  0.0081, -0.0006],\n",
      "        [ 0.0113,  0.0214, -0.0159,  0.0132, -0.0097],\n",
      "        [ 0.0067,  0.0177,  0.0129, -0.0087,  0.0002],\n",
      "        [-0.0082, -0.0150, -0.0183,  0.0076, -0.0126],\n",
      "        [-0.0023, -0.0347, -0.0211,  0.0536, -0.0282]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0372, -0.0153, -0.0474,  0.0554,  0.0245],\n",
      "        [-0.0462,  0.0105, -0.0667, -0.0136,  0.0088],\n",
      "        [ 0.0128,  0.0025,  0.0051, -0.0022,  0.0154],\n",
      "        [-0.0195, -0.0279,  0.0149, -0.0195, -0.0215],\n",
      "        [ 0.0011,  0.0426,  0.0229, -0.0178, -0.0005],\n",
      "        [-0.0451, -0.0255, -0.0928,  0.0286,  0.0083],\n",
      "        [ 0.0048, -0.0215,  0.0279, -0.0301, -0.0204],\n",
      "        [ 0.0287, -0.0067, -0.0248,  0.0053,  0.0177]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0092, grad_fn=<MinBackward1>), tensor(0.8792, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12506499886512756\n",
      "@sample 769: tensor([[ 0.0089, -0.0233, -0.0096,  0.0192, -0.0395],\n",
      "        [ 0.0030, -0.0302, -0.0283,  0.0034,  0.0154],\n",
      "        [ 0.0063,  0.0286,  0.0175, -0.0326,  0.0243],\n",
      "        [ 0.0016, -0.0073,  0.0211,  0.0124, -0.0075],\n",
      "        [-0.0047,  0.0088,  0.0372, -0.0420,  0.0151],\n",
      "        [-0.0028,  0.0440,  0.0092, -0.0420,  0.0449],\n",
      "        [-0.0070, -0.0373, -0.0076,  0.0083,  0.0167],\n",
      "        [-0.0055,  0.0065, -0.0088,  0.0053, -0.0084]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0185, -0.0158,  0.0041,  0.0117, -0.0021],\n",
      "        [ 0.0179,  0.0099,  0.0668, -0.0486, -0.0062],\n",
      "        [-0.0436, -0.0324, -0.0925,  0.0590, -0.0141],\n",
      "        [ 0.0184,  0.0241,  0.0022,  0.0225,  0.0241],\n",
      "        [-0.0120,  0.0154, -0.0135,  0.0247,  0.0172],\n",
      "        [ 0.0171, -0.0097, -0.0718,  0.0337,  0.0188],\n",
      "        [ 0.0186, -0.0011,  0.0586, -0.0332, -0.0105],\n",
      "        [ 0.0194,  0.0143, -0.0264,  0.0017,  0.0203]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0218, grad_fn=<MinBackward1>), tensor(0.9028, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11844819784164429\n",
      "@sample 770: tensor([[-4.6549e-02, -1.2805e-02,  2.9973e-02,  6.2005e-03, -4.7947e-03],\n",
      "        [-1.0139e-02, -1.1465e-03,  2.3391e-02, -2.4057e-02,  3.6277e-05],\n",
      "        [-2.5647e-02,  4.2800e-02,  4.0237e-02, -6.5381e-02,  1.1784e-02],\n",
      "        [-1.3124e-02,  8.8205e-03,  2.6921e-02, -3.5470e-02,  2.5018e-02],\n",
      "        [-1.0425e-02,  5.7506e-02,  3.4846e-02, -6.8165e-02,  3.5199e-02],\n",
      "        [-1.0239e-02, -8.4958e-03, -7.2031e-03, -2.7110e-02,  3.1144e-02],\n",
      "        [ 6.7300e-04, -4.0677e-02, -4.5132e-03, -6.4647e-03,  2.1427e-02],\n",
      "        [ 5.3888e-03, -2.5560e-02, -6.8483e-03, -1.3199e-02, -1.5153e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 3.4560e-02,  6.7192e-03,  5.6233e-02, -3.3937e-02,  1.0459e-02],\n",
      "        [-3.4646e-02,  9.6182e-03, -1.7454e-02,  7.4162e-03, -8.0541e-03],\n",
      "        [-4.9555e-02, -2.0347e-02, -6.5748e-02,  3.7645e-03, -2.7514e-02],\n",
      "        [-1.4587e-02, -1.7385e-02, -4.6059e-02,  4.7760e-02,  3.4349e-02],\n",
      "        [-3.3120e-02, -8.1928e-02, -6.4114e-02,  6.8013e-02, -1.8984e-02],\n",
      "        [ 2.0197e-02, -1.1491e-02, -2.0273e-03, -7.8261e-05,  2.7461e-02],\n",
      "        [-2.3181e-02,  1.4078e-02,  8.6891e-03, -3.6173e-02, -9.1833e-04],\n",
      "        [ 6.7648e-03, -4.1358e-02,  1.5653e-02,  4.3691e-02,  9.3097e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0204, grad_fn=<MinBackward1>), tensor(0.8789, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13139304518699646\n",
      "@sample 771: tensor([[-0.0092, -0.0032,  0.0201,  0.0021, -0.0180],\n",
      "        [ 0.0224, -0.0107, -0.0396,  0.0307, -0.0208],\n",
      "        [-0.0075,  0.0128,  0.0186,  0.0178, -0.0247],\n",
      "        [-0.0091, -0.0192, -0.0007,  0.0156, -0.0126],\n",
      "        [ 0.0368, -0.0279,  0.0186,  0.0357,  0.0021],\n",
      "        [ 0.0105, -0.0006,  0.0143, -0.0137,  0.0167],\n",
      "        [-0.0119,  0.0311,  0.0091, -0.0214,  0.0291],\n",
      "        [ 0.0292,  0.0461, -0.0057, -0.0268,  0.0050]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0051, -0.0186,  0.0333, -0.0406,  0.0087],\n",
      "        [ 0.0563,  0.0301, -0.0070,  0.0314,  0.0150],\n",
      "        [ 0.0088,  0.0260,  0.0336, -0.0114,  0.0267],\n",
      "        [-0.0101, -0.0246, -0.0142, -0.0181,  0.0145],\n",
      "        [-0.0091,  0.0218, -0.0023,  0.0074, -0.0168],\n",
      "        [ 0.0038,  0.0210,  0.0055, -0.0009,  0.0007],\n",
      "        [ 0.0123, -0.0106, -0.0291, -0.0250, -0.0086],\n",
      "        [ 0.0259, -0.0323, -0.0090,  0.0166,  0.0007]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0175, grad_fn=<MinBackward1>), tensor(0.8545, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11571154743432999\n",
      "@sample 772: tensor([[ 0.0133,  0.0674,  0.0369, -0.0279,  0.0213],\n",
      "        [-0.0234, -0.0227,  0.0187, -0.0022,  0.0353],\n",
      "        [-0.0356,  0.0421,  0.0146, -0.0472,  0.0280],\n",
      "        [ 0.0026, -0.0246,  0.0251, -0.0106,  0.0057],\n",
      "        [ 0.0144, -0.0066,  0.0068, -0.0197,  0.0040],\n",
      "        [-0.0107,  0.0596,  0.0110, -0.0253,  0.0217],\n",
      "        [ 0.0134, -0.0197,  0.0142, -0.0066, -0.0067],\n",
      "        [-0.0277, -0.0082,  0.0134, -0.0385,  0.0035]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0292, -0.0164, -0.0693,  0.0057, -0.0184],\n",
      "        [ 0.0190,  0.0080,  0.0813,  0.0148,  0.0024],\n",
      "        [-0.0186, -0.0043, -0.0441, -0.0106,  0.0025],\n",
      "        [-0.0096, -0.0163, -0.0116, -0.0065, -0.0056],\n",
      "        [-0.0070, -0.0092, -0.0421,  0.0048, -0.0081],\n",
      "        [ 0.0102, -0.0209, -0.0424,  0.0075,  0.0023],\n",
      "        [-0.0102,  0.0132, -0.0145, -0.0230,  0.0249],\n",
      "        [ 0.0112, -0.0484,  0.0665, -0.0447, -0.0246]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0094, grad_fn=<MinBackward1>), tensor(0.8920, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11794324964284897\n",
      "@sample 773: tensor([[ 0.0018, -0.0182,  0.0047, -0.0080,  0.0075],\n",
      "        [-0.0129,  0.0212,  0.0073, -0.0119,  0.0232],\n",
      "        [ 0.0219,  0.0438,  0.0146, -0.0417, -0.0012],\n",
      "        [-0.0080, -0.0128,  0.0177, -0.0182,  0.0137],\n",
      "        [-0.0023,  0.0280,  0.0113, -0.0291,  0.0034],\n",
      "        [-0.0014, -0.0079, -0.0088, -0.0352,  0.0042],\n",
      "        [-0.0199,  0.0041,  0.0268, -0.0180, -0.0186],\n",
      "        [ 0.0116,  0.0282,  0.0114, -0.0132,  0.0121]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0008,  0.0009, -0.0123,  0.0194,  0.0046],\n",
      "        [ 0.0196, -0.0373,  0.0151,  0.0059, -0.0048],\n",
      "        [-0.0352,  0.0065, -0.0310,  0.0317,  0.0309],\n",
      "        [-0.0098,  0.0142,  0.0165, -0.0112,  0.0126],\n",
      "        [-0.0020, -0.0151, -0.0428,  0.0029,  0.0088],\n",
      "        [-0.0134, -0.0654, -0.0451, -0.0035, -0.0260],\n",
      "        [ 0.0123, -0.0118,  0.0033,  0.0227, -0.0029],\n",
      "        [ 0.0182,  0.0091, -0.0094, -0.0052, -0.0030]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0153, grad_fn=<MinBackward1>), tensor(0.8504, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09694541990756989\n",
      "@sample 774: tensor([[ 1.4244e-02,  1.7322e-03,  2.2295e-02, -1.0468e-02, -1.1961e-02],\n",
      "        [-4.3540e-02, -6.9911e-02,  1.8621e-02, -2.2240e-02,  4.1695e-02],\n",
      "        [-1.7327e-02,  9.7685e-03, -2.3113e-03, -5.7635e-02,  1.8147e-02],\n",
      "        [-2.9996e-02, -1.3874e-02,  3.5975e-03, -1.8613e-02,  2.0597e-02],\n",
      "        [-2.5154e-02, -3.5792e-02,  2.2767e-02,  1.6944e-02, -3.9225e-03],\n",
      "        [ 4.2836e-05,  2.2596e-02, -6.6407e-03, -1.5836e-02, -5.7884e-03],\n",
      "        [-3.4069e-03, -5.6343e-03,  1.0085e-02, -1.4491e-02, -7.2792e-03],\n",
      "        [ 2.6508e-03, -1.4599e-03,  1.0110e-02, -2.7558e-02,  2.1706e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-2.3257e-03,  6.6485e-03,  2.4370e-02,  3.3447e-03,  2.0341e-02],\n",
      "        [-3.8369e-02, -3.9320e-02,  3.4425e-02, -2.2277e-02, -6.6815e-03],\n",
      "        [-1.4290e-02, -2.1279e-02, -3.3619e-02,  5.6683e-03,  8.4621e-03],\n",
      "        [ 6.7648e-03, -3.3341e-02,  2.4027e-02, -3.6375e-03, -7.3385e-03],\n",
      "        [-5.5135e-03, -1.1459e-02,  3.8031e-02,  1.2923e-03,  2.7378e-02],\n",
      "        [-8.9059e-03,  9.4593e-05,  1.8119e-02,  2.0631e-02,  4.5496e-02],\n",
      "        [-6.1043e-03, -1.9577e-02,  2.7621e-02,  7.3513e-04, -1.5870e-02],\n",
      "        [ 3.1172e-02, -6.7562e-03,  3.7852e-02,  1.1040e-02,  1.3305e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0143, grad_fn=<MinBackward1>), tensor(0.8525, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12561249732971191\n",
      "@sample 775: tensor([[ 0.0186, -0.0219, -0.0214,  0.0199,  0.0039],\n",
      "        [ 0.0073, -0.0002, -0.0005, -0.0244, -0.0022],\n",
      "        [ 0.0257,  0.0540, -0.0202, -0.0431,  0.0240],\n",
      "        [-0.0398, -0.0004, -0.0070,  0.0230, -0.0092],\n",
      "        [ 0.0107, -0.0140,  0.0015, -0.0289, -0.0082],\n",
      "        [ 0.0097, -0.0162, -0.0057, -0.0309, -0.0018],\n",
      "        [ 0.0643, -0.0234, -0.0025,  0.0150, -0.0023],\n",
      "        [-0.0005,  0.0136, -0.0014, -0.0269,  0.0319]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0110,  0.0156,  0.0323, -0.0167,  0.0058],\n",
      "        [-0.0230, -0.0095, -0.0705,  0.0228, -0.0099],\n",
      "        [-0.0590, -0.0710, -0.0567,  0.0234, -0.0067],\n",
      "        [ 0.0252,  0.0141, -0.0290, -0.0106, -0.0099],\n",
      "        [-0.0334,  0.0014, -0.0457, -0.0001,  0.0067],\n",
      "        [-0.0240, -0.0350, -0.0219,  0.0238, -0.0015],\n",
      "        [ 0.0183,  0.0115, -0.0236,  0.0129, -0.0088],\n",
      "        [-0.0089,  0.0050, -0.0534, -0.0062, -0.0273]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0202, grad_fn=<MinBackward1>), tensor(0.9214, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1198979988694191\n",
      "@sample 776: tensor([[ 0.0065, -0.0043, -0.0090,  0.0108, -0.0511],\n",
      "        [-0.0035,  0.0037,  0.0057, -0.0055,  0.0017],\n",
      "        [-0.0289, -0.0231,  0.0085,  0.0109,  0.0117],\n",
      "        [-0.0025,  0.0259, -0.0289, -0.0155,  0.0076],\n",
      "        [ 0.0014,  0.0122, -0.0036,  0.0170, -0.0215],\n",
      "        [ 0.0010, -0.0257,  0.0040, -0.0295, -0.0095],\n",
      "        [ 0.0164,  0.0123, -0.0031, -0.0057, -0.0156],\n",
      "        [ 0.0066,  0.0139,  0.0182, -0.0119,  0.0211]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0348, -0.0127,  0.0412, -0.0204, -0.0117],\n",
      "        [-0.0008, -0.0044,  0.0032,  0.0026, -0.0239],\n",
      "        [ 0.0197, -0.0170,  0.0687, -0.0343, -0.0168],\n",
      "        [ 0.0097, -0.0117, -0.0075, -0.0117,  0.0093],\n",
      "        [ 0.0079,  0.0282,  0.0346, -0.0303, -0.0106],\n",
      "        [-0.0443, -0.0069,  0.0033, -0.0368,  0.0267],\n",
      "        [-0.0084,  0.0070, -0.0324,  0.0059,  0.0176],\n",
      "        [ 0.0064, -0.0237, -0.0404,  0.0112, -0.0190]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0205, grad_fn=<MinBackward1>), tensor(0.8867, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10633422434329987\n",
      "@sample 777: tensor([[ 0.0222,  0.0153,  0.0450, -0.0150, -0.0094],\n",
      "        [ 0.0350, -0.0061,  0.0278, -0.0072,  0.0077],\n",
      "        [ 0.0016, -0.0009, -0.0090, -0.0020, -0.0114],\n",
      "        [-0.0127,  0.0289, -0.0099, -0.0095, -0.0137],\n",
      "        [-0.0204,  0.0101,  0.0030, -0.0243,  0.0071],\n",
      "        [ 0.0008,  0.0355, -0.0217, -0.0145,  0.0043],\n",
      "        [ 0.0105,  0.0173,  0.0112,  0.0014,  0.0138],\n",
      "        [ 0.0176,  0.0183, -0.0255,  0.0187, -0.0012]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0056, -0.0109, -0.0362, -0.0222,  0.0069],\n",
      "        [-0.0090,  0.0095, -0.0096,  0.0211,  0.0276],\n",
      "        [-0.0024, -0.0031,  0.0403, -0.0312, -0.0028],\n",
      "        [-0.0109,  0.0143, -0.0256,  0.0245,  0.0048],\n",
      "        [-0.0250, -0.0140,  0.0105, -0.0027, -0.0110],\n",
      "        [-0.0148, -0.0126, -0.0103, -0.0292, -0.0092],\n",
      "        [-0.0077,  0.0237, -0.0147,  0.0193,  0.0099],\n",
      "        [ 0.0329,  0.0697,  0.0127, -0.0203, -0.0020]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0173, grad_fn=<MinBackward1>), tensor(0.8722, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10963883996009827\n",
      "@sample 778: tensor([[ 0.0233,  0.0206,  0.0308, -0.0013, -0.0053],\n",
      "        [-0.0159,  0.0049, -0.0306,  0.0182, -0.0076],\n",
      "        [-0.0064,  0.0344,  0.0279, -0.0398,  0.0132],\n",
      "        [-0.0302, -0.0062, -0.0132,  0.0136, -0.0274],\n",
      "        [-0.0149,  0.0430,  0.0066, -0.0282,  0.0099],\n",
      "        [ 0.0124, -0.0097, -0.0259,  0.0276, -0.0099],\n",
      "        [ 0.0011, -0.0050, -0.0079, -0.0058, -0.0018],\n",
      "        [ 0.0090, -0.0231,  0.0131,  0.0003,  0.0068]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0030,  0.0195, -0.0561,  0.0471, -0.0152],\n",
      "        [-0.0079,  0.0302, -0.0088, -0.0363, -0.0033],\n",
      "        [-0.0109, -0.0037, -0.0743,  0.0296,  0.0273],\n",
      "        [ 0.0134, -0.0118,  0.0372, -0.0231,  0.0054],\n",
      "        [-0.0339, -0.0061, -0.0988,  0.0502,  0.0197],\n",
      "        [ 0.0242,  0.0288, -0.0036, -0.0171, -0.0120],\n",
      "        [ 0.0155,  0.0075,  0.0182, -0.0224,  0.0025],\n",
      "        [-0.0068, -0.0077,  0.0589,  0.0056,  0.0013]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0173, grad_fn=<MinBackward1>), tensor(0.8708, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11495427042245865\n",
      "@sample 779: tensor([[ 0.0207, -0.0200, -0.0317,  0.0086, -0.0264],\n",
      "        [-0.0062,  0.0391, -0.0094, -0.0427,  0.0253],\n",
      "        [-0.0090, -0.0284, -0.0265,  0.0078, -0.0076],\n",
      "        [-0.0024, -0.0221,  0.0087, -0.0039, -0.0019],\n",
      "        [-0.0024,  0.0029,  0.0210,  0.0115,  0.0012],\n",
      "        [-0.0049,  0.0684,  0.0175, -0.0445,  0.0464],\n",
      "        [-0.0160,  0.0227,  0.0071, -0.0265,  0.0203],\n",
      "        [-0.0231, -0.0099,  0.0145,  0.0131,  0.0070]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0139, -0.0383,  0.0360, -0.0167,  0.0200],\n",
      "        [ 0.0028,  0.0033, -0.0807,  0.0482, -0.0156],\n",
      "        [ 0.0102, -0.0229,  0.0551, -0.0316,  0.0158],\n",
      "        [-0.0037,  0.0099,  0.0249,  0.0024,  0.0074],\n",
      "        [-0.0001,  0.0237, -0.0303, -0.0030,  0.0111],\n",
      "        [ 0.0026, -0.0303, -0.0948,  0.0801,  0.0078],\n",
      "        [ 0.0032, -0.0036, -0.0008,  0.0170,  0.0045],\n",
      "        [-0.0209,  0.0298, -0.0237,  0.0083, -0.0027]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0145, grad_fn=<MinBackward1>), tensor(0.8634, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12247274816036224\n",
      "@sample 780: tensor([[-0.0093, -0.0280, -0.0487,  0.0291, -0.0135],\n",
      "        [-0.0024, -0.0169, -0.0372,  0.0602, -0.0408],\n",
      "        [-0.0025,  0.0373,  0.0031, -0.0221,  0.0321],\n",
      "        [-0.0010, -0.0338,  0.0083, -0.0171,  0.0148],\n",
      "        [ 0.0097,  0.0020, -0.0291,  0.0062,  0.0003],\n",
      "        [ 0.0004, -0.0166, -0.0176,  0.0186, -0.0414],\n",
      "        [ 0.0108, -0.0060, -0.0096,  0.0138,  0.0072],\n",
      "        [ 0.0070, -0.0143, -0.0065,  0.0104,  0.0040]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0188, -0.0006,  0.0074, -0.0266, -0.0068],\n",
      "        [ 0.0479,  0.0573,  0.0236, -0.0159,  0.0453],\n",
      "        [ 0.0082, -0.0024, -0.0573,  0.0419,  0.0061],\n",
      "        [-0.0114, -0.0301,  0.0076, -0.0041, -0.0106],\n",
      "        [ 0.0042,  0.0043,  0.0017,  0.0205, -0.0017],\n",
      "        [ 0.0068,  0.0306, -0.0029, -0.0380,  0.0052],\n",
      "        [ 0.0065,  0.0065,  0.0123,  0.0101, -0.0416],\n",
      "        [ 0.0179,  0.0158, -0.0024,  0.0157,  0.0185]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0060, grad_fn=<MinBackward1>), tensor(0.8851, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11877100169658661\n",
      "@sample 781: tensor([[ 0.0090,  0.0038,  0.0124,  0.0279, -0.0054],\n",
      "        [-0.0178, -0.0141, -0.0075, -0.0081,  0.0200],\n",
      "        [ 0.0079,  0.0067,  0.0192, -0.0260,  0.0394],\n",
      "        [ 0.0207,  0.0083, -0.0389,  0.0157, -0.0089],\n",
      "        [ 0.0213, -0.0170,  0.0090, -0.0040, -0.0231],\n",
      "        [-0.0042, -0.0450, -0.0061,  0.0065,  0.0030],\n",
      "        [ 0.0173,  0.0066,  0.0296, -0.0109, -0.0048],\n",
      "        [ 0.0091,  0.0162,  0.0076, -0.0170, -0.0069]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0116,  0.0321,  0.0010,  0.0047, -0.0025],\n",
      "        [ 0.0335, -0.0132,  0.0304,  0.0096, -0.0041],\n",
      "        [-0.0024, -0.0185, -0.0292,  0.0043, -0.0354],\n",
      "        [ 0.0353,  0.0152,  0.0781, -0.0512, -0.0419],\n",
      "        [-0.0170, -0.0087,  0.0188,  0.0024, -0.0072],\n",
      "        [ 0.0264,  0.0051,  0.0223,  0.0145, -0.0017],\n",
      "        [-0.0211,  0.0305, -0.0234,  0.0377,  0.0131],\n",
      "        [ 0.0025, -0.0218, -0.0006,  0.0382,  0.0147]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0132, grad_fn=<MinBackward1>), tensor(0.8546, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10880422592163086\n",
      "@sample 782: tensor([[ 0.0079,  0.0290,  0.0178, -0.0349,  0.0148],\n",
      "        [-0.0151, -0.0374, -0.0218,  0.0194,  0.0122],\n",
      "        [-0.0066,  0.0005, -0.0039, -0.0071,  0.0031],\n",
      "        [ 0.0090, -0.0222,  0.0025,  0.0354, -0.0063],\n",
      "        [ 0.0027,  0.0083, -0.0073,  0.0098, -0.0450],\n",
      "        [ 0.0248,  0.0125,  0.0193,  0.0268, -0.0060],\n",
      "        [ 0.0048,  0.0755,  0.0156, -0.0651, -0.0030],\n",
      "        [ 0.0268,  0.0551, -0.0108, -0.0080, -0.0083]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0401, -0.0205, -0.0540,  0.0136, -0.0320],\n",
      "        [ 0.0166,  0.0038,  0.0148, -0.0326, -0.0143],\n",
      "        [-0.0044,  0.0252,  0.0072,  0.0055,  0.0260],\n",
      "        [ 0.0517,  0.0487,  0.0520, -0.0072, -0.0145],\n",
      "        [-0.0057,  0.0139, -0.0172, -0.0044, -0.0096],\n",
      "        [ 0.0278,  0.0342, -0.0404,  0.0367, -0.0106],\n",
      "        [-0.0488, -0.0278, -0.0816,  0.0414,  0.0063],\n",
      "        [-0.0234,  0.0168, -0.0415,  0.0221, -0.0026]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0167, grad_fn=<MinBackward1>), tensor(0.8527, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11950421333312988\n",
      "@sample 783: tensor([[ 0.0388,  0.0371,  0.0042, -0.0434,  0.0148],\n",
      "        [-0.0024,  0.0055,  0.0020, -0.0115,  0.0027],\n",
      "        [-0.0097,  0.0082, -0.0213,  0.0128, -0.0095],\n",
      "        [-0.0094,  0.0042,  0.0120,  0.0143,  0.0229],\n",
      "        [-0.0284,  0.0416, -0.0157,  0.0035, -0.0069],\n",
      "        [-0.0007,  0.0713,  0.0008, -0.0391,  0.0627],\n",
      "        [ 0.0041, -0.0015,  0.0115, -0.0058,  0.0287],\n",
      "        [ 0.0164,  0.0176, -0.0100, -0.0128, -0.0095]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0494, -0.0332, -0.0914,  0.0652, -0.0029],\n",
      "        [-0.0190,  0.0118,  0.0204, -0.0139, -0.0025],\n",
      "        [ 0.0135,  0.0149,  0.0398, -0.0275, -0.0183],\n",
      "        [ 0.0266,  0.0189,  0.0220, -0.0413, -0.0239],\n",
      "        [-0.0135,  0.0547, -0.0567, -0.0002,  0.0424],\n",
      "        [ 0.0123, -0.0440, -0.0566,  0.0016, -0.0166],\n",
      "        [ 0.0071,  0.0042, -0.0154, -0.0133, -0.0280],\n",
      "        [-0.0147,  0.0148, -0.0402,  0.0175, -0.0170]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0172, grad_fn=<MinBackward1>), tensor(0.8804, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11255986243486404\n",
      "@sample 784: tensor([[-0.0075, -0.0277,  0.0075,  0.0132,  0.0112],\n",
      "        [-0.0047, -0.0032, -0.0050,  0.0198, -0.0027],\n",
      "        [ 0.0048,  0.0264,  0.0006,  0.0107,  0.0051],\n",
      "        [-0.0347,  0.0700, -0.0241, -0.0892,  0.0081],\n",
      "        [ 0.0013, -0.0144,  0.0342, -0.0101,  0.0193],\n",
      "        [-0.0196,  0.0159,  0.0005, -0.0015, -0.0027],\n",
      "        [-0.0047,  0.0052,  0.0130, -0.0338,  0.0095],\n",
      "        [ 0.0154, -0.0040,  0.0127, -0.0081, -0.0091]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0153,  0.0242,  0.0023, -0.0225,  0.0049],\n",
      "        [ 0.0130,  0.0179,  0.0023,  0.0083, -0.0073],\n",
      "        [-0.0396,  0.0317, -0.0148, -0.0420,  0.0101],\n",
      "        [-0.0158, -0.0224, -0.1026,  0.0616,  0.0515],\n",
      "        [-0.0300, -0.0097, -0.0003,  0.0261, -0.0162],\n",
      "        [-0.0242,  0.0073, -0.0076,  0.0061,  0.0060],\n",
      "        [-0.0251, -0.0139, -0.0603,  0.0152,  0.0002],\n",
      "        [-0.0157, -0.0045, -0.0687,  0.0224, -0.0070]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0176, grad_fn=<MinBackward1>), tensor(0.8846, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10830540955066681\n",
      "@sample 785: tensor([[ 1.8585e-02,  5.8271e-03,  5.6193e-03, -1.1691e-02,  3.6630e-02],\n",
      "        [-8.2182e-03,  2.1354e-02, -1.2505e-02, -3.4662e-03, -4.5465e-03],\n",
      "        [-6.5859e-03,  1.1781e-03,  4.4517e-04,  5.7203e-03,  4.6430e-03],\n",
      "        [ 2.4370e-02,  4.7457e-03,  6.2390e-03, -2.9920e-02,  9.4510e-03],\n",
      "        [ 4.8395e-03,  1.1484e-02, -1.7578e-02, -1.0909e-02, -1.4664e-02],\n",
      "        [ 3.2714e-02,  3.4009e-02, -3.5045e-03, -5.6104e-02,  3.4377e-02],\n",
      "        [-6.3516e-03, -2.7626e-02, -3.4557e-02,  6.6739e-03,  1.3533e-02],\n",
      "        [ 1.7707e-05, -3.0169e-02,  2.0036e-02,  1.5675e-02,  2.1493e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-3.2222e-02, -6.7091e-03, -4.7440e-02, -1.7844e-02, -2.4503e-03],\n",
      "        [ 4.6093e-03, -6.6387e-03, -4.2742e-02,  1.3335e-02, -3.5386e-02],\n",
      "        [-1.8771e-02,  7.1106e-03,  8.9571e-05,  1.7302e-02,  1.7771e-02],\n",
      "        [-1.1843e-02, -1.0611e-02, -5.3907e-02,  3.0323e-03, -2.3935e-03],\n",
      "        [-4.4669e-02,  1.4734e-02, -2.9219e-02,  1.2420e-02,  1.6977e-02],\n",
      "        [-9.1160e-04, -5.4117e-02, -8.2209e-03,  4.3120e-02,  4.0090e-03],\n",
      "        [-5.2047e-03,  1.6131e-02,  4.6082e-02,  8.7843e-03,  1.7276e-02],\n",
      "        [ 8.3994e-04, -1.6969e-02,  5.7544e-04,  4.9512e-03, -8.7890e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0123, grad_fn=<MinBackward1>), tensor(0.8612, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10993978381156921\n",
      "@sample 786: tensor([[ 0.0101, -0.0415, -0.0460, -0.0264,  0.0238],\n",
      "        [ 0.0190,  0.0087,  0.0242,  0.0070, -0.0261],\n",
      "        [ 0.0044, -0.0027, -0.0472,  0.0175, -0.0041],\n",
      "        [ 0.0177, -0.0119,  0.0173,  0.0270, -0.0121],\n",
      "        [-0.0106, -0.0018,  0.0295,  0.0181, -0.0008],\n",
      "        [ 0.0296, -0.0036,  0.0342, -0.0464,  0.0327],\n",
      "        [-0.0067,  0.0088, -0.0074,  0.0124,  0.0068],\n",
      "        [ 0.0069,  0.0158,  0.0067, -0.0064,  0.0126]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0406, -0.0957,  0.0368, -0.0042, -0.0442],\n",
      "        [-0.0143,  0.0197, -0.0144,  0.0173, -0.0214],\n",
      "        [ 0.0229, -0.0482,  0.0483,  0.0133, -0.0327],\n",
      "        [-0.0016,  0.0242, -0.0258, -0.0158,  0.0169],\n",
      "        [ 0.0183,  0.0162,  0.0079, -0.0098, -0.0047],\n",
      "        [-0.0216, -0.0241, -0.0789,  0.0378, -0.0283],\n",
      "        [-0.0049,  0.0098, -0.0033, -0.0112, -0.0135],\n",
      "        [-0.0011, -0.0122, -0.0246, -0.0399, -0.0180]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0164, grad_fn=<MinBackward1>), tensor(0.9263, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.13187450170516968\n",
      "@sample 787: tensor([[ 0.0111,  0.0058, -0.0064,  0.0188,  0.0031],\n",
      "        [ 0.0377, -0.0099, -0.0136,  0.0016,  0.0041],\n",
      "        [ 0.0050,  0.0141,  0.0033, -0.0356,  0.0323],\n",
      "        [ 0.0142,  0.0069,  0.0240,  0.0152,  0.0030],\n",
      "        [-0.0110,  0.0277,  0.0338, -0.0242, -0.0043],\n",
      "        [-0.0045, -0.0219,  0.0351, -0.0100, -0.0060],\n",
      "        [-0.0208, -0.0326,  0.0362, -0.0151,  0.0147],\n",
      "        [ 0.0086, -0.0101, -0.0044, -0.0066,  0.0010]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0271,  0.0076, -0.0317, -0.0078, -0.0022],\n",
      "        [-0.0140, -0.0109,  0.0028,  0.0104, -0.0053],\n",
      "        [-0.0335, -0.0253, -0.0920,  0.0243, -0.0375],\n",
      "        [ 0.0212,  0.0303, -0.0367,  0.0032,  0.0047],\n",
      "        [-0.0200,  0.0063,  0.0060,  0.0027,  0.0265],\n",
      "        [-0.0057, -0.0244,  0.0080,  0.0321,  0.0022],\n",
      "        [ 0.0031,  0.0213,  0.0465,  0.0259,  0.0108],\n",
      "        [-0.0023, -0.0040, -0.0181,  0.0092, -0.0219]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0198, grad_fn=<MinBackward1>), tensor(0.8928, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10436840355396271\n",
      "@sample 788: tensor([[-2.2343e-02, -1.9180e-02,  2.4947e-02, -2.4921e-03, -5.5962e-03],\n",
      "        [ 1.6787e-02,  3.7991e-02, -2.4187e-02,  2.8293e-03,  9.0903e-03],\n",
      "        [ 3.7591e-03, -3.1096e-03,  2.3790e-03,  1.3841e-02,  4.4029e-03],\n",
      "        [ 1.8155e-02,  1.3323e-02, -1.4390e-02,  1.2190e-03, -2.1649e-02],\n",
      "        [-2.5552e-03, -1.3133e-02,  1.6217e-02, -7.0459e-04,  3.7904e-02],\n",
      "        [-6.7737e-03,  3.6174e-02, -8.5440e-04, -5.1716e-04, -7.2204e-05],\n",
      "        [-1.0350e-03, -1.9567e-02,  2.1486e-02, -1.2281e-02,  2.9097e-02],\n",
      "        [ 2.1797e-02, -1.4732e-03, -1.6425e-02,  3.4267e-03, -1.6389e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0299, -0.0183,  0.0440,  0.0041, -0.0017],\n",
      "        [ 0.0008, -0.0088, -0.0188, -0.0070, -0.0095],\n",
      "        [-0.0005,  0.0026,  0.0054, -0.0011, -0.0304],\n",
      "        [-0.0161,  0.0258, -0.0200,  0.0038,  0.0145],\n",
      "        [-0.0095, -0.0233, -0.0168, -0.0210, -0.0125],\n",
      "        [-0.0339,  0.0099, -0.0222,  0.0132, -0.0037],\n",
      "        [-0.0397, -0.0031, -0.0140, -0.0143, -0.0427],\n",
      "        [-0.0299, -0.0208,  0.0285, -0.0425, -0.0186]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0196, grad_fn=<MinBackward1>), tensor(0.8828, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11012092977762222\n",
      "@sample 789: tensor([[ 0.0183,  0.0438,  0.0326, -0.0293,  0.0291],\n",
      "        [ 0.0175, -0.0053, -0.0117, -0.0180,  0.0382],\n",
      "        [-0.0252, -0.0043, -0.0058,  0.0025, -0.0044],\n",
      "        [ 0.0332,  0.0129,  0.0021, -0.0100,  0.0047],\n",
      "        [-0.0078, -0.0151, -0.0154,  0.0255, -0.0068],\n",
      "        [-0.0107, -0.0166,  0.0180, -0.0123,  0.0027],\n",
      "        [-0.0045, -0.0391, -0.0224,  0.0015, -0.0073],\n",
      "        [-0.0158,  0.0269, -0.0018,  0.0239, -0.0020]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0220, -0.0030, -0.0740,  0.0258, -0.0003],\n",
      "        [ 0.0062,  0.0275,  0.0315, -0.0047, -0.0059],\n",
      "        [ 0.0214,  0.0058,  0.0523, -0.0284, -0.0023],\n",
      "        [ 0.0163,  0.0052, -0.0424,  0.0270,  0.0128],\n",
      "        [ 0.0091,  0.0147,  0.0500, -0.0229, -0.0166],\n",
      "        [-0.0067, -0.0093,  0.0144, -0.0047, -0.0069],\n",
      "        [ 0.0268, -0.0162,  0.0366, -0.0206,  0.0395],\n",
      "        [-0.0121,  0.0244, -0.0173, -0.0472, -0.0044]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0182, grad_fn=<MinBackward1>), tensor(0.9124, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11138605326414108\n",
      "@sample 790: tensor([[ 0.0044,  0.0002, -0.0103, -0.0198, -0.0028],\n",
      "        [ 0.0086, -0.0089, -0.0093,  0.0115, -0.0247],\n",
      "        [-0.0142,  0.0222,  0.0293, -0.0179,  0.0169],\n",
      "        [ 0.0193,  0.0263, -0.0059, -0.0075,  0.0073],\n",
      "        [ 0.0209,  0.0013, -0.0239,  0.0017, -0.0084],\n",
      "        [ 0.0182, -0.0152, -0.0031,  0.0338, -0.0491],\n",
      "        [-0.0198, -0.0115, -0.0039,  0.0097, -0.0337],\n",
      "        [-0.0168, -0.0333, -0.0144,  0.0177,  0.0149]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0040, -0.0266, -0.0188,  0.0009,  0.0041],\n",
      "        [ 0.0096,  0.0216,  0.0352, -0.0242,  0.0136],\n",
      "        [-0.0100, -0.0111,  0.0272,  0.0040,  0.0148],\n",
      "        [-0.0040, -0.0112, -0.0393,  0.0194,  0.0005],\n",
      "        [ 0.0065,  0.0167, -0.0501, -0.0145, -0.0157],\n",
      "        [-0.0154,  0.0155, -0.0007,  0.0049, -0.0102],\n",
      "        [ 0.0005,  0.0062,  0.0620, -0.0146,  0.0066],\n",
      "        [ 0.0265,  0.0127,  0.0426, -0.0024,  0.0427]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0172, grad_fn=<MinBackward1>), tensor(0.9091, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11108151823282242\n",
      "@sample 791: tensor([[ 0.0092,  0.0199, -0.0038,  0.0165,  0.0014],\n",
      "        [-0.0177,  0.0011,  0.0026, -0.0413,  0.0280],\n",
      "        [ 0.0110, -0.0403,  0.0143,  0.0326, -0.0187],\n",
      "        [-0.0183, -0.0058,  0.0082,  0.0052,  0.0003],\n",
      "        [-0.0098, -0.0217, -0.0198, -0.0200,  0.0040],\n",
      "        [-0.0131,  0.0060,  0.0153, -0.0170,  0.0127],\n",
      "        [-0.0025, -0.0069,  0.0136,  0.0211, -0.0218],\n",
      "        [-0.0237,  0.0130,  0.0379, -0.0271,  0.0123]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0051,  0.0333, -0.0380,  0.0342,  0.0186],\n",
      "        [ 0.0202, -0.0398, -0.0134, -0.0095, -0.0186],\n",
      "        [-0.0029,  0.0280, -0.0034, -0.0243, -0.0035],\n",
      "        [-0.0231,  0.0296, -0.0254,  0.0346, -0.0011],\n",
      "        [ 0.0068, -0.0246,  0.0129, -0.0061,  0.0442],\n",
      "        [ 0.0303,  0.0512, -0.0230,  0.0201, -0.0169],\n",
      "        [-0.0006,  0.0119,  0.0124, -0.0071,  0.0025],\n",
      "        [-0.0115, -0.0407, -0.0240,  0.0370, -0.0005]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0119, grad_fn=<MinBackward1>), tensor(0.8742, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10662443190813065\n",
      "@sample 792: tensor([[-0.0161, -0.0019, -0.0040,  0.0205,  0.0036],\n",
      "        [-0.0044, -0.0155, -0.0052,  0.0344,  0.0051],\n",
      "        [ 0.0081,  0.0484,  0.0012, -0.0253,  0.0036],\n",
      "        [-0.0189, -0.0037, -0.0078, -0.0053,  0.0014],\n",
      "        [ 0.0182,  0.0013, -0.0103,  0.0239, -0.0014],\n",
      "        [-0.0052, -0.0245, -0.0242,  0.0422, -0.0359],\n",
      "        [ 0.0203, -0.0405,  0.0071,  0.0248, -0.0095],\n",
      "        [ 0.0147, -0.0152, -0.0150,  0.0428, -0.0102]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0148,  0.0274,  0.0191, -0.0335, -0.0091],\n",
      "        [-0.0005, -0.0049, -0.0482,  0.0207, -0.0195],\n",
      "        [-0.0154, -0.0034, -0.0462,  0.0402, -0.0045],\n",
      "        [ 0.0181,  0.0005,  0.0043, -0.0056,  0.0028],\n",
      "        [ 0.0336, -0.0115,  0.0179,  0.0038,  0.0122],\n",
      "        [ 0.0094, -0.0216, -0.0033, -0.0040, -0.0409],\n",
      "        [ 0.0375, -0.0145,  0.0237, -0.0078, -0.0238],\n",
      "        [ 0.0408,  0.0225, -0.0346, -0.0167,  0.0110]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0204, grad_fn=<MinBackward1>), tensor(0.8400, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1261267066001892\n",
      "@sample 793: tensor([[-0.0153,  0.0237,  0.0078,  0.0073, -0.0058],\n",
      "        [ 0.0006,  0.0766,  0.0137, -0.0086,  0.0228],\n",
      "        [ 0.0010,  0.0020,  0.0107, -0.0224,  0.0057],\n",
      "        [-0.0013, -0.0089,  0.0245,  0.0325, -0.0195],\n",
      "        [ 0.0032, -0.0055,  0.0025,  0.0247, -0.0136],\n",
      "        [-0.0064,  0.0340,  0.0504, -0.0223,  0.0081],\n",
      "        [ 0.0099,  0.0436,  0.0265, -0.0014,  0.0187],\n",
      "        [-0.0029, -0.0033, -0.0031,  0.0009,  0.0038]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0094,  0.0113,  0.0484, -0.0378, -0.0137],\n",
      "        [-0.0211, -0.0228, -0.0805,  0.0175,  0.0271],\n",
      "        [-0.0068,  0.0015,  0.0015, -0.0061, -0.0153],\n",
      "        [ 0.0033,  0.0393,  0.0495, -0.0466, -0.0046],\n",
      "        [ 0.0113,  0.0205,  0.0336,  0.0353,  0.0366],\n",
      "        [ 0.0074,  0.0289, -0.0256,  0.0229,  0.0121],\n",
      "        [-0.0198, -0.0016, -0.0595,  0.0300, -0.0068],\n",
      "        [ 0.0103,  0.0016,  0.0064, -0.0003, -0.0030]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0216, grad_fn=<MinBackward1>), tensor(0.8483, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10362616926431656\n",
      "@sample 794: tensor([[ 2.3296e-02,  2.5161e-02, -3.8431e-03, -2.6041e-02,  8.8559e-03],\n",
      "        [-4.5576e-02,  4.7425e-02,  2.8637e-02, -2.9693e-02,  2.2963e-02],\n",
      "        [ 1.6672e-02, -2.2577e-02, -1.8391e-02,  4.2965e-02, -2.0950e-02],\n",
      "        [ 2.7174e-02, -2.9412e-03,  6.3773e-02,  6.0085e-03, -3.6469e-03],\n",
      "        [-2.0029e-02,  9.9894e-03, -6.7547e-05, -5.3646e-04,  1.7979e-02],\n",
      "        [-2.1027e-02, -6.2244e-04, -2.3154e-02,  3.0456e-02,  5.8390e-03],\n",
      "        [ 4.7252e-03,  7.6863e-04,  9.2490e-03,  1.1852e-02,  1.5627e-02],\n",
      "        [-1.3516e-02,  1.9109e-02,  5.3399e-03, -4.1357e-03, -1.5898e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0172,  0.0076, -0.0388,  0.0227, -0.0101],\n",
      "        [-0.0357, -0.0097, -0.0106,  0.0127,  0.0188],\n",
      "        [ 0.0198, -0.0031,  0.0359, -0.0076,  0.0263],\n",
      "        [-0.0228,  0.0192, -0.0348, -0.0054,  0.0055],\n",
      "        [-0.0318, -0.0194, -0.0733,  0.0232, -0.0052],\n",
      "        [ 0.0389,  0.0182,  0.0072,  0.0044,  0.0021],\n",
      "        [-0.0017,  0.0299, -0.0569,  0.0340,  0.0052],\n",
      "        [ 0.0107, -0.0027,  0.0249,  0.0109,  0.0257]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0143, grad_fn=<MinBackward1>), tensor(0.8968, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12188258767127991\n",
      "@sample 795: tensor([[ 0.0066,  0.0237,  0.0218, -0.0181, -0.0104],\n",
      "        [ 0.0025, -0.0007,  0.0039,  0.0096, -0.0254],\n",
      "        [ 0.0151,  0.0026, -0.0048,  0.0014, -0.0092],\n",
      "        [-0.0070, -0.0095,  0.0004,  0.0145, -0.0023],\n",
      "        [-0.0111, -0.0067,  0.0084, -0.0122,  0.0092],\n",
      "        [ 0.0125, -0.0113, -0.0398,  0.0190,  0.0050],\n",
      "        [ 0.0037,  0.0185, -0.0011, -0.0274,  0.0095],\n",
      "        [ 0.0096, -0.0162, -0.0210,  0.0021,  0.0035]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0142, -0.0076, -0.0216,  0.0079,  0.0030],\n",
      "        [ 0.0043,  0.0271,  0.0139, -0.0102,  0.0208],\n",
      "        [-0.0192, -0.0087, -0.0110, -0.0040,  0.0090],\n",
      "        [ 0.0186, -0.0062,  0.0231, -0.0292, -0.0236],\n",
      "        [ 0.0140, -0.0054,  0.0109, -0.0140, -0.0219],\n",
      "        [ 0.0280,  0.0330, -0.0131,  0.0193,  0.0253],\n",
      "        [-0.0190, -0.0112, -0.0054,  0.0089, -0.0006],\n",
      "        [ 0.0046, -0.0067, -0.0224, -0.0199,  0.0023]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0131, grad_fn=<MinBackward1>), tensor(0.8472, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09969935566186905\n",
      "@sample 796: tensor([[-0.0004, -0.0075,  0.0056,  0.0265,  0.0034],\n",
      "        [-0.0141, -0.0129, -0.0125,  0.0232, -0.0097],\n",
      "        [ 0.0105,  0.0108, -0.0023, -0.0177, -0.0223],\n",
      "        [ 0.0218,  0.0240,  0.0170,  0.0374, -0.0269],\n",
      "        [ 0.0055,  0.0039, -0.0092, -0.0121, -0.0061],\n",
      "        [ 0.0033,  0.0526,  0.0234, -0.0208, -0.0042],\n",
      "        [-0.0070, -0.0014,  0.0125, -0.0190,  0.0212],\n",
      "        [ 0.0337, -0.0105,  0.0128,  0.0063, -0.0272]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0259,  0.0149,  0.0008, -0.0415, -0.0023],\n",
      "        [-0.0041,  0.0033,  0.0257, -0.0330,  0.0049],\n",
      "        [-0.0362,  0.0143, -0.0112,  0.0013, -0.0173],\n",
      "        [-0.0112,  0.0441, -0.0455, -0.0252, -0.0216],\n",
      "        [-0.0279, -0.0193, -0.0337,  0.0142, -0.0245],\n",
      "        [ 0.0199, -0.0049, -0.0478,  0.0448,  0.0065],\n",
      "        [-0.0136, -0.0133, -0.0599,  0.0177, -0.0070],\n",
      "        [ 0.0191,  0.0233,  0.0117, -0.0072,  0.0144]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0160, grad_fn=<MinBackward1>), tensor(0.8945, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11056984215974808\n",
      "@sample 797: tensor([[ 0.0143, -0.0115, -0.0142,  0.0063, -0.0350],\n",
      "        [-0.0031, -0.0201, -0.0010,  0.0209, -0.0132],\n",
      "        [ 0.0068, -0.0257, -0.0183,  0.0105, -0.0097],\n",
      "        [ 0.0046, -0.0078, -0.0435, -0.0012,  0.0104],\n",
      "        [ 0.0046,  0.0632,  0.0065, -0.0562,  0.0459],\n",
      "        [-0.0002,  0.0392, -0.0251,  0.0298, -0.0380],\n",
      "        [ 0.0077, -0.0053, -0.0183,  0.0147,  0.0201],\n",
      "        [-0.0231,  0.0001, -0.0196, -0.0098,  0.0200]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0010,  0.0048,  0.0024,  0.0047,  0.0202],\n",
      "        [-0.0183,  0.0108, -0.0160, -0.0279, -0.0246],\n",
      "        [ 0.0003, -0.0142,  0.0340, -0.0222,  0.0045],\n",
      "        [-0.0064,  0.0039, -0.0476, -0.0061,  0.0065],\n",
      "        [-0.0046, -0.0330, -0.0792,  0.0516, -0.0217],\n",
      "        [ 0.0071,  0.0578, -0.0445, -0.0066,  0.0085],\n",
      "        [ 0.0093, -0.0075, -0.0039, -0.0080,  0.0030],\n",
      "        [-0.0018, -0.0119,  0.0348,  0.0128, -0.0039]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0233, grad_fn=<MinBackward1>), tensor(0.8411, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11943195760250092\n",
      "@sample 798: tensor([[-0.0220, -0.0190,  0.0093, -0.0354, -0.0011],\n",
      "        [-0.0119, -0.0093,  0.0080, -0.0087,  0.0019],\n",
      "        [-0.0219,  0.0455,  0.0019, -0.0307,  0.0140],\n",
      "        [-0.0025, -0.0089, -0.0071, -0.0078,  0.0142],\n",
      "        [-0.0090,  0.0007,  0.0121, -0.0015, -0.0172],\n",
      "        [-0.0262, -0.0390, -0.0237,  0.0121,  0.0071],\n",
      "        [-0.0046,  0.0405, -0.0140, -0.0395,  0.0153],\n",
      "        [-0.0097, -0.0293,  0.0015,  0.0188, -0.0031]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0320, -0.0400,  0.0332, -0.0096, -0.0215],\n",
      "        [-0.0290, -0.0113,  0.0184, -0.0020, -0.0242],\n",
      "        [-0.0344, -0.0208, -0.0166,  0.0092, -0.0420],\n",
      "        [ 0.0105, -0.0100, -0.0398,  0.0010,  0.0062],\n",
      "        [ 0.0083, -0.0251,  0.0223, -0.0236,  0.0122],\n",
      "        [-0.0156, -0.0550,  0.0570,  0.0033, -0.0024],\n",
      "        [ 0.0019, -0.0243, -0.0495,  0.0526, -0.0099],\n",
      "        [ 0.0175, -0.0096,  0.0331, -0.0140, -0.0102]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0183, grad_fn=<MinBackward1>), tensor(0.8992, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10828088968992233\n",
      "@sample 799: tensor([[-0.0291, -0.0010,  0.0005,  0.0077,  0.0118],\n",
      "        [ 0.0124, -0.0201, -0.0325, -0.0068,  0.0069],\n",
      "        [ 0.0009, -0.0020,  0.0032, -0.0362, -0.0164],\n",
      "        [ 0.0106,  0.0194,  0.0275, -0.0530, -0.0108],\n",
      "        [ 0.0091, -0.0158, -0.0099,  0.0071, -0.0160],\n",
      "        [ 0.0215,  0.0459, -0.0144, -0.0332,  0.0037],\n",
      "        [-0.0006,  0.0625,  0.0025, -0.0090,  0.0191],\n",
      "        [-0.0373, -0.0268,  0.0090,  0.0269, -0.0170]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0075, -0.0064, -0.0340,  0.0309,  0.0204],\n",
      "        [-0.0317, -0.0088, -0.0030,  0.0119, -0.0120],\n",
      "        [-0.0200, -0.0036,  0.0222,  0.0010,  0.0032],\n",
      "        [-0.0432, -0.0064, -0.0050, -0.0130, -0.0057],\n",
      "        [ 0.0162,  0.0210,  0.0428, -0.0039,  0.0374],\n",
      "        [-0.0154, -0.0421, -0.0563,  0.0738,  0.0317],\n",
      "        [-0.0070,  0.0194, -0.0237,  0.0179, -0.0504],\n",
      "        [ 0.0321, -0.0445,  0.0691, -0.0607, -0.0234]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0182, grad_fn=<MinBackward1>), tensor(0.9012, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1105322614312172\n",
      "@sample 800: tensor([[ 0.0238, -0.0043,  0.0083, -0.0206,  0.0158],\n",
      "        [-0.0431, -0.0116, -0.0186,  0.0240, -0.0160],\n",
      "        [-0.0032, -0.0090, -0.0030,  0.0178,  0.0078],\n",
      "        [ 0.0121,  0.0094,  0.0059,  0.0002, -0.0268],\n",
      "        [-0.0208,  0.0052, -0.0081,  0.0021,  0.0135],\n",
      "        [ 0.0088,  0.0421,  0.0233, -0.0344,  0.0342],\n",
      "        [-0.0160, -0.0256, -0.0213, -0.0045, -0.0002],\n",
      "        [-0.0033, -0.0103,  0.0117, -0.0024,  0.0035]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-5.7308e-02,  3.5865e-02, -1.2137e-02, -2.0273e-02, -8.6036e-04],\n",
      "        [ 4.0625e-02, -9.7417e-03,  3.7668e-02,  5.3187e-03,  1.5263e-02],\n",
      "        [ 1.0863e-02, -2.5854e-06, -5.6126e-03, -2.8680e-03, -7.1006e-03],\n",
      "        [-2.2576e-02,  2.2811e-02,  4.1730e-03, -6.6395e-04,  8.7909e-04],\n",
      "        [ 3.3557e-02,  3.7353e-03,  1.1645e-02, -3.6923e-02,  8.4819e-04],\n",
      "        [-3.0125e-03, -1.1683e-02, -2.0898e-02,  9.1806e-03, -1.5039e-02],\n",
      "        [ 4.6187e-03, -2.0671e-02,  7.2129e-02, -2.8331e-02, -7.9393e-03],\n",
      "        [-2.2983e-02, -2.5459e-02, -1.4378e-02,  7.3695e-03,  5.3346e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0163, grad_fn=<MinBackward1>), tensor(0.8760, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11006541550159454\n",
      "@sample 801: tensor([[ 0.0016, -0.0062,  0.0336,  0.0050,  0.0042],\n",
      "        [-0.0034,  0.0325, -0.0104, -0.0463, -0.0169],\n",
      "        [-0.0039,  0.0158,  0.0037, -0.0141,  0.0112],\n",
      "        [ 0.0037, -0.0069, -0.0167, -0.0310,  0.0009],\n",
      "        [-0.0441, -0.0181, -0.0163, -0.0305,  0.0402],\n",
      "        [-0.0307, -0.0066,  0.0076, -0.0084,  0.0079],\n",
      "        [-0.0081, -0.0243, -0.0118, -0.0217,  0.0059],\n",
      "        [ 0.0094, -0.0103,  0.0079,  0.0226, -0.0024]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0105, -0.0299,  0.0357, -0.0558, -0.0423],\n",
      "        [-0.0082, -0.0722, -0.0632,  0.0680,  0.0268],\n",
      "        [-0.0178, -0.0211, -0.0092, -0.0286, -0.0272],\n",
      "        [-0.0019, -0.0497,  0.0443, -0.0674, -0.0168],\n",
      "        [-0.0140, -0.0650,  0.0301,  0.0208, -0.0226],\n",
      "        [-0.0095, -0.0223,  0.0261, -0.0039, -0.0026],\n",
      "        [-0.0008, -0.0175,  0.0444, -0.0027,  0.0154],\n",
      "        [ 0.0160,  0.0239,  0.0215, -0.0112,  0.0150]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0140, grad_fn=<MinBackward1>), tensor(0.8648, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12042077630758286\n",
      "@sample 802: tensor([[ 0.0192,  0.0036,  0.0054, -0.0099, -0.0041],\n",
      "        [ 0.0099, -0.0089, -0.0081,  0.0292, -0.0301],\n",
      "        [-0.0026, -0.0423, -0.0235,  0.0032,  0.0006],\n",
      "        [ 0.0335,  0.0155, -0.0133, -0.0241, -0.0051],\n",
      "        [-0.0130, -0.0078,  0.0135, -0.0235,  0.0097],\n",
      "        [ 0.0038, -0.0317, -0.0057,  0.0222, -0.0062],\n",
      "        [-0.0304,  0.0125, -0.0142, -0.0435, -0.0162],\n",
      "        [ 0.0124, -0.0151, -0.0349,  0.0157,  0.0058]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0215,  0.0008, -0.0115,  0.0113,  0.0318],\n",
      "        [-0.0083, -0.0317,  0.0404, -0.0388, -0.0336],\n",
      "        [ 0.0008, -0.0109,  0.0340, -0.0163, -0.0163],\n",
      "        [-0.0187, -0.0071, -0.0574, -0.0208, -0.0093],\n",
      "        [-0.0144, -0.0510,  0.0510, -0.0316,  0.0166],\n",
      "        [ 0.0179, -0.0021,  0.0379,  0.0092,  0.0036],\n",
      "        [-0.0152, -0.0200, -0.0355,  0.0126,  0.0204],\n",
      "        [ 0.0054, -0.0131,  0.0001, -0.0035, -0.0087]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0130, grad_fn=<MinBackward1>), tensor(0.8407, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11261841654777527\n",
      "@sample 803: tensor([[ 0.0441,  0.0229,  0.0346, -0.0326, -0.0201],\n",
      "        [-0.0006, -0.0381, -0.0089,  0.0031, -0.0037],\n",
      "        [-0.0053, -0.0252, -0.0120,  0.0378, -0.0046],\n",
      "        [-0.0086,  0.0236,  0.0126, -0.0099, -0.0165],\n",
      "        [ 0.0091,  0.0033,  0.0409,  0.0351, -0.0138],\n",
      "        [-0.0234,  0.0428,  0.0107, -0.0079, -0.0009],\n",
      "        [ 0.0217, -0.0171,  0.0104, -0.0073,  0.0160],\n",
      "        [ 0.0035,  0.0225,  0.0175, -0.0212,  0.0077]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0524, -0.0071, -0.0461,  0.0448, -0.0274],\n",
      "        [-0.0151, -0.0523,  0.0108,  0.0037, -0.0035],\n",
      "        [ 0.0365,  0.0231,  0.0145,  0.0086,  0.0075],\n",
      "        [-0.0384, -0.0233, -0.0155,  0.0142,  0.0125],\n",
      "        [-0.0133,  0.0575,  0.0331, -0.0213, -0.0066],\n",
      "        [-0.0219, -0.0147, -0.0709,  0.0171, -0.0369],\n",
      "        [ 0.0381, -0.0063, -0.0062, -0.0343, -0.0277],\n",
      "        [-0.0281, -0.0255, -0.0378,  0.0116, -0.0041]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0182, grad_fn=<MinBackward1>), tensor(0.8937, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11410810798406601\n",
      "@sample 804: tensor([[ 0.0055,  0.0336,  0.0126, -0.0136, -0.0005],\n",
      "        [-0.0276, -0.0206, -0.0048,  0.0291, -0.0157],\n",
      "        [-0.0051, -0.0310,  0.0034,  0.0155, -0.0356],\n",
      "        [-0.0074,  0.0258,  0.0282,  0.0083,  0.0003],\n",
      "        [-0.0185,  0.0110, -0.0161,  0.0081,  0.0225],\n",
      "        [ 0.0135, -0.0018, -0.0124,  0.0107, -0.0173],\n",
      "        [-0.0409, -0.0077,  0.0071, -0.0051, -0.0205],\n",
      "        [-0.0185,  0.0160,  0.0259, -0.0088,  0.0243]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0082,  0.0334, -0.0367,  0.0123,  0.0042],\n",
      "        [ 0.0165,  0.0269,  0.0367,  0.0059,  0.0128],\n",
      "        [ 0.0365,  0.0077,  0.0542, -0.0102,  0.0410],\n",
      "        [ 0.0038,  0.0133, -0.0164,  0.0065,  0.0004],\n",
      "        [ 0.0213, -0.0060,  0.0152, -0.0188, -0.0017],\n",
      "        [-0.0138,  0.0091,  0.0126, -0.0055,  0.0417],\n",
      "        [-0.0021, -0.0233,  0.0025, -0.0301,  0.0031],\n",
      "        [ 0.0186,  0.0069, -0.0305,  0.0697,  0.0428]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0173, grad_fn=<MinBackward1>), tensor(0.8632, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11619379371404648\n",
      "@sample 805: tensor([[-0.0037,  0.0152,  0.0260, -0.0257,  0.0311],\n",
      "        [-0.0089,  0.0041, -0.0066,  0.0347, -0.0021],\n",
      "        [ 0.0051,  0.0059,  0.0081, -0.0016,  0.0114],\n",
      "        [ 0.0063, -0.0224,  0.0195,  0.0036,  0.0050],\n",
      "        [ 0.0248, -0.0121,  0.0083, -0.0058,  0.0296],\n",
      "        [-0.0186, -0.0256,  0.0297,  0.0080, -0.0068],\n",
      "        [ 0.0055, -0.0045,  0.0080, -0.0007, -0.0182],\n",
      "        [-0.0381, -0.0040, -0.0358,  0.0204,  0.0102]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0209, -0.0300, -0.0354, -0.0220, -0.0381],\n",
      "        [ 0.0023,  0.0265, -0.0060, -0.0408, -0.0074],\n",
      "        [-0.0249,  0.0064, -0.0008,  0.0276,  0.0026],\n",
      "        [-0.0132, -0.0236,  0.0189, -0.0057, -0.0337],\n",
      "        [-0.0156, -0.0038, -0.0500,  0.0286, -0.0045],\n",
      "        [ 0.0070,  0.0016, -0.0016, -0.0356,  0.0179],\n",
      "        [-0.0021, -0.0027, -0.0382, -0.0025,  0.0024],\n",
      "        [ 0.0650,  0.0255,  0.0207,  0.0088,  0.0413]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0084, grad_fn=<MinBackward1>), tensor(0.8791, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10356112569570541\n",
      "@sample 806: tensor([[-0.0091, -0.0318, -0.0242,  0.0093,  0.0023],\n",
      "        [-0.0181, -0.0100, -0.0311, -0.0148,  0.0115],\n",
      "        [ 0.0145, -0.0238, -0.0178,  0.0450, -0.0217],\n",
      "        [-0.0072,  0.0318,  0.0273, -0.0206,  0.0089],\n",
      "        [ 0.0072, -0.0059,  0.0279, -0.0268,  0.0532],\n",
      "        [ 0.0254,  0.0033,  0.0409,  0.0020, -0.0195],\n",
      "        [ 0.0013,  0.0095, -0.0048,  0.0045,  0.0143],\n",
      "        [-0.0185,  0.0053,  0.0108, -0.0081,  0.0073]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0198, -0.0458, -0.0014, -0.0188,  0.0142],\n",
      "        [-0.0291, -0.0552,  0.0162, -0.0080, -0.0401],\n",
      "        [ 0.0101,  0.0405,  0.0395, -0.0415,  0.0111],\n",
      "        [-0.0043,  0.0119, -0.0830,  0.0719,  0.0205],\n",
      "        [-0.0055,  0.0318, -0.0564,  0.0450,  0.0417],\n",
      "        [-0.0497, -0.0067, -0.0197,  0.0530,  0.0225],\n",
      "        [-0.0155,  0.0124, -0.0094, -0.0151, -0.0017],\n",
      "        [-0.0089, -0.0017, -0.0089, -0.0011, -0.0024]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0227, grad_fn=<MinBackward1>), tensor(0.8870, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12137138843536377\n",
      "@sample 807: tensor([[ 0.0229, -0.0012, -0.0161,  0.0269, -0.0362],\n",
      "        [-0.0089,  0.0009, -0.0050,  0.0260,  0.0065],\n",
      "        [-0.0004,  0.0008,  0.0020,  0.0109,  0.0229],\n",
      "        [ 0.0134, -0.0137, -0.0076,  0.0275,  0.0041],\n",
      "        [ 0.0008,  0.0426, -0.0172, -0.0423,  0.0199],\n",
      "        [ 0.0006,  0.0110,  0.0117,  0.0125, -0.0055],\n",
      "        [-0.0259, -0.0016,  0.0117,  0.0219, -0.0052],\n",
      "        [-0.0099, -0.0127,  0.0035,  0.0105,  0.0083]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0036, -0.0206,  0.0006,  0.0200,  0.0248],\n",
      "        [-0.0163,  0.0142,  0.0285,  0.0015, -0.0315],\n",
      "        [ 0.0268, -0.0014, -0.0207,  0.0113, -0.0049],\n",
      "        [ 0.0088,  0.0150, -0.0100, -0.0113, -0.0202],\n",
      "        [-0.0550, -0.0275, -0.0557,  0.0218, -0.0327],\n",
      "        [ 0.0037,  0.0280,  0.0080, -0.0011, -0.0001],\n",
      "        [ 0.0395,  0.0149,  0.0088, -0.0534, -0.0043],\n",
      "        [ 0.0258, -0.0109,  0.0570, -0.0589, -0.0105]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0191, grad_fn=<MinBackward1>), tensor(0.8788, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1124315932393074\n",
      "@sample 808: tensor([[ 0.0185,  0.0198,  0.0116, -0.0207,  0.0073],\n",
      "        [-0.0050, -0.0064, -0.0274,  0.0075,  0.0021],\n",
      "        [-0.0186, -0.0086, -0.0058,  0.0234,  0.0124],\n",
      "        [-0.0263,  0.0085, -0.0159, -0.0342,  0.0176],\n",
      "        [-0.0057, -0.0074, -0.0402,  0.0199,  0.0153],\n",
      "        [ 0.0028, -0.0121, -0.0004,  0.0132, -0.0273],\n",
      "        [ 0.0217, -0.0286,  0.0167, -0.0035,  0.0275],\n",
      "        [-0.0204,  0.0157,  0.0063, -0.0067, -0.0061]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0716, -0.0133,  0.0009,  0.0366,  0.0019],\n",
      "        [-0.0081, -0.0057, -0.0703,  0.0145,  0.0060],\n",
      "        [ 0.0181,  0.0291,  0.0217, -0.0395, -0.0042],\n",
      "        [ 0.0077,  0.0298, -0.0042,  0.0368,  0.0062],\n",
      "        [ 0.0380,  0.0421, -0.0489, -0.0053,  0.0177],\n",
      "        [ 0.0107,  0.0511, -0.0021, -0.0147,  0.0344],\n",
      "        [-0.0004,  0.0598, -0.0273,  0.0057,  0.0264],\n",
      "        [-0.0066, -0.0089,  0.0505, -0.0343, -0.0317]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0131, grad_fn=<MinBackward1>), tensor(0.9059, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11929464340209961\n",
      "@sample 809: tensor([[-0.0064, -0.0210, -0.0184, -0.0008, -0.0080],\n",
      "        [ 0.0024, -0.0184, -0.0072, -0.0017, -0.0045],\n",
      "        [-0.0277,  0.0141, -0.0143,  0.0081, -0.0142],\n",
      "        [-0.0013, -0.0289,  0.0026,  0.0344, -0.0011],\n",
      "        [-0.0100,  0.0075, -0.0057,  0.0064,  0.0070],\n",
      "        [ 0.0231, -0.0168, -0.0219, -0.0252,  0.0085],\n",
      "        [ 0.0225, -0.0353, -0.0112, -0.0354,  0.0394],\n",
      "        [ 0.0186,  0.0110, -0.0327,  0.0024, -0.0135]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0048, -0.0221,  0.0521,  0.0230,  0.0104],\n",
      "        [-0.0285, -0.0181, -0.0100,  0.0095,  0.0070],\n",
      "        [ 0.0098, -0.0039,  0.0594, -0.0223, -0.0221],\n",
      "        [ 0.0220,  0.0196,  0.0599, -0.0268,  0.0022],\n",
      "        [ 0.0112,  0.0265,  0.0136,  0.0063, -0.0100],\n",
      "        [-0.0167, -0.0409, -0.0182,  0.0378,  0.0161],\n",
      "        [-0.0090, -0.0291, -0.0232,  0.0526, -0.0229],\n",
      "        [-0.0018,  0.0227, -0.0301,  0.0129, -0.0353]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0195, grad_fn=<MinBackward1>), tensor(0.8879, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11791791766881943\n",
      "@sample 810: tensor([[ 0.0156, -0.0048, -0.0030, -0.0030, -0.0129],\n",
      "        [ 0.0142, -0.0023, -0.0151,  0.0041, -0.0131],\n",
      "        [ 0.0060,  0.0240,  0.0331, -0.0017,  0.0136],\n",
      "        [ 0.0066, -0.0147,  0.0187, -0.0266, -0.0079],\n",
      "        [-0.0010,  0.0154,  0.0048, -0.0198,  0.0072],\n",
      "        [ 0.0270, -0.0116, -0.0013, -0.0125, -0.0012],\n",
      "        [-0.0059, -0.0128, -0.0110,  0.0072, -0.0157],\n",
      "        [-0.0129,  0.0066,  0.0010,  0.0008, -0.0007]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0101,  0.0026,  0.0037, -0.0199,  0.0122],\n",
      "        [ 0.0032,  0.0059, -0.0158,  0.0046,  0.0242],\n",
      "        [ 0.0131,  0.0069, -0.0521,  0.0152, -0.0037],\n",
      "        [-0.0423, -0.0483,  0.0046, -0.0269,  0.0131],\n",
      "        [-0.0123,  0.0245, -0.0108, -0.0056,  0.0147],\n",
      "        [ 0.0264,  0.0349,  0.0538,  0.0014,  0.0091],\n",
      "        [ 0.0011,  0.0050,  0.0073,  0.0209,  0.0371],\n",
      "        [-0.0215,  0.0211, -0.0394,  0.0037,  0.0110]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0163, grad_fn=<MinBackward1>), tensor(0.8525, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10505568981170654\n",
      "@sample 811: tensor([[ 0.0399, -0.0158, -0.0063,  0.0208,  0.0044],\n",
      "        [-0.0299, -0.0015,  0.0146, -0.0213, -0.0133],\n",
      "        [-0.0097,  0.0011,  0.0070, -0.0010, -0.0310],\n",
      "        [-0.0015, -0.0049,  0.0382, -0.0030, -0.0088],\n",
      "        [ 0.0167,  0.0144,  0.0015, -0.0254, -0.0068],\n",
      "        [-0.0116,  0.0077, -0.0047,  0.0234, -0.0037],\n",
      "        [ 0.0145, -0.0143, -0.0052, -0.0134, -0.0224],\n",
      "        [ 0.0062, -0.0022,  0.0123,  0.0088,  0.0033]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0366,  0.0100, -0.0326,  0.0294,  0.0275],\n",
      "        [ 0.0172, -0.0292, -0.0009, -0.0234, -0.0503],\n",
      "        [ 0.0136,  0.0081, -0.0109,  0.0338,  0.0589],\n",
      "        [ 0.0384,  0.0383, -0.0082,  0.0260,  0.0196],\n",
      "        [ 0.0110, -0.0129,  0.0197, -0.0031,  0.0105],\n",
      "        [ 0.0142,  0.0312,  0.0018,  0.0160, -0.0021],\n",
      "        [-0.0522, -0.0077, -0.0150, -0.0268, -0.0121],\n",
      "        [ 0.0018,  0.0121, -0.0211,  0.0458,  0.0102]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0168, grad_fn=<MinBackward1>), tensor(0.8787, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11151795834302902\n",
      "@sample 812: tensor([[-8.0320e-04, -2.5618e-02, -4.0210e-02,  4.7617e-02, -1.2822e-02],\n",
      "        [ 1.7026e-03,  4.8259e-03,  8.7495e-03, -2.6690e-02,  1.5828e-02],\n",
      "        [ 3.1322e-03, -2.2654e-02, -8.8112e-03,  2.1384e-02, -6.4246e-03],\n",
      "        [-2.1480e-02,  4.6156e-02, -1.4005e-02, -1.2959e-03,  5.1163e-05],\n",
      "        [-9.6248e-03, -1.6469e-02, -4.0503e-02, -2.2968e-02,  2.1671e-02],\n",
      "        [-5.7927e-05, -3.4058e-03, -5.3500e-03, -7.8970e-03,  5.2098e-03],\n",
      "        [-3.1611e-02, -3.4199e-02,  2.0768e-02,  2.5070e-02,  1.8834e-02],\n",
      "        [ 5.2188e-04,  3.7150e-02,  2.9287e-03, -3.4015e-02,  5.2722e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0140,  0.0099,  0.0202, -0.0116, -0.0284],\n",
      "        [-0.0370, -0.0019, -0.0364,  0.0461,  0.0352],\n",
      "        [ 0.0049,  0.0175, -0.0056, -0.0091, -0.0196],\n",
      "        [ 0.0135,  0.0302, -0.0458,  0.0117,  0.0177],\n",
      "        [ 0.0078,  0.0175, -0.0067,  0.0421,  0.0619],\n",
      "        [-0.0034,  0.0065, -0.0444,  0.0144,  0.0011],\n",
      "        [ 0.0308,  0.0033,  0.0431, -0.0214, -0.0076],\n",
      "        [-0.0236, -0.0222, -0.0286,  0.0368,  0.0007]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0155, grad_fn=<MinBackward1>), tensor(0.9081, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11280698329210281\n",
      "@sample 813: tensor([[-0.0053, -0.0139, -0.0074, -0.0205, -0.0010],\n",
      "        [ 0.0328, -0.0088, -0.0016, -0.0071, -0.0136],\n",
      "        [ 0.0339,  0.0012, -0.0130,  0.0226, -0.0313],\n",
      "        [-0.0029,  0.0140, -0.0008, -0.0166,  0.0125],\n",
      "        [-0.0002, -0.0104, -0.0137,  0.0066,  0.0072],\n",
      "        [ 0.0164,  0.0395,  0.0615, -0.0374,  0.0512],\n",
      "        [-0.0101, -0.0049, -0.0023, -0.0126,  0.0069],\n",
      "        [ 0.0187, -0.0169, -0.0234, -0.0147,  0.0255]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0173, -0.0164,  0.0358, -0.0252,  0.0108],\n",
      "        [-0.0237,  0.0058,  0.0019,  0.0159,  0.0246],\n",
      "        [-0.0199,  0.0231, -0.0030,  0.0104,  0.0162],\n",
      "        [-0.0168,  0.0226, -0.0028, -0.0059, -0.0116],\n",
      "        [ 0.0359, -0.0123, -0.0264, -0.0055, -0.0015],\n",
      "        [-0.0014, -0.0130, -0.0227,  0.0442,  0.0003],\n",
      "        [-0.0218, -0.0218, -0.0010,  0.0095,  0.0336],\n",
      "        [ 0.0123, -0.0101,  0.0156, -0.0102, -0.0035]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0144, grad_fn=<MinBackward1>), tensor(0.8575, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11340820044279099\n",
      "@sample 814: tensor([[ 0.0272,  0.0328,  0.0010, -0.0163,  0.0060],\n",
      "        [ 0.0007, -0.0221,  0.0169, -0.0019, -0.0092],\n",
      "        [-0.0142,  0.0174,  0.0175,  0.0205, -0.0065],\n",
      "        [-0.0175,  0.0347, -0.0007,  0.0083, -0.0163],\n",
      "        [-0.0345,  0.0322,  0.0300,  0.0043, -0.0008],\n",
      "        [ 0.0036, -0.0005, -0.0256,  0.0410, -0.0352],\n",
      "        [ 0.0019, -0.0111, -0.0070, -0.0257,  0.0355],\n",
      "        [-0.0036,  0.0188,  0.0249,  0.0226, -0.0142]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0229, -0.0134, -0.0326,  0.0273,  0.0057],\n",
      "        [-0.0228,  0.0151,  0.0036, -0.0130, -0.0090],\n",
      "        [-0.0181, -0.0098, -0.0566, -0.0181,  0.0057],\n",
      "        [-0.0484,  0.0308, -0.0114, -0.0032, -0.0176],\n",
      "        [-0.0092,  0.0076, -0.0594,  0.0203,  0.0063],\n",
      "        [-0.0111, -0.0224,  0.0093, -0.0121, -0.0152],\n",
      "        [ 0.0059, -0.0293, -0.0332, -0.0058, -0.0072],\n",
      "        [-0.0094,  0.0184,  0.0169, -0.0291, -0.0058]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0170, grad_fn=<MinBackward1>), tensor(0.8393, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1157018393278122\n",
      "@sample 815: tensor([[-0.0083, -0.0123, -0.0419,  0.0033, -0.0237],\n",
      "        [-0.0022,  0.0560,  0.0076, -0.0446, -0.0344],\n",
      "        [-0.0016,  0.0073,  0.0033,  0.0102, -0.0340],\n",
      "        [-0.0052,  0.0032,  0.0161,  0.0140, -0.0050],\n",
      "        [ 0.0159, -0.0130, -0.0136,  0.0473, -0.0304],\n",
      "        [ 0.0145,  0.0107,  0.0315, -0.0095, -0.0167],\n",
      "        [ 0.0041,  0.0200,  0.0159, -0.0159,  0.0052],\n",
      "        [ 0.0211, -0.0052,  0.0126,  0.0056, -0.0125]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0069, -0.0184,  0.0120, -0.0397, -0.0063],\n",
      "        [-0.0330, -0.0261,  0.0074,  0.0094, -0.0309],\n",
      "        [ 0.0018,  0.0089,  0.0262, -0.0333, -0.0048],\n",
      "        [ 0.0252, -0.0244, -0.0217, -0.0634, -0.0135],\n",
      "        [ 0.0191,  0.0029,  0.0325, -0.0094,  0.0173],\n",
      "        [-0.0133,  0.0232,  0.0231, -0.0416, -0.0078],\n",
      "        [ 0.0121, -0.0096, -0.0404,  0.0009,  0.0110],\n",
      "        [ 0.0163,  0.0133,  0.0431,  0.0083,  0.0077]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0209, grad_fn=<MinBackward1>), tensor(0.8715, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1128816232085228\n",
      "@sample 816: tensor([[ 0.0120,  0.0101, -0.0124,  0.0086, -0.0111],\n",
      "        [-0.0072,  0.0242,  0.0038, -0.0111, -0.0139],\n",
      "        [-0.0061, -0.0458, -0.0240,  0.0112,  0.0169],\n",
      "        [-0.0010, -0.0026,  0.0036,  0.0068, -0.0143],\n",
      "        [ 0.0354,  0.0252, -0.0069,  0.0160, -0.0251],\n",
      "        [ 0.0058, -0.0187, -0.0088,  0.0572, -0.0209],\n",
      "        [ 0.0130,  0.0269,  0.0364,  0.0138,  0.0022],\n",
      "        [-0.0194, -0.0235,  0.0005,  0.0129, -0.0016]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0129, -0.0140, -0.0205,  0.0324, -0.0105],\n",
      "        [ 0.0033,  0.0149,  0.0385, -0.0370,  0.0085],\n",
      "        [ 0.0163, -0.0234,  0.0094, -0.0244, -0.0350],\n",
      "        [ 0.0072, -0.0029,  0.0037,  0.0073,  0.0195],\n",
      "        [-0.0392,  0.0327,  0.0123, -0.0164, -0.0276],\n",
      "        [ 0.0271,  0.0300,  0.0085, -0.0035, -0.0071],\n",
      "        [ 0.0147,  0.0411, -0.0167, -0.0241, -0.0317],\n",
      "        [ 0.0204, -0.0133,  0.0263, -0.0178, -0.0006]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0211, grad_fn=<MinBackward1>), tensor(0.9158, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10733381658792496\n",
      "@sample 817: tensor([[ 0.0078,  0.0119,  0.0399, -0.0211,  0.0380],\n",
      "        [ 0.0165,  0.0437,  0.0201, -0.0185,  0.0058],\n",
      "        [ 0.0097, -0.0079, -0.0149,  0.0284, -0.0089],\n",
      "        [ 0.0174,  0.0003,  0.0258,  0.0157,  0.0003],\n",
      "        [-0.0162,  0.0281,  0.0097, -0.0082, -0.0177],\n",
      "        [-0.0027,  0.0247,  0.0190,  0.0131, -0.0094],\n",
      "        [-0.0041,  0.0219,  0.0031,  0.0076, -0.0201],\n",
      "        [-0.0118,  0.0032, -0.0088,  0.0315, -0.0188]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0337,  0.0114, -0.0261, -0.0032, -0.0598],\n",
      "        [-0.0365, -0.0052, -0.0514,  0.0262, -0.0013],\n",
      "        [ 0.0114,  0.0336, -0.0151, -0.0064,  0.0066],\n",
      "        [-0.0017,  0.0110,  0.0229, -0.0447, -0.0027],\n",
      "        [-0.0157,  0.0212, -0.0094, -0.0209,  0.0244],\n",
      "        [ 0.0392,  0.0197, -0.0097,  0.0117,  0.0094],\n",
      "        [-0.0055,  0.0091, -0.0103,  0.0114,  0.0157],\n",
      "        [ 0.0215, -0.0065,  0.0249, -0.0414, -0.0323]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0178, grad_fn=<MinBackward1>), tensor(0.9016, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10488872975111008\n",
      "@sample 818: tensor([[-0.0270,  0.0664,  0.0086, -0.0717,  0.0075],\n",
      "        [-0.0341,  0.0494,  0.0378,  0.0068, -0.0448],\n",
      "        [-0.0219,  0.0013, -0.0029,  0.0040, -0.0025],\n",
      "        [ 0.0046, -0.0347,  0.0048,  0.0277,  0.0070],\n",
      "        [ 0.0039,  0.0070, -0.0118, -0.0023, -0.0027],\n",
      "        [-0.0355, -0.0302, -0.0090, -0.0237,  0.0357],\n",
      "        [ 0.0146,  0.0177, -0.0123, -0.0283,  0.0168],\n",
      "        [-0.0061,  0.0144,  0.0361, -0.0103,  0.0033]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0462, -0.0324, -0.0293,  0.0886, -0.0029],\n",
      "        [-0.0310,  0.0123, -0.0388, -0.0035,  0.0071],\n",
      "        [ 0.0096,  0.0183, -0.0053, -0.0399, -0.0018],\n",
      "        [ 0.0191, -0.0119,  0.0051,  0.0186, -0.0181],\n",
      "        [ 0.0051,  0.0145,  0.0062, -0.0113,  0.0007],\n",
      "        [ 0.0086, -0.0295,  0.0494, -0.0334, -0.0019],\n",
      "        [ 0.0023,  0.0045, -0.0102,  0.0113,  0.0043],\n",
      "        [-0.0086,  0.0109, -0.0191,  0.0455,  0.0172]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0153, grad_fn=<MinBackward1>), tensor(0.8408, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12323570251464844\n",
      "@sample 819: tensor([[-3.6377e-02,  5.2182e-04,  1.3338e-03, -2.9581e-02,  1.0481e-02],\n",
      "        [ 1.8150e-02,  1.3908e-02, -1.3261e-02, -2.5267e-03, -3.7574e-03],\n",
      "        [ 1.2637e-03, -1.3819e-02,  3.6846e-02, -3.2638e-02,  2.6394e-02],\n",
      "        [ 1.3865e-02, -1.9866e-02,  2.1353e-03, -2.3591e-02,  3.0980e-02],\n",
      "        [-1.0245e-05,  2.3926e-03,  1.1925e-02,  4.8663e-03,  7.8834e-04],\n",
      "        [ 4.2173e-03, -7.8682e-03, -6.1071e-03,  1.4763e-02, -1.4445e-02],\n",
      "        [ 1.0669e-02, -4.5542e-02, -1.5555e-02,  9.0548e-03,  2.6515e-03],\n",
      "        [-7.9448e-03,  4.3981e-04,  3.5430e-03,  4.8699e-03,  2.7486e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0172, -0.0522,  0.0387, -0.0381, -0.0385],\n",
      "        [-0.0149, -0.0122, -0.0436,  0.0150, -0.0203],\n",
      "        [ 0.0153, -0.0272,  0.0255, -0.0243, -0.0582],\n",
      "        [ 0.0165, -0.0259,  0.0227,  0.0101, -0.0015],\n",
      "        [-0.0012,  0.0136,  0.0142, -0.0142,  0.0036],\n",
      "        [ 0.0118,  0.0520, -0.0063,  0.0272,  0.0099],\n",
      "        [ 0.0215,  0.0147,  0.0718, -0.0150, -0.0156],\n",
      "        [-0.0111,  0.0092,  0.0095, -0.0198, -0.0288]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0142, grad_fn=<MinBackward1>), tensor(0.8750, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11758873611688614\n",
      "@sample 820: tensor([[-0.0128, -0.0127,  0.0104, -0.0238,  0.0184],\n",
      "        [ 0.0057,  0.0052,  0.0171, -0.0172, -0.0142],\n",
      "        [ 0.0193,  0.0311, -0.0294, -0.0020,  0.0265],\n",
      "        [ 0.0001, -0.0099, -0.0013,  0.0122, -0.0207],\n",
      "        [-0.0022,  0.0166,  0.0448, -0.0471,  0.0192],\n",
      "        [-0.0006,  0.0076,  0.0189, -0.0139,  0.0283],\n",
      "        [ 0.0116,  0.0378,  0.0346, -0.0246,  0.0171],\n",
      "        [ 0.0075, -0.0344, -0.0134,  0.0117, -0.0340]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0089, -0.0213,  0.0126,  0.0180,  0.0166],\n",
      "        [-0.0032, -0.0128,  0.0849, -0.0305, -0.0201],\n",
      "        [-0.0206,  0.0252, -0.0203,  0.0464,  0.0189],\n",
      "        [-0.0235,  0.0029,  0.0132, -0.0136, -0.0038],\n",
      "        [-0.0294, -0.0103, -0.0633,  0.0197,  0.0285],\n",
      "        [-0.0277, -0.0505, -0.0725,  0.0093, -0.0048],\n",
      "        [-0.0037, -0.0086, -0.0691,  0.0318, -0.0045],\n",
      "        [-0.0047, -0.0296,  0.0005, -0.0167,  0.0132]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0082, grad_fn=<MinBackward1>), tensor(0.8436, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10430449992418289\n",
      "@sample 821: tensor([[-0.0248,  0.0165,  0.0187, -0.0383,  0.0130],\n",
      "        [ 0.0039, -0.0023,  0.0030, -0.0011, -0.0098],\n",
      "        [ 0.0163,  0.0334, -0.0097, -0.0487,  0.0052],\n",
      "        [-0.0225,  0.0011, -0.0027, -0.0062, -0.0058],\n",
      "        [ 0.0062, -0.0016,  0.0175, -0.0306,  0.0053],\n",
      "        [-0.0257, -0.0103,  0.0041, -0.0327,  0.0060],\n",
      "        [-0.0023, -0.0277, -0.0294,  0.0176, -0.0322],\n",
      "        [ 0.0051, -0.0384,  0.0013,  0.0185, -0.0056]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0253, -0.0237,  0.0352, -0.0241, -0.0108],\n",
      "        [-0.0108,  0.0082,  0.0339, -0.0099, -0.0160],\n",
      "        [-0.0567, -0.0462, -0.0582,  0.0511,  0.0032],\n",
      "        [-0.0126, -0.0014,  0.0087,  0.0090,  0.0013],\n",
      "        [-0.0247, -0.0248, -0.0369,  0.0499,  0.0049],\n",
      "        [ 0.0497, -0.0158, -0.0475,  0.0402,  0.0028],\n",
      "        [-0.0018,  0.0068,  0.0332, -0.0451,  0.0079],\n",
      "        [ 0.0109, -0.0123, -0.0045, -0.0004, -0.0058]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0045, grad_fn=<MinBackward1>), tensor(0.8848, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11864904314279556\n",
      "@sample 822: tensor([[-0.0169,  0.0066,  0.0219, -0.0086, -0.0058],\n",
      "        [-0.0189, -0.0113,  0.0253, -0.0221,  0.0095],\n",
      "        [-0.0098,  0.0029, -0.0327,  0.0164, -0.0045],\n",
      "        [-0.0187,  0.0034,  0.0031, -0.0157,  0.0072],\n",
      "        [-0.0282,  0.0198,  0.0154, -0.0216,  0.0194],\n",
      "        [-0.0115, -0.0168,  0.0031, -0.0061,  0.0163],\n",
      "        [ 0.0130, -0.0167, -0.0008, -0.0344,  0.0340],\n",
      "        [ 0.0274, -0.0295,  0.0080, -0.0371,  0.0282]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0035,  0.0165,  0.0168,  0.0142, -0.0018],\n",
      "        [ 0.0122, -0.0038,  0.0144, -0.0026,  0.0403],\n",
      "        [ 0.0290,  0.0037,  0.0290, -0.0058, -0.0013],\n",
      "        [ 0.0077,  0.0123,  0.0053,  0.0094,  0.0138],\n",
      "        [ 0.0170, -0.0237, -0.0025,  0.0048,  0.0014],\n",
      "        [-0.0146, -0.0192,  0.0104, -0.0172,  0.0154],\n",
      "        [-0.0252, -0.0148, -0.0526, -0.0163, -0.0171],\n",
      "        [-0.0413, -0.0180, -0.0528,  0.0462, -0.0340]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0196, grad_fn=<MinBackward1>), tensor(0.8743, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09886348247528076\n",
      "@sample 823: tensor([[-0.0229, -0.0162, -0.0156, -0.0066,  0.0006],\n",
      "        [-0.0055, -0.0237, -0.0109,  0.0079,  0.0012],\n",
      "        [ 0.0111,  0.0004, -0.0024,  0.0055, -0.0248],\n",
      "        [ 0.0014,  0.0037, -0.0218,  0.0116, -0.0322],\n",
      "        [ 0.0038,  0.0037, -0.0263, -0.0302,  0.0063],\n",
      "        [-0.0062,  0.0216, -0.0022, -0.0190,  0.0030],\n",
      "        [ 0.0113, -0.0244, -0.0077,  0.0183,  0.0026],\n",
      "        [ 0.0031, -0.0015,  0.0192, -0.0156,  0.0105]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0466, -0.0244,  0.0682,  0.0033,  0.0136],\n",
      "        [ 0.0281,  0.0026,  0.0109, -0.0263, -0.0342],\n",
      "        [ 0.0041,  0.0066, -0.0375,  0.0114,  0.0385],\n",
      "        [-0.0328, -0.0225, -0.0101, -0.0002, -0.0167],\n",
      "        [-0.0077, -0.0132, -0.0455,  0.0264,  0.0239],\n",
      "        [ 0.0023, -0.0150, -0.0249,  0.0310, -0.0105],\n",
      "        [ 0.0066, -0.0198, -0.0091,  0.0011,  0.0016],\n",
      "        [-0.0035, -0.0076, -0.0294,  0.0029, -0.0081]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0181, grad_fn=<MinBackward1>), tensor(0.9115, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11247827112674713\n",
      "@sample 824: tensor([[ 0.0043, -0.0194, -0.0115,  0.0143, -0.0039],\n",
      "        [ 0.0024, -0.0219,  0.0035, -0.0033,  0.0294],\n",
      "        [-0.0027,  0.0058,  0.0018,  0.0063,  0.0156],\n",
      "        [-0.0040,  0.0007,  0.0114, -0.0117, -0.0058],\n",
      "        [-0.0283,  0.0318,  0.0062, -0.0207,  0.0203],\n",
      "        [ 0.0148,  0.0046,  0.0070,  0.0184, -0.0081],\n",
      "        [-0.0205, -0.0120, -0.0104,  0.0133, -0.0097],\n",
      "        [-0.0475, -0.0129, -0.0021, -0.0149,  0.0237]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0088,  0.0190,  0.0222, -0.0055, -0.0226],\n",
      "        [-0.0131, -0.0065, -0.0137,  0.0022,  0.0022],\n",
      "        [ 0.0093,  0.0020,  0.0092,  0.0149,  0.0207],\n",
      "        [ 0.0038, -0.0263, -0.0004,  0.0291, -0.0262],\n",
      "        [ 0.0061, -0.0308, -0.0646,  0.0694,  0.0028],\n",
      "        [ 0.0184,  0.0382,  0.0304, -0.0354,  0.0038],\n",
      "        [ 0.0240, -0.0195,  0.0480, -0.0059,  0.0230],\n",
      "        [ 0.0349, -0.0071,  0.0193, -0.0170, -0.0226]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0198, grad_fn=<MinBackward1>), tensor(0.8578, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12071524560451508\n",
      "@sample 825: tensor([[-1.6565e-03, -1.7718e-02,  7.7826e-03,  6.4952e-03,  1.3012e-03],\n",
      "        [-1.7398e-02, -8.8358e-04,  6.1430e-05, -1.8528e-02,  1.4913e-02],\n",
      "        [ 3.0448e-02,  4.6788e-03, -1.0579e-02,  8.0537e-03,  1.3634e-02],\n",
      "        [-3.4992e-02, -2.1813e-02, -4.7357e-02,  2.8373e-03,  9.2291e-03],\n",
      "        [-1.7802e-02,  1.7187e-02,  5.6014e-03, -3.1824e-02,  1.0828e-02],\n",
      "        [ 1.9995e-02,  2.1584e-02, -1.8512e-02,  4.0931e-03,  2.5422e-03],\n",
      "        [-4.4479e-03, -7.0354e-04, -2.2854e-02,  1.6747e-02, -2.5280e-02],\n",
      "        [ 1.2375e-02, -7.6907e-03, -2.6005e-03, -1.5542e-05, -1.1938e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-1.0503e-02,  4.0644e-03, -2.0489e-02,  3.2885e-02,  3.8931e-02],\n",
      "        [-7.3358e-04,  7.9813e-03, -3.2037e-02, -5.3793e-06, -1.6593e-02],\n",
      "        [-7.5447e-03, -1.4983e-02, -9.8704e-03, -1.0410e-02, -3.0508e-02],\n",
      "        [ 2.0192e-02, -1.5307e-02,  5.2080e-02, -3.6398e-02, -2.2052e-02],\n",
      "        [-3.3587e-02, -2.6040e-02, -5.8218e-02,  1.2601e-02, -2.7028e-02],\n",
      "        [ 4.1680e-02,  4.3720e-02, -1.9383e-02,  3.7514e-02,  4.6069e-03],\n",
      "        [ 7.3745e-03,  2.2680e-02, -4.9120e-02, -8.8006e-04,  4.2408e-02],\n",
      "        [-1.5239e-02, -1.3328e-02,  3.5582e-02,  7.1067e-03,  1.2500e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0224, grad_fn=<MinBackward1>), tensor(0.8359, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10833918303251266\n",
      "@sample 826: tensor([[-0.0296, -0.0062, -0.0303,  0.0060, -0.0017],\n",
      "        [ 0.0202, -0.0100, -0.0034,  0.0286, -0.0115],\n",
      "        [ 0.0033, -0.0145,  0.0023, -0.0170,  0.0149],\n",
      "        [ 0.0116,  0.0126,  0.0175, -0.0297, -0.0116],\n",
      "        [-0.0080,  0.0243,  0.0070, -0.0319,  0.0178],\n",
      "        [ 0.0077, -0.0218,  0.0045, -0.0030,  0.0074],\n",
      "        [-0.0210,  0.0159, -0.0174, -0.0054,  0.0283],\n",
      "        [ 0.0392, -0.0123, -0.0049,  0.0150, -0.0228]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0401, -0.0089,  0.0168, -0.0160,  0.0354],\n",
      "        [ 0.0217,  0.0401, -0.0209,  0.0150,  0.0244],\n",
      "        [-0.0316,  0.0260, -0.0120, -0.0141, -0.0118],\n",
      "        [-0.0332, -0.0204, -0.0665,  0.0639,  0.0303],\n",
      "        [-0.0092,  0.0148, -0.0300,  0.0178,  0.0057],\n",
      "        [ 0.0018,  0.0055, -0.0032,  0.0054,  0.0168],\n",
      "        [-0.0191,  0.0105, -0.0134, -0.0084, -0.0010],\n",
      "        [-0.0267,  0.0105,  0.0185,  0.0079,  0.0053]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0163, grad_fn=<MinBackward1>), tensor(0.8862, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10414286702871323\n",
      "@sample 827: tensor([[-0.0023, -0.0080,  0.0115,  0.0155, -0.0168],\n",
      "        [-0.0011,  0.0831,  0.0542, -0.0406, -0.0018],\n",
      "        [ 0.0154, -0.0211, -0.0257, -0.0129,  0.0116],\n",
      "        [-0.0241,  0.0016, -0.0176,  0.0161, -0.0390],\n",
      "        [-0.0064,  0.0453,  0.0217, -0.0449,  0.0267],\n",
      "        [ 0.0264, -0.0012,  0.0064, -0.0097,  0.0094],\n",
      "        [-0.0070, -0.0108, -0.0102,  0.0272, -0.0055],\n",
      "        [ 0.0030,  0.0233, -0.0004, -0.0085,  0.0290]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0052, -0.0113, -0.0094,  0.0082, -0.0028],\n",
      "        [-0.0347,  0.0257, -0.0613,  0.0505, -0.0197],\n",
      "        [-0.0124, -0.0213,  0.0202, -0.0041, -0.0097],\n",
      "        [ 0.0149, -0.0118,  0.0384, -0.0438,  0.0178],\n",
      "        [-0.0160, -0.0381, -0.0472,  0.0397, -0.0237],\n",
      "        [ 0.0053, -0.0010, -0.0915,  0.0440,  0.0042],\n",
      "        [-0.0053,  0.0099,  0.0040, -0.0242, -0.0205],\n",
      "        [-0.0097,  0.0524, -0.0490,  0.0157,  0.0027]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0164, grad_fn=<MinBackward1>), tensor(0.8823, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10433471202850342\n",
      "@sample 828: tensor([[ 0.0027,  0.0156,  0.0151, -0.0139,  0.0204],\n",
      "        [ 0.0111, -0.0121, -0.0080,  0.0402, -0.0077],\n",
      "        [ 0.0188, -0.0112, -0.0377,  0.0499, -0.0155],\n",
      "        [-0.0009, -0.0144,  0.0049, -0.0166,  0.0276],\n",
      "        [ 0.0044, -0.0076, -0.0433, -0.0071, -0.0225],\n",
      "        [-0.0016, -0.0164, -0.0420,  0.0173, -0.0042],\n",
      "        [-0.0108, -0.0080, -0.0097,  0.0086, -0.0007],\n",
      "        [ 0.0018, -0.0232,  0.0025,  0.0087, -0.0074]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0055, -0.0034, -0.0118,  0.0017,  0.0086],\n",
      "        [ 0.0181,  0.0442,  0.0041,  0.0044, -0.0135],\n",
      "        [ 0.0234,  0.0071, -0.0076, -0.0011,  0.0126],\n",
      "        [ 0.0011, -0.0074, -0.0388, -0.0088, -0.0016],\n",
      "        [-0.0117, -0.0154, -0.0296, -0.0264, -0.0267],\n",
      "        [ 0.0448, -0.0106,  0.0265, -0.0086,  0.0043],\n",
      "        [ 0.0037, -0.0335,  0.0543, -0.0299, -0.0200],\n",
      "        [ 0.0193, -0.0495,  0.0369, -0.0159, -0.0271]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0135, grad_fn=<MinBackward1>), tensor(0.9004, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1132112517952919\n",
      "@sample 829: tensor([[-2.3748e-03,  8.5558e-03, -1.0625e-02,  3.3773e-02,  6.0059e-03],\n",
      "        [ 1.3148e-02,  6.7450e-03, -1.6949e-02, -3.2971e-02,  3.4499e-03],\n",
      "        [ 8.1959e-03,  7.9269e-03,  4.1505e-03,  1.1326e-02, -8.9936e-03],\n",
      "        [-1.2729e-02,  1.6989e-02, -5.0198e-03, -7.8945e-03,  8.9478e-03],\n",
      "        [-2.0343e-02,  3.8010e-03, -8.1577e-04, -1.0700e-02,  6.0650e-03],\n",
      "        [ 2.3370e-02,  8.0468e-03,  1.1928e-05,  3.5223e-03,  5.8163e-03],\n",
      "        [-5.9602e-02,  4.5112e-02, -1.7372e-02,  1.6808e-02,  1.2159e-02],\n",
      "        [ 1.8995e-02,  3.3473e-02, -7.3274e-03, -2.1941e-02,  2.4551e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0285,  0.0227, -0.0170,  0.0041, -0.0399],\n",
      "        [-0.0204, -0.0365, -0.0096,  0.0155,  0.0037],\n",
      "        [ 0.0017,  0.0066,  0.0225, -0.0318,  0.0070],\n",
      "        [-0.0146,  0.0018, -0.0197,  0.0047, -0.0009],\n",
      "        [ 0.0105, -0.0237, -0.0011, -0.0034, -0.0438],\n",
      "        [-0.0327,  0.0219, -0.0219,  0.0085, -0.0240],\n",
      "        [ 0.0101,  0.0298, -0.0102,  0.0098,  0.0088],\n",
      "        [-0.0094, -0.0191, -0.0375,  0.0421, -0.0187]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0150, grad_fn=<MinBackward1>), tensor(0.8481, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11376520246267319\n",
      "@sample 830: tensor([[-0.0104,  0.0501,  0.0481, -0.0733,  0.0591],\n",
      "        [-0.0020, -0.0061, -0.0027,  0.0373, -0.0275],\n",
      "        [-0.0049, -0.0157, -0.0160, -0.0008,  0.0172],\n",
      "        [ 0.0024, -0.0334, -0.0298,  0.0289, -0.0030],\n",
      "        [-0.0055,  0.0054, -0.0114, -0.0136,  0.0228],\n",
      "        [ 0.0106, -0.0072, -0.0198,  0.0189,  0.0076],\n",
      "        [ 0.0144, -0.0135, -0.0039,  0.0126,  0.0015],\n",
      "        [ 0.0169,  0.0035,  0.0015,  0.0009,  0.0075]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0062, -0.0273, -0.0219,  0.0201,  0.0116],\n",
      "        [ 0.0193,  0.0223,  0.0619, -0.0610,  0.0075],\n",
      "        [ 0.0060, -0.0168,  0.0483, -0.0445, -0.0405],\n",
      "        [ 0.0171,  0.0261,  0.0377, -0.0143, -0.0200],\n",
      "        [-0.0037, -0.0073,  0.0267,  0.0031, -0.0034],\n",
      "        [ 0.0221,  0.0313, -0.0078, -0.0314, -0.0257],\n",
      "        [ 0.0131,  0.0114,  0.0048, -0.0122, -0.0018],\n",
      "        [ 0.0021,  0.0103,  0.0008, -0.0027,  0.0130]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0203, grad_fn=<MinBackward1>), tensor(0.9138, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11510492116212845\n",
      "@sample 831: tensor([[ 0.0302,  0.0393,  0.0194, -0.0225,  0.0226],\n",
      "        [ 0.0198, -0.0201, -0.0061,  0.0444, -0.0393],\n",
      "        [-0.0193, -0.0085, -0.0090, -0.0212,  0.0094],\n",
      "        [ 0.0169,  0.0069,  0.0087, -0.0446,  0.0379],\n",
      "        [ 0.0153, -0.0169,  0.0110, -0.0027, -0.0132],\n",
      "        [ 0.0037, -0.0010,  0.0134,  0.0091,  0.0061],\n",
      "        [ 0.0068, -0.0062,  0.0021,  0.0089, -0.0038],\n",
      "        [ 0.0154,  0.0057, -0.0023,  0.0277, -0.0069]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0394,  0.0204, -0.0676,  0.0214, -0.0129],\n",
      "        [-0.0076,  0.0189,  0.0140, -0.0244, -0.0008],\n",
      "        [-0.0245, -0.0174,  0.0085,  0.0126,  0.0017],\n",
      "        [-0.0120, -0.0272, -0.0808,  0.0529, -0.0262],\n",
      "        [-0.0492,  0.0109,  0.0078, -0.0203, -0.0415],\n",
      "        [ 0.0011,  0.0172, -0.0271, -0.0305,  0.0237],\n",
      "        [-0.0177,  0.0033, -0.0420, -0.0018,  0.0124],\n",
      "        [ 0.0049,  0.0516, -0.0049, -0.0202, -0.0131]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0157, grad_fn=<MinBackward1>), tensor(0.8857, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.112366683781147\n",
      "@sample 832: tensor([[ 0.0016, -0.0127, -0.0073, -0.0047,  0.0113],\n",
      "        [ 0.0051,  0.0025, -0.0002, -0.0014,  0.0007],\n",
      "        [ 0.0180,  0.0153,  0.0015,  0.0080, -0.0073],\n",
      "        [ 0.0111, -0.0086,  0.0059, -0.0026,  0.0151],\n",
      "        [-0.0197,  0.0103,  0.0007,  0.0183, -0.0174],\n",
      "        [-0.0173,  0.0071,  0.0170,  0.0149, -0.0035],\n",
      "        [-0.0200,  0.0149, -0.0092,  0.0274, -0.0004],\n",
      "        [ 0.0174, -0.0313, -0.0019,  0.0142, -0.0083]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0122, -0.0043, -0.0018,  0.0170,  0.0112],\n",
      "        [ 0.0207,  0.0261,  0.0589, -0.0547, -0.0244],\n",
      "        [ 0.0186, -0.0100, -0.0052, -0.0396, -0.0462],\n",
      "        [-0.0018, -0.0140, -0.0039, -0.0074,  0.0052],\n",
      "        [ 0.0459,  0.0184,  0.0239, -0.0366, -0.0061],\n",
      "        [ 0.0311,  0.0394,  0.0482, -0.0015,  0.0139],\n",
      "        [ 0.0239,  0.0327,  0.0129, -0.0318, -0.0051],\n",
      "        [ 0.0009,  0.0382, -0.0195,  0.0267,  0.0064]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0176, grad_fn=<MinBackward1>), tensor(0.8509, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1180003210902214\n",
      "@sample 833: tensor([[ 0.0049,  0.0228,  0.0430, -0.0253, -0.0014],\n",
      "        [ 0.0165, -0.0041,  0.0447,  0.0241, -0.0172],\n",
      "        [ 0.0010, -0.0104, -0.0031,  0.0081, -0.0031],\n",
      "        [ 0.0061, -0.0048,  0.0184,  0.0294, -0.0034],\n",
      "        [ 0.0206, -0.0232, -0.0066,  0.0559, -0.0320],\n",
      "        [ 0.0194,  0.0017, -0.0017,  0.0115, -0.0010],\n",
      "        [ 0.0118,  0.0220, -0.0050,  0.0131, -0.0091],\n",
      "        [ 0.0049, -0.0054,  0.0201,  0.0106, -0.0267]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0263,  0.0115, -0.0706,  0.0434,  0.0200],\n",
      "        [-0.0131,  0.0390,  0.0078,  0.0159, -0.0623],\n",
      "        [-0.0100,  0.0149, -0.0251, -0.0247, -0.0151],\n",
      "        [ 0.0139,  0.0867,  0.0241,  0.0170,  0.0324],\n",
      "        [ 0.0186,  0.0424,  0.0033,  0.0122, -0.0035],\n",
      "        [ 0.0007,  0.0140, -0.0029,  0.0049,  0.0106],\n",
      "        [-0.0028,  0.0357, -0.0314, -0.0178, -0.0047],\n",
      "        [ 0.0362, -0.0240,  0.0455, -0.0172, -0.0067]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0153, grad_fn=<MinBackward1>), tensor(0.8795, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11942574381828308\n",
      "@sample 834: tensor([[-0.0155,  0.0483,  0.0214, -0.0065, -0.0082],\n",
      "        [-0.0163,  0.0025, -0.0175,  0.0183,  0.0039],\n",
      "        [ 0.0154, -0.0038,  0.0127, -0.0083,  0.0152],\n",
      "        [-0.0085,  0.0309,  0.0300, -0.0189,  0.0039],\n",
      "        [ 0.0283,  0.0034,  0.0219,  0.0006,  0.0213],\n",
      "        [-0.0135, -0.0229, -0.0156,  0.0124,  0.0371],\n",
      "        [ 0.0384, -0.0181,  0.0035,  0.0035, -0.0331],\n",
      "        [-0.0008, -0.0135,  0.0133,  0.0101, -0.0008]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0240,  0.0193, -0.0185, -0.0143,  0.0158],\n",
      "        [ 0.0061, -0.0202,  0.0308, -0.0006, -0.0403],\n",
      "        [-0.0064,  0.0277, -0.0197,  0.0052,  0.0169],\n",
      "        [-0.0050,  0.0082,  0.0153, -0.0230, -0.0355],\n",
      "        [ 0.0029, -0.0248, -0.0109,  0.0298,  0.0124],\n",
      "        [ 0.0144, -0.0038, -0.0043, -0.0068, -0.0038],\n",
      "        [-0.0085,  0.0006, -0.0040, -0.0623, -0.0236],\n",
      "        [ 0.0061,  0.0055,  0.0168,  0.0052, -0.0038]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0118, grad_fn=<MinBackward1>), tensor(0.8562, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1063268855214119\n",
      "@sample 835: tensor([[-0.0297,  0.0095,  0.0384, -0.0089,  0.0489],\n",
      "        [ 0.0022,  0.0025, -0.0163,  0.0112,  0.0148],\n",
      "        [ 0.0129,  0.0176,  0.0270, -0.0247,  0.0024],\n",
      "        [-0.0231, -0.0327,  0.0026,  0.0102,  0.0060],\n",
      "        [ 0.0128, -0.0451, -0.0011,  0.0092, -0.0069],\n",
      "        [ 0.0072, -0.0122,  0.0053, -0.0198,  0.0365],\n",
      "        [-0.0094, -0.0079,  0.0040, -0.0069,  0.0013],\n",
      "        [-0.0036,  0.0047,  0.0284, -0.0101,  0.0208]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0174,  0.0355,  0.0133, -0.0053, -0.0120],\n",
      "        [-0.0071,  0.0079,  0.0354, -0.0125, -0.0115],\n",
      "        [-0.0226, -0.0153, -0.0435,  0.0175, -0.0067],\n",
      "        [-0.0131,  0.0369, -0.0335, -0.0141,  0.0068],\n",
      "        [-0.0094,  0.0111, -0.0436,  0.0037,  0.0053],\n",
      "        [-0.0137,  0.0045, -0.0553,  0.0524,  0.0074],\n",
      "        [-0.0051, -0.0070,  0.0155,  0.0135,  0.0064],\n",
      "        [-0.0197, -0.0065, -0.0135,  0.0300,  0.0154]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0139, grad_fn=<MinBackward1>), tensor(0.9270, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10557620972394943\n",
      "@sample 836: tensor([[-0.0141,  0.0201,  0.0125,  0.0008,  0.0105],\n",
      "        [ 0.0063,  0.0184,  0.0249, -0.0004,  0.0115],\n",
      "        [-0.0087,  0.0197,  0.0122, -0.0077, -0.0053],\n",
      "        [ 0.0014,  0.0178,  0.0286, -0.0029, -0.0105],\n",
      "        [ 0.0048, -0.0274, -0.0456, -0.0150, -0.0213],\n",
      "        [ 0.0296,  0.0534,  0.0092, -0.0338,  0.0123],\n",
      "        [ 0.0233, -0.0192, -0.0176,  0.0192, -0.0137],\n",
      "        [ 0.0202,  0.0130, -0.0015, -0.0277, -0.0054]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0175,  0.0304, -0.0780,  0.0377,  0.0051],\n",
      "        [-0.0283,  0.0175, -0.0455,  0.0261,  0.0105],\n",
      "        [-0.0149, -0.0063, -0.0085,  0.0068,  0.0086],\n",
      "        [ 0.0152,  0.0131, -0.0410,  0.0117,  0.0210],\n",
      "        [ 0.0073, -0.0593, -0.0094,  0.0238, -0.0046],\n",
      "        [-0.0285, -0.0276, -0.0564,  0.0253,  0.0015],\n",
      "        [ 0.0062,  0.0011,  0.0002, -0.0404, -0.0236],\n",
      "        [ 0.0133, -0.0108,  0.0308,  0.0011, -0.0108]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0121, grad_fn=<MinBackward1>), tensor(0.8983, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1089424192905426\n",
      "@sample 837: tensor([[ 0.0046, -0.0418, -0.0127, -0.0083, -0.0048],\n",
      "        [ 0.0088,  0.0116, -0.0082, -0.0080,  0.0030],\n",
      "        [-0.0208, -0.0180,  0.0185, -0.0112,  0.0134],\n",
      "        [-0.0152, -0.0251, -0.0369,  0.0078,  0.0329],\n",
      "        [-0.0170,  0.0161, -0.0051, -0.0104,  0.0227],\n",
      "        [ 0.0113, -0.0032, -0.0199, -0.0151,  0.0073],\n",
      "        [ 0.0041, -0.0047,  0.0188, -0.0227,  0.0120],\n",
      "        [ 0.0007, -0.0416, -0.0189, -0.0108,  0.0207]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0165, -0.0231,  0.0493, -0.0277,  0.0081],\n",
      "        [-0.0020, -0.0075,  0.0031, -0.0068,  0.0070],\n",
      "        [-0.0446,  0.0014,  0.0291,  0.0180, -0.0241],\n",
      "        [ 0.0224, -0.0056, -0.0111, -0.0045, -0.0400],\n",
      "        [ 0.0113, -0.0083,  0.0224, -0.0105,  0.0012],\n",
      "        [-0.0127, -0.0178, -0.0388,  0.0111, -0.0083],\n",
      "        [-0.0142, -0.0067, -0.0251,  0.0246,  0.0075],\n",
      "        [-0.0240, -0.0185, -0.0097, -0.0010, -0.0243]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0196, grad_fn=<MinBackward1>), tensor(0.8616, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10935863852500916\n",
      "@sample 838: tensor([[-0.0051, -0.0222,  0.0029, -0.0461,  0.0105],\n",
      "        [-0.0065, -0.0246, -0.0198,  0.0094, -0.0200],\n",
      "        [ 0.0082, -0.0004,  0.0181, -0.0123,  0.0062],\n",
      "        [-0.0018,  0.0054,  0.0093,  0.0031, -0.0092],\n",
      "        [ 0.0365, -0.0089, -0.0085,  0.0101, -0.0330],\n",
      "        [-0.0128,  0.0198, -0.0009, -0.0108, -0.0097],\n",
      "        [-0.0469,  0.0030,  0.0018, -0.0266, -0.0127],\n",
      "        [ 0.0156, -0.0180,  0.0285, -0.0321,  0.0071]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0195, -0.0599,  0.0350,  0.0125,  0.0149],\n",
      "        [ 0.0209, -0.0011,  0.0095,  0.0398,  0.0222],\n",
      "        [-0.0227, -0.0007, -0.0121,  0.0140,  0.0117],\n",
      "        [-0.0165, -0.0050,  0.0162,  0.0313,  0.0116],\n",
      "        [-0.0225,  0.0197, -0.0058, -0.0125,  0.0123],\n",
      "        [-0.0110,  0.0116, -0.0013,  0.0054, -0.0127],\n",
      "        [-0.0178,  0.0096,  0.0164, -0.0134,  0.0229],\n",
      "        [-0.0178, -0.0232,  0.0457, -0.0032,  0.0021]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0204, grad_fn=<MinBackward1>), tensor(0.8653, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11100559681653976\n",
      "@sample 839: tensor([[-0.0223,  0.0137,  0.0174, -0.0195,  0.0183],\n",
      "        [ 0.0085, -0.0236, -0.0241, -0.0068,  0.0264],\n",
      "        [ 0.0199, -0.0091, -0.0175,  0.0138,  0.0013],\n",
      "        [-0.0099,  0.0036, -0.0081, -0.0036,  0.0122],\n",
      "        [-0.0058, -0.0037,  0.0007, -0.0041,  0.0006],\n",
      "        [-0.0003,  0.0150,  0.0171, -0.0261,  0.0065],\n",
      "        [-0.0058, -0.0163, -0.0335, -0.0313, -0.0207],\n",
      "        [-0.0132, -0.0185,  0.0025, -0.0156,  0.0170]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0259, -0.0152, -0.0296,  0.0049,  0.0255],\n",
      "        [ 0.0458, -0.0385, -0.0164,  0.0120,  0.0597],\n",
      "        [-0.0093,  0.0025,  0.0045,  0.0013,  0.0182],\n",
      "        [-0.0154,  0.0021,  0.0141, -0.0341, -0.0092],\n",
      "        [-0.0122,  0.0017,  0.0182, -0.0339, -0.0005],\n",
      "        [-0.0250, -0.0398, -0.0405,  0.0253, -0.0192],\n",
      "        [-0.0241, -0.0855,  0.0400, -0.0125,  0.0083],\n",
      "        [-0.0221, -0.0220,  0.0345, -0.0137, -0.0081]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0144, grad_fn=<MinBackward1>), tensor(0.8791, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11327826976776123\n",
      "@sample 840: tensor([[ 0.0157, -0.0446,  0.0239,  0.0090, -0.0026],\n",
      "        [ 0.0061, -0.0011,  0.0009,  0.0110,  0.0268],\n",
      "        [ 0.0061,  0.0378, -0.0010, -0.0593,  0.0221],\n",
      "        [ 0.0153,  0.0117,  0.0073, -0.0014, -0.0134],\n",
      "        [ 0.0295, -0.0099,  0.0101,  0.0211,  0.0023],\n",
      "        [ 0.0103, -0.0365,  0.0040, -0.0272, -0.0039],\n",
      "        [ 0.0154,  0.0162,  0.0143, -0.0122,  0.0105],\n",
      "        [-0.0015,  0.0012, -0.0168, -0.0124,  0.0102]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0043, -0.0009,  0.0034, -0.0037, -0.0060],\n",
      "        [-0.0217, -0.0021,  0.0013,  0.0138, -0.0002],\n",
      "        [-0.0006, -0.0478, -0.0213,  0.0306,  0.0144],\n",
      "        [-0.0136, -0.0152, -0.0225,  0.0300,  0.0058],\n",
      "        [ 0.0126,  0.0058, -0.0479,  0.0117,  0.0125],\n",
      "        [ 0.0285, -0.0345,  0.0389, -0.0104, -0.0593],\n",
      "        [ 0.0089, -0.0166, -0.0136, -0.0175,  0.0093],\n",
      "        [-0.0266, -0.0118, -0.0111, -0.0001,  0.0234]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0106, grad_fn=<MinBackward1>), tensor(0.9069, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10712124407291412\n",
      "@sample 841: tensor([[-2.5882e-03, -2.0557e-02, -5.2182e-03,  2.2813e-02, -5.8850e-03],\n",
      "        [-7.3675e-03,  7.0526e-03, -7.6383e-03,  7.6090e-03, -2.9871e-02],\n",
      "        [ 4.4258e-03, -1.8800e-02, -1.1671e-02,  3.5480e-03,  1.3170e-02],\n",
      "        [ 2.1409e-03, -7.0155e-05,  2.5430e-02, -3.2991e-03, -1.5143e-02],\n",
      "        [ 7.8456e-05,  3.5885e-02, -2.3811e-02, -2.0611e-02, -1.2240e-04],\n",
      "        [ 2.1256e-02, -1.5628e-03,  1.6760e-02,  6.6626e-03, -1.9237e-03],\n",
      "        [-2.5051e-02,  1.0470e-02,  1.5983e-02, -2.2814e-02,  1.5009e-02],\n",
      "        [-1.5144e-02, -1.7685e-02, -6.7711e-03,  1.6369e-02, -9.1428e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0101,  0.0337,  0.0184,  0.0043,  0.0240],\n",
      "        [-0.0068,  0.0080,  0.0194, -0.0210, -0.0175],\n",
      "        [ 0.0248,  0.0119,  0.0363, -0.0129, -0.0103],\n",
      "        [-0.0040, -0.0086, -0.0120, -0.0023, -0.0010],\n",
      "        [-0.0151, -0.0074, -0.0525,  0.0236, -0.0217],\n",
      "        [-0.0072, -0.0101, -0.0200, -0.0017,  0.0067],\n",
      "        [-0.0485,  0.0231,  0.0041, -0.0391, -0.0050],\n",
      "        [ 0.0073, -0.0387,  0.0039,  0.0067,  0.0233]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0156, grad_fn=<MinBackward1>), tensor(0.8558, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10479491204023361\n",
      "@sample 842: tensor([[-0.0067,  0.0342,  0.0326, -0.0229,  0.0141],\n",
      "        [-0.0032, -0.0087,  0.0009,  0.0092, -0.0280],\n",
      "        [ 0.0260, -0.0209,  0.0087,  0.0397, -0.0323],\n",
      "        [-0.0211,  0.0037,  0.0423,  0.0050,  0.0191],\n",
      "        [-0.0076,  0.0128,  0.0075, -0.0055, -0.0201],\n",
      "        [ 0.0299, -0.0367, -0.0040,  0.0350, -0.0204],\n",
      "        [ 0.0080,  0.0157, -0.0058, -0.0022, -0.0014],\n",
      "        [ 0.0058,  0.0561, -0.0024, -0.0504,  0.0159]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0240, -0.0049, -0.0424,  0.0075, -0.0094],\n",
      "        [ 0.0102, -0.0159,  0.0115,  0.0138,  0.0021],\n",
      "        [ 0.0016,  0.0143, -0.0048,  0.0053,  0.0353],\n",
      "        [ 0.0234,  0.0134,  0.0223, -0.0261, -0.0101],\n",
      "        [-0.0484,  0.0052, -0.0149, -0.0051,  0.0027],\n",
      "        [ 0.0151, -0.0356,  0.0220, -0.0400, -0.0036],\n",
      "        [-0.0035, -0.0175, -0.0437,  0.0014, -0.0023],\n",
      "        [-0.0494, -0.0419, -0.0447,  0.0166, -0.0409]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0191, grad_fn=<MinBackward1>), tensor(0.8800, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10614889860153198\n",
      "@sample 843: tensor([[-3.5505e-04,  4.1928e-02,  3.2172e-02,  1.8359e-02, -2.9174e-02],\n",
      "        [-1.0680e-02,  1.7371e-02, -4.1873e-02, -2.9687e-02, -9.4990e-03],\n",
      "        [ 1.9962e-02,  3.7977e-03,  7.4394e-03,  3.7178e-02, -3.2296e-02],\n",
      "        [ 4.5216e-03, -5.5737e-03, -2.8469e-03,  1.6307e-02, -1.8784e-02],\n",
      "        [-9.3713e-03, -1.7619e-02,  1.6216e-02, -7.5787e-03,  1.2658e-02],\n",
      "        [-2.8507e-02,  1.1087e-02,  1.3313e-02, -3.0711e-02,  7.3522e-03],\n",
      "        [ 8.1502e-03,  1.2483e-02, -7.1786e-05, -4.2833e-03, -2.5561e-03],\n",
      "        [ 1.9412e-02, -1.5038e-03, -1.2460e-03,  2.4269e-02, -4.7009e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0150, -0.0049, -0.0118,  0.0222, -0.0009],\n",
      "        [ 0.0072, -0.0080,  0.0410,  0.0125,  0.0018],\n",
      "        [ 0.0326,  0.0324, -0.0013, -0.0104,  0.0147],\n",
      "        [ 0.0170, -0.0031, -0.0330,  0.0151, -0.0078],\n",
      "        [ 0.0168,  0.0104, -0.0044,  0.0205,  0.0033],\n",
      "        [-0.0134, -0.0081, -0.0090, -0.0142,  0.0099],\n",
      "        [-0.0139,  0.0063, -0.0279,  0.0171, -0.0064],\n",
      "        [ 0.0378,  0.0153,  0.0441, -0.0201,  0.0238]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0140, grad_fn=<MinBackward1>), tensor(0.8755, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11026180535554886\n",
      "@sample 844: tensor([[-1.1154e-02,  6.1645e-02,  9.6733e-03, -2.7001e-02,  7.5575e-03],\n",
      "        [-2.8643e-03, -1.0964e-02,  1.8023e-02, -9.6118e-03, -3.5264e-04],\n",
      "        [ 4.6060e-03,  2.3970e-02,  3.3966e-04, -1.6931e-02,  5.6828e-03],\n",
      "        [-3.9974e-03,  3.2472e-03, -1.7031e-02,  7.2283e-03, -3.0800e-02],\n",
      "        [-8.3543e-04,  2.7991e-02,  2.9177e-02, -1.6189e-02,  1.3531e-02],\n",
      "        [ 3.9238e-03,  4.9983e-02,  1.6992e-02, -4.8484e-02,  1.1718e-02],\n",
      "        [-3.5837e-02,  1.2213e-03,  2.0790e-02, -5.6165e-03, -2.7825e-02],\n",
      "        [-4.1737e-05, -2.7250e-03,  2.7193e-02,  4.3954e-03,  1.1285e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0403,  0.0028, -0.1090,  0.0445,  0.0059],\n",
      "        [-0.0169, -0.0208,  0.0094,  0.0210, -0.0038],\n",
      "        [-0.0026, -0.0213, -0.0430,  0.0322,  0.0256],\n",
      "        [-0.0003, -0.0213,  0.0072, -0.0285,  0.0100],\n",
      "        [-0.0178, -0.0176, -0.0081,  0.0173,  0.0300],\n",
      "        [-0.0386,  0.0132, -0.0006, -0.0038, -0.0148],\n",
      "        [ 0.0093,  0.0020, -0.0006, -0.0184, -0.0090],\n",
      "        [ 0.0096,  0.0126,  0.0058, -0.0060, -0.0026]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0161, grad_fn=<MinBackward1>), tensor(0.8871, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10430219024419785\n",
      "@sample 845: tensor([[-0.0096,  0.0337, -0.0166, -0.0430,  0.0199],\n",
      "        [ 0.0243, -0.0004, -0.0094,  0.0154, -0.0357],\n",
      "        [ 0.0268, -0.0087, -0.0075,  0.0122, -0.0179],\n",
      "        [-0.0119, -0.0177,  0.0249,  0.0059, -0.0116],\n",
      "        [-0.0026,  0.0030, -0.0125,  0.0077, -0.0033],\n",
      "        [-0.0042,  0.0022,  0.0283,  0.0171,  0.0014],\n",
      "        [-0.0101,  0.0068,  0.0162, -0.0161,  0.0152],\n",
      "        [ 0.0085,  0.0213,  0.0007,  0.0017, -0.0008]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0086, -0.0279, -0.0579,  0.0449, -0.0099],\n",
      "        [ 0.0082,  0.0408,  0.0281, -0.0219,  0.0368],\n",
      "        [-0.0039,  0.0079, -0.0288, -0.0265, -0.0046],\n",
      "        [ 0.0187, -0.0042,  0.0364, -0.0075, -0.0049],\n",
      "        [-0.0033,  0.0010,  0.0257, -0.0427, -0.0100],\n",
      "        [ 0.0214,  0.0164, -0.0036, -0.0189,  0.0268],\n",
      "        [ 0.0093,  0.0180, -0.0526,  0.0457,  0.0017],\n",
      "        [ 0.0410,  0.0270, -0.0837, -0.0050,  0.0025]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0172, grad_fn=<MinBackward1>), tensor(0.8504, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10133988410234451\n",
      "@sample 846: tensor([[-0.0134,  0.0110, -0.0181,  0.0218, -0.0118],\n",
      "        [-0.0218,  0.0376, -0.0005, -0.0195,  0.0302],\n",
      "        [-0.0003,  0.0286,  0.0242, -0.0606,  0.0513],\n",
      "        [ 0.0169, -0.0118, -0.0503,  0.0324, -0.0084],\n",
      "        [ 0.0190,  0.0563, -0.0262,  0.0234, -0.0142],\n",
      "        [ 0.0015, -0.0101,  0.0241,  0.0010, -0.0086],\n",
      "        [-0.0124, -0.0216,  0.0032,  0.0002, -0.0252],\n",
      "        [ 0.0096, -0.0148,  0.0144,  0.0094, -0.0080]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0084, -0.0308, -0.0157, -0.0180,  0.0084],\n",
      "        [-0.0021,  0.0077,  0.0079,  0.0546,  0.0055],\n",
      "        [-0.0285, -0.0268, -0.0807,  0.0895, -0.0032],\n",
      "        [ 0.0342, -0.0187,  0.0144, -0.0040,  0.0168],\n",
      "        [ 0.0301,  0.0447,  0.0413, -0.0297, -0.0290],\n",
      "        [ 0.0051, -0.0039, -0.0579,  0.0007, -0.0044],\n",
      "        [-0.0314,  0.0116,  0.0168, -0.0114,  0.0005],\n",
      "        [ 0.0149,  0.0020,  0.0439, -0.0064,  0.0114]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0185, grad_fn=<MinBackward1>), tensor(0.9026, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1197480857372284\n",
      "@sample 847: tensor([[ 0.0111, -0.0030, -0.0223, -0.0048, -0.0085],\n",
      "        [ 0.0143,  0.0127,  0.0186, -0.0332,  0.0061],\n",
      "        [ 0.0160,  0.0143,  0.0211, -0.0027, -0.0028],\n",
      "        [ 0.0037,  0.0317,  0.0368, -0.0419, -0.0036],\n",
      "        [-0.0314,  0.0064, -0.0140, -0.0073, -0.0099],\n",
      "        [ 0.0293,  0.0264, -0.0053, -0.0097, -0.0160],\n",
      "        [ 0.0107,  0.0117,  0.0376, -0.0186, -0.0136],\n",
      "        [-0.0055, -0.0019, -0.0244,  0.0168, -0.0250]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0195, -0.0293,  0.0064, -0.0019, -0.0045],\n",
      "        [-0.0114, -0.0052,  0.0007,  0.0418, -0.0015],\n",
      "        [-0.0116,  0.0098, -0.0160,  0.0104,  0.0215],\n",
      "        [-0.0214, -0.0073, -0.1136,  0.0658, -0.0079],\n",
      "        [-0.0465, -0.0047, -0.0013, -0.0365, -0.0456],\n",
      "        [-0.0325, -0.0072, -0.0495, -0.0180, -0.0049],\n",
      "        [-0.0316,  0.0142, -0.0205,  0.0150, -0.0170],\n",
      "        [-0.0220, -0.0032, -0.0171, -0.0318,  0.0051]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0179, grad_fn=<MinBackward1>), tensor(0.8686, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10663000494241714\n",
      "@sample 848: tensor([[ 0.0343,  0.0068,  0.0048,  0.0122, -0.0203],\n",
      "        [ 0.0135, -0.0038,  0.0118,  0.0013, -0.0002],\n",
      "        [ 0.0009,  0.0180, -0.0148, -0.0224, -0.0074],\n",
      "        [-0.0051, -0.0040,  0.0133,  0.0015,  0.0095],\n",
      "        [-0.0022,  0.0360,  0.0187, -0.0414, -0.0012],\n",
      "        [-0.0073,  0.0184,  0.0095,  0.0237,  0.0044],\n",
      "        [-0.0231,  0.0228,  0.0002, -0.0138,  0.0075],\n",
      "        [-0.0069, -0.0081, -0.0013,  0.0159, -0.0155]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0168, -0.0139, -0.0474,  0.0386,  0.0438],\n",
      "        [-0.0069,  0.0463, -0.0364,  0.0087, -0.0002],\n",
      "        [-0.0185,  0.0204, -0.0364,  0.0071, -0.0224],\n",
      "        [ 0.0121, -0.0131,  0.0281, -0.0041, -0.0073],\n",
      "        [-0.0306,  0.0291, -0.0553, -0.0309, -0.0080],\n",
      "        [ 0.0179,  0.0294, -0.0141,  0.0127,  0.0009],\n",
      "        [-0.0245, -0.0077, -0.0383, -0.0139, -0.0144],\n",
      "        [ 0.0211, -0.0242,  0.0700, -0.0169,  0.0212]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0188, grad_fn=<MinBackward1>), tensor(0.8603, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10436666756868362\n",
      "@sample 849: tensor([[-0.0061,  0.0086, -0.0269,  0.0028,  0.0017],\n",
      "        [-0.0174,  0.0469,  0.0069,  0.0297, -0.0255],\n",
      "        [ 0.0007,  0.0057, -0.0465,  0.0364, -0.0190],\n",
      "        [ 0.0183,  0.0096,  0.0145, -0.0096,  0.0027],\n",
      "        [ 0.0140, -0.0045,  0.0154, -0.0015,  0.0119],\n",
      "        [ 0.0070,  0.0075, -0.0399,  0.0227, -0.0280],\n",
      "        [ 0.0128,  0.0145,  0.0038,  0.0138, -0.0015],\n",
      "        [ 0.0039,  0.0176, -0.0271,  0.0268,  0.0012]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0189,  0.0273,  0.0066, -0.0159,  0.0296],\n",
      "        [ 0.0314,  0.0798,  0.0009, -0.0425,  0.0402],\n",
      "        [ 0.0334,  0.0325,  0.0556, -0.0405,  0.0088],\n",
      "        [-0.0101,  0.0242,  0.0131,  0.0116, -0.0431],\n",
      "        [-0.0227,  0.0094,  0.0072,  0.0176,  0.0034],\n",
      "        [ 0.0084,  0.0026,  0.0042, -0.0068,  0.0032],\n",
      "        [ 0.0140,  0.0128, -0.0170,  0.0298,  0.0098],\n",
      "        [-0.0129,  0.0329, -0.0484,  0.0097,  0.0283]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0063, grad_fn=<MinBackward1>), tensor(0.8715, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1166614517569542\n",
      "@sample 850: tensor([[ 0.0013,  0.0398,  0.0093, -0.0038, -0.0196],\n",
      "        [-0.0225,  0.0104,  0.0258, -0.0373,  0.0255],\n",
      "        [-0.0064,  0.0271,  0.0364,  0.0013, -0.0256],\n",
      "        [-0.0111, -0.0001,  0.0089, -0.0302,  0.0016],\n",
      "        [-0.0015,  0.0008,  0.0090,  0.0131,  0.0069],\n",
      "        [ 0.0127,  0.0030,  0.0233, -0.0182,  0.0018],\n",
      "        [ 0.0061,  0.0073, -0.0087, -0.0154,  0.0086],\n",
      "        [ 0.0194,  0.0329, -0.0188,  0.0039, -0.0423]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-2.8828e-02, -5.3809e-03, -6.4207e-02,  1.8421e-02,  2.9750e-02],\n",
      "        [ 2.7034e-03, -3.8439e-02,  3.1943e-02, -1.5528e-02,  3.2171e-03],\n",
      "        [-9.7059e-04,  3.9779e-05, -2.5882e-03, -1.0626e-02, -3.9750e-02],\n",
      "        [-1.3726e-02, -2.2114e-02, -4.9976e-02,  4.8468e-03,  1.6695e-02],\n",
      "        [-1.1097e-02,  2.2098e-02, -1.6789e-02, -6.7553e-03, -4.3508e-02],\n",
      "        [-1.7268e-02,  2.1709e-02,  1.4379e-02, -5.1561e-03, -4.6526e-03],\n",
      "        [-2.6522e-02, -3.0135e-02, -1.0800e-01,  2.4434e-02, -2.4120e-02],\n",
      "        [ 8.7353e-03,  4.4315e-02, -3.6800e-02, -6.9240e-03, -3.2326e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0206, grad_fn=<MinBackward1>), tensor(0.8344, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10050825774669647\n",
      "@sample 851: tensor([[-0.0009, -0.0043, -0.0375,  0.0327, -0.0085],\n",
      "        [-0.0072, -0.0163, -0.0170,  0.0146,  0.0319],\n",
      "        [ 0.0266,  0.0116,  0.0043, -0.0144,  0.0110],\n",
      "        [ 0.0035, -0.0068, -0.0103,  0.0153, -0.0017],\n",
      "        [-0.0159, -0.0173,  0.0124,  0.0082, -0.0072],\n",
      "        [-0.0430, -0.0080, -0.0039, -0.0080,  0.0252],\n",
      "        [ 0.0193,  0.0483, -0.0156, -0.0031, -0.0021],\n",
      "        [-0.0249,  0.0045, -0.0288,  0.0291, -0.0029]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0073,  0.0195, -0.0115,  0.0018, -0.0055],\n",
      "        [ 0.0064,  0.0176,  0.0395, -0.0404, -0.0220],\n",
      "        [ 0.0032, -0.0051, -0.0336,  0.0291,  0.0286],\n",
      "        [ 0.0134,  0.0024, -0.0105, -0.0084, -0.0120],\n",
      "        [ 0.0116, -0.0279, -0.0195, -0.0359,  0.0078],\n",
      "        [ 0.0112, -0.0196,  0.0096, -0.0261, -0.0229],\n",
      "        [-0.0154,  0.0328, -0.0142, -0.0062,  0.0087],\n",
      "        [ 0.0307,  0.0213,  0.0147, -0.0162,  0.0262]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0178, grad_fn=<MinBackward1>), tensor(0.9141, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1010625958442688\n",
      "@sample 852: tensor([[-0.0051,  0.0002,  0.0098, -0.0190,  0.0172],\n",
      "        [ 0.0054, -0.0257,  0.0058, -0.0160,  0.0194],\n",
      "        [ 0.0128,  0.0149, -0.0248,  0.0142, -0.0044],\n",
      "        [ 0.0055,  0.0007,  0.0093, -0.0123,  0.0088],\n",
      "        [-0.0258, -0.0228, -0.0002,  0.0040, -0.0075],\n",
      "        [-0.0058, -0.0121, -0.0057,  0.0156,  0.0106],\n",
      "        [-0.0125,  0.0050, -0.0040, -0.0127,  0.0062],\n",
      "        [ 0.0294,  0.0217, -0.0125, -0.0411,  0.0404]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0105, -0.0130, -0.0323,  0.0049,  0.0140],\n",
      "        [-0.0232,  0.0003,  0.0130, -0.0007, -0.0173],\n",
      "        [-0.0101, -0.0029,  0.0306,  0.0033,  0.0044],\n",
      "        [-0.0060, -0.0157,  0.0027, -0.0415, -0.0307],\n",
      "        [ 0.0108, -0.0144,  0.0269, -0.0215,  0.0197],\n",
      "        [ 0.0302,  0.0146,  0.0295, -0.0124,  0.0298],\n",
      "        [-0.0138, -0.0141,  0.0299, -0.0073, -0.0054],\n",
      "        [-0.0604, -0.0194, -0.0894,  0.0794,  0.0409]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0121, grad_fn=<MinBackward1>), tensor(0.9104, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10249942541122437\n",
      "@sample 853: tensor([[ 0.0025, -0.0089, -0.0116, -0.0205,  0.0184],\n",
      "        [ 0.0042, -0.0362, -0.0112, -0.0236,  0.0300],\n",
      "        [-0.0025, -0.0167, -0.0250, -0.0198,  0.0141],\n",
      "        [-0.0042, -0.0200,  0.0069,  0.0047,  0.0198],\n",
      "        [-0.0158, -0.0057,  0.0292,  0.0281,  0.0045],\n",
      "        [ 0.0011, -0.0092, -0.0063,  0.0283, -0.0003],\n",
      "        [ 0.0017,  0.0377,  0.0183, -0.0376,  0.0088],\n",
      "        [ 0.0117, -0.0163, -0.0213,  0.0121,  0.0125]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 1.7626e-02, -8.7626e-03,  4.9567e-02, -1.1472e-02,  6.2687e-04],\n",
      "        [-1.7508e-02, -4.7612e-02,  2.2275e-02, -2.7826e-02, -2.1141e-02],\n",
      "        [-6.8596e-03, -3.1613e-02,  1.4250e-03,  6.5012e-03,  2.1694e-02],\n",
      "        [-9.1594e-04, -1.8626e-05,  2.0199e-02,  1.2049e-02, -1.3633e-02],\n",
      "        [ 1.3964e-02,  4.2994e-02,  2.9944e-03,  7.1945e-03,  5.4109e-03],\n",
      "        [ 3.5582e-02,  6.9625e-03,  2.8034e-03, -8.3725e-04, -2.3334e-02],\n",
      "        [-3.4896e-02,  7.2841e-03, -3.3061e-02, -1.4355e-02, -4.4354e-03],\n",
      "        [-1.2944e-02, -3.6049e-02, -1.3325e-02,  2.9404e-03, -1.9852e-04]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0156, grad_fn=<MinBackward1>), tensor(0.8711, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11000052094459534\n",
      "@sample 854: tensor([[ 0.0127, -0.0306,  0.0218,  0.0237, -0.0190],\n",
      "        [-0.0099, -0.0180,  0.0126,  0.0065,  0.0150],\n",
      "        [ 0.0003, -0.0053,  0.0290,  0.0034,  0.0304],\n",
      "        [ 0.0002,  0.0051, -0.0008,  0.0324,  0.0013],\n",
      "        [ 0.0112, -0.0015,  0.0097, -0.0004,  0.0080],\n",
      "        [ 0.0040,  0.0652,  0.0358, -0.0557,  0.0550],\n",
      "        [ 0.0259,  0.0182,  0.0095, -0.0431, -0.0104],\n",
      "        [ 0.0138, -0.0232,  0.0172,  0.0012, -0.0211]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0186,  0.0394, -0.0477,  0.0016,  0.0092],\n",
      "        [-0.0169, -0.0231, -0.0159, -0.0201, -0.0153],\n",
      "        [ 0.0082,  0.0220, -0.0384,  0.0151,  0.0108],\n",
      "        [-0.0174,  0.0127,  0.0155,  0.0005, -0.0002],\n",
      "        [-0.0411,  0.0060, -0.0413,  0.0279,  0.0084],\n",
      "        [-0.0144, -0.0341, -0.0551,  0.0702,  0.0101],\n",
      "        [-0.0524, -0.0117, -0.0289,  0.0651,  0.0002],\n",
      "        [ 0.0090,  0.0172,  0.0234,  0.0122,  0.0479]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0213, grad_fn=<MinBackward1>), tensor(0.8278, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10491542518138885\n",
      "@sample 855: tensor([[-0.0263,  0.0031, -0.0024, -0.0406,  0.0300],\n",
      "        [ 0.0207, -0.0092, -0.0300,  0.0192,  0.0088],\n",
      "        [ 0.0060, -0.0137,  0.0115, -0.0050,  0.0188],\n",
      "        [-0.0151,  0.0174,  0.0023, -0.0188,  0.0169],\n",
      "        [ 0.0219, -0.0278, -0.0006, -0.0081,  0.0170],\n",
      "        [-0.0174, -0.0354, -0.0152,  0.0251, -0.0233],\n",
      "        [-0.0263, -0.0457, -0.0038, -0.0064, -0.0019],\n",
      "        [-0.0042, -0.0017,  0.0369, -0.0348,  0.0159]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0301, -0.0400, -0.0293,  0.0112, -0.0163],\n",
      "        [-0.0307, -0.0356, -0.0567,  0.0066, -0.0193],\n",
      "        [ 0.0185, -0.0067, -0.0543,  0.0292,  0.0013],\n",
      "        [-0.0074, -0.0055, -0.0432,  0.0075,  0.0021],\n",
      "        [-0.0423, -0.0190, -0.0249,  0.0012, -0.0136],\n",
      "        [-0.0049,  0.0027,  0.0221, -0.0391,  0.0041],\n",
      "        [-0.0271,  0.0063, -0.0007,  0.0092,  0.0045],\n",
      "        [-0.0290,  0.0176,  0.0189,  0.0103, -0.0022]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0135, grad_fn=<MinBackward1>), tensor(0.9079, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09429816156625748\n",
      "@sample 856: tensor([[-0.0018,  0.0179, -0.0084,  0.0053, -0.0101],\n",
      "        [ 0.0011,  0.0110, -0.0159,  0.0197,  0.0003],\n",
      "        [-0.0174, -0.0106,  0.0174, -0.0029,  0.0073],\n",
      "        [-0.0198, -0.0235, -0.0218, -0.0162, -0.0039],\n",
      "        [ 0.0055,  0.0135,  0.0067, -0.0381,  0.0106],\n",
      "        [-0.0109,  0.0103,  0.0114, -0.0124, -0.0120],\n",
      "        [ 0.0021, -0.0512,  0.0001,  0.0223,  0.0098],\n",
      "        [ 0.0072,  0.0036,  0.0347, -0.0614,  0.0590]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0283,  0.0250, -0.0313,  0.0026, -0.0006],\n",
      "        [-0.0232,  0.0235, -0.0319,  0.0059,  0.0222],\n",
      "        [-0.0191, -0.0225, -0.0137, -0.0087, -0.0196],\n",
      "        [-0.0019, -0.0342,  0.0425,  0.0037, -0.0169],\n",
      "        [-0.0188,  0.0020, -0.0310,  0.0305, -0.0188],\n",
      "        [ 0.0185, -0.0214,  0.0302, -0.0390, -0.0125],\n",
      "        [-0.0112, -0.0220,  0.0595, -0.0217,  0.0041],\n",
      "        [-0.0246, -0.0530,  0.0094,  0.0196, -0.0181]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0153, grad_fn=<MinBackward1>), tensor(0.9204, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10572846978902817\n",
      "@sample 857: tensor([[ 0.0225, -0.0132,  0.0150,  0.0188,  0.0114],\n",
      "        [ 0.0113, -0.0323, -0.0019,  0.0181, -0.0045],\n",
      "        [-0.0302,  0.0067,  0.0205, -0.0061, -0.0018],\n",
      "        [ 0.0063, -0.0169, -0.0072,  0.0242,  0.0026],\n",
      "        [-0.0197,  0.0105,  0.0521, -0.0050,  0.0233],\n",
      "        [ 0.0093, -0.0028,  0.0258,  0.0114,  0.0066],\n",
      "        [ 0.0244,  0.0090, -0.0155,  0.0095, -0.0144],\n",
      "        [ 0.0055, -0.0166,  0.0202, -0.0033,  0.0082]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0208, -0.0269, -0.0199,  0.0105, -0.0048],\n",
      "        [ 0.0247,  0.0116,  0.0012, -0.0063, -0.0040],\n",
      "        [-0.0187, -0.0430,  0.0131, -0.0207,  0.0267],\n",
      "        [ 0.0134,  0.0138, -0.0099,  0.0111,  0.0055],\n",
      "        [-0.0112, -0.0216,  0.0046, -0.0039, -0.0024],\n",
      "        [ 0.0196,  0.0056,  0.0082, -0.0225, -0.0309],\n",
      "        [-0.0382, -0.0020, -0.0599,  0.0279,  0.0194],\n",
      "        [-0.0262, -0.0030, -0.0114,  0.0213, -0.0145]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0217, grad_fn=<MinBackward1>), tensor(0.8653, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10335727035999298\n",
      "@sample 858: tensor([[ 0.0056, -0.0158, -0.0039, -0.0120, -0.0043],\n",
      "        [ 0.0131, -0.0049, -0.0342,  0.0529, -0.0313],\n",
      "        [ 0.0211,  0.0498, -0.0143, -0.0690,  0.0126],\n",
      "        [-0.0041, -0.0112,  0.0127, -0.0088,  0.0210],\n",
      "        [ 0.0088, -0.0309, -0.0037,  0.0075, -0.0008],\n",
      "        [-0.0167, -0.0152,  0.0097,  0.0217, -0.0173],\n",
      "        [ 0.0283, -0.0009,  0.0110, -0.0104,  0.0139],\n",
      "        [ 0.0289, -0.0105, -0.0232, -0.0156,  0.0009]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0320, -0.0158, -0.0292, -0.0372, -0.0303],\n",
      "        [ 0.0618,  0.0358,  0.0577, -0.0007,  0.0122],\n",
      "        [ 0.0278, -0.0148, -0.0014,  0.0667,  0.0163],\n",
      "        [ 0.0174, -0.0243,  0.0412, -0.0236, -0.0279],\n",
      "        [-0.0062,  0.0052,  0.0249, -0.0203, -0.0135],\n",
      "        [ 0.0204,  0.0202,  0.0506, -0.0172, -0.0173],\n",
      "        [-0.0125, -0.0339, -0.0168,  0.0350, -0.0268],\n",
      "        [-0.0069,  0.0280,  0.0075, -0.0009,  0.0163]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0135, grad_fn=<MinBackward1>), tensor(0.8582, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11726119369268417\n",
      "@sample 859: tensor([[ 0.0159,  0.0225, -0.0011, -0.0030, -0.0003],\n",
      "        [ 0.0005, -0.0213, -0.0225,  0.0424,  0.0189],\n",
      "        [ 0.0037, -0.0066, -0.0235,  0.0054,  0.0079],\n",
      "        [-0.0198, -0.0160, -0.0087, -0.0006,  0.0047],\n",
      "        [-0.0120, -0.0139,  0.0021, -0.0165,  0.0218],\n",
      "        [-0.0054, -0.0089,  0.0441,  0.0230, -0.0204],\n",
      "        [-0.0173, -0.0135,  0.0048, -0.0194,  0.0029],\n",
      "        [-0.0107,  0.0191, -0.0123,  0.0015,  0.0080]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0030,  0.0249, -0.0439,  0.0343, -0.0307],\n",
      "        [ 0.0273,  0.0076, -0.0058, -0.0200, -0.0226],\n",
      "        [ 0.0434, -0.0170,  0.0569,  0.0002,  0.0216],\n",
      "        [ 0.0083, -0.0295,  0.0203, -0.0096, -0.0338],\n",
      "        [ 0.0008,  0.0176,  0.0033, -0.0012,  0.0082],\n",
      "        [-0.0027,  0.0209, -0.0011,  0.0104,  0.0046],\n",
      "        [-0.0049, -0.0229,  0.0238,  0.0131, -0.0004],\n",
      "        [-0.0007,  0.0388, -0.0434, -0.0131, -0.0193]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0133, grad_fn=<MinBackward1>), tensor(0.8449, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10752514004707336\n",
      "@sample 860: tensor([[-0.0107,  0.0132, -0.0182,  0.0275, -0.0316],\n",
      "        [ 0.0053,  0.0246,  0.0068, -0.0175,  0.0013],\n",
      "        [-0.0012,  0.0195, -0.0152, -0.0032, -0.0262],\n",
      "        [ 0.0042,  0.0567,  0.0005, -0.0531,  0.0264],\n",
      "        [-0.0178,  0.0508, -0.0232, -0.0198,  0.0561],\n",
      "        [-0.0030,  0.0236,  0.0129, -0.0028, -0.0122],\n",
      "        [-0.0185, -0.0176, -0.0577, -0.0369, -0.0058],\n",
      "        [ 0.0106, -0.0062, -0.0153,  0.0235, -0.0132]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0099, -0.0047,  0.0032, -0.0195,  0.0128],\n",
      "        [-0.0243, -0.0099, -0.0475,  0.0022, -0.0012],\n",
      "        [-0.0176, -0.0289, -0.0168,  0.0293,  0.0162],\n",
      "        [-0.0607, -0.0213, -0.0265,  0.0210,  0.0032],\n",
      "        [-0.0066, -0.0110, -0.0564,  0.0506,  0.0072],\n",
      "        [ 0.0202,  0.0043, -0.0071,  0.0242, -0.0245],\n",
      "        [ 0.0008, -0.0865,  0.0603, -0.0064, -0.0076],\n",
      "        [ 0.0397, -0.0015,  0.0205,  0.0103,  0.0138]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0155, grad_fn=<MinBackward1>), tensor(0.9052, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12265875190496445\n",
      "@sample 861: tensor([[ 1.4206e-02, -3.4608e-03, -3.2720e-02,  1.8197e-02, -1.0933e-02],\n",
      "        [-1.2022e-02, -1.5327e-02, -1.9611e-02,  7.6817e-04, -1.7879e-02],\n",
      "        [ 1.4126e-02,  1.4699e-02,  7.4857e-03,  1.2726e-05, -1.9455e-02],\n",
      "        [ 2.4600e-02,  1.3893e-02, -6.6387e-03,  9.8646e-03,  1.3483e-02],\n",
      "        [-2.5878e-03,  8.3461e-05, -1.7404e-02,  8.4826e-03, -1.2284e-02],\n",
      "        [-2.6224e-03,  5.0240e-02, -1.9429e-02, -4.0683e-02,  2.0595e-02],\n",
      "        [ 1.2151e-02, -2.3209e-02, -1.9400e-02,  3.5581e-02,  1.4196e-02],\n",
      "        [-3.8636e-03,  3.6974e-02,  2.7896e-02, -1.5039e-02,  1.8126e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0218, -0.0272, -0.0264, -0.0027,  0.0305],\n",
      "        [-0.0027, -0.0379,  0.0491, -0.0030,  0.0009],\n",
      "        [-0.0213,  0.0441, -0.0039,  0.0302,  0.0092],\n",
      "        [-0.0085,  0.0022, -0.0530,  0.0436,  0.0020],\n",
      "        [ 0.0008, -0.0048, -0.0340,  0.0188,  0.0157],\n",
      "        [-0.0057, -0.0398, -0.0663,  0.0219, -0.0094],\n",
      "        [-0.0016,  0.0096, -0.0095,  0.0032,  0.0274],\n",
      "        [-0.0049, -0.0074, -0.0492,  0.0391,  0.0080]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0149, grad_fn=<MinBackward1>), tensor(0.8867, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10577840358018875\n",
      "@sample 862: tensor([[ 0.0190,  0.0031,  0.0006, -0.0112, -0.0099],\n",
      "        [-0.0023, -0.0010, -0.0055, -0.0053, -0.0045],\n",
      "        [ 0.0030,  0.0234,  0.0327, -0.0246,  0.0210],\n",
      "        [ 0.0129, -0.0014, -0.0163, -0.0012, -0.0025],\n",
      "        [-0.0281, -0.0120, -0.0027, -0.0055, -0.0258],\n",
      "        [-0.0143,  0.0943,  0.0180, -0.0353,  0.0113],\n",
      "        [ 0.0055,  0.0149, -0.0058, -0.0052, -0.0251],\n",
      "        [ 0.0085,  0.0563, -0.0036, -0.0351, -0.0012]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0032,  0.0162,  0.0004,  0.0108, -0.0190],\n",
      "        [ 0.0094,  0.0018,  0.0098, -0.0040,  0.0071],\n",
      "        [-0.0250, -0.0099, -0.0315,  0.0396, -0.0017],\n",
      "        [-0.0282, -0.0239, -0.0544,  0.0573,  0.0207],\n",
      "        [ 0.0108, -0.0167,  0.0478, -0.0242,  0.0053],\n",
      "        [ 0.0059, -0.0304, -0.0719,  0.0319,  0.0034],\n",
      "        [-0.0328, -0.0138,  0.0091,  0.0252,  0.0045],\n",
      "        [-0.0647,  0.0600, -0.0187,  0.0173, -0.0035]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0200, grad_fn=<MinBackward1>), tensor(0.8482, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10233448445796967\n",
      "@sample 863: tensor([[ 0.0111,  0.0128,  0.0033, -0.0124,  0.0080],\n",
      "        [-0.0063,  0.0224,  0.0124, -0.0124, -0.0322],\n",
      "        [-0.0090,  0.0250,  0.0234, -0.0167,  0.0130],\n",
      "        [-0.0088, -0.0263, -0.0159,  0.0338, -0.0168],\n",
      "        [ 0.0012, -0.0044, -0.0051, -0.0502,  0.0325],\n",
      "        [ 0.0252,  0.0193,  0.0029, -0.0090, -0.0008],\n",
      "        [ 0.0062, -0.0260, -0.0054,  0.0402, -0.0019],\n",
      "        [ 0.0167, -0.0059, -0.0101,  0.0101, -0.0173]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0012, -0.0077, -0.0457,  0.0058, -0.0025],\n",
      "        [ 0.0115,  0.0147,  0.0069, -0.0002, -0.0172],\n",
      "        [ 0.0076, -0.0223, -0.0457,  0.0044, -0.0357],\n",
      "        [ 0.0105, -0.0055,  0.0169, -0.0223,  0.0118],\n",
      "        [-0.0544, -0.0688, -0.1179,  0.0575, -0.0039],\n",
      "        [-0.0032,  0.0009, -0.0028,  0.0132, -0.0153],\n",
      "        [ 0.0435,  0.0020, -0.0157, -0.0220, -0.0288],\n",
      "        [ 0.0252,  0.0170, -0.0094,  0.0183,  0.0188]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0151, grad_fn=<MinBackward1>), tensor(0.8768, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09625452756881714\n",
      "@sample 864: tensor([[-0.0358, -0.0057,  0.0149, -0.0014,  0.0004],\n",
      "        [ 0.0306, -0.0022, -0.0172, -0.0388,  0.0090],\n",
      "        [-0.0108,  0.0006, -0.0198,  0.0410, -0.0076],\n",
      "        [ 0.0003,  0.0276,  0.0115, -0.0010, -0.0150],\n",
      "        [ 0.0222,  0.0045,  0.0024,  0.0134, -0.0307],\n",
      "        [ 0.0128, -0.0201,  0.0273,  0.0253, -0.0264],\n",
      "        [ 0.0018, -0.0033,  0.0037,  0.0004,  0.0041],\n",
      "        [ 0.0153, -0.0219,  0.0180, -0.0028, -0.0106]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0027,  0.0181,  0.0044,  0.0200,  0.0133],\n",
      "        [-0.0065,  0.0044, -0.0492,  0.0190,  0.0159],\n",
      "        [ 0.0297,  0.0195,  0.0314, -0.0040, -0.0055],\n",
      "        [ 0.0099,  0.0307, -0.0003, -0.0435, -0.0177],\n",
      "        [ 0.0061,  0.0465,  0.0035, -0.0146,  0.0246],\n",
      "        [-0.0027, -0.0385,  0.0046,  0.0064, -0.0208],\n",
      "        [ 0.0006, -0.0074, -0.0163,  0.0260,  0.0199],\n",
      "        [-0.0262,  0.0185,  0.0032,  0.0023, -0.0169]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0180, grad_fn=<MinBackward1>), tensor(0.8392, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11939394474029541\n",
      "@sample 865: tensor([[-0.0228,  0.0382,  0.0209, -0.0496,  0.0136],\n",
      "        [-0.0030,  0.0044,  0.0276, -0.0184,  0.0076],\n",
      "        [-0.0277,  0.0066,  0.0010, -0.0251,  0.0002],\n",
      "        [-0.0339,  0.0282,  0.0229,  0.0348, -0.0068],\n",
      "        [-0.0011, -0.0082,  0.0129,  0.0312, -0.0069],\n",
      "        [-0.0141,  0.0186, -0.0219,  0.0191, -0.0013],\n",
      "        [ 0.0268,  0.0083, -0.0260,  0.0402, -0.0121],\n",
      "        [ 0.0085, -0.0350, -0.0076,  0.0211, -0.0021]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0060, -0.0309, -0.0321, -0.0070,  0.0045],\n",
      "        [-0.0294,  0.0149, -0.0956,  0.0514,  0.0182],\n",
      "        [ 0.0051, -0.0292,  0.0397, -0.0251, -0.0547],\n",
      "        [ 0.0072,  0.0120, -0.0376,  0.0101, -0.0328],\n",
      "        [ 0.0390,  0.0046,  0.0076, -0.0136,  0.0136],\n",
      "        [ 0.0076,  0.0199,  0.0248, -0.0084, -0.0025],\n",
      "        [-0.0185, -0.0057, -0.0412,  0.0154,  0.0073],\n",
      "        [ 0.0151,  0.0407,  0.0431, -0.0022,  0.0246]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0159, grad_fn=<MinBackward1>), tensor(0.8529, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1178298369050026\n",
      "@sample 866: tensor([[ 0.0110, -0.0115, -0.0303,  0.0309,  0.0214],\n",
      "        [-0.0286,  0.0437,  0.0282, -0.0108, -0.0173],\n",
      "        [-0.0021,  0.0056, -0.0087,  0.0439, -0.0156],\n",
      "        [ 0.0019,  0.0090, -0.0007,  0.0009, -0.0163],\n",
      "        [ 0.0263, -0.0203, -0.0007,  0.0123, -0.0022],\n",
      "        [ 0.0117, -0.0039, -0.0067,  0.0354, -0.0087],\n",
      "        [ 0.0102, -0.0029, -0.0137, -0.0014, -0.0003],\n",
      "        [ 0.0089, -0.0121, -0.0137,  0.0483, -0.0075]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0599,  0.0328, -0.0465,  0.0202,  0.0202],\n",
      "        [-0.0143,  0.0132, -0.0199,  0.0038, -0.0221],\n",
      "        [ 0.0447,  0.0232,  0.0488, -0.0584, -0.0426],\n",
      "        [ 0.0205, -0.0059,  0.0497, -0.0191, -0.0176],\n",
      "        [-0.0048,  0.0008, -0.0195, -0.0261, -0.0226],\n",
      "        [-0.0123,  0.0315,  0.0209, -0.0100,  0.0229],\n",
      "        [ 0.0142,  0.0160,  0.0176, -0.0292, -0.0086],\n",
      "        [ 0.0153,  0.0278, -0.0363, -0.0114,  0.0006]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0185, grad_fn=<MinBackward1>), tensor(0.8875, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10981763154268265\n",
      "@sample 867: tensor([[-0.0079, -0.0139, -0.0227, -0.0112,  0.0165],\n",
      "        [-0.0042, -0.0040,  0.0083,  0.0171, -0.0075],\n",
      "        [ 0.0223, -0.0183, -0.0242,  0.0342, -0.0141],\n",
      "        [-0.0209,  0.0206, -0.0181,  0.0131,  0.0179],\n",
      "        [-0.0159, -0.0161, -0.0046,  0.0319, -0.0279],\n",
      "        [ 0.0140, -0.0022, -0.0088, -0.0050,  0.0033],\n",
      "        [-0.0264,  0.0296,  0.0224, -0.0210,  0.0254],\n",
      "        [ 0.0110, -0.0190, -0.0298,  0.0021, -0.0030]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0287, -0.0657,  0.0143, -0.0051, -0.0128],\n",
      "        [ 0.0119,  0.0242,  0.0188, -0.0040, -0.0007],\n",
      "        [ 0.0105,  0.0127, -0.0091, -0.0181, -0.0158],\n",
      "        [ 0.0236,  0.0206, -0.0022, -0.0426, -0.0072],\n",
      "        [-0.0027,  0.0154, -0.0018,  0.0175,  0.0151],\n",
      "        [ 0.0268,  0.0106, -0.0037,  0.0174,  0.0078],\n",
      "        [ 0.0008,  0.0272,  0.0034,  0.0184,  0.0103],\n",
      "        [ 0.0145,  0.0238,  0.0220, -0.0162,  0.0261]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0161, grad_fn=<MinBackward1>), tensor(0.8793, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11548592895269394\n",
      "@sample 868: tensor([[ 0.0171,  0.0082, -0.0095,  0.0132, -0.0059],\n",
      "        [-0.0238, -0.0102,  0.0187,  0.0118,  0.0283],\n",
      "        [-0.0071,  0.0318,  0.0153, -0.0336,  0.0231],\n",
      "        [-0.0233,  0.0283, -0.0039, -0.0097,  0.0292],\n",
      "        [-0.0191, -0.0255, -0.0011, -0.0116,  0.0239],\n",
      "        [-0.0306,  0.0076, -0.0004, -0.0031, -0.0116],\n",
      "        [ 0.0093, -0.0078, -0.0080, -0.0351,  0.0156],\n",
      "        [-0.0260,  0.0204,  0.0310,  0.0043,  0.0091]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0076, -0.0037, -0.0436,  0.0089, -0.0380],\n",
      "        [ 0.0355, -0.0118, -0.0396,  0.0121,  0.0005],\n",
      "        [-0.0199, -0.0291, -0.0295,  0.0394,  0.0048],\n",
      "        [ 0.0319,  0.0290, -0.0364, -0.0025,  0.0073],\n",
      "        [-0.0082, -0.0094,  0.0312, -0.0226, -0.0130],\n",
      "        [-0.0184, -0.0271,  0.0151, -0.0099, -0.0204],\n",
      "        [-0.0222, -0.0008, -0.0289,  0.0147, -0.0186],\n",
      "        [-0.0214,  0.0416, -0.0415,  0.0268, -0.0119]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0129, grad_fn=<MinBackward1>), tensor(0.8220, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10537136346101761\n",
      "@sample 869: tensor([[-1.9936e-02, -3.0772e-02,  4.2845e-02,  9.7038e-04, -1.7203e-05],\n",
      "        [ 5.9252e-03,  1.9032e-02, -2.5296e-02, -3.1010e-02, -9.2294e-03],\n",
      "        [-8.9704e-03,  1.4610e-02,  2.5393e-02, -9.8115e-03,  1.5222e-05],\n",
      "        [-2.4152e-02,  2.4406e-02, -1.6966e-02, -2.0408e-03,  1.3199e-02],\n",
      "        [-2.8032e-02,  1.8844e-02,  1.4000e-02, -2.6989e-02,  6.2106e-03],\n",
      "        [ 8.9784e-03, -1.7357e-02,  4.2608e-03,  3.0445e-02, -1.6082e-02],\n",
      "        [ 2.8484e-02,  4.1839e-02,  2.2988e-02, -3.1963e-02,  2.6037e-02],\n",
      "        [ 2.3323e-02, -1.2594e-02, -1.5181e-02,  1.8379e-02, -2.9373e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0133,  0.0086,  0.0185, -0.0002, -0.0037],\n",
      "        [-0.0135, -0.0414, -0.0264,  0.0374,  0.0230],\n",
      "        [-0.0200, -0.0068,  0.0152, -0.0343, -0.0270],\n",
      "        [ 0.0313, -0.0363,  0.0096, -0.0359, -0.0414],\n",
      "        [-0.0104, -0.0291, -0.0073, -0.0063, -0.0369],\n",
      "        [ 0.0171,  0.0202, -0.0019,  0.0038,  0.0060],\n",
      "        [ 0.0149, -0.0332,  0.0256,  0.0176, -0.0095],\n",
      "        [ 0.0202,  0.0176,  0.0225, -0.0156, -0.0097]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0172, grad_fn=<MinBackward1>), tensor(0.8898, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10627105832099915\n",
      "@sample 870: tensor([[-1.4341e-02, -8.3749e-03, -5.1131e-03, -1.6177e-02,  8.5365e-03],\n",
      "        [-3.3691e-02, -2.8386e-02, -5.8384e-03,  3.0106e-02,  1.7846e-02],\n",
      "        [-1.7054e-02,  6.1049e-03,  4.2286e-03,  7.6064e-03,  9.8145e-03],\n",
      "        [-2.3203e-02, -1.5992e-02,  1.6634e-02, -2.5008e-02,  8.7612e-03],\n",
      "        [-3.6851e-02,  4.3278e-02,  1.2550e-02, -3.7948e-02, -6.5327e-03],\n",
      "        [ 5.2363e-03,  9.5619e-03, -6.2538e-03,  6.1093e-03, -2.7471e-03],\n",
      "        [ 7.7526e-03,  2.0807e-02,  1.2189e-05, -6.3748e-03,  3.7912e-03],\n",
      "        [-4.3394e-02,  3.9074e-04,  6.8286e-03, -5.0185e-02,  5.6516e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0222, -0.0081, -0.0093, -0.0051, -0.0141],\n",
      "        [ 0.0229,  0.0113,  0.0469, -0.0356, -0.0013],\n",
      "        [ 0.0167,  0.0191,  0.0185,  0.0030, -0.0158],\n",
      "        [-0.0108, -0.0209,  0.0752, -0.0385, -0.0231],\n",
      "        [-0.0092, -0.0216,  0.0068,  0.0224,  0.0120],\n",
      "        [ 0.0104,  0.0089,  0.0188, -0.0123, -0.0005],\n",
      "        [ 0.0118, -0.0088, -0.0044, -0.0024,  0.0055],\n",
      "        [ 0.0153, -0.0158, -0.0250,  0.0624, -0.0168]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0169, grad_fn=<MinBackward1>), tensor(0.8664, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09431885182857513\n",
      "@sample 871: tensor([[-0.0037, -0.0175,  0.0025,  0.0099,  0.0091],\n",
      "        [-0.0084, -0.0158, -0.0038, -0.0072,  0.0089],\n",
      "        [ 0.0218,  0.0222,  0.0209, -0.0133,  0.0164],\n",
      "        [-0.0055, -0.0117,  0.0134, -0.0250, -0.0089],\n",
      "        [-0.0038, -0.0094,  0.0005,  0.0281, -0.0087],\n",
      "        [-0.0360, -0.0075, -0.0123,  0.0259, -0.0160],\n",
      "        [ 0.0024, -0.0133,  0.0010,  0.0057, -0.0065],\n",
      "        [ 0.0144, -0.0205, -0.0158,  0.0288, -0.0247]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0056,  0.0120, -0.0004,  0.0333,  0.0164],\n",
      "        [-0.0022,  0.0124, -0.0127,  0.0116,  0.0167],\n",
      "        [-0.0111,  0.0474,  0.0081,  0.0098, -0.0092],\n",
      "        [-0.0065, -0.0330,  0.0076, -0.0137, -0.0123],\n",
      "        [ 0.0300,  0.0428, -0.0194,  0.0249,  0.0256],\n",
      "        [ 0.0241,  0.0223,  0.0042, -0.0239,  0.0025],\n",
      "        [-0.0050,  0.0065,  0.0255, -0.0121, -0.0150],\n",
      "        [-0.0193,  0.0085,  0.0166, -0.0465,  0.0180]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0153, grad_fn=<MinBackward1>), tensor(0.8666, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11393971741199493\n",
      "@sample 872: tensor([[ 9.2280e-04,  1.5584e-02, -8.9445e-04, -2.3206e-02,  8.1903e-03],\n",
      "        [ 2.5554e-02, -3.1736e-03,  8.2630e-03,  7.8941e-03, -7.8961e-03],\n",
      "        [ 1.9304e-02, -4.2537e-03, -5.2034e-03,  4.1350e-03, -1.9199e-02],\n",
      "        [-7.8554e-03, -1.1872e-02,  1.6650e-02, -2.8391e-04, -8.6477e-03],\n",
      "        [ 2.5329e-02, -4.2817e-03,  1.0531e-02,  4.1136e-02, -1.4940e-02],\n",
      "        [ 1.8034e-02, -3.1181e-02,  8.6213e-04, -1.8064e-02,  6.9955e-03],\n",
      "        [ 2.4735e-03,  3.3228e-02,  2.1704e-02, -2.6456e-02, -6.6876e-05],\n",
      "        [ 7.3404e-03, -1.9078e-02,  8.2724e-03, -2.2493e-02, -6.9512e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0357, -0.0098, -0.0073,  0.0392,  0.0104],\n",
      "        [-0.0233, -0.0192,  0.0270, -0.0009, -0.0243],\n",
      "        [-0.0077,  0.0048, -0.0105,  0.0058, -0.0195],\n",
      "        [ 0.0001, -0.0297,  0.0376,  0.0082, -0.0132],\n",
      "        [ 0.0261,  0.0298, -0.0206,  0.0234, -0.0092],\n",
      "        [-0.0098,  0.0008,  0.0563, -0.0064, -0.0243],\n",
      "        [-0.0266, -0.0123, -0.0214,  0.0029, -0.0059],\n",
      "        [-0.0089, -0.0146,  0.0342, -0.0132, -0.0358]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0158, grad_fn=<MinBackward1>), tensor(0.9082, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10878261923789978\n",
      "@sample 873: tensor([[ 0.0227,  0.0501, -0.0024, -0.0286,  0.0145],\n",
      "        [ 0.0145, -0.0218,  0.0101,  0.0103,  0.0062],\n",
      "        [ 0.0019,  0.0043,  0.0092, -0.0145,  0.0013],\n",
      "        [ 0.0203, -0.0034,  0.0218,  0.0134, -0.0128],\n",
      "        [ 0.0032,  0.0018,  0.0083, -0.0094,  0.0121],\n",
      "        [-0.0100,  0.0177,  0.0130,  0.0032, -0.0049],\n",
      "        [ 0.0174,  0.0095,  0.0002, -0.0240,  0.0159],\n",
      "        [-0.0050, -0.0095, -0.0072,  0.0184,  0.0125]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0261, -0.0015, -0.0463,  0.0412,  0.0210],\n",
      "        [ 0.0169, -0.0076,  0.0527, -0.0268, -0.0065],\n",
      "        [-0.0101, -0.0381, -0.0296,  0.0107,  0.0116],\n",
      "        [ 0.0222,  0.0252,  0.0498,  0.0062,  0.0076],\n",
      "        [-0.0090, -0.0149,  0.0027,  0.0126, -0.0073],\n",
      "        [ 0.0308,  0.0525,  0.0433, -0.0118,  0.0244],\n",
      "        [-0.0174, -0.0197, -0.0395,  0.0361, -0.0378],\n",
      "        [ 0.0016,  0.0284,  0.0551,  0.0217, -0.0029]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0148, grad_fn=<MinBackward1>), tensor(0.8616, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10243625938892365\n",
      "@sample 874: tensor([[-0.0325, -0.0111,  0.0545, -0.0151,  0.0187],\n",
      "        [-0.0121, -0.0044,  0.0279, -0.0174,  0.0134],\n",
      "        [-0.0331,  0.0028, -0.0021, -0.0025, -0.0020],\n",
      "        [-0.0354,  0.0037, -0.0075, -0.0065,  0.0033],\n",
      "        [ 0.0034,  0.0200, -0.0181, -0.0148,  0.0084],\n",
      "        [ 0.0111,  0.0233, -0.0155,  0.0317, -0.0578],\n",
      "        [ 0.0307, -0.0103,  0.0066, -0.0205,  0.0092],\n",
      "        [ 0.0184, -0.0350,  0.0074,  0.0144, -0.0135]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0118,  0.0078,  0.0053,  0.0006, -0.0185],\n",
      "        [-0.0005,  0.0371, -0.0279,  0.0149,  0.0128],\n",
      "        [ 0.0013,  0.0003,  0.0384, -0.0253, -0.0244],\n",
      "        [ 0.0329,  0.0063,  0.0449, -0.0247,  0.0106],\n",
      "        [ 0.0098,  0.0166, -0.0082, -0.0171, -0.0332],\n",
      "        [ 0.0066,  0.0029,  0.0439, -0.0264, -0.0419],\n",
      "        [-0.0310, -0.0195, -0.0061,  0.0170, -0.0067],\n",
      "        [ 0.0103,  0.0225,  0.0228, -0.0094,  0.0076]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0178, grad_fn=<MinBackward1>), tensor(0.8402, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10806726664304733\n",
      "@sample 875: tensor([[-0.0117, -0.0166, -0.0078,  0.0127,  0.0063],\n",
      "        [-0.0098, -0.0223,  0.0092, -0.0189,  0.0052],\n",
      "        [-0.0056, -0.0023, -0.0021, -0.0169,  0.0179],\n",
      "        [-0.0012,  0.0096,  0.0252, -0.0069, -0.0017],\n",
      "        [ 0.0110, -0.0010,  0.0141, -0.0282, -0.0009],\n",
      "        [ 0.0222,  0.0072,  0.0038, -0.0155, -0.0187],\n",
      "        [-0.0101,  0.0109, -0.0088, -0.0200, -0.0015],\n",
      "        [ 0.0080, -0.0150,  0.0262,  0.0341, -0.0058]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0181,  0.0184, -0.0139, -0.0102,  0.0042],\n",
      "        [-0.0302, -0.0202,  0.0067, -0.0272, -0.0099],\n",
      "        [-0.0100, -0.0149, -0.0002,  0.0253, -0.0103],\n",
      "        [-0.0117,  0.0002,  0.0278,  0.0319, -0.0090],\n",
      "        [-0.0028, -0.0302,  0.0389,  0.0023,  0.0187],\n",
      "        [-0.0300, -0.0077, -0.0077,  0.0130,  0.0022],\n",
      "        [-0.0058, -0.0402,  0.0554, -0.0263, -0.0231],\n",
      "        [ 0.0297,  0.0246,  0.0074, -0.0263, -0.0128]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0136, grad_fn=<MinBackward1>), tensor(0.9089, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11063411086797714\n",
      "@sample 876: tensor([[ 0.0168,  0.0043,  0.0041, -0.0082, -0.0015],\n",
      "        [ 0.0103,  0.0065,  0.0222,  0.0035, -0.0044],\n",
      "        [ 0.0069, -0.0350,  0.0041,  0.0031, -0.0074],\n",
      "        [-0.0049, -0.0383,  0.0186, -0.0024, -0.0088],\n",
      "        [ 0.0238, -0.0225,  0.0163,  0.0078,  0.0183],\n",
      "        [ 0.0031,  0.0121,  0.0074,  0.0135, -0.0013],\n",
      "        [ 0.0127,  0.0014, -0.0146,  0.0337, -0.0055],\n",
      "        [-0.0244,  0.0051,  0.0062, -0.0382, -0.0177]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0112, -0.0089, -0.0033,  0.0247,  0.0289],\n",
      "        [-0.0015,  0.0054, -0.0091, -0.0276,  0.0341],\n",
      "        [-0.0267, -0.0076, -0.0094,  0.0052,  0.0011],\n",
      "        [ 0.0035,  0.0030,  0.0272, -0.0088,  0.0170],\n",
      "        [-0.0177, -0.0037, -0.0220,  0.0161,  0.0105],\n",
      "        [ 0.0112,  0.0037, -0.0108,  0.0042,  0.0004],\n",
      "        [ 0.0174,  0.0315, -0.0212, -0.0196, -0.0022],\n",
      "        [ 0.0233, -0.0121,  0.0392, -0.0280,  0.0184]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0162, grad_fn=<MinBackward1>), tensor(0.8913, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09839747101068497\n",
      "@sample 877: tensor([[-0.0209,  0.0078,  0.0061, -0.0037,  0.0149],\n",
      "        [ 0.0183, -0.0203, -0.0081,  0.0131, -0.0201],\n",
      "        [-0.0227,  0.0274,  0.0228, -0.0274,  0.0071],\n",
      "        [ 0.0420,  0.0009, -0.0049, -0.0034,  0.0030],\n",
      "        [ 0.0036, -0.0123, -0.0084, -0.0028, -0.0266],\n",
      "        [ 0.0171, -0.0144,  0.0088, -0.0045,  0.0067],\n",
      "        [-0.0149, -0.0402,  0.0162,  0.0064,  0.0275],\n",
      "        [-0.0224, -0.0209, -0.0087, -0.0069,  0.0073]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0184,  0.0111,  0.0308, -0.0432, -0.0012],\n",
      "        [ 0.0077, -0.0077, -0.0089,  0.0292,  0.0133],\n",
      "        [ 0.0029,  0.0375, -0.0325,  0.0542,  0.0148],\n",
      "        [-0.0166,  0.0326, -0.0376,  0.0181,  0.0226],\n",
      "        [-0.0234, -0.0139, -0.0172, -0.0062,  0.0004],\n",
      "        [ 0.0417,  0.0019, -0.0155,  0.0233,  0.0111],\n",
      "        [ 0.0106,  0.0028,  0.0141,  0.0345,  0.0413],\n",
      "        [ 0.0176, -0.0053,  0.0186, -0.0224,  0.0249]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0142, grad_fn=<MinBackward1>), tensor(0.8592, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10478250682353973\n",
      "@sample 878: tensor([[-1.2710e-03, -1.4998e-02, -3.8089e-02, -2.1790e-03,  1.7897e-02],\n",
      "        [-1.2650e-02,  2.0595e-02,  2.1678e-02, -1.8816e-02,  2.6173e-02],\n",
      "        [ 3.0711e-03,  2.3056e-03, -2.6150e-03,  3.2678e-03,  4.0889e-05],\n",
      "        [ 9.2172e-03, -3.5691e-04, -8.5528e-03, -6.8615e-03, -5.9917e-03],\n",
      "        [ 1.5020e-02,  1.9903e-02,  1.0767e-02, -1.5101e-02,  6.8374e-03],\n",
      "        [ 7.1613e-03,  2.9380e-02, -1.8817e-02, -3.8954e-02,  2.3922e-02],\n",
      "        [-3.2508e-05, -2.4906e-02, -7.4054e-03, -1.0253e-03,  1.0565e-05],\n",
      "        [-1.4931e-02, -1.1065e-02, -1.6332e-02, -1.7486e-02,  4.2304e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0152, -0.0019,  0.0060,  0.0050, -0.0139],\n",
      "        [-0.0131,  0.0076, -0.0928,  0.0264,  0.0025],\n",
      "        [ 0.0024,  0.0016,  0.0001, -0.0119, -0.0014],\n",
      "        [-0.0104,  0.0004, -0.0217, -0.0014, -0.0096],\n",
      "        [-0.0460, -0.0103, -0.0544,  0.0263,  0.0040],\n",
      "        [-0.0088,  0.0007, -0.0387,  0.0191,  0.0237],\n",
      "        [-0.0002, -0.0152,  0.0393, -0.0155, -0.0077],\n",
      "        [-0.0013, -0.0157, -0.0345,  0.0381,  0.0272]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0123, grad_fn=<MinBackward1>), tensor(0.8734, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.0978323370218277\n",
      "@sample 879: tensor([[ 0.0063, -0.0002, -0.0270,  0.0128, -0.0007],\n",
      "        [ 0.0224,  0.0136,  0.0146,  0.0026, -0.0207],\n",
      "        [ 0.0151, -0.0097, -0.0089, -0.0196, -0.0086],\n",
      "        [-0.0076,  0.0116, -0.0153, -0.0200,  0.0066],\n",
      "        [-0.0112, -0.0091, -0.0163,  0.0181,  0.0295],\n",
      "        [ 0.0066, -0.0286,  0.0024,  0.0129, -0.0170],\n",
      "        [-0.0161, -0.0308, -0.0351, -0.0014,  0.0290],\n",
      "        [ 0.0132, -0.0217, -0.0124,  0.0227,  0.0071]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0174,  0.0036,  0.0146, -0.0601, -0.0127],\n",
      "        [-0.0160,  0.0261,  0.0158, -0.0282,  0.0096],\n",
      "        [-0.0614, -0.0151,  0.0449,  0.0235,  0.0006],\n",
      "        [-0.0014, -0.0186, -0.0124,  0.0243,  0.0110],\n",
      "        [ 0.0159,  0.0342,  0.0787, -0.0241, -0.0278],\n",
      "        [-0.0193, -0.0054, -0.0246,  0.0005,  0.0169],\n",
      "        [-0.0094, -0.0193,  0.0298,  0.0073, -0.0012],\n",
      "        [ 0.0176,  0.0202,  0.0008, -0.0067,  0.0129]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0126, grad_fn=<MinBackward1>), tensor(0.8180, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1060183197259903\n",
      "@sample 880: tensor([[ 0.0034,  0.0074, -0.0199, -0.0168, -0.0173],\n",
      "        [-0.0059,  0.0099,  0.0199,  0.0329, -0.0139],\n",
      "        [-0.0213,  0.0422,  0.0135, -0.0576,  0.0144],\n",
      "        [ 0.0183, -0.0150,  0.0015, -0.0358, -0.0043],\n",
      "        [ 0.0129,  0.0019, -0.0131,  0.0244, -0.0068],\n",
      "        [-0.0165,  0.0088,  0.0166,  0.0225, -0.0016],\n",
      "        [-0.0058, -0.0235, -0.0474,  0.0363, -0.0097],\n",
      "        [ 0.0196,  0.0045, -0.0032,  0.0110,  0.0062]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0346, -0.0339,  0.0374, -0.0175,  0.0335],\n",
      "        [ 0.0026,  0.0146, -0.0275, -0.0068,  0.0173],\n",
      "        [-0.0302, -0.0157, -0.0241,  0.0027,  0.0122],\n",
      "        [-0.0464, -0.0208,  0.0176, -0.0365, -0.0234],\n",
      "        [ 0.0093,  0.0150, -0.0290,  0.0121,  0.0008],\n",
      "        [-0.0039,  0.0289, -0.0179,  0.0152,  0.0148],\n",
      "        [ 0.0472,  0.0106,  0.0140, -0.0236, -0.0041],\n",
      "        [ 0.0025,  0.0130,  0.0016, -0.0157, -0.0083]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0076, grad_fn=<MinBackward1>), tensor(0.8661, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10563089698553085\n",
      "@sample 881: tensor([[ 1.6745e-02, -1.1083e-02, -1.3889e-03,  4.0691e-03, -1.7440e-02],\n",
      "        [ 5.0045e-03, -1.3307e-02,  5.5092e-03, -2.5964e-02,  2.3575e-02],\n",
      "        [-1.6218e-04,  2.3172e-02, -1.3194e-02, -1.4010e-02,  1.2301e-02],\n",
      "        [-2.3347e-05,  2.5824e-02,  7.9611e-03,  4.2950e-03,  1.3854e-02],\n",
      "        [ 1.1526e-02, -6.9703e-03, -3.4940e-03,  2.1585e-02, -2.5809e-02],\n",
      "        [ 1.4575e-02, -6.4812e-03,  2.6358e-03,  1.2402e-03, -2.0756e-02],\n",
      "        [-1.6146e-02, -7.4826e-03, -1.1638e-02, -1.0522e-02, -6.0388e-03],\n",
      "        [ 4.0404e-02,  1.5695e-02, -2.4314e-02,  1.5539e-02,  1.5317e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0021, -0.0097, -0.0320,  0.0075,  0.0152],\n",
      "        [-0.0069, -0.0177, -0.0038, -0.0217, -0.0161],\n",
      "        [ 0.0124,  0.0157, -0.0266,  0.0257, -0.0053],\n",
      "        [ 0.0032,  0.0033, -0.0592,  0.0280,  0.0145],\n",
      "        [-0.0227, -0.0004, -0.0219,  0.0040,  0.0083],\n",
      "        [-0.0125, -0.0132, -0.0432,  0.0586,  0.0152],\n",
      "        [ 0.0002, -0.0435,  0.0383, -0.0043, -0.0192],\n",
      "        [ 0.0214,  0.0308, -0.0536,  0.0166, -0.0162]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0167, grad_fn=<MinBackward1>), tensor(0.8637, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10139087587594986\n",
      "@sample 882: tensor([[-0.0007, -0.0261, -0.0020,  0.0357, -0.0012],\n",
      "        [-0.0531,  0.0015, -0.0159, -0.0202,  0.0181],\n",
      "        [ 0.0099, -0.0220, -0.0042,  0.0348, -0.0512],\n",
      "        [ 0.0114,  0.0511,  0.0153, -0.0281,  0.0148],\n",
      "        [ 0.0207,  0.0140, -0.0125,  0.0194, -0.0352],\n",
      "        [ 0.0080, -0.0225,  0.0113, -0.0164,  0.0014],\n",
      "        [ 0.0065,  0.0334, -0.0015, -0.0082,  0.0147],\n",
      "        [ 0.0455,  0.0109,  0.0010, -0.0103,  0.0106]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0095,  0.0230,  0.0198, -0.0182,  0.0028],\n",
      "        [ 0.0091, -0.0648,  0.0418, -0.0312, -0.0189],\n",
      "        [ 0.0204,  0.0085,  0.0151, -0.0256,  0.0293],\n",
      "        [-0.0056,  0.0084, -0.0945,  0.0457,  0.0001],\n",
      "        [-0.0065,  0.0086,  0.0309,  0.0023, -0.0285],\n",
      "        [-0.0306, -0.0123,  0.0170, -0.0352, -0.0308],\n",
      "        [-0.0115, -0.0332, -0.0549,  0.0253,  0.0130],\n",
      "        [-0.0278,  0.0041, -0.0453, -0.0376, -0.0152]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0104, grad_fn=<MinBackward1>), tensor(0.9352, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.12199966609477997\n",
      "@sample 883: tensor([[ 0.0149,  0.0241,  0.0185, -0.0361, -0.0038],\n",
      "        [ 0.0029, -0.0190, -0.0108, -0.0187, -0.0049],\n",
      "        [ 0.0091, -0.0193, -0.0292,  0.0459, -0.0310],\n",
      "        [-0.0088,  0.0502,  0.0125, -0.0542, -0.0045],\n",
      "        [-0.0035, -0.0111, -0.0034, -0.0044,  0.0024],\n",
      "        [ 0.0070,  0.0078,  0.0091, -0.0089,  0.0362],\n",
      "        [-0.0016, -0.0207,  0.0055, -0.0105,  0.0018],\n",
      "        [-0.0387, -0.0103,  0.0087, -0.0087,  0.0138]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0475, -0.0656, -0.0650,  0.0033, -0.0188],\n",
      "        [ 0.0137, -0.0395,  0.0201,  0.0084,  0.0075],\n",
      "        [-0.0008,  0.0091,  0.0358, -0.0589, -0.0199],\n",
      "        [-0.0308, -0.0032, -0.0064,  0.0342,  0.0313],\n",
      "        [-0.0289,  0.0085, -0.0022, -0.0075,  0.0302],\n",
      "        [-0.0386,  0.0001, -0.0335,  0.0173, -0.0057],\n",
      "        [-0.0095, -0.0238, -0.0067, -0.0077,  0.0050],\n",
      "        [-0.0005, -0.0224,  0.0308, -0.0046,  0.0200]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0179, grad_fn=<MinBackward1>), tensor(0.8754, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11218464374542236\n",
      "@sample 884: tensor([[-6.1993e-03,  4.7228e-02,  8.7923e-03, -5.9185e-03,  1.4905e-02],\n",
      "        [ 1.0997e-02,  4.8196e-02,  6.5202e-03, -2.0185e-02,  2.6296e-02],\n",
      "        [-6.2216e-03, -3.0032e-02,  1.4052e-03,  1.3351e-02,  5.7883e-03],\n",
      "        [ 1.9935e-03, -1.6412e-02, -1.5964e-03, -1.0668e-02,  1.6023e-03],\n",
      "        [ 1.4040e-03,  1.6065e-03, -4.5761e-02,  1.8517e-03, -3.8808e-02],\n",
      "        [-6.4277e-04, -1.9409e-02, -1.8094e-02, -8.3373e-03, -2.5576e-03],\n",
      "        [ 2.2045e-02, -1.5059e-02, -2.0899e-03,  2.2825e-03, -1.6375e-02],\n",
      "        [-2.4622e-04, -8.1077e-05,  1.5632e-02,  4.4052e-03, -3.1131e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0491,  0.0479,  0.0006, -0.0046,  0.0150],\n",
      "        [-0.0074, -0.0070, -0.0108, -0.0031, -0.0110],\n",
      "        [ 0.0149,  0.0056,  0.0480, -0.0035,  0.0210],\n",
      "        [-0.0102, -0.0273,  0.0256, -0.0400, -0.0202],\n",
      "        [-0.0311, -0.0308,  0.0155, -0.0313,  0.0063],\n",
      "        [-0.0152, -0.0134, -0.0155, -0.0174,  0.0136],\n",
      "        [ 0.0103,  0.0133, -0.0076, -0.0004, -0.0102],\n",
      "        [-0.0186,  0.0540, -0.0428,  0.0377,  0.0251]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0110, grad_fn=<MinBackward1>), tensor(0.8390, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11048275977373123\n",
      "@sample 885: tensor([[-0.0098, -0.0048, -0.0075,  0.0291, -0.0006],\n",
      "        [ 0.0368, -0.0185,  0.0030,  0.0114,  0.0279],\n",
      "        [-0.0297, -0.0141, -0.0172, -0.0151,  0.0089],\n",
      "        [-0.0012,  0.0069, -0.0147,  0.0042,  0.0124],\n",
      "        [ 0.0102, -0.0136,  0.0109,  0.0143, -0.0163],\n",
      "        [-0.0130, -0.0096, -0.0225,  0.0321, -0.0284],\n",
      "        [-0.0008, -0.0117, -0.0266,  0.0365,  0.0120],\n",
      "        [-0.0123, -0.0047, -0.0118, -0.0015,  0.0120]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0157,  0.0036,  0.0069,  0.0007,  0.0006],\n",
      "        [ 0.0155,  0.0048, -0.0096,  0.0306,  0.0329],\n",
      "        [-0.0070, -0.0107, -0.0077, -0.0128, -0.0193],\n",
      "        [-0.0066, -0.0006,  0.0036,  0.0207, -0.0083],\n",
      "        [-0.0084,  0.0118, -0.0437, -0.0097,  0.0297],\n",
      "        [ 0.0280,  0.0022,  0.0312, -0.0278,  0.0402],\n",
      "        [ 0.0306,  0.0102, -0.0236,  0.0109, -0.0093],\n",
      "        [ 0.0093, -0.0075, -0.0157, -0.0119, -0.0122]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0141, grad_fn=<MinBackward1>), tensor(0.9002, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09489630162715912\n",
      "@sample 886: tensor([[ 2.1091e-02,  1.5013e-02, -2.5376e-03,  1.8344e-02, -8.6932e-03],\n",
      "        [ 3.6633e-03, -1.6576e-02, -1.2619e-02,  1.2453e-02, -4.2096e-03],\n",
      "        [ 2.7923e-02, -1.4223e-03,  1.6299e-02, -7.8240e-03,  2.6937e-02],\n",
      "        [ 1.6898e-03,  1.4581e-02, -1.5018e-02, -3.5617e-02,  3.2283e-02],\n",
      "        [-3.7161e-02, -5.2726e-03,  2.0572e-02,  5.5950e-03, -3.4819e-03],\n",
      "        [ 3.2708e-02, -9.4686e-03, -3.0802e-04,  1.5064e-02, -1.4161e-02],\n",
      "        [-4.4909e-03,  1.1286e-02, -1.8642e-02, -2.1726e-02,  3.7037e-05],\n",
      "        [ 3.3242e-03, -2.1852e-02,  9.0686e-03,  7.8159e-03, -1.3631e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-8.2055e-03,  1.3358e-02, -2.4229e-02, -9.1695e-03,  1.9514e-02],\n",
      "        [ 9.3420e-03, -1.4619e-02, -6.6565e-03,  2.9442e-03,  6.6581e-03],\n",
      "        [ 3.1178e-02,  1.1931e-02, -9.0699e-02,  7.6677e-02,  3.4512e-02],\n",
      "        [-4.9322e-02, -6.7578e-03, -7.6949e-02,  4.2948e-02, -2.6740e-02],\n",
      "        [ 2.0795e-02, -8.0149e-02, -2.2506e-02, -6.0324e-03, -4.2504e-03],\n",
      "        [-2.7087e-02,  1.4413e-02, -5.1257e-02,  1.6068e-02,  3.9011e-05],\n",
      "        [ 1.9509e-02, -2.6380e-02,  5.5509e-02, -2.0755e-02, -9.6513e-03],\n",
      "        [-1.2720e-02,  5.0947e-02,  7.0075e-03,  1.0251e-02,  2.1368e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0170, grad_fn=<MinBackward1>), tensor(0.8821, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11657752841711044\n",
      "@sample 887: tensor([[ 0.0055,  0.0022,  0.0142, -0.0074, -0.0143],\n",
      "        [-0.0016, -0.0024, -0.0220,  0.0122, -0.0161],\n",
      "        [-0.0028,  0.0020, -0.0087,  0.0352,  0.0110],\n",
      "        [-0.0211,  0.0268,  0.0101, -0.0086,  0.0128],\n",
      "        [-0.0186, -0.0105,  0.0223, -0.0179,  0.0064],\n",
      "        [-0.0190, -0.0021,  0.0207,  0.0063,  0.0033],\n",
      "        [ 0.0012, -0.0161,  0.0047, -0.0099,  0.0062],\n",
      "        [ 0.0002,  0.0054,  0.0125,  0.0036, -0.0084]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0219,  0.0082,  0.0022, -0.0565, -0.0212],\n",
      "        [ 0.0159,  0.0063,  0.0399, -0.0404,  0.0026],\n",
      "        [ 0.0477,  0.0107,  0.0543, -0.0458, -0.0394],\n",
      "        [ 0.0139,  0.0071, -0.0317,  0.0115,  0.0425],\n",
      "        [ 0.0032, -0.0268,  0.0179, -0.0125, -0.0093],\n",
      "        [ 0.0259, -0.0020, -0.0104,  0.0170,  0.0267],\n",
      "        [ 0.0147, -0.0024,  0.0069,  0.0448,  0.0062],\n",
      "        [-0.0060,  0.0049, -0.0377,  0.0076,  0.0062]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0088, grad_fn=<MinBackward1>), tensor(0.8326, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1048678532242775\n",
      "@sample 888: tensor([[-0.0237,  0.0099, -0.0090,  0.0078,  0.0034],\n",
      "        [-0.0148,  0.0222, -0.0041, -0.0322,  0.0444],\n",
      "        [-0.0117,  0.0140, -0.0007, -0.0008,  0.0109],\n",
      "        [-0.0306,  0.0044, -0.0022,  0.0446, -0.0120],\n",
      "        [-0.0281,  0.0049, -0.0198,  0.0112, -0.0105],\n",
      "        [-0.0055,  0.0019, -0.0015,  0.0154, -0.0161],\n",
      "        [-0.0237,  0.0017, -0.0064,  0.0182,  0.0017],\n",
      "        [-0.0005,  0.0047, -0.0049, -0.0005,  0.0139]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0077, -0.0021,  0.0110, -0.0093,  0.0177],\n",
      "        [ 0.0106, -0.0226, -0.0390, -0.0201,  0.0307],\n",
      "        [ 0.0064,  0.0387, -0.0153,  0.0152, -0.0016],\n",
      "        [ 0.0124,  0.0099, -0.0306,  0.0314, -0.0189],\n",
      "        [ 0.0037,  0.0140,  0.0117, -0.0136, -0.0039],\n",
      "        [ 0.0337,  0.0101, -0.0269,  0.0099,  0.0092],\n",
      "        [ 0.0185,  0.0191,  0.0015, -0.0015,  0.0127],\n",
      "        [-0.0022, -0.0082, -0.0261, -0.0235, -0.0133]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0116, grad_fn=<MinBackward1>), tensor(0.8723, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09193563461303711\n",
      "@sample 889: tensor([[-0.0053, -0.0376,  0.0021,  0.0123, -0.0218],\n",
      "        [ 0.0119,  0.0231,  0.0256, -0.0193,  0.0084],\n",
      "        [ 0.0085,  0.0150,  0.0138, -0.0081,  0.0001],\n",
      "        [ 0.0076,  0.0113,  0.0065, -0.0288,  0.0072],\n",
      "        [ 0.0163,  0.0104,  0.0108,  0.0229, -0.0131],\n",
      "        [-0.0080,  0.0127,  0.0003, -0.0053,  0.0209],\n",
      "        [-0.0023, -0.0061,  0.0131, -0.0019, -0.0064],\n",
      "        [-0.0038, -0.0016, -0.0002, -0.0043,  0.0148]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0291,  0.0144,  0.0467,  0.0132,  0.0060],\n",
      "        [-0.0181,  0.0012, -0.0581,  0.0524,  0.0255],\n",
      "        [ 0.0035,  0.0488, -0.0318, -0.0153, -0.0028],\n",
      "        [-0.0270,  0.0038, -0.0124, -0.0064, -0.0315],\n",
      "        [ 0.0028,  0.0108, -0.0053, -0.0031,  0.0092],\n",
      "        [-0.0116,  0.0296, -0.0371,  0.0150, -0.0076],\n",
      "        [ 0.0263, -0.0063,  0.0011, -0.0385, -0.0494],\n",
      "        [ 0.0366,  0.0188, -0.0167, -0.0091,  0.0107]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0135, grad_fn=<MinBackward1>), tensor(0.8372, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10077487677335739\n",
      "@sample 890: tensor([[ 4.9948e-04, -6.1670e-03,  9.4731e-03,  9.1133e-03,  1.0055e-02],\n",
      "        [-1.0532e-02,  1.8771e-02, -1.4740e-02,  6.8500e-03,  3.9018e-03],\n",
      "        [-9.0410e-03,  5.6670e-03, -1.3707e-02,  6.9229e-03,  1.5751e-02],\n",
      "        [ 7.8856e-03, -1.4706e-03,  4.0572e-03, -1.7263e-02,  1.0373e-02],\n",
      "        [ 6.0448e-03,  4.1217e-02, -4.9306e-03, -2.5361e-02,  1.8610e-02],\n",
      "        [ 1.2888e-03,  8.0282e-03, -2.0208e-03,  1.6748e-02, -6.3396e-03],\n",
      "        [-2.4781e-02,  3.0009e-02,  1.6668e-02, -2.6693e-02, -1.4452e-02],\n",
      "        [-1.2279e-04, -2.4853e-02,  2.3534e-02,  2.5225e-02,  2.4363e-06]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0102,  0.0107,  0.0105, -0.0054, -0.0121],\n",
      "        [-0.0066,  0.0185, -0.0561,  0.0033,  0.0323],\n",
      "        [-0.0058, -0.0280, -0.0022, -0.0199, -0.0369],\n",
      "        [-0.0098, -0.0039, -0.0179, -0.0227, -0.0245],\n",
      "        [-0.0064,  0.0398, -0.0438,  0.0253,  0.0347],\n",
      "        [ 0.0058,  0.0248, -0.0025, -0.0161, -0.0068],\n",
      "        [-0.0185, -0.0017, -0.0504,  0.0269,  0.0353],\n",
      "        [ 0.0109,  0.0172,  0.0323, -0.0125, -0.0144]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0180, grad_fn=<MinBackward1>), tensor(0.9010, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09807883203029633\n",
      "@sample 891: tensor([[ 0.0105, -0.0190, -0.0195,  0.0371, -0.0168],\n",
      "        [ 0.0303,  0.0309,  0.0687, -0.0304,  0.0085],\n",
      "        [-0.0276,  0.0086,  0.0039,  0.0277, -0.0007],\n",
      "        [ 0.0136,  0.0187,  0.0128, -0.0025, -0.0195],\n",
      "        [-0.0249,  0.0119, -0.0283, -0.0019,  0.0123],\n",
      "        [ 0.0106,  0.0105,  0.0110, -0.0015, -0.0142],\n",
      "        [-0.0025,  0.0186, -0.0043,  0.0131, -0.0003],\n",
      "        [-0.0062,  0.0345,  0.0256, -0.0232, -0.0058]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0150, -0.0037, -0.0259, -0.0058, -0.0121],\n",
      "        [-0.0234, -0.0342, -0.0624,  0.0594, -0.0041],\n",
      "        [ 0.0159,  0.0062, -0.0219, -0.0153,  0.0366],\n",
      "        [ 0.0146,  0.0145,  0.0183,  0.0250,  0.0229],\n",
      "        [ 0.0217, -0.0409,  0.0214, -0.0210, -0.0244],\n",
      "        [ 0.0055,  0.0141, -0.0049, -0.0065, -0.0119],\n",
      "        [ 0.0257,  0.0196,  0.0167,  0.0130,  0.0115],\n",
      "        [-0.0107,  0.0197, -0.0136,  0.0417,  0.0501]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0196, grad_fn=<MinBackward1>), tensor(0.9163, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10706126689910889\n",
      "@sample 892: tensor([[-0.0129,  0.0198,  0.0122, -0.0044, -0.0297],\n",
      "        [-0.0219, -0.0012,  0.0079,  0.0112, -0.0245],\n",
      "        [-0.0051, -0.0023, -0.0302,  0.0122, -0.0148],\n",
      "        [ 0.0059, -0.0221,  0.0042,  0.0315, -0.0007],\n",
      "        [-0.0101,  0.0127, -0.0115,  0.0064, -0.0292],\n",
      "        [ 0.0020,  0.0267, -0.0108, -0.0049,  0.0009],\n",
      "        [-0.0170, -0.0165,  0.0203,  0.0019,  0.0126],\n",
      "        [ 0.0101,  0.0271, -0.0010, -0.0222, -0.0045]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0011, -0.0116, -0.0101, -0.0024, -0.0059],\n",
      "        [ 0.0028, -0.0239,  0.0112,  0.0032, -0.0076],\n",
      "        [ 0.0131, -0.0086, -0.0048, -0.0135,  0.0026],\n",
      "        [ 0.0396,  0.0235,  0.0021,  0.0220,  0.0231],\n",
      "        [ 0.0304,  0.0221,  0.0131,  0.0008,  0.0138],\n",
      "        [ 0.0094, -0.0152, -0.0206, -0.0040, -0.0019],\n",
      "        [-0.0036,  0.0152, -0.0134,  0.0063,  0.0228],\n",
      "        [-0.0084, -0.0165,  0.0244, -0.0030, -0.0294]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0132, grad_fn=<MinBackward1>), tensor(0.8488, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.090416319668293\n",
      "@sample 893: tensor([[-0.0120, -0.0043,  0.0029,  0.0032, -0.0026],\n",
      "        [ 0.0121, -0.0016,  0.0006,  0.0063, -0.0237],\n",
      "        [ 0.0006, -0.0060, -0.0231, -0.0047,  0.0140],\n",
      "        [ 0.0260,  0.0022,  0.0025, -0.0183,  0.0165],\n",
      "        [-0.0162, -0.0057,  0.0130, -0.0129, -0.0047],\n",
      "        [-0.0114, -0.0284, -0.0266,  0.0333, -0.0126],\n",
      "        [ 0.0298, -0.0275,  0.0046,  0.0039,  0.0064],\n",
      "        [ 0.0158,  0.0036,  0.0032, -0.0058,  0.0223]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0047,  0.0057,  0.0132, -0.0113,  0.0170],\n",
      "        [ 0.0182, -0.0009,  0.0215, -0.0218, -0.0454],\n",
      "        [ 0.0044, -0.0246, -0.0236, -0.0059,  0.0083],\n",
      "        [-0.0224, -0.0071,  0.0144,  0.0298,  0.0186],\n",
      "        [ 0.0039,  0.0035,  0.0203, -0.0272, -0.0006],\n",
      "        [-0.0041, -0.0136,  0.0080, -0.0315, -0.0148],\n",
      "        [ 0.0148,  0.0195, -0.0525,  0.0350,  0.0047],\n",
      "        [-0.0043, -0.0024,  0.0025, -0.0006, -0.0032]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0173, grad_fn=<MinBackward1>), tensor(0.8869, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10224148631095886\n",
      "@sample 894: tensor([[ 0.0099, -0.0021,  0.0113,  0.0251, -0.0247],\n",
      "        [-0.0198, -0.0049,  0.0072, -0.0181,  0.0250],\n",
      "        [ 0.0366, -0.0253, -0.0085,  0.0153,  0.0189],\n",
      "        [-0.0184, -0.0032,  0.0021, -0.0265,  0.0073],\n",
      "        [ 0.0171, -0.0081, -0.0019, -0.0167, -0.0017],\n",
      "        [-0.0307,  0.0393,  0.0287, -0.0539,  0.0131],\n",
      "        [-0.0042,  0.0076, -0.0038, -0.0276,  0.0187],\n",
      "        [-0.0040,  0.0430, -0.0065,  0.0115, -0.0078]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0068,  0.0156, -0.0005, -0.0175, -0.0200],\n",
      "        [ 0.0018, -0.0357,  0.0349, -0.0095, -0.0217],\n",
      "        [-0.0037,  0.0182, -0.0438,  0.0028, -0.0164],\n",
      "        [-0.0046, -0.0573,  0.0501,  0.0061, -0.0292],\n",
      "        [-0.0292, -0.0085, -0.0215,  0.0136,  0.0058],\n",
      "        [-0.0161,  0.0037, -0.0434,  0.0097, -0.0034],\n",
      "        [-0.0044, -0.0040,  0.0236, -0.0347, -0.0124],\n",
      "        [ 0.0081, -0.0113, -0.0271, -0.0028, -0.0011]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0176, grad_fn=<MinBackward1>), tensor(0.9106, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10828877985477448\n",
      "@sample 895: tensor([[-0.0340,  0.0117,  0.0323, -0.0127, -0.0003],\n",
      "        [-0.0065, -0.0186, -0.0012,  0.0058, -0.0296],\n",
      "        [ 0.0131, -0.0152, -0.0038,  0.0304, -0.0197],\n",
      "        [ 0.0158,  0.0249,  0.0418, -0.0033, -0.0173],\n",
      "        [ 0.0018,  0.0334,  0.0233, -0.0397,  0.0008],\n",
      "        [-0.0128, -0.0029, -0.0010, -0.0115,  0.0059],\n",
      "        [ 0.0017,  0.0158,  0.0233, -0.0175,  0.0213],\n",
      "        [-0.0212,  0.0006,  0.0099, -0.0104, -0.0084]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0289, -0.0020,  0.0081, -0.0340, -0.0386],\n",
      "        [ 0.0070, -0.0167,  0.0557, -0.0338, -0.0044],\n",
      "        [-0.0139,  0.0004, -0.0054,  0.0229,  0.0192],\n",
      "        [-0.0383,  0.0068, -0.0211,  0.0241, -0.0237],\n",
      "        [ 0.0065, -0.0669, -0.0162, -0.0212, -0.0119],\n",
      "        [-0.0086,  0.0083, -0.0001, -0.0322, -0.0059],\n",
      "        [-0.0400, -0.0100, -0.0224,  0.0123, -0.0036],\n",
      "        [-0.0246, -0.0212,  0.0218, -0.0272, -0.0380]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0075, grad_fn=<MinBackward1>), tensor(0.8464, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11151091009378433\n",
      "@sample 896: tensor([[-0.0361,  0.0091,  0.0315, -0.0388,  0.0100],\n",
      "        [-0.0053, -0.0235,  0.0058,  0.0252,  0.0038],\n",
      "        [ 0.0171,  0.0298,  0.0158, -0.0255,  0.0095],\n",
      "        [ 0.0079, -0.0073,  0.0149, -0.0056, -0.0122],\n",
      "        [-0.0075, -0.0173,  0.0516, -0.0037,  0.0279],\n",
      "        [ 0.0062,  0.0293,  0.0461, -0.0281,  0.0053],\n",
      "        [-0.0036, -0.0191,  0.0025,  0.0053, -0.0098],\n",
      "        [ 0.0260, -0.0101,  0.0330,  0.0051,  0.0142]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0786, -0.0231, -0.1028,  0.0272,  0.0013],\n",
      "        [-0.0046,  0.0176,  0.0096, -0.0221, -0.0088],\n",
      "        [-0.0366,  0.0117, -0.0394, -0.0137, -0.0529],\n",
      "        [-0.0412, -0.0046,  0.0153, -0.0028,  0.0143],\n",
      "        [-0.0072,  0.0469,  0.0416, -0.0095, -0.0047],\n",
      "        [-0.0393,  0.0154, -0.0482,  0.0078, -0.0082],\n",
      "        [-0.0051,  0.0209,  0.0353, -0.0253,  0.0054],\n",
      "        [-0.0422, -0.0227, -0.0512,  0.0023,  0.0243]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0168, grad_fn=<MinBackward1>), tensor(0.8674, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11240516602993011\n",
      "@sample 897: tensor([[ 0.0299,  0.0080,  0.0009, -0.0047,  0.0148],\n",
      "        [ 0.0012,  0.0010, -0.0099,  0.0035, -0.0024],\n",
      "        [-0.0172,  0.0037,  0.0200,  0.0008,  0.0121],\n",
      "        [ 0.0160, -0.0057, -0.0078, -0.0022,  0.0275],\n",
      "        [ 0.0063,  0.0036,  0.0040,  0.0026, -0.0180],\n",
      "        [ 0.0013, -0.0133,  0.0088,  0.0004,  0.0217],\n",
      "        [-0.0046,  0.0121, -0.0231,  0.0196, -0.0145],\n",
      "        [ 0.0139,  0.0112,  0.0088,  0.0136, -0.0151]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0003, -0.0010, -0.0757,  0.0401, -0.0299],\n",
      "        [-0.0203, -0.0204,  0.0086, -0.0030, -0.0076],\n",
      "        [ 0.0048,  0.0215, -0.0211, -0.0065, -0.0203],\n",
      "        [ 0.0297,  0.0008, -0.0147,  0.0016,  0.0145],\n",
      "        [-0.0185,  0.0075,  0.0065, -0.0085, -0.0047],\n",
      "        [-0.0097, -0.0072, -0.0056, -0.0112, -0.0059],\n",
      "        [ 0.0149,  0.0145, -0.0134,  0.0039,  0.0221],\n",
      "        [ 0.0146,  0.0273, -0.0069, -0.0043, -0.0003]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0139, grad_fn=<MinBackward1>), tensor(0.8625, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10028429329395294\n",
      "@sample 898: tensor([[-0.0256,  0.0208,  0.0123, -0.0274,  0.0114],\n",
      "        [ 0.0113, -0.0005, -0.0099, -0.0359,  0.0138],\n",
      "        [-0.0039,  0.0315, -0.0015, -0.0143,  0.0172],\n",
      "        [ 0.0130,  0.0333, -0.0064, -0.0080,  0.0103],\n",
      "        [ 0.0063, -0.0513, -0.0175, -0.0094,  0.0195],\n",
      "        [ 0.0086, -0.0127, -0.0018,  0.0091,  0.0005],\n",
      "        [-0.0238, -0.0061,  0.0289, -0.0091,  0.0071],\n",
      "        [ 0.0143,  0.0347, -0.0015, -0.0231,  0.0150]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0029, -0.0188, -0.0142, -0.0668, -0.0301],\n",
      "        [-0.0152,  0.0011, -0.0065, -0.0005, -0.0086],\n",
      "        [ 0.0005, -0.0127, -0.0554,  0.0072, -0.0109],\n",
      "        [-0.0108,  0.0060, -0.0368,  0.0163,  0.0110],\n",
      "        [-0.0312, -0.0850, -0.0140, -0.0027, -0.0274],\n",
      "        [ 0.0038,  0.0073, -0.0275,  0.0152,  0.0188],\n",
      "        [ 0.0024,  0.0478, -0.0009,  0.0216, -0.0002],\n",
      "        [ 0.0042, -0.0314, -0.0248, -0.0092, -0.0349]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0112, grad_fn=<MinBackward1>), tensor(0.8402, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11134873330593109\n",
      "@sample 899: tensor([[ 0.0102, -0.0304, -0.0171,  0.0038,  0.0263],\n",
      "        [ 0.0009, -0.0284, -0.0127,  0.0150, -0.0219],\n",
      "        [ 0.0041, -0.0001, -0.0199,  0.0312, -0.0027],\n",
      "        [ 0.0070, -0.0122, -0.0059,  0.0230, -0.0186],\n",
      "        [ 0.0098, -0.0059, -0.0025,  0.0063,  0.0190],\n",
      "        [-0.0049, -0.0136, -0.0019, -0.0339,  0.0013],\n",
      "        [ 0.0097, -0.0097, -0.0153,  0.0234, -0.0311],\n",
      "        [ 0.0147, -0.0275, -0.0107,  0.0372, -0.0537]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0153,  0.0270, -0.0163, -0.0144, -0.0113],\n",
      "        [-0.0054, -0.0032, -0.0213,  0.0357,  0.0198],\n",
      "        [ 0.0176,  0.0265,  0.0176,  0.0073,  0.0103],\n",
      "        [-0.0117,  0.0019,  0.0050, -0.0111, -0.0028],\n",
      "        [-0.0062,  0.0101, -0.0129,  0.0182, -0.0088],\n",
      "        [-0.0234, -0.0016,  0.0301, -0.0038, -0.0059],\n",
      "        [ 0.0203,  0.0197,  0.0116, -0.0008,  0.0053],\n",
      "        [ 0.0019,  0.0222,  0.0712, -0.0346,  0.0074]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0198, grad_fn=<MinBackward1>), tensor(0.8944, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10761290788650513\n",
      "@sample 900: tensor([[ 1.2496e-02, -5.9579e-03,  1.0513e-02, -1.7020e-02,  2.6996e-02],\n",
      "        [-1.2897e-03,  2.9642e-02,  1.8999e-06, -1.3079e-02,  2.3496e-02],\n",
      "        [ 1.8473e-03, -1.4287e-02,  3.1416e-02, -8.0489e-03,  2.4862e-02],\n",
      "        [-1.8815e-03, -7.5478e-03, -3.0576e-03,  2.4347e-03,  1.6662e-04],\n",
      "        [ 2.4281e-02,  1.4474e-02, -3.0053e-02, -1.3162e-02,  1.2699e-04],\n",
      "        [-1.1405e-02,  2.1078e-02,  3.7572e-02, -3.4209e-02, -1.6096e-02],\n",
      "        [-3.4075e-02,  6.8668e-03,  2.2002e-02, -4.1679e-03, -5.4973e-04],\n",
      "        [-1.7055e-02, -2.9121e-02, -5.4711e-03, -4.4425e-04,  1.1625e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-1.9310e-02,  3.6308e-02,  8.6869e-03, -6.6748e-03, -1.3205e-02],\n",
      "        [-1.6075e-02, -4.4241e-03, -9.1549e-02,  3.4803e-02,  1.9989e-02],\n",
      "        [-4.0036e-02, -2.5368e-02, -3.3035e-02,  9.2777e-03, -9.8429e-03],\n",
      "        [ 1.2699e-02,  3.7554e-03, -3.1363e-02, -1.2533e-04,  2.2860e-02],\n",
      "        [-2.2193e-02, -1.0618e-02, -1.0122e-02, -3.0327e-02, -1.3634e-02],\n",
      "        [-1.8757e-02, -3.5230e-03, -1.3238e-02,  1.1938e-02,  8.8945e-03],\n",
      "        [-2.3879e-05,  3.8240e-02,  1.0266e-02, -2.7638e-02, -3.1956e-03],\n",
      "        [ 4.6330e-03, -1.3100e-02, -6.7834e-03,  8.8858e-03,  1.0227e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0123, grad_fn=<MinBackward1>), tensor(0.8650, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10744643956422806\n",
      "@sample 901: tensor([[-0.0282, -0.0098, -0.0137, -0.0150,  0.0013],\n",
      "        [-0.0208, -0.0026,  0.0118, -0.0207,  0.0094],\n",
      "        [-0.0170,  0.0083,  0.0043, -0.0019, -0.0043],\n",
      "        [ 0.0145, -0.0029,  0.0002,  0.0196,  0.0087],\n",
      "        [ 0.0133, -0.0128, -0.0225,  0.0345, -0.0068],\n",
      "        [ 0.0078,  0.0177,  0.0175,  0.0027, -0.0155],\n",
      "        [ 0.0123,  0.0032,  0.0009, -0.0333,  0.0091],\n",
      "        [ 0.0085,  0.0147,  0.0227, -0.0128,  0.0012]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0045, -0.0468,  0.0378, -0.0055, -0.0016],\n",
      "        [ 0.0229, -0.0223,  0.0274,  0.0082, -0.0040],\n",
      "        [ 0.0047, -0.0012,  0.0243,  0.0340,  0.0262],\n",
      "        [ 0.0344,  0.0274,  0.0245, -0.0211,  0.0159],\n",
      "        [ 0.0135,  0.0194,  0.0266, -0.0225,  0.0055],\n",
      "        [-0.0346, -0.0034,  0.0008,  0.0287, -0.0028],\n",
      "        [-0.0478,  0.0175, -0.0173, -0.0020, -0.0106],\n",
      "        [-0.0021, -0.0049,  0.0365, -0.0378, -0.0242]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0136, grad_fn=<MinBackward1>), tensor(0.8874, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09753978997468948\n",
      "@sample 902: tensor([[ 0.0125,  0.0201, -0.0054, -0.0060, -0.0168],\n",
      "        [ 0.0122, -0.0091,  0.0052,  0.0182, -0.0235],\n",
      "        [-0.0069, -0.0091,  0.0087,  0.0055,  0.0181],\n",
      "        [ 0.0063, -0.0215, -0.0030, -0.0056,  0.0151],\n",
      "        [-0.0095, -0.0023, -0.0024, -0.0164, -0.0062],\n",
      "        [ 0.0256, -0.0125, -0.0128,  0.0664, -0.0339],\n",
      "        [ 0.0317, -0.0279,  0.0238,  0.0126, -0.0086],\n",
      "        [-0.0058, -0.0169, -0.0268,  0.0104, -0.0084]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-2.5575e-02, -1.7544e-02, -2.0493e-02,  1.1679e-02,  7.6774e-04],\n",
      "        [-2.8602e-03,  1.6245e-02,  5.6574e-03, -6.1389e-03,  1.1194e-02],\n",
      "        [-1.4515e-02, -6.7735e-04,  4.5086e-04,  4.5292e-03, -1.6046e-02],\n",
      "        [-2.8954e-02, -2.4003e-02, -1.2403e-02,  2.3568e-02,  1.7911e-02],\n",
      "        [-2.4115e-02, -1.8900e-02,  1.4266e-04, -6.1954e-03,  1.7167e-02],\n",
      "        [-1.6660e-03,  4.6589e-02,  6.3984e-03, -2.5330e-02, -3.4894e-02],\n",
      "        [-1.2838e-02,  1.1556e-02,  1.3531e-02,  2.0932e-02,  2.1137e-02],\n",
      "        [ 2.1093e-05,  3.1860e-03,  3.0752e-02, -3.3398e-02, -4.1936e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0173, grad_fn=<MinBackward1>), tensor(0.8778, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09512900561094284\n",
      "@sample 903: tensor([[ 0.0300, -0.0056,  0.0023, -0.0255, -0.0157],\n",
      "        [ 0.0135, -0.0040, -0.0394,  0.0153,  0.0007],\n",
      "        [-0.0030,  0.0051, -0.0105, -0.0002,  0.0175],\n",
      "        [ 0.0110,  0.0508, -0.0056,  0.0040, -0.0091],\n",
      "        [-0.0042,  0.0044,  0.0136, -0.0105, -0.0054],\n",
      "        [ 0.0049,  0.0075,  0.0211, -0.0363,  0.0159],\n",
      "        [ 0.0026,  0.0166,  0.0134, -0.0115,  0.0344],\n",
      "        [-0.0102,  0.0174,  0.0034, -0.0029,  0.0116]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0124,  0.0058, -0.0189,  0.0403,  0.0002],\n",
      "        [-0.0048, -0.0193,  0.0031,  0.0028,  0.0096],\n",
      "        [-0.0137, -0.0067, -0.0447,  0.0083, -0.0172],\n",
      "        [ 0.0076, -0.0164, -0.0240,  0.0031,  0.0144],\n",
      "        [-0.0056,  0.0007, -0.0316, -0.0025, -0.0127],\n",
      "        [ 0.0093, -0.0117, -0.0791,  0.0435,  0.0510],\n",
      "        [ 0.0050, -0.0219,  0.0071,  0.0452,  0.0281],\n",
      "        [-0.0146,  0.0319, -0.0008,  0.0087,  0.0048]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0161, grad_fn=<MinBackward1>), tensor(0.8823, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09641232341527939\n",
      "@sample 904: tensor([[ 0.0211, -0.0122, -0.0223, -0.0113, -0.0101],\n",
      "        [ 0.0180,  0.0036,  0.0070, -0.0301,  0.0153],\n",
      "        [-0.0095, -0.0144, -0.0066,  0.0095,  0.0119],\n",
      "        [ 0.0112,  0.0384,  0.0316, -0.0511,  0.0070],\n",
      "        [-0.0041, -0.0159, -0.0020,  0.0063,  0.0195],\n",
      "        [ 0.0022, -0.0139, -0.0126,  0.0022,  0.0084],\n",
      "        [-0.0229,  0.0062, -0.0305, -0.0020, -0.0054],\n",
      "        [ 0.0054,  0.0078,  0.0034, -0.0038,  0.0067]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0165, -0.0039,  0.0439,  0.0570,  0.0260],\n",
      "        [-0.0100, -0.0105, -0.0194,  0.0052, -0.0056],\n",
      "        [-0.0027,  0.0164,  0.0449, -0.0043, -0.0022],\n",
      "        [-0.0268, -0.0311, -0.0708,  0.0536,  0.0266],\n",
      "        [ 0.0071, -0.0065,  0.0382, -0.0010, -0.0277],\n",
      "        [-0.0037,  0.0033, -0.0083, -0.0358, -0.0224],\n",
      "        [ 0.0029, -0.0316,  0.0574, -0.0406,  0.0100],\n",
      "        [ 0.0021,  0.0110,  0.0266, -0.0366,  0.0096]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0100, grad_fn=<MinBackward1>), tensor(0.8502, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10818628966808319\n",
      "@sample 905: tensor([[-5.5265e-03, -3.3305e-03,  3.1665e-03,  2.2786e-03, -1.4854e-03],\n",
      "        [-5.3399e-03, -3.8773e-05, -1.1715e-02,  6.4728e-03,  7.8009e-03],\n",
      "        [ 1.5700e-02,  3.1341e-02,  5.2194e-03,  3.6526e-03,  3.8282e-03],\n",
      "        [-2.9900e-02, -1.7002e-02, -4.4973e-02,  1.3401e-02, -3.4435e-02],\n",
      "        [-8.1009e-04, -1.1824e-02, -9.0386e-03,  2.7847e-03,  6.7049e-03],\n",
      "        [-2.7041e-03,  1.5358e-02, -2.7634e-02,  5.5344e-03,  1.1547e-02],\n",
      "        [ 3.1611e-02, -1.7813e-02, -7.5926e-02,  3.4148e-02, -2.8850e-02],\n",
      "        [-1.4909e-02,  6.7217e-03,  4.0372e-04,  1.2301e-02,  1.8078e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0098,  0.0177,  0.0077,  0.0531,  0.0264],\n",
      "        [-0.0025,  0.0030,  0.0174, -0.0215,  0.0136],\n",
      "        [ 0.0043, -0.0008, -0.0571,  0.0074, -0.0070],\n",
      "        [-0.0081, -0.0200,  0.0550, -0.0364, -0.0121],\n",
      "        [ 0.0080, -0.0170,  0.0115, -0.0144,  0.0027],\n",
      "        [ 0.0464, -0.0011,  0.0650, -0.0472, -0.0573],\n",
      "        [ 0.0525,  0.0021,  0.0126,  0.0171,  0.0128],\n",
      "        [-0.0104,  0.0208, -0.0041,  0.0108,  0.0157]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0136, grad_fn=<MinBackward1>), tensor(0.9001, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11041170358657837\n",
      "@sample 906: tensor([[ 0.0029,  0.0040, -0.0015,  0.0389,  0.0067],\n",
      "        [-0.0071, -0.0035, -0.0206, -0.0182, -0.0008],\n",
      "        [ 0.0185,  0.0068, -0.0212,  0.0046,  0.0094],\n",
      "        [-0.0214,  0.0066, -0.0352, -0.0136,  0.0179],\n",
      "        [-0.0122, -0.0097, -0.0004,  0.0015,  0.0193],\n",
      "        [ 0.0009,  0.0045, -0.0049, -0.0262,  0.0132],\n",
      "        [-0.0022, -0.0100, -0.0015, -0.0184,  0.0213],\n",
      "        [-0.0056,  0.0006, -0.0030,  0.0325, -0.0032]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0123,  0.0313, -0.0251,  0.0096,  0.0013],\n",
      "        [-0.0258, -0.0029,  0.0192, -0.0106, -0.0014],\n",
      "        [-0.0089, -0.0030, -0.0198,  0.0297,  0.0079],\n",
      "        [ 0.0228, -0.0282,  0.0637, -0.0093,  0.0157],\n",
      "        [ 0.0143,  0.0225,  0.0332, -0.0212, -0.0134],\n",
      "        [ 0.0029,  0.0017, -0.0331, -0.0149,  0.0073],\n",
      "        [-0.0061, -0.0140, -0.0022,  0.0081, -0.0181],\n",
      "        [ 0.0109,  0.0508, -0.0064,  0.0203,  0.0103]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0189, grad_fn=<MinBackward1>), tensor(0.8264, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10073887556791306\n",
      "@sample 907: tensor([[-0.0124,  0.0430, -0.0065, -0.0236,  0.0071],\n",
      "        [ 0.0027,  0.0490, -0.0345, -0.0250,  0.0314],\n",
      "        [-0.0135,  0.0161,  0.0051,  0.0179, -0.0185],\n",
      "        [-0.0120,  0.0136, -0.0004,  0.0057, -0.0099],\n",
      "        [-0.0186,  0.0124, -0.0152, -0.0006,  0.0234],\n",
      "        [-0.0034,  0.0090, -0.0131,  0.0169, -0.0196],\n",
      "        [-0.0006, -0.0058,  0.0050, -0.0039, -0.0249],\n",
      "        [-0.0234,  0.0243,  0.0157,  0.0172,  0.0095]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0002, -0.0053, -0.0015,  0.0155, -0.0151],\n",
      "        [-0.0396, -0.0280, -0.0324,  0.0695, -0.0565],\n",
      "        [ 0.0408,  0.0451,  0.0173, -0.0290, -0.0085],\n",
      "        [ 0.0280,  0.0212,  0.0172, -0.0349,  0.0049],\n",
      "        [ 0.0026,  0.0182, -0.0051,  0.0122,  0.0018],\n",
      "        [-0.0091,  0.0185, -0.0140, -0.0220, -0.0107],\n",
      "        [-0.0175,  0.0250,  0.0003,  0.0020,  0.0260],\n",
      "        [ 0.0213,  0.0184, -0.0040, -0.0169, -0.0287]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0192, grad_fn=<MinBackward1>), tensor(0.9132, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10420111566781998\n",
      "@sample 908: tensor([[ 0.0009, -0.0458,  0.0180, -0.0034, -0.0098],\n",
      "        [ 0.0104,  0.0496,  0.0240, -0.0287,  0.0234],\n",
      "        [ 0.0142,  0.0039, -0.0215,  0.0399,  0.0030],\n",
      "        [-0.0011,  0.0010,  0.0040,  0.0303, -0.0098],\n",
      "        [-0.0043, -0.0133, -0.0398,  0.0032, -0.0107],\n",
      "        [ 0.0212, -0.0216, -0.0171,  0.0015,  0.0129],\n",
      "        [-0.0040,  0.0093, -0.0421,  0.0100, -0.0074],\n",
      "        [ 0.0103,  0.0276, -0.0319,  0.0020,  0.0113]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0043,  0.0312,  0.0299,  0.0033,  0.0338],\n",
      "        [-0.0417, -0.0072, -0.0844,  0.0276, -0.0086],\n",
      "        [ 0.0185,  0.0552,  0.0139, -0.0140, -0.0012],\n",
      "        [ 0.0084,  0.0330, -0.0195, -0.0140, -0.0059],\n",
      "        [-0.0120, -0.0062,  0.0404, -0.0326, -0.0258],\n",
      "        [ 0.0011,  0.0021,  0.0414, -0.0050,  0.0087],\n",
      "        [ 0.0170,  0.0239, -0.0022, -0.0183,  0.0149],\n",
      "        [ 0.0125, -0.0106,  0.0073,  0.0010,  0.0018]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0149, grad_fn=<MinBackward1>), tensor(0.9018, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1085069477558136\n",
      "@sample 909: tensor([[-0.0231, -0.0142, -0.0120,  0.0303, -0.0178],\n",
      "        [-0.0075,  0.0105, -0.0041, -0.0263,  0.0005],\n",
      "        [ 0.0158, -0.0249, -0.0106,  0.0135, -0.0154],\n",
      "        [-0.0031, -0.0117, -0.0013, -0.0015,  0.0288],\n",
      "        [-0.0412,  0.0008, -0.0061, -0.0035,  0.0126],\n",
      "        [-0.0004,  0.0213,  0.0010,  0.0021,  0.0002],\n",
      "        [-0.0246, -0.0214,  0.0122, -0.0177, -0.0060],\n",
      "        [ 0.0035,  0.0146, -0.0150,  0.0198,  0.0004]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0079, -0.0094,  0.0119, -0.0217, -0.0299],\n",
      "        [-0.0086, -0.0234, -0.0281, -0.0082, -0.0247],\n",
      "        [ 0.0142, -0.0017, -0.0111, -0.0088, -0.0123],\n",
      "        [-0.0167, -0.0202,  0.0440, -0.0288, -0.0161],\n",
      "        [ 0.0238, -0.0168,  0.0533, -0.0237,  0.0016],\n",
      "        [ 0.0013,  0.0108,  0.0021,  0.0226,  0.0370],\n",
      "        [-0.0040, -0.0215,  0.0088, -0.0171, -0.0182],\n",
      "        [-0.0048,  0.0035, -0.0196, -0.0078, -0.0195]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0086, grad_fn=<MinBackward1>), tensor(0.8943, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09533122181892395\n",
      "@sample 910: tensor([[-0.0141,  0.0150,  0.0103,  0.0162, -0.0123],\n",
      "        [ 0.0217,  0.0087, -0.0251,  0.0014,  0.0007],\n",
      "        [ 0.0188, -0.0145,  0.0149, -0.0227, -0.0298],\n",
      "        [ 0.0264,  0.0242, -0.0011, -0.0263,  0.0414],\n",
      "        [-0.0192,  0.0049, -0.0130,  0.0220, -0.0018],\n",
      "        [-0.0078, -0.0209, -0.0113,  0.0146,  0.0012],\n",
      "        [-0.0016, -0.0253,  0.0099,  0.0105,  0.0077],\n",
      "        [ 0.0203, -0.0138, -0.0170,  0.0214,  0.0184]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 1.5242e-02,  3.3340e-02, -4.3961e-03, -8.3556e-03, -3.4387e-03],\n",
      "        [ 2.0372e-02,  2.0490e-02, -1.2113e-02,  9.3519e-03, -1.1441e-02],\n",
      "        [-3.8094e-02, -2.5419e-03,  9.1223e-04, -2.7745e-02, -8.7818e-03],\n",
      "        [ 3.8417e-03, -4.8245e-03, -5.7693e-02,  3.1992e-02, -1.6028e-02],\n",
      "        [ 2.6620e-02,  1.9237e-02,  3.9413e-02, -2.3358e-03,  1.2831e-02],\n",
      "        [ 2.0522e-03,  1.8288e-02,  4.2361e-02, -3.2190e-02, -8.9800e-04],\n",
      "        [-2.5788e-03,  3.0434e-02,  5.0813e-05, -3.7612e-02, -1.4159e-02],\n",
      "        [-6.4361e-03, -7.4078e-03,  1.5878e-02,  1.1910e-03,  5.2746e-04]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0143, grad_fn=<MinBackward1>), tensor(0.8619, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09479271620512009\n",
      "@sample 911: tensor([[-0.0101, -0.0138,  0.0010, -0.0075,  0.0193],\n",
      "        [ 0.0148,  0.0196,  0.0200, -0.0035, -0.0185],\n",
      "        [ 0.0181, -0.0065,  0.0216, -0.0293,  0.0218],\n",
      "        [ 0.0002, -0.0028, -0.0072,  0.0002,  0.0056],\n",
      "        [ 0.0216, -0.0415,  0.0002,  0.0124, -0.0059],\n",
      "        [ 0.0002,  0.0392, -0.0066, -0.0492,  0.0217],\n",
      "        [ 0.0094, -0.0111,  0.0052,  0.0078, -0.0069],\n",
      "        [ 0.0146,  0.0033,  0.0291,  0.0114, -0.0117]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-2.6154e-02, -1.8056e-02, -1.1106e-02, -2.0717e-02,  6.5024e-03],\n",
      "        [ 8.7552e-05, -2.2783e-02, -4.8752e-02,  5.7312e-02, -5.9832e-03],\n",
      "        [-1.3068e-03, -2.2572e-02,  1.8664e-02,  1.7692e-02, -7.7073e-05],\n",
      "        [ 3.2862e-03, -5.6983e-03, -1.9061e-02,  2.8033e-03,  7.9423e-03],\n",
      "        [-7.0841e-03, -4.6935e-03,  5.5365e-02, -2.3835e-02,  3.4551e-03],\n",
      "        [-3.3153e-02, -5.0672e-02, -6.5680e-02,  4.8840e-02, -1.1218e-02],\n",
      "        [ 3.2169e-02,  3.5056e-03,  4.4254e-03,  1.0477e-02,  2.3489e-02],\n",
      "        [-2.5875e-03,  2.2876e-02,  1.9829e-02,  4.3256e-02,  9.5707e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0156, grad_fn=<MinBackward1>), tensor(0.8585, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09280826151371002\n",
      "@sample 912: tensor([[ 0.0102,  0.0363,  0.0045, -0.0453,  0.0176],\n",
      "        [ 0.0121, -0.0109,  0.0274,  0.0036,  0.0040],\n",
      "        [-0.0244, -0.0094,  0.0229, -0.0088,  0.0147],\n",
      "        [-0.0125, -0.0152,  0.0209,  0.0012, -0.0078],\n",
      "        [-0.0026, -0.0062,  0.0020, -0.0019, -0.0041],\n",
      "        [ 0.0072,  0.0008,  0.0092,  0.0032,  0.0023],\n",
      "        [ 0.0022, -0.0141, -0.0015, -0.0195,  0.0179],\n",
      "        [ 0.0195, -0.0345,  0.0238,  0.0205,  0.0153]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0041, -0.0393, -0.0697,  0.0217,  0.0189],\n",
      "        [-0.0243,  0.0140, -0.0151,  0.0012, -0.0147],\n",
      "        [ 0.0032, -0.0057,  0.0054,  0.0023, -0.0191],\n",
      "        [ 0.0203,  0.0351,  0.0314, -0.0213,  0.0137],\n",
      "        [-0.0110, -0.0053, -0.0378,  0.0048, -0.0263],\n",
      "        [-0.0005, -0.0253, -0.0445,  0.0106,  0.0282],\n",
      "        [-0.0172,  0.0183, -0.0562,  0.0029,  0.0266],\n",
      "        [ 0.0015,  0.0124, -0.0197, -0.0060,  0.0116]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0178, grad_fn=<MinBackward1>), tensor(0.8656, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10570928454399109\n",
      "@sample 913: tensor([[-0.0070,  0.0314, -0.0175, -0.0017,  0.0283],\n",
      "        [-0.0299,  0.0089,  0.0290, -0.0201,  0.0027],\n",
      "        [-0.0147, -0.0180,  0.0156,  0.0124, -0.0288],\n",
      "        [-0.0280,  0.0227,  0.0430,  0.0200, -0.0081],\n",
      "        [-0.0097, -0.0120, -0.0069,  0.0158, -0.0057],\n",
      "        [-0.0004, -0.0032,  0.0103,  0.0072, -0.0031],\n",
      "        [ 0.0253, -0.0033,  0.0104, -0.0014, -0.0101],\n",
      "        [-0.0036,  0.0280,  0.0205, -0.0318,  0.0308]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0174,  0.0271, -0.0148, -0.0114, -0.0002],\n",
      "        [-0.0350, -0.0210,  0.0012, -0.0068, -0.0242],\n",
      "        [-0.0002,  0.0389, -0.0027, -0.0297,  0.0169],\n",
      "        [ 0.0030,  0.0231, -0.0253, -0.0224, -0.0057],\n",
      "        [-0.0017, -0.0186, -0.0031,  0.0254, -0.0134],\n",
      "        [ 0.0076, -0.0060,  0.0186, -0.0175, -0.0289],\n",
      "        [-0.0005,  0.0096, -0.0131,  0.0263,  0.0243],\n",
      "        [-0.0062, -0.0120, -0.0384,  0.0366,  0.0027]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0156, grad_fn=<MinBackward1>), tensor(0.8858, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09182633459568024\n",
      "@sample 914: tensor([[-0.0028, -0.0004,  0.0341, -0.0202,  0.0040],\n",
      "        [ 0.0290, -0.0279,  0.0017, -0.0038, -0.0043],\n",
      "        [-0.0056,  0.0095,  0.0113, -0.0112,  0.0110],\n",
      "        [-0.0130, -0.0220,  0.0109, -0.0124,  0.0028],\n",
      "        [ 0.0171, -0.0233,  0.0088, -0.0030,  0.0051],\n",
      "        [ 0.0042,  0.0001,  0.0115,  0.0020, -0.0240],\n",
      "        [ 0.0290, -0.0270,  0.0313,  0.0071, -0.0081],\n",
      "        [ 0.0293,  0.0127,  0.0124, -0.0166,  0.0221]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0305,  0.0106, -0.0453,  0.0047, -0.0290],\n",
      "        [-0.0115, -0.0017, -0.0025,  0.0130, -0.0147],\n",
      "        [-0.0083, -0.0167,  0.0024,  0.0067, -0.0157],\n",
      "        [ 0.0069,  0.0080, -0.0413,  0.0018,  0.0007],\n",
      "        [ 0.0131,  0.0144, -0.0262, -0.0031, -0.0151],\n",
      "        [-0.0451,  0.0128,  0.0147, -0.0034, -0.0057],\n",
      "        [ 0.0088,  0.0104,  0.0258, -0.0123, -0.0110],\n",
      "        [-0.0282, -0.0454, -0.0124,  0.0315,  0.0094]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0165, grad_fn=<MinBackward1>), tensor(0.8902, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1041489839553833\n",
      "@sample 915: tensor([[-1.8127e-02, -2.3439e-02,  6.2905e-03, -2.6148e-02, -7.9543e-03],\n",
      "        [ 1.4079e-02,  9.5573e-04, -2.0343e-02,  4.5424e-03, -2.0379e-02],\n",
      "        [ 1.2913e-03, -2.3972e-02, -2.5684e-03, -2.8677e-02, -1.3822e-03],\n",
      "        [ 1.1347e-02, -2.5018e-03,  7.4257e-03, -5.9941e-03,  1.0686e-02],\n",
      "        [-2.1816e-05, -4.1220e-02,  1.7379e-02, -1.5295e-03, -1.1624e-02],\n",
      "        [ 3.7258e-03, -7.5961e-03,  5.7933e-03, -3.9167e-03, -1.3730e-02],\n",
      "        [-1.1279e-03, -2.2962e-02, -1.6652e-02,  7.6596e-03,  2.9111e-03],\n",
      "        [-1.0722e-02,  1.0199e-03,  1.2577e-02, -1.8319e-02, -1.2816e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0027, -0.0083, -0.0026,  0.0242,  0.0474],\n",
      "        [-0.0108,  0.0002,  0.0127,  0.0016,  0.0308],\n",
      "        [-0.0346, -0.0368,  0.0257, -0.0252, -0.0249],\n",
      "        [-0.0092,  0.0190,  0.0203, -0.0016,  0.0057],\n",
      "        [ 0.0039, -0.0285,  0.0343, -0.0146,  0.0133],\n",
      "        [-0.0119, -0.0050, -0.0144,  0.0034, -0.0179],\n",
      "        [-0.0072, -0.0271,  0.0186, -0.0061, -0.0164],\n",
      "        [-0.0362, -0.0156, -0.0028,  0.0096,  0.0156]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0100, grad_fn=<MinBackward1>), tensor(0.8800, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10411176085472107\n",
      "@sample 916: tensor([[-2.5013e-02,  4.6199e-03, -5.8402e-03, -2.3328e-02,  1.3164e-02],\n",
      "        [-2.8130e-03, -1.6624e-02, -9.9392e-03,  1.3207e-02, -1.1868e-02],\n",
      "        [-2.5792e-02, -5.8430e-03, -4.0668e-03,  2.0233e-03,  1.9126e-02],\n",
      "        [ 2.8480e-04, -8.1609e-03,  5.2556e-02, -4.9864e-02,  3.1528e-02],\n",
      "        [-1.2413e-02,  7.1589e-03, -1.0376e-02, -1.0999e-03, -1.0302e-02],\n",
      "        [-6.6592e-03, -1.1746e-03, -3.0451e-02, -1.2389e-02, -9.5979e-03],\n",
      "        [-1.6782e-03,  1.8103e-02,  1.8439e-02, -4.0690e-02, -1.0003e-03],\n",
      "        [-1.5733e-02, -9.6594e-03,  1.7881e-05,  1.1568e-02,  1.9362e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0054, -0.0243,  0.0444, -0.0392, -0.0280],\n",
      "        [ 0.0262, -0.0210,  0.0263, -0.0006,  0.0071],\n",
      "        [ 0.0250, -0.0219, -0.0374,  0.0310,  0.0077],\n",
      "        [ 0.0088, -0.0367, -0.0501,  0.0415, -0.0223],\n",
      "        [-0.0011,  0.0179,  0.0015, -0.0370,  0.0106],\n",
      "        [-0.0106, -0.0330,  0.0140, -0.0084,  0.0013],\n",
      "        [-0.0049, -0.0112, -0.0242,  0.0269, -0.0204],\n",
      "        [ 0.0181,  0.0064, -0.0247,  0.0211, -0.0121]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0201, grad_fn=<MinBackward1>), tensor(0.8618, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.0974518358707428\n",
      "@sample 917: tensor([[-0.0066, -0.0246,  0.0149, -0.0078,  0.0053],\n",
      "        [-0.0383, -0.0168, -0.0123, -0.0135,  0.0192],\n",
      "        [ 0.0155, -0.0041,  0.0261,  0.0089, -0.0269],\n",
      "        [ 0.0133,  0.0379,  0.0251, -0.0018,  0.0019],\n",
      "        [ 0.0204,  0.0261, -0.0191, -0.0261,  0.0075],\n",
      "        [ 0.0028, -0.0274,  0.0087,  0.0222, -0.0336],\n",
      "        [-0.0237, -0.0250, -0.0063, -0.0063, -0.0039],\n",
      "        [ 0.0234, -0.0021, -0.0206, -0.0082, -0.0176]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-2.5578e-03, -4.9591e-02,  4.1838e-02, -1.9511e-02, -9.8953e-03],\n",
      "        [-3.4442e-03,  1.0845e-02,  1.2698e-02, -4.0965e-02, -2.2912e-02],\n",
      "        [-1.7943e-02, -1.8106e-02,  3.1256e-02, -1.1249e-02, -1.4248e-02],\n",
      "        [-2.1072e-02,  9.3367e-03, -6.2280e-02, -5.9115e-03, -3.4100e-02],\n",
      "        [-2.1813e-02, -2.6368e-02, -6.2157e-02,  5.7397e-02,  2.6163e-02],\n",
      "        [ 1.1426e-02, -1.7616e-02,  2.4828e-02,  2.8555e-02,  2.2399e-02],\n",
      "        [-9.8892e-05,  1.4795e-03,  1.7631e-02, -1.5634e-02, -6.9942e-04],\n",
      "        [-7.7113e-03, -9.5880e-03, -1.7456e-02, -1.2060e-02, -2.1269e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0164, grad_fn=<MinBackward1>), tensor(0.9101, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10588425397872925\n",
      "@sample 918: tensor([[-0.0168, -0.0081,  0.0029, -0.0192,  0.0141],\n",
      "        [ 0.0035,  0.0249,  0.0444, -0.0017, -0.0158],\n",
      "        [-0.0235, -0.0063, -0.0177,  0.0264, -0.0090],\n",
      "        [ 0.0196,  0.0199, -0.0095, -0.0179, -0.0479],\n",
      "        [-0.0052,  0.0327,  0.0147,  0.0144, -0.0215],\n",
      "        [ 0.0156,  0.0106, -0.0154, -0.0066, -0.0024],\n",
      "        [-0.0194,  0.0028, -0.0012, -0.0240, -0.0052],\n",
      "        [-0.0047,  0.0189,  0.0079, -0.0434,  0.0354]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0045, -0.0061,  0.0089, -0.0180,  0.0126],\n",
      "        [-0.0079,  0.0159, -0.0146,  0.0532,  0.0335],\n",
      "        [ 0.0145,  0.0103,  0.0168, -0.0213,  0.0022],\n",
      "        [-0.0293, -0.0215, -0.0046,  0.0025, -0.0020],\n",
      "        [ 0.0174,  0.0365, -0.0230, -0.0046, -0.0042],\n",
      "        [-0.0192, -0.0279, -0.0100, -0.0049,  0.0279],\n",
      "        [-0.0134, -0.0194,  0.0108, -0.0257,  0.0168],\n",
      "        [-0.0270, -0.0415, -0.1012,  0.0479,  0.0301]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0167, grad_fn=<MinBackward1>), tensor(0.8944, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11375344544649124\n",
      "@sample 919: tensor([[-9.3689e-03,  4.1513e-03,  8.7364e-03, -6.1644e-02,  1.8188e-02],\n",
      "        [-3.9684e-03,  4.7987e-02,  1.8335e-02, -3.9811e-02,  2.5562e-02],\n",
      "        [-5.8919e-04, -3.0244e-02, -6.9606e-04,  1.6364e-02, -9.1653e-04],\n",
      "        [ 5.6963e-03, -1.0170e-02, -3.9093e-02,  2.2857e-02, -1.1421e-02],\n",
      "        [-1.2798e-02,  4.2945e-02,  2.7814e-02, -2.5280e-02,  9.3010e-03],\n",
      "        [ 1.9222e-02, -5.8137e-03, -1.3345e-02, -4.3928e-03,  2.2255e-02],\n",
      "        [ 2.0265e-02, -4.0829e-03, -8.7787e-03, -9.2477e-05,  3.3270e-03],\n",
      "        [-7.3093e-03,  6.0919e-04, -1.9686e-02,  2.8362e-02, -5.3657e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0166, -0.0194, -0.0892,  0.0368, -0.0261],\n",
      "        [-0.0091, -0.0421, -0.0139,  0.0420, -0.0546],\n",
      "        [-0.0016, -0.0200,  0.0212, -0.0114,  0.0069],\n",
      "        [ 0.0093, -0.0165,  0.0126, -0.0266, -0.0229],\n",
      "        [ 0.0094, -0.0030, -0.0494,  0.0449,  0.0436],\n",
      "        [-0.0206, -0.0084, -0.0615,  0.0426,  0.0215],\n",
      "        [ 0.0209,  0.0018,  0.0201,  0.0073,  0.0190],\n",
      "        [ 0.0150,  0.0068,  0.0188, -0.0277, -0.0134]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0138, grad_fn=<MinBackward1>), tensor(0.8761, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10079441219568253\n",
      "@sample 920: tensor([[-0.0083,  0.0157,  0.0050, -0.0124,  0.0014],\n",
      "        [ 0.0125,  0.0158,  0.0249, -0.0156,  0.0272],\n",
      "        [ 0.0182,  0.0172, -0.0060, -0.0035, -0.0116],\n",
      "        [ 0.0129, -0.0205, -0.0178,  0.0342, -0.0113],\n",
      "        [-0.0226,  0.0062, -0.0073, -0.0014, -0.0141],\n",
      "        [-0.0130,  0.0143, -0.0169,  0.0247, -0.0314],\n",
      "        [ 0.0121, -0.0100,  0.0027, -0.0057, -0.0103],\n",
      "        [-0.0230,  0.0011, -0.0117,  0.0196,  0.0119]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0170,  0.0348,  0.0135, -0.0179, -0.0130],\n",
      "        [-0.0091,  0.0053, -0.0189,  0.0133, -0.0025],\n",
      "        [ 0.0012,  0.0123, -0.0291, -0.0036,  0.0058],\n",
      "        [ 0.0178,  0.0083, -0.0302, -0.0028,  0.0190],\n",
      "        [ 0.0160, -0.0204,  0.0128, -0.0053,  0.0079],\n",
      "        [ 0.0401, -0.0003, -0.0015, -0.0198,  0.0177],\n",
      "        [-0.0218, -0.0114, -0.0075,  0.0083,  0.0073],\n",
      "        [ 0.0427,  0.0579,  0.0444, -0.0015, -0.0159]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0171, grad_fn=<MinBackward1>), tensor(0.8548, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09655182808637619\n",
      "@sample 921: tensor([[-0.0019, -0.0172, -0.0001,  0.0244, -0.0313],\n",
      "        [ 0.0145, -0.0131, -0.0030,  0.0280,  0.0026],\n",
      "        [-0.0146,  0.0086, -0.0053, -0.0047,  0.0036],\n",
      "        [ 0.0127,  0.0508,  0.0340, -0.0292,  0.0257],\n",
      "        [ 0.0078,  0.0075, -0.0060,  0.0129, -0.0310],\n",
      "        [-0.0241, -0.0204, -0.0101,  0.0158, -0.0101],\n",
      "        [-0.0158,  0.0351,  0.0112, -0.0266,  0.0115],\n",
      "        [-0.0146, -0.0124, -0.0097,  0.0264, -0.0259]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-1.0482e-02,  1.0730e-04,  9.5968e-03, -2.0829e-03, -1.1766e-02],\n",
      "        [ 2.2167e-02,  2.8217e-03, -1.7842e-02, -1.8787e-02, -1.0681e-02],\n",
      "        [ 9.5153e-04,  2.2779e-02, -3.5137e-03, -1.8957e-02,  1.5544e-04],\n",
      "        [ 2.0493e-02, -9.7622e-03, -7.4019e-02,  4.0327e-03, -1.9716e-02],\n",
      "        [ 1.5636e-02, -4.1771e-02, -2.8464e-02,  1.5380e-02, -2.8607e-03],\n",
      "        [ 1.8067e-02, -6.6410e-03, -1.7468e-02,  3.4590e-03, -2.0542e-02],\n",
      "        [-4.3292e-02, -4.6840e-02, -5.7281e-02,  4.7520e-02,  6.9182e-05],\n",
      "        [ 9.6678e-03,  9.6870e-03,  2.6302e-02,  8.6017e-03, -1.3809e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0137, grad_fn=<MinBackward1>), tensor(0.8606, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1006401851773262\n",
      "@sample 922: tensor([[-0.0161,  0.0192, -0.0276, -0.0085,  0.0078],\n",
      "        [ 0.0054, -0.0166, -0.0022,  0.0082,  0.0010],\n",
      "        [-0.0139,  0.0323,  0.0234, -0.0278, -0.0065],\n",
      "        [ 0.0088,  0.0142, -0.0071,  0.0076, -0.0237],\n",
      "        [ 0.0106, -0.0148,  0.0226,  0.0031,  0.0006],\n",
      "        [ 0.0181,  0.0085, -0.0236,  0.0216, -0.0006],\n",
      "        [ 0.0135, -0.0034, -0.0134, -0.0027,  0.0170],\n",
      "        [ 0.0161,  0.0175,  0.0074, -0.0255,  0.0393]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0011, -0.0056, -0.0806,  0.0153, -0.0008],\n",
      "        [-0.0088,  0.0144, -0.0030, -0.0029, -0.0079],\n",
      "        [-0.0286,  0.0080, -0.0307,  0.0569,  0.0084],\n",
      "        [ 0.0094, -0.0042, -0.0386,  0.0371, -0.0076],\n",
      "        [-0.0199, -0.0189,  0.0024, -0.0103, -0.0141],\n",
      "        [ 0.0179,  0.0310, -0.0276,  0.0009,  0.0084],\n",
      "        [ 0.0123, -0.0059,  0.0297, -0.0260, -0.0134],\n",
      "        [-0.0098, -0.0085, -0.0324, -0.0018, -0.0289]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0119, grad_fn=<MinBackward1>), tensor(0.8651, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09550371766090393\n",
      "@sample 923: tensor([[ 0.0084, -0.0013,  0.0002,  0.0044,  0.0050],\n",
      "        [ 0.0033,  0.0071, -0.0058,  0.0019, -0.0078],\n",
      "        [ 0.0026,  0.0255,  0.0323, -0.0134,  0.0104],\n",
      "        [ 0.0212, -0.0025, -0.0013,  0.0173, -0.0058],\n",
      "        [-0.0101,  0.0074,  0.0020,  0.0145, -0.0159],\n",
      "        [ 0.0405, -0.0116,  0.0046,  0.0213, -0.0108],\n",
      "        [ 0.0078, -0.0184,  0.0115, -0.0053, -0.0177],\n",
      "        [ 0.0218,  0.0260, -0.0097, -0.0034, -0.0145]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0104,  0.0072, -0.0064, -0.0153, -0.0188],\n",
      "        [-0.0174, -0.0016, -0.0529,  0.0109, -0.0268],\n",
      "        [-0.0163, -0.0001, -0.0485,  0.0240, -0.0032],\n",
      "        [ 0.0008,  0.0105, -0.0051,  0.0036,  0.0061],\n",
      "        [ 0.0007,  0.0079,  0.0216, -0.0328,  0.0033],\n",
      "        [-0.0072,  0.0001, -0.0388,  0.0163, -0.0291],\n",
      "        [-0.0266, -0.0194, -0.0198,  0.0242, -0.0127],\n",
      "        [-0.0044, -0.0017, -0.0242,  0.0195,  0.0042]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0168, grad_fn=<MinBackward1>), tensor(0.8678, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10884689539670944\n",
      "@sample 924: tensor([[ 0.0229,  0.0186, -0.0186,  0.0300, -0.0157],\n",
      "        [-0.0008,  0.0094, -0.0017, -0.0056,  0.0123],\n",
      "        [-0.0303,  0.0221,  0.0165, -0.0162,  0.0091],\n",
      "        [ 0.0056, -0.0115, -0.0054,  0.0127, -0.0032],\n",
      "        [-0.0326,  0.0088,  0.0131,  0.0180,  0.0090],\n",
      "        [-0.0149, -0.0178, -0.0139, -0.0276,  0.0137],\n",
      "        [-0.0067,  0.0041,  0.0011,  0.0185, -0.0073],\n",
      "        [-0.0054, -0.0161, -0.0260,  0.0200, -0.0216]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0192,  0.0375,  0.0005, -0.0099,  0.0250],\n",
      "        [ 0.0168,  0.0013, -0.0131,  0.0150,  0.0097],\n",
      "        [-0.0160,  0.0146, -0.0167,  0.0007, -0.0065],\n",
      "        [ 0.0074,  0.0222, -0.0575,  0.0216, -0.0037],\n",
      "        [ 0.0357,  0.0451, -0.0404,  0.0464, -0.0019],\n",
      "        [ 0.0004, -0.0167, -0.0180, -0.0323,  0.0158],\n",
      "        [-0.0009,  0.0219,  0.0146, -0.0102,  0.0090],\n",
      "        [ 0.0136, -0.0139,  0.0422,  0.0007,  0.0241]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0147, grad_fn=<MinBackward1>), tensor(0.9011, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1048734113574028\n",
      "@sample 925: tensor([[-0.0191,  0.0114, -0.0100,  0.0289, -0.0100],\n",
      "        [-0.0293, -0.0183, -0.0337, -0.0056, -0.0237],\n",
      "        [-0.0176,  0.0455,  0.0103,  0.0018, -0.0025],\n",
      "        [-0.0154,  0.0089, -0.0274,  0.0249, -0.0113],\n",
      "        [-0.0338,  0.0224, -0.0131,  0.0144,  0.0163],\n",
      "        [-0.0285,  0.0183, -0.0120, -0.0010, -0.0019],\n",
      "        [ 0.0164,  0.0091, -0.0026, -0.0050, -0.0036],\n",
      "        [ 0.0068, -0.0061, -0.0046, -0.0138,  0.0083]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0078, -0.0016, -0.0480, -0.0068, -0.0009],\n",
      "        [ 0.0118, -0.0338,  0.0292, -0.0060,  0.0266],\n",
      "        [ 0.0141,  0.0103, -0.0227,  0.0173,  0.0016],\n",
      "        [ 0.0531,  0.0014,  0.0303, -0.0275,  0.0176],\n",
      "        [-0.0123,  0.0310, -0.0085, -0.0238,  0.0108],\n",
      "        [ 0.0169, -0.0061,  0.0083, -0.0285,  0.0021],\n",
      "        [-0.0137,  0.0063,  0.0050,  0.0274,  0.0214],\n",
      "        [-0.0002, -0.0139, -0.0194,  0.0191,  0.0011]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0198, grad_fn=<MinBackward1>), tensor(0.8672, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1076580062508583\n",
      "@sample 926: tensor([[ 4.4244e-03, -2.8255e-02,  3.2776e-02, -2.6985e-02,  6.8585e-03],\n",
      "        [-6.6774e-03,  2.6911e-05, -5.8071e-03, -3.1809e-03,  1.2428e-04],\n",
      "        [-5.4526e-03,  2.2933e-02, -1.4736e-03, -1.6927e-02, -1.7454e-02],\n",
      "        [-7.0971e-03, -7.0709e-03,  4.0310e-03,  2.2116e-02,  5.7867e-03],\n",
      "        [-2.3946e-03,  2.3255e-03, -2.3501e-02,  2.1157e-02, -1.2572e-02],\n",
      "        [ 4.9064e-03, -1.1871e-02,  1.1541e-03,  7.1307e-03, -3.0529e-03],\n",
      "        [-1.0660e-02, -1.1554e-02, -2.0478e-02,  2.5605e-02, -1.6622e-05],\n",
      "        [ 1.2788e-02,  1.4857e-02,  2.2316e-02, -6.2712e-03,  1.6533e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0137, -0.0317,  0.0317,  0.0067, -0.0059],\n",
      "        [ 0.0184, -0.0022, -0.0176, -0.0031,  0.0122],\n",
      "        [-0.0321, -0.0223,  0.0258, -0.0346, -0.0606],\n",
      "        [ 0.0002,  0.0254,  0.0263, -0.0142,  0.0052],\n",
      "        [ 0.0110, -0.0002, -0.0130, -0.0492, -0.0237],\n",
      "        [-0.0091,  0.0338,  0.0007,  0.0070, -0.0090],\n",
      "        [ 0.0216,  0.0188,  0.0214,  0.0021,  0.0176],\n",
      "        [-0.0046, -0.0029, -0.0124,  0.0081, -0.0043]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0193, grad_fn=<MinBackward1>), tensor(0.8770, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09029781073331833\n",
      "@sample 927: tensor([[ 1.0918e-02,  3.3155e-02,  1.1671e-02, -2.5690e-03,  4.0054e-03],\n",
      "        [ 3.8779e-04, -9.4754e-03, -4.9133e-03,  7.9451e-03, -1.1432e-02],\n",
      "        [-4.5052e-03,  1.4428e-02,  1.1303e-02, -1.0456e-02, -3.8824e-03],\n",
      "        [ 1.2435e-02,  9.8476e-03, -8.8734e-03, -1.2702e-02,  5.6862e-03],\n",
      "        [ 6.1898e-04, -3.5157e-02, -2.3975e-03, -6.3019e-03,  6.9752e-03],\n",
      "        [ 9.3727e-03,  6.8518e-03,  3.1390e-04, -8.2852e-03, -6.4638e-03],\n",
      "        [-2.6654e-05,  2.7600e-02,  1.1132e-03,  6.3456e-03, -8.7351e-03],\n",
      "        [-5.1190e-03,  2.8458e-03, -1.5605e-02, -1.1854e-02,  1.0703e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0190, -0.0008, -0.0096,  0.0059,  0.0051],\n",
      "        [ 0.0277, -0.0222,  0.0513, -0.0191, -0.0402],\n",
      "        [-0.0028, -0.0229,  0.0028,  0.0104,  0.0148],\n",
      "        [-0.0171, -0.0409, -0.0140, -0.0046, -0.0133],\n",
      "        [-0.0421, -0.0399,  0.0481, -0.0036, -0.0276],\n",
      "        [ 0.0073,  0.0180,  0.0078,  0.0067,  0.0120],\n",
      "        [-0.0108,  0.0147,  0.0110, -0.0569,  0.0054],\n",
      "        [ 0.0183, -0.0088,  0.0087, -0.0072,  0.0092]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0162, grad_fn=<MinBackward1>), tensor(0.8158, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10675059258937836\n",
      "@sample 928: tensor([[ 1.1155e-02, -5.2243e-05, -9.9773e-03,  5.4506e-03, -2.5140e-02],\n",
      "        [ 2.0386e-02, -1.5169e-02, -2.7858e-02,  4.1325e-03,  3.9949e-03],\n",
      "        [ 1.1411e-02, -1.6195e-02,  2.8635e-02, -5.1933e-03, -2.6506e-02],\n",
      "        [ 3.9200e-02,  1.3676e-03,  1.0755e-02,  1.2855e-02, -2.6324e-02],\n",
      "        [-3.4628e-02,  2.9968e-02,  1.0714e-02, -1.2786e-02,  9.7397e-03],\n",
      "        [-3.5968e-02,  1.1743e-02, -2.3016e-02,  5.3837e-03,  1.8967e-02],\n",
      "        [ 9.0742e-04,  1.1696e-02,  1.3185e-02, -3.3978e-02,  2.3765e-02],\n",
      "        [ 2.6400e-03, -2.4525e-02,  1.4345e-02,  1.8538e-03,  8.2421e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0031,  0.0426,  0.0170,  0.0002, -0.0130],\n",
      "        [-0.0050,  0.0241,  0.0290,  0.0057,  0.0344],\n",
      "        [ 0.0095,  0.0033,  0.0645, -0.0116, -0.0051],\n",
      "        [ 0.0078,  0.0114,  0.0144, -0.0261,  0.0133],\n",
      "        [ 0.0173, -0.0041,  0.0592, -0.0001, -0.0199],\n",
      "        [ 0.0108, -0.0024, -0.0174, -0.0006,  0.0026],\n",
      "        [-0.0214, -0.0419, -0.0217,  0.0317, -0.0179],\n",
      "        [-0.0113,  0.0078,  0.0054,  0.0363,  0.0060]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0138, grad_fn=<MinBackward1>), tensor(0.9046, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.108696848154068\n",
      "@sample 929: tensor([[ 0.0092, -0.0250, -0.0196,  0.0206, -0.0239],\n",
      "        [-0.0065,  0.0019,  0.0075, -0.0125,  0.0064],\n",
      "        [ 0.0024,  0.0238,  0.0101, -0.0079, -0.0044],\n",
      "        [ 0.0110, -0.0078, -0.0071, -0.0010,  0.0104],\n",
      "        [-0.0005,  0.0206,  0.0029, -0.0014, -0.0121],\n",
      "        [-0.0101,  0.0179,  0.0206, -0.0248, -0.0065],\n",
      "        [-0.0079,  0.0012,  0.0092, -0.0279,  0.0222],\n",
      "        [ 0.0213, -0.0486,  0.0025,  0.0162, -0.0059]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0002,  0.0071, -0.0022,  0.0133,  0.0137],\n",
      "        [ 0.0093,  0.0340,  0.0285, -0.0053,  0.0175],\n",
      "        [ 0.0219, -0.0073,  0.0149, -0.0230, -0.0132],\n",
      "        [ 0.0090,  0.0044,  0.0110,  0.0135, -0.0186],\n",
      "        [ 0.0196,  0.0252,  0.0087,  0.0257,  0.0266],\n",
      "        [-0.0233,  0.0023, -0.0531,  0.0440,  0.0175],\n",
      "        [-0.0103,  0.0338, -0.0323,  0.0065,  0.0132],\n",
      "        [ 0.0095, -0.0074,  0.0121, -0.0067,  0.0290]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0118, grad_fn=<MinBackward1>), tensor(0.8652, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09667341411113739\n",
      "@sample 930: tensor([[ 1.0792e-02, -9.4891e-03, -4.3096e-03,  9.1028e-04,  7.8725e-03],\n",
      "        [-2.1639e-02, -1.5927e-02, -5.5324e-03,  1.1490e-02, -1.1935e-03],\n",
      "        [-1.0255e-02,  5.9488e-03,  2.6209e-03,  4.6347e-03,  1.1051e-02],\n",
      "        [-1.2788e-02,  8.0052e-04,  2.2813e-02, -2.8575e-02,  1.5245e-02],\n",
      "        [-1.1221e-02, -3.1400e-02, -3.6944e-03,  1.0981e-03,  1.7347e-02],\n",
      "        [ 1.7837e-02,  1.6386e-02,  1.2518e-02, -1.0469e-02,  2.2991e-02],\n",
      "        [-4.0767e-03,  1.2815e-02,  4.6333e-03, -3.0026e-05,  1.2972e-02],\n",
      "        [ 1.3180e-02, -1.0688e-02, -1.8875e-02,  6.5976e-03,  2.1998e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0005,  0.0045, -0.0057, -0.0115, -0.0273],\n",
      "        [-0.0044,  0.0116,  0.0298, -0.0063, -0.0184],\n",
      "        [ 0.0141, -0.0049,  0.0062, -0.0340, -0.0367],\n",
      "        [-0.0014, -0.0090,  0.0066,  0.0137,  0.0111],\n",
      "        [ 0.0110,  0.0032, -0.0016, -0.0075, -0.0055],\n",
      "        [-0.0152, -0.0057, -0.0312, -0.0017, -0.0224],\n",
      "        [ 0.0155,  0.0034,  0.0005,  0.0005,  0.0096],\n",
      "        [-0.0013, -0.0283, -0.0091, -0.0179, -0.0267]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0104, grad_fn=<MinBackward1>), tensor(0.8784, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09405262023210526\n",
      "@sample 931: tensor([[-0.0192, -0.0178,  0.0190,  0.0286, -0.0072],\n",
      "        [-0.0048,  0.0134,  0.0089, -0.0074,  0.0074],\n",
      "        [ 0.0009, -0.0058,  0.0053,  0.0038,  0.0104],\n",
      "        [-0.0024,  0.0029,  0.0377, -0.0181,  0.0017],\n",
      "        [-0.0531,  0.0230,  0.0138, -0.0217, -0.0143],\n",
      "        [ 0.0149, -0.0185,  0.0068,  0.0021, -0.0138],\n",
      "        [ 0.0332, -0.0138,  0.0189,  0.0011,  0.0251],\n",
      "        [-0.0092,  0.0064,  0.0161,  0.0112,  0.0003]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0034,  0.0132,  0.0085,  0.0106, -0.0087],\n",
      "        [-0.0180,  0.0013,  0.0136, -0.0312, -0.0099],\n",
      "        [ 0.0197, -0.0169,  0.0323,  0.0059,  0.0251],\n",
      "        [-0.0213,  0.0100,  0.0043, -0.0131, -0.0425],\n",
      "        [-0.0249, -0.0017,  0.0337, -0.0200,  0.0111],\n",
      "        [ 0.0014,  0.0046,  0.0035,  0.0222, -0.0062],\n",
      "        [-0.0072,  0.0056, -0.0500,  0.0695,  0.0110],\n",
      "        [ 0.0114, -0.0023, -0.0151, -0.0042, -0.0283]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0198, grad_fn=<MinBackward1>), tensor(0.8164, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09917862713336945\n",
      "@sample 932: tensor([[ 0.0095,  0.0131, -0.0034, -0.0103,  0.0140],\n",
      "        [-0.0186, -0.0225,  0.0026,  0.0279, -0.0151],\n",
      "        [ 0.0078, -0.0116,  0.0110, -0.0067,  0.0311],\n",
      "        [ 0.0051,  0.0188, -0.0085,  0.0116, -0.0329],\n",
      "        [ 0.0168,  0.0333, -0.0026, -0.0115,  0.0020],\n",
      "        [-0.0022,  0.0257, -0.0062, -0.0126,  0.0086],\n",
      "        [-0.0058, -0.0023,  0.0185,  0.0064,  0.0094],\n",
      "        [ 0.0094,  0.0251,  0.0141, -0.0141,  0.0320]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0003,  0.0025,  0.0173,  0.0218, -0.0125],\n",
      "        [ 0.0036, -0.0116,  0.0135, -0.0085,  0.0037],\n",
      "        [ 0.0035, -0.0187,  0.0007, -0.0008,  0.0099],\n",
      "        [-0.0341, -0.0342, -0.0438,  0.0142, -0.0055],\n",
      "        [ 0.0021, -0.0174, -0.0096,  0.0088,  0.0129],\n",
      "        [-0.0072, -0.0084, -0.0322,  0.0183,  0.0202],\n",
      "        [ 0.0312,  0.0163,  0.0415,  0.0127, -0.0313],\n",
      "        [ 0.0028,  0.0213, -0.0214,  0.0048, -0.0022]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0114, grad_fn=<MinBackward1>), tensor(0.8167, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09851560741662979\n",
      "@sample 933: tensor([[ 0.0048,  0.0055,  0.0308,  0.0043,  0.0174],\n",
      "        [ 0.0269, -0.0096,  0.0212, -0.0336,  0.0207],\n",
      "        [ 0.0023,  0.0224,  0.0028, -0.0090, -0.0102],\n",
      "        [-0.0371, -0.0160,  0.0144, -0.0202,  0.0208],\n",
      "        [ 0.0010, -0.0298,  0.0018,  0.0200,  0.0292],\n",
      "        [-0.0082, -0.0166,  0.0028, -0.0139,  0.0134],\n",
      "        [ 0.0052, -0.0117,  0.0191,  0.0095, -0.0057],\n",
      "        [ 0.0199, -0.0087,  0.0141,  0.0146, -0.0024]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0028, -0.0017,  0.0090, -0.0337, -0.0223],\n",
      "        [-0.0047,  0.0069, -0.0213,  0.0490,  0.0163],\n",
      "        [-0.0107,  0.0276,  0.0006, -0.0126, -0.0237],\n",
      "        [ 0.0015, -0.0229,  0.0312, -0.0185,  0.0045],\n",
      "        [ 0.0002,  0.0116, -0.0012, -0.0073, -0.0180],\n",
      "        [-0.0030,  0.0081,  0.0024,  0.0028,  0.0123],\n",
      "        [-0.0108,  0.0066, -0.0036, -0.0038,  0.0034],\n",
      "        [ 0.0245,  0.0114,  0.0084, -0.0033, -0.0142]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0141, grad_fn=<MinBackward1>), tensor(0.8444, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08550497889518738\n",
      "@sample 934: tensor([[ 0.0062, -0.0100, -0.0025, -0.0053,  0.0164],\n",
      "        [ 0.0141,  0.0008,  0.0046,  0.0033, -0.0064],\n",
      "        [-0.0276,  0.0408,  0.0058, -0.0147, -0.0046],\n",
      "        [ 0.0259,  0.0110, -0.0072,  0.0191, -0.0217],\n",
      "        [ 0.0014, -0.0009, -0.0091, -0.0277,  0.0331],\n",
      "        [ 0.0104,  0.0132, -0.0021,  0.0137, -0.0062],\n",
      "        [-0.0231, -0.0153, -0.0207, -0.0004,  0.0224],\n",
      "        [ 0.0044, -0.0053,  0.0056, -0.0139,  0.0114]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0111, -0.0445,  0.0244, -0.0232, -0.0154],\n",
      "        [ 0.0034,  0.0018, -0.0065, -0.0072, -0.0007],\n",
      "        [-0.0185,  0.0465, -0.0228, -0.0017, -0.0049],\n",
      "        [-0.0016,  0.0458, -0.0164, -0.0002,  0.0108],\n",
      "        [-0.0410, -0.0396, -0.1143, -0.0173, -0.0062],\n",
      "        [ 0.0256,  0.0208, -0.0255,  0.0276,  0.0159],\n",
      "        [ 0.0175, -0.0229,  0.0449,  0.0015, -0.0050],\n",
      "        [ 0.0098,  0.0110, -0.0214,  0.0123, -0.0244]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0189, grad_fn=<MinBackward1>), tensor(0.8582, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1061190739274025\n",
      "@sample 935: tensor([[ 0.0176,  0.0523,  0.0291, -0.0421, -0.0038],\n",
      "        [-0.0037,  0.0059, -0.0185, -0.0220,  0.0369],\n",
      "        [-0.0007,  0.0508,  0.0002, -0.0397,  0.0326],\n",
      "        [-0.0277, -0.0093, -0.0017,  0.0063,  0.0006],\n",
      "        [ 0.0222, -0.0192,  0.0220, -0.0227,  0.0132],\n",
      "        [ 0.0085,  0.0045, -0.0059, -0.0002, -0.0043],\n",
      "        [-0.0248,  0.0051,  0.0141, -0.0128,  0.0373],\n",
      "        [-0.0134, -0.0166, -0.0150,  0.0075,  0.0198]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0193, -0.0315, -0.0474,  0.0030,  0.0081],\n",
      "        [ 0.0040,  0.0102, -0.0398,  0.0139, -0.0078],\n",
      "        [-0.0113, -0.0154, -0.0742,  0.0442, -0.0107],\n",
      "        [-0.0033,  0.0075,  0.0284, -0.0106, -0.0019],\n",
      "        [-0.0077, -0.0231, -0.0320,  0.0180,  0.0076],\n",
      "        [-0.0076,  0.0116, -0.0243,  0.0145, -0.0055],\n",
      "        [ 0.0252,  0.0053,  0.0600,  0.0041, -0.0003],\n",
      "        [ 0.0216, -0.0523,  0.0170,  0.0107, -0.0189]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0145, grad_fn=<MinBackward1>), tensor(0.8484, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10429307818412781\n",
      "@sample 936: tensor([[-0.0023,  0.0099,  0.0060, -0.0473,  0.0248],\n",
      "        [ 0.0147, -0.0114, -0.0025,  0.0080,  0.0148],\n",
      "        [ 0.0276,  0.0222, -0.0025, -0.0226, -0.0132],\n",
      "        [-0.0304,  0.0287,  0.0130, -0.0336,  0.0154],\n",
      "        [ 0.0166,  0.0432, -0.0281, -0.0522, -0.0038],\n",
      "        [-0.0163, -0.0075,  0.0031, -0.0136,  0.0323],\n",
      "        [ 0.0203,  0.0223,  0.0086,  0.0173, -0.0095],\n",
      "        [-0.0303, -0.0169,  0.0038,  0.0035,  0.0139]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0228, -0.0395, -0.0184,  0.0237,  0.0084],\n",
      "        [ 0.0094, -0.0072, -0.0522,  0.0291,  0.0142],\n",
      "        [-0.0119,  0.0020, -0.0374,  0.0160, -0.0103],\n",
      "        [-0.0161, -0.0281, -0.0125,  0.0484, -0.0027],\n",
      "        [-0.0158, -0.0306, -0.0089,  0.0047,  0.0061],\n",
      "        [-0.0185, -0.0476, -0.0100,  0.0177,  0.0197],\n",
      "        [ 0.0179,  0.0213, -0.0181,  0.0339,  0.0043],\n",
      "        [-0.0141, -0.0361,  0.0172, -0.0091,  0.0018]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0162, grad_fn=<MinBackward1>), tensor(0.8723, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10377892106771469\n",
      "@sample 937: tensor([[ 0.0195,  0.0271,  0.0115, -0.0283,  0.0179],\n",
      "        [-0.0179, -0.0286, -0.0082,  0.0014,  0.0003],\n",
      "        [ 0.0091, -0.0003, -0.0002, -0.0264,  0.0135],\n",
      "        [-0.0096, -0.0190,  0.0154, -0.0019, -0.0151],\n",
      "        [ 0.0233, -0.0081,  0.0064, -0.0170, -0.0088],\n",
      "        [ 0.0229,  0.0177, -0.0114, -0.0132,  0.0043],\n",
      "        [ 0.0152, -0.0173, -0.0072,  0.0311, -0.0199],\n",
      "        [ 0.0039, -0.0153,  0.0022,  0.0210,  0.0010]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0176, -0.0211, -0.0220,  0.0294, -0.0045],\n",
      "        [-0.0066, -0.0135,  0.0459, -0.0119, -0.0092],\n",
      "        [-0.0173, -0.0199, -0.0102,  0.0367,  0.0117],\n",
      "        [-0.0161, -0.0002,  0.0383, -0.0283,  0.0083],\n",
      "        [ 0.0005, -0.0025, -0.0482, -0.0058, -0.0190],\n",
      "        [-0.0048, -0.0134, -0.0411,  0.0221,  0.0129],\n",
      "        [-0.0005, -0.0100,  0.0096,  0.0156, -0.0127],\n",
      "        [ 0.0043,  0.0029, -0.0236,  0.0091, -0.0100]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0147, grad_fn=<MinBackward1>), tensor(0.8364, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09872600436210632\n",
      "@sample 938: tensor([[ 0.0026,  0.0080,  0.0070, -0.0204,  0.0184],\n",
      "        [ 0.0164,  0.0044,  0.0114,  0.0218, -0.0177],\n",
      "        [ 0.0098, -0.0087, -0.0313,  0.0139, -0.0254],\n",
      "        [ 0.0314, -0.0130,  0.0075,  0.0233, -0.0006],\n",
      "        [ 0.0174, -0.0136, -0.0024,  0.0103, -0.0082],\n",
      "        [-0.0055, -0.0108, -0.0039,  0.0203, -0.0047],\n",
      "        [-0.0344,  0.0167,  0.0144, -0.0254, -0.0041],\n",
      "        [-0.0057,  0.0382,  0.0152, -0.0142,  0.0320]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0339, -0.0102, -0.0213,  0.0219,  0.0018],\n",
      "        [ 0.0127,  0.0438,  0.0159, -0.0220,  0.0133],\n",
      "        [ 0.0030,  0.0105,  0.0182,  0.0068,  0.0193],\n",
      "        [-0.0034,  0.0465, -0.0200, -0.0331, -0.0163],\n",
      "        [-0.0013, -0.0006, -0.0116,  0.0084,  0.0016],\n",
      "        [ 0.0147,  0.0172, -0.0087, -0.0177, -0.0053],\n",
      "        [ 0.0149,  0.0238, -0.0795,  0.0331,  0.0153],\n",
      "        [-0.0012, -0.0206, -0.0101,  0.0392, -0.0023]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0114, grad_fn=<MinBackward1>), tensor(0.9109, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09516534209251404\n",
      "@sample 939: tensor([[ 0.0173, -0.0083, -0.0309,  0.0110, -0.0102],\n",
      "        [-0.0032,  0.0409,  0.0201, -0.0326, -0.0168],\n",
      "        [-0.0029,  0.0005, -0.0018,  0.0050, -0.0125],\n",
      "        [ 0.0201, -0.0360, -0.0311, -0.0148,  0.0337],\n",
      "        [ 0.0090,  0.0193,  0.0407, -0.0274,  0.0123],\n",
      "        [-0.0235,  0.0025,  0.0102, -0.0054,  0.0175],\n",
      "        [ 0.0039, -0.0309, -0.0123,  0.0248, -0.0125],\n",
      "        [-0.0030,  0.0130, -0.0024, -0.0112, -0.0192]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0206,  0.0110,  0.0157, -0.0283, -0.0033],\n",
      "        [-0.0256, -0.0170, -0.0437,  0.0859,  0.0247],\n",
      "        [-0.0209, -0.0223, -0.0612,  0.0365,  0.0155],\n",
      "        [ 0.0028, -0.0098,  0.0323,  0.0155,  0.0195],\n",
      "        [ 0.0018, -0.0126,  0.0091,  0.0374,  0.0101],\n",
      "        [-0.0068, -0.0333, -0.0210, -0.0101,  0.0015],\n",
      "        [ 0.0128, -0.0186,  0.0203, -0.0197, -0.0088],\n",
      "        [-0.0011, -0.0136, -0.0595, -0.0169,  0.0197]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0152, grad_fn=<MinBackward1>), tensor(0.8667, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1017310693860054\n",
      "@sample 940: tensor([[ 9.7881e-03, -4.0025e-02, -2.2240e-02,  1.0312e-02, -1.4102e-02],\n",
      "        [-1.5269e-02, -4.1581e-03, -7.8406e-03,  2.2271e-03,  1.7402e-03],\n",
      "        [ 2.6222e-03, -4.8135e-03, -2.4619e-02,  1.8343e-02, -1.5233e-02],\n",
      "        [-1.2639e-02,  4.6594e-03,  2.3168e-02, -1.1893e-02,  1.8502e-02],\n",
      "        [-8.1640e-05, -4.9032e-03, -5.1345e-03,  2.6952e-02, -1.2918e-02],\n",
      "        [ 2.7984e-02,  9.1463e-03, -2.0584e-02, -8.8122e-03,  6.5578e-03],\n",
      "        [ 7.3992e-03,  5.9593e-04,  2.1002e-03,  4.6860e-03, -1.4130e-02],\n",
      "        [ 2.2048e-02,  1.5709e-02,  3.2423e-02,  1.3746e-02, -1.5474e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0013,  0.0071,  0.0211, -0.0211, -0.0036],\n",
      "        [ 0.0080, -0.0249, -0.0079,  0.0029, -0.0174],\n",
      "        [ 0.0094, -0.0016,  0.0114, -0.0485, -0.0087],\n",
      "        [ 0.0079,  0.0028, -0.0321,  0.0019, -0.0308],\n",
      "        [ 0.0024,  0.0406, -0.0222, -0.0023,  0.0189],\n",
      "        [-0.0179, -0.0229, -0.0351,  0.0197, -0.0056],\n",
      "        [-0.0027,  0.0221,  0.0081, -0.0064,  0.0189],\n",
      "        [-0.0165, -0.0061, -0.0086,  0.0270,  0.0016]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0159, grad_fn=<MinBackward1>), tensor(0.8500, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09622067958116531\n",
      "@sample 941: tensor([[-0.0276, -0.0033, -0.0156, -0.0229, -0.0072],\n",
      "        [ 0.0130, -0.0341, -0.0201,  0.0252, -0.0180],\n",
      "        [ 0.0083, -0.0004, -0.0159,  0.0230, -0.0340],\n",
      "        [ 0.0137, -0.0002, -0.0070,  0.0127,  0.0012],\n",
      "        [ 0.0156, -0.0221, -0.0008, -0.0075,  0.0028],\n",
      "        [ 0.0054,  0.0149,  0.0236, -0.0125,  0.0176],\n",
      "        [ 0.0012,  0.0136, -0.0103, -0.0149,  0.0159],\n",
      "        [ 0.0087, -0.0178,  0.0195, -0.0013, -0.0030]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0092, -0.0846,  0.0292, -0.0492,  0.0062],\n",
      "        [ 0.0159,  0.0231,  0.0141, -0.0049,  0.0052],\n",
      "        [ 0.0395,  0.0098,  0.0348, -0.0260,  0.0011],\n",
      "        [-0.0155, -0.0046, -0.0473, -0.0202, -0.0353],\n",
      "        [ 0.0106, -0.0251,  0.0312,  0.0398,  0.0175],\n",
      "        [-0.0162,  0.0061, -0.0569,  0.0325,  0.0061],\n",
      "        [-0.0027,  0.0245,  0.0186, -0.0213, -0.0209],\n",
      "        [-0.0225,  0.0125, -0.0109,  0.0236,  0.0585]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0112, grad_fn=<MinBackward1>), tensor(0.9009, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1086612120270729\n",
      "@sample 942: tensor([[ 0.0073, -0.0270, -0.0268,  0.0367,  0.0168],\n",
      "        [-0.0151,  0.0038,  0.0197,  0.0260, -0.0187],\n",
      "        [-0.0019,  0.0012, -0.0251,  0.0265, -0.0007],\n",
      "        [ 0.0349, -0.0138, -0.0167,  0.0242, -0.0171],\n",
      "        [ 0.0193,  0.0124, -0.0050,  0.0201, -0.0387],\n",
      "        [ 0.0040,  0.0237, -0.0056, -0.0045,  0.0026],\n",
      "        [-0.0237,  0.0291, -0.0056, -0.0017, -0.0053],\n",
      "        [-0.0105, -0.0239, -0.0282,  0.0258, -0.0259]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0317, -0.0071,  0.0136, -0.0090, -0.0299],\n",
      "        [-0.0042,  0.0186, -0.0211,  0.0122, -0.0153],\n",
      "        [ 0.0350,  0.0125,  0.0062, -0.0408, -0.0140],\n",
      "        [ 0.0111,  0.0240,  0.0077,  0.0054, -0.0199],\n",
      "        [-0.0065,  0.0158,  0.0337, -0.0319, -0.0259],\n",
      "        [-0.0180,  0.0174, -0.0082, -0.0147, -0.0183],\n",
      "        [ 0.0381,  0.0249,  0.0306,  0.0101,  0.0183],\n",
      "        [ 0.0024,  0.0116,  0.0421, -0.0165,  0.0002]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0153, grad_fn=<MinBackward1>), tensor(0.9068, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1011444479227066\n",
      "@sample 943: tensor([[ 2.3964e-02, -3.2629e-03, -1.0677e-02,  2.1784e-02,  1.2573e-02],\n",
      "        [-4.2621e-02, -2.3814e-02,  7.8122e-03,  1.6375e-02, -4.6135e-03],\n",
      "        [ 1.3983e-02, -4.3861e-03, -2.9243e-02,  1.5463e-02, -1.5022e-02],\n",
      "        [ 2.3448e-02, -2.3395e-02, -1.5937e-02,  3.2453e-02, -1.2226e-02],\n",
      "        [-2.7643e-03, -9.8273e-03, -5.8798e-03, -3.8823e-03,  2.7783e-03],\n",
      "        [ 1.1878e-03, -2.1485e-03,  1.2928e-02, -8.1018e-03,  6.6995e-04],\n",
      "        [ 6.2729e-05, -7.7473e-03, -1.1828e-02,  2.6968e-02, -3.3691e-02],\n",
      "        [ 8.8997e-03, -4.1921e-03, -2.1666e-03, -6.4268e-03, -2.1371e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0118,  0.0180, -0.0087, -0.0179, -0.0243],\n",
      "        [ 0.0238, -0.0163,  0.0339,  0.0085, -0.0058],\n",
      "        [ 0.0097, -0.0186,  0.0218, -0.0032,  0.0169],\n",
      "        [ 0.0059,  0.0213,  0.0173, -0.0054, -0.0028],\n",
      "        [-0.0210, -0.0230,  0.0439,  0.0226, -0.0078],\n",
      "        [-0.0192, -0.0182,  0.0397, -0.0240, -0.0181],\n",
      "        [-0.0182,  0.0214, -0.0146,  0.0114,  0.0242],\n",
      "        [ 0.0098, -0.0049,  0.0044,  0.0179, -0.0036]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0081, grad_fn=<MinBackward1>), tensor(0.8755, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10802596807479858\n",
      "@sample 944: tensor([[ 0.0029, -0.0149,  0.0016,  0.0202, -0.0061],\n",
      "        [-0.0057, -0.0079,  0.0238,  0.0096, -0.0010],\n",
      "        [ 0.0074,  0.0127,  0.0185, -0.0208,  0.0192],\n",
      "        [-0.0121,  0.0039, -0.0080,  0.0073,  0.0088],\n",
      "        [ 0.0157,  0.0008, -0.0275,  0.0328,  0.0005],\n",
      "        [-0.0209, -0.0092, -0.0085,  0.0204, -0.0307],\n",
      "        [ 0.0117,  0.0284, -0.0151, -0.0229, -0.0071],\n",
      "        [ 0.0218,  0.0139,  0.0227,  0.0328, -0.0435]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0058, -0.0085,  0.0197, -0.0029, -0.0010],\n",
      "        [-0.0404,  0.0232,  0.0099,  0.0158,  0.0129],\n",
      "        [-0.0148, -0.0132, -0.0214,  0.0182,  0.0060],\n",
      "        [-0.0361, -0.0064, -0.0230, -0.0077, -0.0307],\n",
      "        [ 0.0450,  0.0666,  0.0041, -0.0286, -0.0080],\n",
      "        [ 0.0197, -0.0042,  0.0640, -0.0517, -0.0404],\n",
      "        [ 0.0046, -0.0295,  0.0056,  0.0094,  0.0059],\n",
      "        [-0.0205, -0.0056, -0.0323,  0.0206,  0.0285]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0195, grad_fn=<MinBackward1>), tensor(0.8353, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10898429155349731\n",
      "@sample 945: tensor([[ 0.0102, -0.0218, -0.0025,  0.0337, -0.0154],\n",
      "        [ 0.0068, -0.0120, -0.0099,  0.0181, -0.0128],\n",
      "        [ 0.0018, -0.0157, -0.0239,  0.0150, -0.0197],\n",
      "        [-0.0008, -0.0192,  0.0015, -0.0011, -0.0328],\n",
      "        [ 0.0139, -0.0142, -0.0396,  0.0178, -0.0349],\n",
      "        [-0.0187,  0.0094,  0.0152,  0.0175,  0.0014],\n",
      "        [ 0.0028,  0.0092, -0.0007, -0.0219,  0.0164],\n",
      "        [ 0.0209, -0.0177, -0.0079,  0.0423, -0.0345]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0079,  0.0454,  0.0173, -0.0294,  0.0108],\n",
      "        [ 0.0120,  0.0331,  0.0224, -0.0069,  0.0161],\n",
      "        [ 0.0049,  0.0453,  0.0348, -0.0076,  0.0180],\n",
      "        [ 0.0110, -0.0128,  0.0334, -0.0293,  0.0183],\n",
      "        [-0.0014,  0.0189,  0.0028, -0.0230, -0.0104],\n",
      "        [ 0.0209,  0.0355,  0.0255, -0.0099,  0.0023],\n",
      "        [ 0.0050, -0.0109, -0.0167,  0.0167, -0.0262],\n",
      "        [ 0.0164,  0.0280,  0.0421, -0.0064,  0.0120]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0134, grad_fn=<MinBackward1>), tensor(0.8616, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10359001159667969\n",
      "@sample 946: tensor([[-3.4691e-02,  3.0391e-02,  2.0013e-02, -3.2814e-02, -2.7120e-03],\n",
      "        [-9.2723e-03,  4.6399e-03,  2.6999e-02, -1.2578e-02, -3.7009e-03],\n",
      "        [-1.4587e-02,  1.2058e-02,  5.5581e-03,  1.5365e-03, -1.8295e-02],\n",
      "        [ 1.4144e-02, -1.3685e-03,  1.7233e-03,  9.2238e-04,  9.1070e-03],\n",
      "        [ 8.4870e-03, -6.8173e-03,  1.1739e-02,  3.0098e-02,  6.2830e-03],\n",
      "        [ 4.8822e-03, -9.5574e-03, -3.1889e-02,  4.1649e-03,  1.9861e-04],\n",
      "        [-1.3730e-02,  4.8317e-02, -7.0855e-05, -1.6188e-02,  1.1163e-02],\n",
      "        [-3.6891e-03, -2.0283e-03,  2.9140e-02, -3.3306e-02,  6.9390e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0158, -0.0186,  0.0011, -0.0153,  0.0050],\n",
      "        [-0.0102,  0.0055,  0.0448, -0.0377, -0.0115],\n",
      "        [-0.0296,  0.0227, -0.0120, -0.0127,  0.0007],\n",
      "        [-0.0069,  0.0096,  0.0066, -0.0119,  0.0077],\n",
      "        [-0.0034,  0.0157, -0.0167, -0.0095, -0.0257],\n",
      "        [ 0.0046, -0.0069,  0.0133, -0.0232,  0.0073],\n",
      "        [ 0.0133,  0.0156, -0.0454,  0.0121,  0.0077],\n",
      "        [-0.0432,  0.0005, -0.0508,  0.0446,  0.0030]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0117, grad_fn=<MinBackward1>), tensor(0.8608, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10136415809392929\n",
      "@sample 947: tensor([[-0.0094,  0.0196,  0.0243,  0.0020,  0.0259],\n",
      "        [-0.0072, -0.0220,  0.0318,  0.0060, -0.0035],\n",
      "        [ 0.0029,  0.0031, -0.0206,  0.0054, -0.0303],\n",
      "        [ 0.0149, -0.0118, -0.0094, -0.0336,  0.0045],\n",
      "        [-0.0041, -0.0078, -0.0025,  0.0168, -0.0019],\n",
      "        [ 0.0235,  0.0187, -0.0073, -0.0313,  0.0137],\n",
      "        [ 0.0164, -0.0086, -0.0047, -0.0109,  0.0144],\n",
      "        [ 0.0008,  0.0207,  0.0044, -0.0294,  0.0388]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0330,  0.0179, -0.0574,  0.0275, -0.0016],\n",
      "        [ 0.0270, -0.0224,  0.0259, -0.0067,  0.0041],\n",
      "        [-0.0258, -0.0114,  0.0294, -0.0104,  0.0066],\n",
      "        [-0.0657, -0.0226, -0.0219,  0.0223,  0.0218],\n",
      "        [ 0.0106,  0.0176,  0.0270, -0.0214,  0.0059],\n",
      "        [-0.0373, -0.0202, -0.0682,  0.0600, -0.0122],\n",
      "        [-0.0329, -0.0155, -0.0051,  0.0116,  0.0145],\n",
      "        [-0.0273,  0.0088, -0.0346,  0.0202,  0.0056]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0198, grad_fn=<MinBackward1>), tensor(0.8476, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09683533757925034\n",
      "@sample 948: tensor([[-0.0178, -0.0016,  0.0072,  0.0081,  0.0155],\n",
      "        [ 0.0091,  0.0039,  0.0234, -0.0314,  0.0043],\n",
      "        [ 0.0084, -0.0214, -0.0110,  0.0146, -0.0192],\n",
      "        [ 0.0131,  0.0086, -0.0078, -0.0118,  0.0038],\n",
      "        [-0.0207, -0.0134, -0.0191, -0.0158,  0.0248],\n",
      "        [ 0.0003, -0.0365,  0.0243, -0.0236, -0.0121],\n",
      "        [ 0.0025,  0.0034,  0.0292, -0.0039,  0.0019],\n",
      "        [-0.0135,  0.0009, -0.0030,  0.0053,  0.0041]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0170,  0.0346,  0.0129,  0.0056, -0.0184],\n",
      "        [-0.0497, -0.0036,  0.0106,  0.0133,  0.0282],\n",
      "        [-0.0057,  0.0201, -0.0205,  0.0080,  0.0002],\n",
      "        [-0.0084, -0.0084,  0.0294, -0.0334,  0.0002],\n",
      "        [ 0.0016, -0.0195, -0.0847,  0.0239,  0.0419],\n",
      "        [-0.0128,  0.0044, -0.0267, -0.0037, -0.0053],\n",
      "        [-0.0121,  0.0198,  0.0092, -0.0190,  0.0040],\n",
      "        [ 0.0223,  0.0212,  0.0037, -0.0178, -0.0050]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0170, grad_fn=<MinBackward1>), tensor(0.8952, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09892113506793976\n",
      "@sample 949: tensor([[-0.0065, -0.0064,  0.0169, -0.0013, -0.0101],\n",
      "        [ 0.0111,  0.0232,  0.0067, -0.0257, -0.0077],\n",
      "        [-0.0069, -0.0176,  0.0327, -0.0463,  0.0288],\n",
      "        [ 0.0191, -0.0016, -0.0128, -0.0194,  0.0086],\n",
      "        [-0.0275,  0.0085,  0.0080, -0.0230,  0.0152],\n",
      "        [ 0.0015,  0.0335,  0.0114, -0.0007, -0.0081],\n",
      "        [-0.0084,  0.0077, -0.0016, -0.0172,  0.0043],\n",
      "        [ 0.0117, -0.0171,  0.0071, -0.0039,  0.0002]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-4.3925e-04, -1.8275e-03, -1.4481e-02, -6.7105e-03,  2.2606e-02],\n",
      "        [-2.6094e-02,  3.0930e-03, -5.4857e-02,  2.4796e-02,  1.8908e-02],\n",
      "        [-4.0902e-02, -1.7232e-02, -2.7258e-02,  1.6021e-02,  3.2468e-02],\n",
      "        [-2.7424e-02,  3.0033e-05,  1.4353e-02, -1.1425e-02, -2.0702e-02],\n",
      "        [-1.7648e-02, -5.3360e-03,  2.2547e-02, -2.3328e-02, -2.4515e-02],\n",
      "        [ 2.9385e-02, -7.4365e-03, -1.7048e-02, -4.7959e-03, -3.5585e-03],\n",
      "        [-4.0023e-03, -2.7106e-02,  2.6855e-03,  2.2422e-03,  9.1233e-03],\n",
      "        [-2.3781e-02, -7.9981e-03, -1.1575e-03, -2.0804e-02,  2.2675e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0138, grad_fn=<MinBackward1>), tensor(0.8985, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09507077932357788\n",
      "@sample 950: tensor([[ 1.5491e-02, -2.9986e-02, -3.8588e-03, -5.0728e-04,  4.1972e-02],\n",
      "        [ 6.7270e-03, -1.9715e-02, -1.9792e-03, -2.4600e-02,  7.3660e-03],\n",
      "        [ 2.8497e-04,  6.8043e-03, -1.7501e-02, -1.1813e-02, -3.3020e-03],\n",
      "        [ 5.2288e-03,  1.8000e-02,  3.9717e-02, -2.0976e-02, -1.9122e-02],\n",
      "        [ 2.5925e-03, -8.0462e-03, -4.6080e-03,  3.4721e-02, -1.4626e-02],\n",
      "        [-1.6613e-02, -1.2434e-02,  3.8317e-03,  4.9606e-03,  6.8030e-03],\n",
      "        [-1.7844e-02, -2.7776e-05, -1.3909e-02, -5.6635e-03,  1.2340e-03],\n",
      "        [-1.3255e-02, -1.0596e-03,  1.6184e-02, -1.5277e-02,  1.4298e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0109, -0.0121, -0.0105, -0.0371, -0.0310],\n",
      "        [ 0.0046, -0.0070, -0.0137, -0.0408,  0.0014],\n",
      "        [-0.0135, -0.0086,  0.0189, -0.0295, -0.0068],\n",
      "        [-0.0324, -0.0173, -0.0164, -0.0200, -0.0416],\n",
      "        [ 0.0224,  0.0171, -0.0277, -0.0048,  0.0463],\n",
      "        [ 0.0298, -0.0027,  0.0341, -0.0195,  0.0031],\n",
      "        [-0.0274,  0.0189, -0.0325,  0.0046, -0.0273],\n",
      "        [ 0.0093, -0.0157,  0.0555, -0.0335, -0.0147]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0090, grad_fn=<MinBackward1>), tensor(0.8592, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11273247748613358\n",
      "@sample 951: tensor([[-0.0079,  0.0015, -0.0034,  0.0010,  0.0056],\n",
      "        [ 0.0028,  0.0338,  0.0180, -0.0514,  0.0342],\n",
      "        [-0.0208,  0.0139, -0.0055, -0.0098,  0.0072],\n",
      "        [-0.0074, -0.0072, -0.0036,  0.0158,  0.0089],\n",
      "        [-0.0170,  0.0005, -0.0005, -0.0136,  0.0235],\n",
      "        [ 0.0084, -0.0118, -0.0032,  0.0057,  0.0030],\n",
      "        [ 0.0183, -0.0007, -0.0015,  0.0088,  0.0011],\n",
      "        [-0.0227,  0.0023, -0.0020, -0.0254, -0.0080]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0150,  0.0138, -0.0195, -0.0082, -0.0131],\n",
      "        [-0.0084, -0.0050, -0.0902,  0.0479, -0.0023],\n",
      "        [ 0.0024, -0.0299,  0.0175,  0.0021,  0.0038],\n",
      "        [ 0.0281, -0.0116,  0.0249, -0.0015,  0.0028],\n",
      "        [ 0.0039, -0.0465,  0.0130,  0.0079, -0.0465],\n",
      "        [ 0.0160,  0.0072,  0.0439,  0.0022, -0.0014],\n",
      "        [-0.0083, -0.0214, -0.0030, -0.0095,  0.0179],\n",
      "        [-0.0227, -0.0367,  0.0192, -0.0083,  0.0125]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0149, grad_fn=<MinBackward1>), tensor(0.8921, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11029882729053497\n",
      "@sample 952: tensor([[-0.0019, -0.0130,  0.0026,  0.0118,  0.0088],\n",
      "        [ 0.0063,  0.0235,  0.0110, -0.0217,  0.0123],\n",
      "        [ 0.0182, -0.0371, -0.0330,  0.0228, -0.0343],\n",
      "        [-0.0210, -0.0044, -0.0073, -0.0050, -0.0029],\n",
      "        [ 0.0154, -0.0054,  0.0099,  0.0181,  0.0152],\n",
      "        [-0.0013, -0.0011, -0.0235, -0.0124,  0.0080],\n",
      "        [ 0.0074, -0.0072,  0.0240, -0.0255,  0.0292],\n",
      "        [-0.0067, -0.0061, -0.0008,  0.0013,  0.0053]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0134, -0.0108,  0.0225, -0.0058, -0.0004],\n",
      "        [-0.0073, -0.0196, -0.0533,  0.0258,  0.0216],\n",
      "        [ 0.0147, -0.0201,  0.0671, -0.0257,  0.0048],\n",
      "        [-0.0172,  0.0060,  0.0143,  0.0146,  0.0345],\n",
      "        [ 0.0086,  0.0162, -0.0130,  0.0472, -0.0070],\n",
      "        [ 0.0025,  0.0037, -0.0083, -0.0152,  0.0029],\n",
      "        [ 0.0050,  0.0160,  0.0238, -0.0055, -0.0263],\n",
      "        [ 0.0109,  0.0282,  0.0226, -0.0316, -0.0123]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0146, grad_fn=<MinBackward1>), tensor(0.8700, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10116766393184662\n",
      "@sample 953: tensor([[ 0.0019,  0.0392,  0.0109,  0.0021, -0.0114],\n",
      "        [ 0.0092,  0.0056,  0.0037, -0.0004,  0.0056],\n",
      "        [ 0.0207,  0.0045,  0.0026,  0.0390, -0.0158],\n",
      "        [ 0.0027, -0.0055,  0.0050, -0.0070,  0.0058],\n",
      "        [-0.0158, -0.0124, -0.0197,  0.0028, -0.0016],\n",
      "        [-0.0073, -0.0002, -0.0012,  0.0095, -0.0112],\n",
      "        [ 0.0028,  0.0295,  0.0022, -0.0331,  0.0248],\n",
      "        [-0.0037,  0.0254, -0.0141,  0.0261, -0.0197]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0069, -0.0310, -0.0322, -0.0049, -0.0095],\n",
      "        [ 0.0326,  0.0065, -0.0070, -0.0245, -0.0077],\n",
      "        [ 0.0164,  0.0303, -0.0223,  0.0364,  0.0048],\n",
      "        [-0.0031, -0.0052, -0.0044, -0.0022,  0.0043],\n",
      "        [-0.0125,  0.0003,  0.0215, -0.0028, -0.0138],\n",
      "        [-0.0040,  0.0114,  0.0266, -0.0312, -0.0115],\n",
      "        [-0.0129, -0.0075, -0.0326,  0.0085, -0.0294],\n",
      "        [ 0.0032,  0.0089, -0.0032,  0.0046, -0.0060]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0165, grad_fn=<MinBackward1>), tensor(0.8758, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09461702406406403\n",
      "@sample 954: tensor([[ 0.0131,  0.0222,  0.0019, -0.0048, -0.0023],\n",
      "        [ 0.0341,  0.0107,  0.0208, -0.0413,  0.0285],\n",
      "        [ 0.0066, -0.0112,  0.0155,  0.0101,  0.0051],\n",
      "        [-0.0154,  0.0321,  0.0128, -0.0143,  0.0060],\n",
      "        [-0.0043,  0.0413,  0.0062, -0.0159,  0.0220],\n",
      "        [ 0.0292, -0.0118, -0.0117,  0.0102,  0.0231],\n",
      "        [-0.0114, -0.0061, -0.0295,  0.0198, -0.0036],\n",
      "        [-0.0081, -0.0147,  0.0144, -0.0095, -0.0111]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0113,  0.0210, -0.0307,  0.0193,  0.0074],\n",
      "        [ 0.0283, -0.0243, -0.0887,  0.0293,  0.0125],\n",
      "        [-0.0471,  0.0011, -0.0472,  0.0152, -0.0045],\n",
      "        [ 0.0033,  0.0288, -0.0191,  0.0015,  0.0014],\n",
      "        [-0.0234,  0.0256, -0.0360, -0.0014, -0.0264],\n",
      "        [ 0.0227,  0.0175, -0.0349, -0.0216, -0.0262],\n",
      "        [ 0.0291,  0.0329, -0.0068, -0.0076,  0.0083],\n",
      "        [-0.0085,  0.0347, -0.0339, -0.0147,  0.0223]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0140, grad_fn=<MinBackward1>), tensor(0.9033, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10494868457317352\n",
      "@sample 955: tensor([[ 0.0044, -0.0023, -0.0089,  0.0238,  0.0178],\n",
      "        [-0.0224,  0.0231, -0.0010,  0.0097,  0.0086],\n",
      "        [-0.0261,  0.0052,  0.0226,  0.0032, -0.0163],\n",
      "        [ 0.0028,  0.0130, -0.0016, -0.0083, -0.0149],\n",
      "        [ 0.0031,  0.0266,  0.0034,  0.0079,  0.0130],\n",
      "        [ 0.0273, -0.0030,  0.0062,  0.0104, -0.0174],\n",
      "        [ 0.0007, -0.0037, -0.0077,  0.0012,  0.0111],\n",
      "        [-0.0176, -0.0062,  0.0056,  0.0300, -0.0036]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0095,  0.0338, -0.0125, -0.0204, -0.0009],\n",
      "        [-0.0093,  0.0194, -0.0260, -0.0113, -0.0249],\n",
      "        [-0.0002,  0.0341,  0.0478, -0.0187,  0.0280],\n",
      "        [-0.0393,  0.0170, -0.0104,  0.0178,  0.0253],\n",
      "        [ 0.0240,  0.0103, -0.0470,  0.0297,  0.0092],\n",
      "        [-0.0188,  0.0076, -0.0281,  0.0230,  0.0172],\n",
      "        [-0.0132, -0.0214,  0.0240,  0.0072, -0.0007],\n",
      "        [ 0.0079,  0.0132,  0.0307, -0.0141,  0.0142]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0115, grad_fn=<MinBackward1>), tensor(0.8413, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10077469050884247\n",
      "@sample 956: tensor([[-0.0003,  0.0075, -0.0058,  0.0053,  0.0081],\n",
      "        [-0.0135,  0.0055, -0.0252,  0.0030,  0.0401],\n",
      "        [ 0.0087,  0.0106, -0.0088,  0.0086,  0.0075],\n",
      "        [ 0.0122, -0.0312, -0.0342,  0.0385, -0.0341],\n",
      "        [ 0.0112, -0.0072,  0.0339,  0.0073, -0.0013],\n",
      "        [-0.0104,  0.0314,  0.0115, -0.0333,  0.0489],\n",
      "        [ 0.0408,  0.0206, -0.0071,  0.0015, -0.0265],\n",
      "        [ 0.0038, -0.0064,  0.0011, -0.0146,  0.0354]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0148,  0.0207, -0.0077, -0.0116, -0.0091],\n",
      "        [ 0.0014, -0.0005, -0.0494,  0.0173, -0.0255],\n",
      "        [ 0.0189,  0.0067,  0.0310, -0.0210, -0.0128],\n",
      "        [ 0.0196, -0.0104,  0.0295,  0.0099, -0.0073],\n",
      "        [-0.0157, -0.0052,  0.0096,  0.0202,  0.0063],\n",
      "        [-0.0138, -0.0071, -0.1016,  0.0565,  0.0149],\n",
      "        [-0.0413,  0.0038, -0.0377, -0.0008, -0.0049],\n",
      "        [ 0.0037, -0.0145, -0.0046, -0.0081, -0.0332]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0137, grad_fn=<MinBackward1>), tensor(0.8491, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11032047867774963\n",
      "@sample 957: tensor([[ 0.0052,  0.0055, -0.0061,  0.0117, -0.0157],\n",
      "        [ 0.0020,  0.0178,  0.0294,  0.0091,  0.0070],\n",
      "        [ 0.0245,  0.0393,  0.0070,  0.0058, -0.0111],\n",
      "        [-0.0515, -0.0055,  0.0122, -0.0141,  0.0114],\n",
      "        [-0.0082,  0.0039, -0.0035,  0.0050,  0.0085],\n",
      "        [ 0.0045, -0.0093,  0.0416,  0.0126, -0.0063],\n",
      "        [-0.0124, -0.0169, -0.0082,  0.0089, -0.0114],\n",
      "        [ 0.0278,  0.0272,  0.0166, -0.0188, -0.0182]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0090,  0.0121, -0.0034, -0.0025,  0.0345],\n",
      "        [-0.0273,  0.0011, -0.0143,  0.0165,  0.0069],\n",
      "        [ 0.0150,  0.0240, -0.0428,  0.0351,  0.0152],\n",
      "        [-0.0119, -0.0086,  0.0023, -0.0277, -0.0108],\n",
      "        [ 0.0155, -0.0152,  0.0320,  0.0114, -0.0113],\n",
      "        [-0.0011,  0.0069,  0.0089,  0.0192,  0.0045],\n",
      "        [ 0.0127, -0.0105,  0.0190,  0.0150, -0.0031],\n",
      "        [ 0.0135,  0.0241, -0.0314, -0.0017, -0.0159]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0168, grad_fn=<MinBackward1>), tensor(0.8690, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10489743947982788\n",
      "@sample 958: tensor([[ 0.0029,  0.0328,  0.0158, -0.0339,  0.0420],\n",
      "        [-0.0045, -0.0249,  0.0029,  0.0086, -0.0166],\n",
      "        [ 0.0308, -0.0380, -0.0609,  0.0212,  0.0090],\n",
      "        [-0.0053,  0.0194, -0.0273,  0.0175,  0.0281],\n",
      "        [ 0.0150, -0.0155,  0.0128,  0.0247,  0.0025],\n",
      "        [-0.0198, -0.0129,  0.0182, -0.0131,  0.0161],\n",
      "        [-0.0083,  0.0121,  0.0076,  0.0210, -0.0150],\n",
      "        [ 0.0144, -0.0027,  0.0007,  0.0043,  0.0049]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0045, -0.0009, -0.0363,  0.0273, -0.0052],\n",
      "        [-0.0119,  0.0071,  0.0132,  0.0088,  0.0236],\n",
      "        [ 0.0298,  0.0237, -0.0114, -0.0074, -0.0319],\n",
      "        [-0.0189,  0.0019, -0.0193, -0.0302, -0.0358],\n",
      "        [ 0.0072, -0.0078, -0.0108,  0.0124, -0.0076],\n",
      "        [-0.0103, -0.0034,  0.0167, -0.0134, -0.0186],\n",
      "        [ 0.0093,  0.0454, -0.0240,  0.0149,  0.0241],\n",
      "        [ 0.0153,  0.0006, -0.0154,  0.0079, -0.0027]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0174, grad_fn=<MinBackward1>), tensor(0.8320, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10311024636030197\n",
      "@sample 959: tensor([[ 0.0426, -0.0396, -0.0232, -0.0088, -0.0084],\n",
      "        [ 0.0070,  0.0121, -0.0127,  0.0157, -0.0148],\n",
      "        [ 0.0120, -0.0373, -0.0137,  0.0180, -0.0075],\n",
      "        [-0.0012, -0.0339, -0.0011, -0.0088, -0.0192],\n",
      "        [-0.0078, -0.0021, -0.0238,  0.0072, -0.0192],\n",
      "        [ 0.0039, -0.0143, -0.0018, -0.0020,  0.0035],\n",
      "        [ 0.0121, -0.0119, -0.0042,  0.0012, -0.0217],\n",
      "        [ 0.0056,  0.0040, -0.0013, -0.0129,  0.0387]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0051, -0.0168,  0.0342, -0.0189, -0.0033],\n",
      "        [ 0.0504, -0.0029, -0.0158,  0.0150, -0.0065],\n",
      "        [ 0.0071, -0.0287, -0.0177,  0.0171, -0.0092],\n",
      "        [-0.0285, -0.0358, -0.0018,  0.0176, -0.0198],\n",
      "        [-0.0377, -0.0162, -0.0177, -0.0002, -0.0065],\n",
      "        [-0.0023, -0.0079, -0.0320,  0.0298,  0.0134],\n",
      "        [-0.0132, -0.0045,  0.0375, -0.0330, -0.0194],\n",
      "        [ 0.0055, -0.0281, -0.0443, -0.0123, -0.0066]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0137, grad_fn=<MinBackward1>), tensor(0.9179, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10167708992958069\n",
      "@sample 960: tensor([[ 0.0038, -0.0146, -0.0043,  0.0038, -0.0038],\n",
      "        [ 0.0131, -0.0054, -0.0028, -0.0052,  0.0138],\n",
      "        [ 0.0024, -0.0207, -0.0091,  0.0121,  0.0101],\n",
      "        [ 0.0033, -0.0068,  0.0236, -0.0149,  0.0006],\n",
      "        [-0.0167, -0.0022, -0.0152, -0.0202, -0.0150],\n",
      "        [-0.0001,  0.0004, -0.0034,  0.0009,  0.0010],\n",
      "        [ 0.0143, -0.0132, -0.0021,  0.0004, -0.0036],\n",
      "        [-0.0042,  0.0140,  0.0028, -0.0186,  0.0025]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-1.3159e-02, -9.4975e-04,  2.5308e-03,  1.7144e-02,  1.9118e-02],\n",
      "        [-1.5593e-02, -5.3958e-03, -3.2560e-02, -1.3407e-02,  1.2810e-02],\n",
      "        [ 1.6218e-02,  1.2361e-02,  1.9560e-02, -2.2627e-02, -4.8407e-03],\n",
      "        [-1.5494e-02, -6.5892e-03,  8.9571e-05,  4.1231e-02,  1.5522e-02],\n",
      "        [-6.8465e-03, -1.9030e-02,  1.5477e-02, -3.4574e-02, -2.5190e-02],\n",
      "        [-2.7835e-03,  2.8880e-03,  1.8335e-02,  1.1666e-02,  2.0839e-02],\n",
      "        [ 1.0334e-03,  5.3201e-03,  2.7408e-02, -7.1509e-03, -9.9924e-03],\n",
      "        [ 3.7345e-03,  1.4789e-02,  3.7959e-02,  1.4529e-02,  8.1706e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0200, grad_fn=<MinBackward1>), tensor(0.8680, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.0893331840634346\n",
      "@sample 961: tensor([[-0.0102,  0.0080,  0.0254, -0.0234, -0.0321],\n",
      "        [ 0.0099, -0.0251,  0.0099,  0.0143,  0.0089],\n",
      "        [ 0.0038, -0.0024, -0.0082,  0.0097, -0.0077],\n",
      "        [-0.0080, -0.0176, -0.0153,  0.0206, -0.0059],\n",
      "        [-0.0009,  0.0055, -0.0020, -0.0276, -0.0067],\n",
      "        [-0.0242,  0.0025, -0.0126,  0.0031, -0.0155],\n",
      "        [-0.0139,  0.0237,  0.0182, -0.0362,  0.0330],\n",
      "        [-0.0125, -0.0147,  0.0183,  0.0007,  0.0005]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0455,  0.0337, -0.0012,  0.0063, -0.0213],\n",
      "        [ 0.0178, -0.0096,  0.0210, -0.0245, -0.0193],\n",
      "        [-0.0032, -0.0047,  0.0069,  0.0054,  0.0172],\n",
      "        [ 0.0153,  0.0107,  0.0088, -0.0087, -0.0077],\n",
      "        [-0.0266, -0.0426,  0.0537, -0.0396, -0.0155],\n",
      "        [ 0.0197, -0.0176,  0.0554, -0.0320,  0.0043],\n",
      "        [-0.0038, -0.0002, -0.0251,  0.0134,  0.0326],\n",
      "        [ 0.0273,  0.0046,  0.0188, -0.0251,  0.0008]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0146, grad_fn=<MinBackward1>), tensor(0.8479, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09889186918735504\n",
      "@sample 962: tensor([[ 0.0045, -0.0019, -0.0063,  0.0191, -0.0024],\n",
      "        [-0.0008,  0.0033, -0.0072, -0.0061, -0.0015],\n",
      "        [-0.0315,  0.0057, -0.0109, -0.0057,  0.0114],\n",
      "        [-0.0242,  0.0182,  0.0087, -0.0079, -0.0012],\n",
      "        [-0.0051, -0.0249,  0.0211,  0.0046,  0.0226],\n",
      "        [-0.0170,  0.0428, -0.0314, -0.0203,  0.0319],\n",
      "        [ 0.0074,  0.0132,  0.0102, -0.0081, -0.0091],\n",
      "        [-0.0042, -0.0145, -0.0067,  0.0026,  0.0038]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0046, -0.0013, -0.0130, -0.0190,  0.0072],\n",
      "        [-0.0079, -0.0010,  0.0234,  0.0220,  0.0005],\n",
      "        [ 0.0287,  0.0162, -0.0176,  0.0106,  0.0168],\n",
      "        [ 0.0089, -0.0179,  0.0098, -0.0365, -0.0024],\n",
      "        [ 0.0224, -0.0131,  0.0021, -0.0071,  0.0009],\n",
      "        [ 0.0111, -0.0029, -0.0296, -0.0251,  0.0243],\n",
      "        [ 0.0006,  0.0328,  0.0140, -0.0267,  0.0124],\n",
      "        [-0.0054, -0.0088, -0.0195, -0.0108,  0.0076]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0153, grad_fn=<MinBackward1>), tensor(0.8574, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10372703522443771\n",
      "@sample 963: tensor([[-0.0164,  0.0222, -0.0028, -0.0188,  0.0155],\n",
      "        [ 0.0126, -0.0095, -0.0115, -0.0254,  0.0022],\n",
      "        [-0.0091, -0.0154, -0.0094,  0.0130, -0.0120],\n",
      "        [-0.0363,  0.0456,  0.0024, -0.0108,  0.0123],\n",
      "        [ 0.0039, -0.0316, -0.0109, -0.0007,  0.0058],\n",
      "        [-0.0038,  0.0202, -0.0166, -0.0373,  0.0359],\n",
      "        [-0.0088,  0.0101,  0.0135, -0.0491,  0.0140],\n",
      "        [-0.0415,  0.0266, -0.0140, -0.0029,  0.0044]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0079,  0.0226, -0.0116,  0.0064, -0.0089],\n",
      "        [ 0.0020, -0.0286,  0.0285, -0.0084,  0.0331],\n",
      "        [-0.0093,  0.0076,  0.0142, -0.0128,  0.0182],\n",
      "        [ 0.0056,  0.0161, -0.0535,  0.0697,  0.0026],\n",
      "        [-0.0152, -0.0033, -0.0007, -0.0172, -0.0060],\n",
      "        [-0.0038, -0.0304, -0.0529,  0.0349, -0.0360],\n",
      "        [-0.0305, -0.0390,  0.0057, -0.0148,  0.0091],\n",
      "        [ 0.0279, -0.0082,  0.0147,  0.0106, -0.0011]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0145, grad_fn=<MinBackward1>), tensor(0.8510, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10419284552335739\n",
      "@sample 964: tensor([[ 0.0054, -0.0207, -0.0198,  0.0152, -0.0189],\n",
      "        [-0.0004, -0.0084,  0.0070, -0.0027,  0.0007],\n",
      "        [-0.0077,  0.0302,  0.0116, -0.0200,  0.0464],\n",
      "        [-0.0186,  0.0103,  0.0061, -0.0169,  0.0227],\n",
      "        [-0.0170, -0.0320,  0.0334,  0.0084,  0.0117],\n",
      "        [-0.0018, -0.0048, -0.0072,  0.0024, -0.0111],\n",
      "        [-0.0079,  0.0227, -0.0152, -0.0227,  0.0131],\n",
      "        [ 0.0007, -0.0019,  0.0048,  0.0173, -0.0218]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0083,  0.0032,  0.0445, -0.0064,  0.0298],\n",
      "        [-0.0141,  0.0036, -0.0045,  0.0153,  0.0136],\n",
      "        [-0.0068, -0.0222, -0.0511,  0.0238, -0.0024],\n",
      "        [-0.0052, -0.0022, -0.0530,  0.0307,  0.0094],\n",
      "        [ 0.0008,  0.0145,  0.0066, -0.0007, -0.0054],\n",
      "        [ 0.0240,  0.0262, -0.0078, -0.0053, -0.0001],\n",
      "        [-0.0101, -0.0260, -0.0249,  0.0032, -0.0361],\n",
      "        [-0.0025, -0.0097, -0.0078,  0.0148, -0.0024]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0127, grad_fn=<MinBackward1>), tensor(0.8581, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09996072947978973\n",
      "@sample 965: tensor([[ 0.0097,  0.0229, -0.0043, -0.0144,  0.0169],\n",
      "        [-0.0138, -0.0285,  0.0028,  0.0098, -0.0344],\n",
      "        [ 0.0222, -0.0077, -0.0270,  0.0124, -0.0038],\n",
      "        [-0.0139,  0.0051,  0.0095,  0.0071, -0.0203],\n",
      "        [-0.0054, -0.0077, -0.0079, -0.0148,  0.0049],\n",
      "        [-0.0022, -0.0219, -0.0347,  0.0110, -0.0153],\n",
      "        [-0.0243,  0.0047,  0.0068,  0.0079, -0.0165],\n",
      "        [-0.0156, -0.0033, -0.0088,  0.0041, -0.0007]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-1.6894e-02, -1.3165e-02, -2.4991e-02,  2.3228e-02,  3.0640e-02],\n",
      "        [ 6.7358e-03, -1.3206e-03,  5.7485e-03, -1.3185e-03,  4.8193e-03],\n",
      "        [-2.0552e-03, -1.2281e-02,  2.1738e-02, -8.3424e-03, -2.4448e-02],\n",
      "        [-9.7709e-03, -6.7022e-03, -1.2825e-02,  9.3651e-03,  1.2318e-02],\n",
      "        [-2.1675e-02, -3.4498e-02,  9.8216e-03, -1.6969e-02,  8.9573e-03],\n",
      "        [ 1.4312e-04, -1.0508e-02,  4.0304e-02, -1.0346e-02,  9.7558e-03],\n",
      "        [-6.0342e-05,  7.4370e-03,  2.9569e-02,  1.3654e-02,  9.3798e-03],\n",
      "        [ 1.3719e-02, -1.9454e-03,  3.0253e-03,  2.3423e-02,  2.3669e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0147, grad_fn=<MinBackward1>), tensor(0.8169, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10297061502933502\n",
      "@sample 966: tensor([[ 0.0070,  0.0060, -0.0076,  0.0231, -0.0116],\n",
      "        [ 0.0125, -0.0064, -0.0149, -0.0003, -0.0091],\n",
      "        [ 0.0148,  0.0148,  0.0059, -0.0084, -0.0174],\n",
      "        [ 0.0052,  0.0147,  0.0306,  0.0113, -0.0110],\n",
      "        [ 0.0079, -0.0104, -0.0067,  0.0223,  0.0096],\n",
      "        [-0.0122, -0.0159, -0.0062,  0.0249, -0.0023],\n",
      "        [ 0.0070,  0.0286,  0.0223, -0.0285,  0.0048],\n",
      "        [-0.0064, -0.0055,  0.0007,  0.0233, -0.0005]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0132,  0.0171, -0.0017, -0.0040, -0.0099],\n",
      "        [ 0.0019,  0.0032, -0.0029,  0.0051,  0.0101],\n",
      "        [-0.0223, -0.0106, -0.0159,  0.0111,  0.0125],\n",
      "        [ 0.0117,  0.0062,  0.0468, -0.0106, -0.0232],\n",
      "        [ 0.0129,  0.0253,  0.0210, -0.0323, -0.0234],\n",
      "        [ 0.0318,  0.0111,  0.0238,  0.0193,  0.0003],\n",
      "        [-0.0721,  0.0129, -0.0876,  0.0422, -0.0129],\n",
      "        [ 0.0130,  0.0155,  0.0166, -0.0297, -0.0054]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0130, grad_fn=<MinBackward1>), tensor(0.8396, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09042356163263321\n",
      "@sample 967: tensor([[-2.3470e-02,  6.8986e-03,  1.0231e-02, -1.6634e-02,  1.2098e-02],\n",
      "        [-6.9987e-03, -1.4424e-02, -2.0793e-02,  2.1507e-02,  4.3976e-03],\n",
      "        [-3.7666e-03,  1.8199e-02, -4.3282e-03, -1.0670e-02, -5.4266e-03],\n",
      "        [ 1.2049e-02,  3.5335e-03,  8.5284e-03,  2.4275e-02, -1.1156e-03],\n",
      "        [ 2.3119e-03,  4.2157e-03, -2.9453e-03,  5.0053e-05, -1.0701e-02],\n",
      "        [ 2.6115e-03, -6.9644e-03, -1.7742e-02, -1.1438e-02, -3.6895e-05],\n",
      "        [-5.8236e-03,  1.5840e-04, -7.2329e-04, -1.7842e-02, -1.0362e-02],\n",
      "        [ 1.0859e-02,  9.3047e-03, -9.0385e-03,  9.7189e-03, -1.3604e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0096, -0.0041,  0.0089, -0.0126,  0.0029],\n",
      "        [ 0.0213, -0.0001, -0.0341, -0.0125,  0.0142],\n",
      "        [-0.0096,  0.0324, -0.0025, -0.0033,  0.0155],\n",
      "        [ 0.0219,  0.0189,  0.0048,  0.0110, -0.0024],\n",
      "        [-0.0075,  0.0189, -0.0049,  0.0266,  0.0121],\n",
      "        [-0.0227, -0.0034,  0.0174, -0.0117,  0.0201],\n",
      "        [-0.0341, -0.0262,  0.0147, -0.0200,  0.0036],\n",
      "        [ 0.0246,  0.0141,  0.0308, -0.0008, -0.0023]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0146, grad_fn=<MinBackward1>), tensor(0.8926, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09663980454206467\n",
      "@sample 968: tensor([[ 0.0167,  0.0540, -0.0008, -0.0078,  0.0001],\n",
      "        [ 0.0506,  0.0219,  0.0065,  0.0091, -0.0158],\n",
      "        [ 0.0068,  0.0303,  0.0226, -0.0433,  0.0011],\n",
      "        [-0.0010,  0.0010, -0.0109,  0.0019, -0.0237],\n",
      "        [ 0.0020,  0.0188,  0.0145,  0.0095, -0.0205],\n",
      "        [ 0.0025,  0.0065, -0.0013,  0.0026,  0.0052],\n",
      "        [ 0.0139, -0.0166,  0.0190, -0.0299,  0.0065],\n",
      "        [ 0.0237, -0.0093, -0.0117, -0.0121,  0.0140]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0167, -0.0267, -0.0422,  0.0272,  0.0106],\n",
      "        [ 0.0204,  0.0760,  0.0153, -0.0112, -0.0203],\n",
      "        [-0.0161,  0.0144, -0.0159,  0.0160, -0.0168],\n",
      "        [ 0.0065, -0.0091,  0.0021, -0.0171,  0.0013],\n",
      "        [ 0.0197,  0.0252, -0.0005, -0.0148,  0.0236],\n",
      "        [-0.0026,  0.0095, -0.0176,  0.0085,  0.0087],\n",
      "        [-0.0210, -0.0174, -0.0107,  0.0169,  0.0235],\n",
      "        [-0.0020, -0.0026,  0.0008,  0.0216, -0.0184]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0076, grad_fn=<MinBackward1>), tensor(0.9014, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10218708217144012\n",
      "@sample 969: tensor([[-0.0086, -0.0075,  0.0218, -0.0002,  0.0151],\n",
      "        [ 0.0081,  0.0094,  0.0179, -0.0208, -0.0035],\n",
      "        [-0.0040,  0.0241, -0.0037,  0.0105, -0.0324],\n",
      "        [-0.0200,  0.0086,  0.0203,  0.0099,  0.0149],\n",
      "        [-0.0073, -0.0019,  0.0202, -0.0069, -0.0309],\n",
      "        [-0.0002,  0.0244,  0.0161, -0.0384,  0.0092],\n",
      "        [-0.0105, -0.0274,  0.0178,  0.0134, -0.0137],\n",
      "        [ 0.0431, -0.0254, -0.0167,  0.0084,  0.0034]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0375, -0.0189, -0.0129,  0.0235,  0.0086],\n",
      "        [-0.0176, -0.0118, -0.0215, -0.0210, -0.0084],\n",
      "        [-0.0339,  0.0126, -0.0204,  0.0146,  0.0073],\n",
      "        [-0.0017, -0.0026, -0.0217,  0.0042,  0.0181],\n",
      "        [-0.0161,  0.0067, -0.0114,  0.0062,  0.0057],\n",
      "        [-0.0125, -0.0260,  0.0004,  0.0199, -0.0110],\n",
      "        [ 0.0321, -0.0160,  0.0596, -0.0137, -0.0063],\n",
      "        [-0.0356,  0.0120, -0.0153, -0.0107, -0.0216]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0125, grad_fn=<MinBackward1>), tensor(0.8787, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09379235655069351\n",
      "@sample 970: tensor([[-0.0148,  0.0440,  0.0128, -0.0487,  0.0281],\n",
      "        [-0.0309, -0.0063,  0.0075,  0.0444, -0.0118],\n",
      "        [-0.0179, -0.0075, -0.0025, -0.0002,  0.0089],\n",
      "        [-0.0319, -0.0066, -0.0027,  0.0089, -0.0163],\n",
      "        [-0.0230,  0.0090,  0.0034, -0.0207, -0.0073],\n",
      "        [-0.0108,  0.0193, -0.0067, -0.0123, -0.0173],\n",
      "        [-0.0184,  0.0064, -0.0275, -0.0278,  0.0264],\n",
      "        [-0.0281, -0.0008, -0.0090, -0.0013,  0.0115]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0374, -0.0022, -0.0903,  0.0314,  0.0246],\n",
      "        [ 0.0267,  0.0239,  0.0101, -0.0075, -0.0121],\n",
      "        [ 0.0166, -0.0299,  0.0155,  0.0108, -0.0300],\n",
      "        [-0.0094, -0.0033,  0.0265, -0.0279, -0.0088],\n",
      "        [-0.0233, -0.0046, -0.0009, -0.0063, -0.0376],\n",
      "        [-0.0212,  0.0041, -0.0111,  0.0201,  0.0015],\n",
      "        [ 0.0050, -0.0097, -0.0170, -0.0312,  0.0050],\n",
      "        [ 0.0112, -0.0217,  0.0479, -0.0227, -0.0027]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0054, grad_fn=<MinBackward1>), tensor(0.8471, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10697237402200699\n",
      "@sample 971: tensor([[ 2.7051e-02, -1.6765e-02, -1.2259e-02, -7.3893e-03,  6.1055e-03],\n",
      "        [-9.0535e-03,  1.0411e-02,  4.0098e-03, -4.4685e-03, -6.8961e-03],\n",
      "        [-1.1147e-05,  8.9503e-03, -1.2598e-02,  1.1037e-02, -8.7569e-03],\n",
      "        [-1.5662e-02, -2.3653e-02,  7.5516e-03, -6.9057e-03,  2.7163e-02],\n",
      "        [-2.2561e-03, -1.7848e-03, -1.8862e-02,  1.4228e-02, -1.5514e-02],\n",
      "        [ 3.7535e-03, -3.1358e-02,  1.3622e-02, -7.4733e-03,  5.8083e-03],\n",
      "        [-4.3359e-03,  1.7930e-02,  6.9565e-03, -1.1285e-02,  9.4246e-03],\n",
      "        [ 2.2291e-02,  3.7829e-03, -3.3713e-03, -1.8841e-02,  1.2172e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0125, -0.0292, -0.0005, -0.0083,  0.0230],\n",
      "        [-0.0053,  0.0162, -0.0362,  0.0035,  0.0095],\n",
      "        [-0.0087, -0.0334,  0.0185, -0.0179, -0.0152],\n",
      "        [ 0.0057, -0.0172,  0.0148, -0.0101, -0.0203],\n",
      "        [ 0.0278,  0.0046, -0.0082, -0.0063,  0.0129],\n",
      "        [ 0.0098, -0.0071,  0.0018, -0.0077, -0.0029],\n",
      "        [-0.0067, -0.0274, -0.0328,  0.0295,  0.0003],\n",
      "        [-0.0402, -0.0149, -0.0182,  0.0319, -0.0153]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0103, grad_fn=<MinBackward1>), tensor(0.8531, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09490100294351578\n",
      "@sample 972: tensor([[-0.0014, -0.0174,  0.0095,  0.0147,  0.0001],\n",
      "        [ 0.0077,  0.0104, -0.0234,  0.0579, -0.0063],\n",
      "        [-0.0038, -0.0019,  0.0064, -0.0175,  0.0090],\n",
      "        [ 0.0194, -0.0059, -0.0010,  0.0043, -0.0131],\n",
      "        [ 0.0168,  0.0512,  0.0425, -0.0362,  0.0501],\n",
      "        [-0.0050,  0.0103, -0.0144,  0.0447, -0.0465],\n",
      "        [-0.0018,  0.0098, -0.0166,  0.0141, -0.0005],\n",
      "        [-0.0052, -0.0104,  0.0123, -0.0128,  0.0059]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0034,  0.0124,  0.0163, -0.0097,  0.0011],\n",
      "        [ 0.0381,  0.0331,  0.0431, -0.0023, -0.0262],\n",
      "        [-0.0220,  0.0050,  0.0340,  0.0193, -0.0316],\n",
      "        [-0.0024,  0.0082,  0.0170,  0.0117,  0.0018],\n",
      "        [-0.0070, -0.0176, -0.0347,  0.0298, -0.0138],\n",
      "        [ 0.0213,  0.0134, -0.0129, -0.0218,  0.0173],\n",
      "        [-0.0316, -0.0102,  0.0255,  0.0071, -0.0011],\n",
      "        [ 0.0059,  0.0068,  0.0423, -0.0050, -0.0103]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0131, grad_fn=<MinBackward1>), tensor(0.8818, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09962759912014008\n",
      "@sample 973: tensor([[ 0.0185, -0.0318, -0.0093, -0.0101,  0.0340],\n",
      "        [-0.0342, -0.0015,  0.0009, -0.0273,  0.0169],\n",
      "        [ 0.0241, -0.0009,  0.0060,  0.0034, -0.0279],\n",
      "        [ 0.0150,  0.0073,  0.0056,  0.0013, -0.0078],\n",
      "        [-0.0068, -0.0254, -0.0258, -0.0032,  0.0025],\n",
      "        [ 0.0117, -0.0190,  0.0207,  0.0104, -0.0289],\n",
      "        [-0.0181, -0.0242, -0.0114, -0.0116, -0.0049],\n",
      "        [-0.0215,  0.0073, -0.0184, -0.0079,  0.0168]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0032, -0.0285,  0.0220,  0.0222, -0.0042],\n",
      "        [ 0.0140,  0.0014, -0.0040,  0.0003, -0.0199],\n",
      "        [-0.0064, -0.0234,  0.0511, -0.0253, -0.0468],\n",
      "        [-0.0077, -0.0080, -0.0339,  0.0031,  0.0075],\n",
      "        [ 0.0002, -0.0463, -0.0092, -0.0325,  0.0218],\n",
      "        [ 0.0062,  0.0058,  0.0470, -0.0134, -0.0151],\n",
      "        [-0.0125, -0.0555,  0.0191, -0.0023,  0.0153],\n",
      "        [-0.0159, -0.0379, -0.0153,  0.0078,  0.0049]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0190, grad_fn=<MinBackward1>), tensor(0.8892, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11569468677043915\n",
      "@sample 974: tensor([[ 0.0087, -0.0119, -0.0053, -0.0066,  0.0063],\n",
      "        [-0.0120,  0.0317,  0.0494, -0.0090,  0.0496],\n",
      "        [ 0.0293, -0.0146, -0.0055,  0.0084, -0.0248],\n",
      "        [ 0.0084,  0.0078,  0.0090, -0.0005, -0.0011],\n",
      "        [ 0.0240,  0.0115, -0.0142,  0.0204, -0.0254],\n",
      "        [ 0.0244,  0.0373,  0.0018, -0.0100,  0.0072],\n",
      "        [-0.0225,  0.0168,  0.0251, -0.0110,  0.0107],\n",
      "        [-0.0607,  0.0064,  0.0121, -0.0286,  0.0210]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0152,  0.0160, -0.0138,  0.0093,  0.0104],\n",
      "        [ 0.0118,  0.0028, -0.0462, -0.0298, -0.0073],\n",
      "        [ 0.0099, -0.0243,  0.0090,  0.0141,  0.0329],\n",
      "        [-0.0081, -0.0202, -0.0014,  0.0157,  0.0063],\n",
      "        [ 0.0046,  0.0088,  0.0163, -0.0319, -0.0220],\n",
      "        [-0.0377, -0.0308, -0.0709,  0.0473,  0.0053],\n",
      "        [ 0.0084,  0.0096,  0.0140, -0.0180, -0.0219],\n",
      "        [-0.0220, -0.0535,  0.0274, -0.0118, -0.0317]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0157, grad_fn=<MinBackward1>), tensor(0.8531, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1046186313033104\n",
      "@sample 975: tensor([[ 0.0126,  0.0158, -0.0116,  0.0175,  0.0090],\n",
      "        [ 0.0017, -0.0131,  0.0352, -0.0172,  0.0184],\n",
      "        [ 0.0204, -0.0064, -0.0207,  0.0178, -0.0101],\n",
      "        [-0.0063,  0.0193,  0.0008,  0.0097, -0.0026],\n",
      "        [ 0.0145,  0.0191,  0.0064, -0.0023,  0.0023],\n",
      "        [-0.0192,  0.0125,  0.0232,  0.0237,  0.0040],\n",
      "        [-0.0109,  0.0139, -0.0012,  0.0201,  0.0098],\n",
      "        [-0.0099,  0.0100, -0.0098,  0.0037, -0.0024]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0053,  0.0282, -0.0460,  0.0187,  0.0140],\n",
      "        [-0.0222, -0.0046,  0.0024, -0.0107,  0.0081],\n",
      "        [ 0.0277,  0.0065,  0.0452,  0.0105, -0.0133],\n",
      "        [-0.0396, -0.0072, -0.0323, -0.0069, -0.0413],\n",
      "        [ 0.0046,  0.0028, -0.0243,  0.0161,  0.0343],\n",
      "        [-0.0054,  0.0305, -0.0600,  0.0074, -0.0157],\n",
      "        [-0.0274,  0.0229, -0.0072, -0.0010,  0.0080],\n",
      "        [ 0.0228,  0.0288,  0.0114,  0.0069,  0.0231]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0198, grad_fn=<MinBackward1>), tensor(0.8643, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09939157217741013\n",
      "@sample 976: tensor([[ 0.0071,  0.0097, -0.0254, -0.0107, -0.0157],\n",
      "        [ 0.0180, -0.0156, -0.0119,  0.0171, -0.0225],\n",
      "        [ 0.0311,  0.0352,  0.0126, -0.0555,  0.0404],\n",
      "        [-0.0053, -0.0154, -0.0267,  0.0153,  0.0065],\n",
      "        [-0.0121, -0.0146, -0.0161,  0.0147,  0.0055],\n",
      "        [-0.0022,  0.0077, -0.0120, -0.0083,  0.0349],\n",
      "        [-0.0096, -0.0013,  0.0091, -0.0015,  0.0039],\n",
      "        [-0.0025,  0.0273, -0.0099, -0.0215,  0.0110]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-4.2394e-02,  3.0036e-03, -5.6177e-05,  7.7378e-03,  6.3278e-03],\n",
      "        [ 3.3212e-02, -8.9414e-03,  2.1769e-02, -2.3689e-02,  2.0652e-03],\n",
      "        [-3.5216e-02, -3.9426e-02, -1.5221e-02,  3.4164e-02, -2.4972e-02],\n",
      "        [-4.3725e-03, -1.4800e-02, -4.3731e-03, -7.0781e-03, -7.5294e-03],\n",
      "        [-3.4834e-03,  9.0741e-03, -1.9329e-02,  8.3957e-03,  8.2610e-03],\n",
      "        [ 3.6172e-02, -1.7654e-03,  1.5016e-02, -1.8901e-02, -2.5280e-02],\n",
      "        [-8.0506e-03, -1.9971e-02,  2.5331e-02, -2.0889e-02, -2.4627e-02],\n",
      "        [-1.0341e-02, -3.0078e-02, -5.9710e-02,  2.3128e-02, -1.2153e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0119, grad_fn=<MinBackward1>), tensor(0.8638, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09936624020338058\n",
      "@sample 977: tensor([[ 0.0130, -0.0086,  0.0001,  0.0028, -0.0075],\n",
      "        [-0.0272, -0.0066, -0.0041,  0.0293, -0.0177],\n",
      "        [ 0.0267,  0.0022, -0.0034, -0.0206,  0.0198],\n",
      "        [-0.0035,  0.0016,  0.0014,  0.0055,  0.0027],\n",
      "        [-0.0151,  0.0096,  0.0172, -0.0116,  0.0364],\n",
      "        [ 0.0054,  0.0171,  0.0163, -0.0160,  0.0083],\n",
      "        [-0.0058, -0.0123,  0.0127,  0.0021,  0.0089],\n",
      "        [-0.0042, -0.0172,  0.0158, -0.0059,  0.0215]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0020,  0.0170, -0.0319,  0.0108, -0.0176],\n",
      "        [ 0.0283, -0.0064,  0.0395, -0.0293, -0.0115],\n",
      "        [-0.0175, -0.0249, -0.0232, -0.0166, -0.0389],\n",
      "        [ 0.0039,  0.0119,  0.0206, -0.0162, -0.0020],\n",
      "        [ 0.0050,  0.0252, -0.0041, -0.0375, -0.0198],\n",
      "        [-0.0145,  0.0108, -0.0529,  0.0238, -0.0398],\n",
      "        [-0.0176, -0.0295, -0.0280, -0.0059, -0.0190],\n",
      "        [ 0.0184, -0.0058,  0.0178,  0.0243, -0.0370]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0164, grad_fn=<MinBackward1>), tensor(0.8283, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10283681750297546\n",
      "@sample 978: tensor([[ 0.0026, -0.0107, -0.0203,  0.0188, -0.0196],\n",
      "        [-0.0063, -0.0035,  0.0089,  0.0126, -0.0133],\n",
      "        [ 0.0061, -0.0224, -0.0192,  0.0210, -0.0150],\n",
      "        [-0.0030, -0.0126, -0.0056,  0.0058, -0.0150],\n",
      "        [-0.0159, -0.0171, -0.0104,  0.0183,  0.0013],\n",
      "        [ 0.0269,  0.0210, -0.0022, -0.0289,  0.0040],\n",
      "        [ 0.0045,  0.0287, -0.0087,  0.0085, -0.0216],\n",
      "        [ 0.0174, -0.0060, -0.0042,  0.0101, -0.0055]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0150, -0.0228, -0.0122,  0.0093,  0.0293],\n",
      "        [ 0.0339,  0.0111,  0.0199, -0.0245,  0.0083],\n",
      "        [-0.0162,  0.0032,  0.0163, -0.0072, -0.0163],\n",
      "        [-0.0023, -0.0152,  0.0100, -0.0052,  0.0279],\n",
      "        [ 0.0018, -0.0125, -0.0252, -0.0081,  0.0208],\n",
      "        [-0.0428,  0.0129, -0.0434,  0.0383,  0.0263],\n",
      "        [ 0.0055,  0.0378, -0.0169, -0.0069, -0.0159],\n",
      "        [ 0.0093, -0.0049,  0.0293, -0.0265, -0.0131]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0120, grad_fn=<MinBackward1>), tensor(0.8486, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09768842160701752\n",
      "@sample 979: tensor([[-1.3775e-02, -7.1967e-03,  1.6429e-02, -1.7656e-02, -2.0929e-02],\n",
      "        [-2.6186e-02,  1.0934e-02, -2.7191e-02,  1.4228e-02,  1.2304e-02],\n",
      "        [-6.5442e-03, -1.0735e-02, -9.7553e-03,  1.4929e-02, -1.4108e-02],\n",
      "        [ 1.3879e-02,  3.1141e-03, -5.6809e-03, -5.4830e-03, -1.4992e-02],\n",
      "        [-2.9807e-03, -3.5817e-03, -3.3422e-02,  2.6377e-02, -2.8495e-02],\n",
      "        [ 1.5517e-02, -1.8865e-02, -8.3265e-03,  1.4492e-02, -4.4561e-03],\n",
      "        [ 1.5320e-02,  4.7565e-05, -3.1474e-03,  1.0092e-02, -7.2572e-03],\n",
      "        [ 2.0143e-02,  6.2613e-03,  1.3563e-02,  1.7136e-02, -3.3404e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0212,  0.0092,  0.0146, -0.0151,  0.0277],\n",
      "        [ 0.0174,  0.0073, -0.0046,  0.0046,  0.0195],\n",
      "        [ 0.0252,  0.0167,  0.0253, -0.0078,  0.0127],\n",
      "        [ 0.0095,  0.0052,  0.0073,  0.0021, -0.0002],\n",
      "        [ 0.0331, -0.0088,  0.0334, -0.0168,  0.0103],\n",
      "        [ 0.0066,  0.0026,  0.0110,  0.0063,  0.0022],\n",
      "        [ 0.0038,  0.0033,  0.0084, -0.0132, -0.0028],\n",
      "        [-0.0008,  0.0470, -0.0066, -0.0257, -0.0089]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0129, grad_fn=<MinBackward1>), tensor(0.8392, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09252316504716873\n",
      "@sample 980: tensor([[-1.0529e-02,  2.8464e-04,  3.4241e-04,  1.2043e-02, -2.0234e-02],\n",
      "        [-1.9288e-02,  1.0896e-02,  1.4495e-02,  4.5164e-03,  4.4022e-03],\n",
      "        [ 1.2029e-02, -2.6185e-02, -1.1495e-02,  1.7108e-02, -5.5751e-04],\n",
      "        [ 3.5478e-02, -3.4245e-03, -3.5481e-02, -8.5974e-03, -1.8458e-02],\n",
      "        [-4.7578e-03,  1.3728e-02,  1.2977e-02, -2.2449e-02, -4.8963e-03],\n",
      "        [-5.9835e-05, -8.2871e-04,  1.6490e-02,  3.7089e-02, -3.5021e-02],\n",
      "        [-1.4309e-02, -1.9335e-02,  1.9540e-02,  3.3151e-02, -4.3354e-03],\n",
      "        [ 2.0613e-02, -1.8965e-02,  1.5051e-02,  2.3525e-02, -9.7696e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0048,  0.0021,  0.0300, -0.0359,  0.0044],\n",
      "        [ 0.0048,  0.0141, -0.0140, -0.0053,  0.0206],\n",
      "        [-0.0132,  0.0016, -0.0015,  0.0004, -0.0133],\n",
      "        [-0.0101, -0.0010, -0.0409, -0.0067,  0.0126],\n",
      "        [ 0.0018, -0.0001, -0.0323, -0.0406, -0.0347],\n",
      "        [ 0.0089,  0.0283, -0.0085, -0.0225, -0.0196],\n",
      "        [ 0.0347,  0.0346, -0.0038,  0.0078, -0.0151],\n",
      "        [ 0.0109,  0.0345, -0.0096, -0.0262, -0.0037]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0085, grad_fn=<MinBackward1>), tensor(0.8153, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09444402903318405\n",
      "@sample 981: tensor([[-0.0309, -0.0047, -0.0027,  0.0108, -0.0004],\n",
      "        [ 0.0009, -0.0313,  0.0041,  0.0204, -0.0166],\n",
      "        [ 0.0146, -0.0166, -0.0067,  0.0309, -0.0085],\n",
      "        [ 0.0012, -0.0060, -0.0034, -0.0138,  0.0138],\n",
      "        [-0.0109,  0.0228,  0.0029, -0.0095,  0.0074],\n",
      "        [ 0.0108,  0.0025,  0.0157,  0.0095,  0.0025],\n",
      "        [ 0.0117,  0.0098,  0.0255,  0.0003, -0.0291],\n",
      "        [-0.0114, -0.0058, -0.0201, -0.0185,  0.0212]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0425, -0.0181,  0.0331, -0.0283,  0.0048],\n",
      "        [ 0.0026, -0.0040,  0.0161, -0.0298, -0.0272],\n",
      "        [-0.0097,  0.0045, -0.0055,  0.0026,  0.0324],\n",
      "        [-0.0164, -0.0056, -0.0594,  0.0181, -0.0022],\n",
      "        [ 0.0106,  0.0575, -0.0328,  0.0106,  0.0211],\n",
      "        [ 0.0200,  0.0457, -0.0050, -0.0117, -0.0184],\n",
      "        [-0.0303,  0.0328, -0.0417, -0.0197, -0.0329],\n",
      "        [-0.0052, -0.0006, -0.0186, -0.0065,  0.0058]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0143, grad_fn=<MinBackward1>), tensor(0.8710, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09689333289861679\n",
      "@sample 982: tensor([[-0.0051, -0.0178, -0.0024,  0.0072, -0.0086],\n",
      "        [-0.0045,  0.0363, -0.0003, -0.0139, -0.0101],\n",
      "        [ 0.0148,  0.0430,  0.0589, -0.0237,  0.0208],\n",
      "        [ 0.0226, -0.0179, -0.0344,  0.0424,  0.0051],\n",
      "        [-0.0025,  0.0056,  0.0013, -0.0067, -0.0025],\n",
      "        [ 0.0039, -0.0063, -0.0035,  0.0330,  0.0035],\n",
      "        [ 0.0175, -0.0172, -0.0089, -0.0034, -0.0207],\n",
      "        [ 0.0077, -0.0104, -0.0108,  0.0038, -0.0153]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0304, -0.0255,  0.0292, -0.0165, -0.0211],\n",
      "        [ 0.0171, -0.0008, -0.0382,  0.0052, -0.0077],\n",
      "        [-0.0207, -0.0421, -0.0514,  0.0599, -0.0253],\n",
      "        [ 0.0358,  0.0654,  0.0256,  0.0017,  0.0156],\n",
      "        [ 0.0014,  0.0242, -0.0103, -0.0104, -0.0020],\n",
      "        [ 0.0451,  0.0216,  0.0536,  0.0041, -0.0082],\n",
      "        [-0.0055,  0.0045,  0.0008, -0.0239,  0.0092],\n",
      "        [-0.0008, -0.0095, -0.0065, -0.0100,  0.0026]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0170, grad_fn=<MinBackward1>), tensor(0.8568, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10725545138120651\n",
      "@sample 983: tensor([[-1.8525e-02, -2.4444e-02,  1.9267e-02,  4.8072e-03,  2.5898e-02],\n",
      "        [-2.7417e-02, -2.2925e-02,  8.9236e-03,  1.2983e-02,  5.7961e-03],\n",
      "        [-1.4674e-02, -2.2812e-02, -1.3842e-02,  3.9212e-04,  1.3193e-02],\n",
      "        [ 1.4353e-02, -4.6930e-03,  4.7734e-03,  1.1234e-03, -1.0983e-02],\n",
      "        [-2.4008e-02,  1.1859e-02,  2.2171e-02, -1.7744e-02, -9.9585e-03],\n",
      "        [-2.0682e-02, -6.2422e-03, -1.3023e-03, -8.1840e-04, -2.1885e-04],\n",
      "        [-2.4094e-03, -1.9675e-02, -3.4016e-03,  4.4301e-03,  7.2135e-03],\n",
      "        [-2.5885e-02, -1.4502e-03,  1.3340e-02,  4.1842e-05,  1.3493e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0245,  0.0085, -0.0182,  0.0341, -0.0127],\n",
      "        [ 0.0126, -0.0151,  0.0285, -0.0271, -0.0044],\n",
      "        [ 0.0098,  0.0277,  0.0065, -0.0012, -0.0032],\n",
      "        [ 0.0056,  0.0241,  0.0052, -0.0219,  0.0154],\n",
      "        [-0.0293,  0.0084, -0.0371, -0.0073,  0.0063],\n",
      "        [ 0.0033, -0.0215,  0.0056, -0.0197, -0.0100],\n",
      "        [ 0.0136,  0.0122,  0.0172,  0.0015,  0.0107],\n",
      "        [-0.0030, -0.0007,  0.0222, -0.0193, -0.0222]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0176, grad_fn=<MinBackward1>), tensor(0.8603, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.0924522653222084\n",
      "@sample 984: tensor([[ 0.0056, -0.0153,  0.0127, -0.0067,  0.0020],\n",
      "        [ 0.0307, -0.0044,  0.0227,  0.0010, -0.0173],\n",
      "        [-0.0023,  0.0023,  0.0467, -0.0334,  0.0284],\n",
      "        [ 0.0078, -0.0069, -0.0217, -0.0186,  0.0353],\n",
      "        [ 0.0202, -0.0229,  0.0211, -0.0222,  0.0009],\n",
      "        [ 0.0044,  0.0358,  0.0470, -0.0182,  0.0055],\n",
      "        [-0.0032,  0.0284,  0.0001, -0.0283,  0.0071],\n",
      "        [ 0.0257, -0.0002,  0.0263, -0.0258, -0.0038]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0126,  0.0564, -0.0157,  0.0147,  0.0254],\n",
      "        [ 0.0215, -0.0050,  0.0188,  0.0340, -0.0030],\n",
      "        [-0.0169, -0.0198, -0.0105,  0.0360, -0.0127],\n",
      "        [ 0.0047, -0.0261,  0.0181, -0.0192,  0.0081],\n",
      "        [-0.0259,  0.0317,  0.0091, -0.0081, -0.0039],\n",
      "        [-0.0306,  0.0468, -0.0254,  0.0546,  0.0139],\n",
      "        [-0.0462, -0.0312, -0.0588,  0.0153, -0.0272],\n",
      "        [-0.0124, -0.0278, -0.0183, -0.0194,  0.0064]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0136, grad_fn=<MinBackward1>), tensor(0.8647, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09780815243721008\n",
      "@sample 985: tensor([[ 0.0207, -0.0094,  0.0044,  0.0066, -0.0043],\n",
      "        [-0.0002, -0.0219,  0.0025, -0.0283,  0.0093],\n",
      "        [ 0.0035,  0.0462,  0.0152, -0.0183,  0.0156],\n",
      "        [-0.0033,  0.0101,  0.0081, -0.0197,  0.0153],\n",
      "        [ 0.0161, -0.0003, -0.0157, -0.0184, -0.0007],\n",
      "        [ 0.0018,  0.0305, -0.0008, -0.0198,  0.0203],\n",
      "        [-0.0063, -0.0113, -0.0009,  0.0073, -0.0049],\n",
      "        [ 0.0117, -0.0102,  0.0108,  0.0006,  0.0049]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0027, -0.0072, -0.0214,  0.0110, -0.0216],\n",
      "        [-0.0109, -0.0177,  0.0077, -0.0061,  0.0039],\n",
      "        [-0.0418, -0.0432, -0.1022,  0.0871,  0.0145],\n",
      "        [-0.0098, -0.0019, -0.0528,  0.0447,  0.0347],\n",
      "        [-0.0262,  0.0076, -0.0111,  0.0127,  0.0299],\n",
      "        [-0.0186, -0.0260, -0.0583,  0.0211,  0.0096],\n",
      "        [ 0.0173,  0.0139,  0.0031, -0.0056,  0.0030],\n",
      "        [-0.0095,  0.0053,  0.0091, -0.0115,  0.0135]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0141, grad_fn=<MinBackward1>), tensor(0.8491, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09050853550434113\n",
      "@sample 986: tensor([[-0.0170, -0.0129, -0.0087, -0.0304,  0.0299],\n",
      "        [ 0.0029,  0.0021,  0.0213,  0.0092,  0.0021],\n",
      "        [ 0.0146, -0.0141, -0.0030, -0.0156, -0.0060],\n",
      "        [-0.0172, -0.0163,  0.0003, -0.0116,  0.0179],\n",
      "        [ 0.0227, -0.0049,  0.0502, -0.0027,  0.0046],\n",
      "        [ 0.0164, -0.0250,  0.0058, -0.0212,  0.0014],\n",
      "        [-0.0162, -0.0098,  0.0071, -0.0066, -0.0204],\n",
      "        [ 0.0101, -0.0255,  0.0238, -0.0228,  0.0075]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0101, -0.0144,  0.0231, -0.0262,  0.0102],\n",
      "        [ 0.0077,  0.0101, -0.0067, -0.0035, -0.0012],\n",
      "        [-0.0198, -0.0367,  0.0171,  0.0009,  0.0198],\n",
      "        [-0.0202, -0.0022, -0.0032, -0.0088, -0.0210],\n",
      "        [-0.0225, -0.0019, -0.0125,  0.0343, -0.0246],\n",
      "        [-0.0047, -0.0253, -0.0009, -0.0005,  0.0269],\n",
      "        [ 0.0148, -0.0297,  0.0437, -0.0246,  0.0257],\n",
      "        [-0.0567,  0.0076,  0.0125,  0.0353, -0.0012]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0173, grad_fn=<MinBackward1>), tensor(0.8380, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09983377158641815\n",
      "@sample 987: tensor([[-0.0215,  0.0290,  0.0103, -0.0157,  0.0027],\n",
      "        [-0.0056,  0.0078,  0.0078, -0.0402,  0.0049],\n",
      "        [-0.0408,  0.0224,  0.0358,  0.0011, -0.0130],\n",
      "        [-0.0069, -0.0216, -0.0044, -0.0306,  0.0170],\n",
      "        [ 0.0134, -0.0009,  0.0134, -0.0033,  0.0118],\n",
      "        [-0.0192,  0.0170,  0.0053, -0.0161,  0.0026],\n",
      "        [-0.0078,  0.0107,  0.0295, -0.0187,  0.0039],\n",
      "        [-0.0171,  0.0055,  0.0244, -0.0102, -0.0020]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0135,  0.0342, -0.0176,  0.0073,  0.0026],\n",
      "        [-0.0156, -0.0135,  0.0205, -0.0261, -0.0061],\n",
      "        [ 0.0142, -0.0098, -0.0140, -0.0218,  0.0009],\n",
      "        [ 0.0157, -0.0584,  0.0495,  0.0044, -0.0334],\n",
      "        [-0.0094, -0.0087, -0.0144,  0.0048,  0.0121],\n",
      "        [-0.0005,  0.0121,  0.0021, -0.0066, -0.0013],\n",
      "        [ 0.0070,  0.0192,  0.0046,  0.0109,  0.0033],\n",
      "        [-0.0102,  0.0321, -0.0026,  0.0084, -0.0189]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0129, grad_fn=<MinBackward1>), tensor(0.8484, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09185466915369034\n",
      "@sample 988: tensor([[-0.0246, -0.0333, -0.0189,  0.0067, -0.0157],\n",
      "        [-0.0160, -0.0047, -0.0289, -0.0123, -0.0192],\n",
      "        [ 0.0017, -0.0253, -0.0128,  0.0039, -0.0140],\n",
      "        [ 0.0064, -0.0064,  0.0215, -0.0350,  0.0053],\n",
      "        [ 0.0333,  0.0096,  0.0130, -0.0428, -0.0014],\n",
      "        [-0.0184,  0.0023,  0.0037,  0.0079,  0.0087],\n",
      "        [-0.0318,  0.0232,  0.0248, -0.0604,  0.0327],\n",
      "        [ 0.0108, -0.0158,  0.0126,  0.0059,  0.0045]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0089, -0.0312,  0.0497, -0.0196,  0.0129],\n",
      "        [ 0.0155, -0.0381,  0.0136,  0.0061,  0.0078],\n",
      "        [ 0.0111, -0.0357,  0.0118,  0.0142,  0.0325],\n",
      "        [-0.0222, -0.0121, -0.0040,  0.0079,  0.0125],\n",
      "        [-0.0183, -0.0032, -0.0176,  0.0234, -0.0083],\n",
      "        [-0.0123,  0.0115,  0.0056, -0.0005,  0.0028],\n",
      "        [-0.0081, -0.0154, -0.0339,  0.0183, -0.0216],\n",
      "        [-0.0081,  0.0075, -0.0066,  0.0118,  0.0141]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0137, grad_fn=<MinBackward1>), tensor(0.8161, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09592347592115402\n",
      "@sample 989: tensor([[-0.0003,  0.0085, -0.0212,  0.0012,  0.0090],\n",
      "        [ 0.0269,  0.0153, -0.0237,  0.0214,  0.0066],\n",
      "        [-0.0118,  0.0008,  0.0194,  0.0077, -0.0083],\n",
      "        [ 0.0166,  0.0023, -0.0137, -0.0141, -0.0027],\n",
      "        [-0.0063,  0.0099, -0.0224,  0.0088,  0.0191],\n",
      "        [ 0.0101, -0.0291,  0.0056,  0.0037, -0.0024],\n",
      "        [-0.0089, -0.0141,  0.0066, -0.0109,  0.0147],\n",
      "        [-0.0019,  0.0189, -0.0046,  0.0104,  0.0184]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0126, -0.0143, -0.0457,  0.0041,  0.0082],\n",
      "        [ 0.0340,  0.0076, -0.0564,  0.0072,  0.0329],\n",
      "        [-0.0024,  0.0067, -0.0014, -0.0018,  0.0033],\n",
      "        [-0.0199, -0.0089, -0.0297, -0.0073, -0.0099],\n",
      "        [ 0.0300,  0.0049,  0.0227, -0.0184, -0.0120],\n",
      "        [ 0.0016, -0.0231,  0.0025, -0.0102, -0.0115],\n",
      "        [-0.0049, -0.0122, -0.0104,  0.0205,  0.0121],\n",
      "        [ 0.0106, -0.0172, -0.0378,  0.0143, -0.0136]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0125, grad_fn=<MinBackward1>), tensor(0.8992, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09793872386217117\n",
      "@sample 990: tensor([[ 1.3846e-02, -4.3236e-03, -2.5295e-04,  9.4676e-03, -1.3878e-02],\n",
      "        [-1.1922e-02, -5.5980e-03,  2.2361e-02,  5.9723e-03,  1.4841e-02],\n",
      "        [-1.0403e-02,  2.0049e-02,  1.0831e-02, -1.3679e-02,  2.3253e-02],\n",
      "        [-1.0684e-02, -1.1518e-02, -1.2741e-02, -2.4981e-03,  1.9238e-02],\n",
      "        [ 1.8228e-03,  5.2889e-03, -1.3975e-02,  1.8811e-02,  5.5305e-03],\n",
      "        [-1.5201e-02,  4.3933e-03, -1.1076e-02,  8.8178e-03,  3.3539e-03],\n",
      "        [ 3.0510e-06,  5.6471e-03, -1.1823e-03, -1.2608e-02, -3.1456e-02],\n",
      "        [ 1.2354e-02, -1.1341e-03, -3.5298e-02,  2.2692e-02, -1.7289e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0223,  0.0210, -0.0136,  0.0217,  0.0131],\n",
      "        [ 0.0018,  0.0150, -0.0199,  0.0012,  0.0002],\n",
      "        [-0.0133, -0.0063, -0.0430,  0.0259, -0.0017],\n",
      "        [ 0.0223,  0.0264, -0.0260, -0.0050, -0.0113],\n",
      "        [ 0.0204,  0.0060,  0.0004,  0.0002, -0.0089],\n",
      "        [-0.0009,  0.0332, -0.0025, -0.0069, -0.0068],\n",
      "        [-0.0371, -0.0012, -0.0196, -0.0418, -0.0190],\n",
      "        [-0.0058, -0.0017,  0.0383,  0.0031,  0.0003]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0117, grad_fn=<MinBackward1>), tensor(0.8904, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1002560704946518\n",
      "@sample 991: tensor([[ 4.1842e-03,  2.3391e-02, -2.4398e-02, -3.2816e-02,  1.3509e-02],\n",
      "        [ 5.9914e-03,  2.1811e-02,  6.4706e-03,  2.0897e-03,  9.8524e-03],\n",
      "        [-5.6976e-03, -2.6471e-02,  2.4312e-03,  1.1004e-02, -9.5095e-03],\n",
      "        [ 1.0571e-02, -2.4015e-03, -6.8962e-03,  1.9118e-05, -2.3983e-02],\n",
      "        [ 2.7264e-02,  1.2306e-02,  3.6779e-03, -7.6439e-03, -6.3255e-03],\n",
      "        [-1.0108e-02,  1.2207e-03, -1.2820e-02, -5.8212e-03,  1.0334e-02],\n",
      "        [-1.5802e-03,  1.4370e-02,  9.8941e-04,  7.2670e-03, -1.3809e-02],\n",
      "        [-5.0493e-03, -8.8151e-03, -2.8386e-03, -1.5729e-02,  9.4879e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0208, -0.0249, -0.0640,  0.0019,  0.0288],\n",
      "        [ 0.0041,  0.0266, -0.0346,  0.0298,  0.0105],\n",
      "        [ 0.0105,  0.0057,  0.0070,  0.0034,  0.0137],\n",
      "        [-0.0049, -0.0140,  0.0451,  0.0117, -0.0189],\n",
      "        [-0.0288,  0.0032, -0.0315, -0.0109, -0.0153],\n",
      "        [-0.0124, -0.0226,  0.0419, -0.0081, -0.0121],\n",
      "        [ 0.0030,  0.0002, -0.0005,  0.0094,  0.0157],\n",
      "        [ 0.0160, -0.0171,  0.0104, -0.0055, -0.0175]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0152, grad_fn=<MinBackward1>), tensor(0.8466, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.0851033553481102\n",
      "@sample 992: tensor([[ 0.0112,  0.0391, -0.0208, -0.0237,  0.0266],\n",
      "        [ 0.0035, -0.0079, -0.0260, -0.0072, -0.0087],\n",
      "        [ 0.0260,  0.0315, -0.0136,  0.0059, -0.0055],\n",
      "        [ 0.0082,  0.0355,  0.0102, -0.0429,  0.0019],\n",
      "        [-0.0084, -0.0045, -0.0047, -0.0048,  0.0049],\n",
      "        [-0.0114, -0.0112, -0.0132, -0.0002, -0.0072],\n",
      "        [ 0.0013,  0.0065, -0.0053, -0.0182, -0.0029],\n",
      "        [-0.0024,  0.0082, -0.0210,  0.0285, -0.0129]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0045,  0.0138, -0.0467,  0.0294,  0.0411],\n",
      "        [ 0.0186, -0.0131,  0.0162,  0.0046, -0.0209],\n",
      "        [-0.0122,  0.0128, -0.0516,  0.0308,  0.0189],\n",
      "        [-0.0254, -0.0002, -0.0116,  0.0316, -0.0204],\n",
      "        [-0.0039, -0.0153, -0.0263,  0.0157,  0.0295],\n",
      "        [-0.0233, -0.0058, -0.0047,  0.0163,  0.0062],\n",
      "        [-0.0010,  0.0091, -0.0131,  0.0046,  0.0278],\n",
      "        [ 0.0429,  0.0261,  0.0012, -0.0060,  0.0313]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0110, grad_fn=<MinBackward1>), tensor(0.8435, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09536360949277878\n",
      "@sample 993: tensor([[-6.3803e-03,  4.0066e-03,  1.8614e-02, -2.8344e-03, -4.8208e-03],\n",
      "        [ 9.6347e-03, -2.6604e-02, -9.1110e-04,  1.9115e-02, -1.9121e-03],\n",
      "        [-7.4155e-03, -7.0929e-03, -8.8731e-03,  2.3080e-02, -2.0356e-02],\n",
      "        [ 1.2844e-02,  9.1619e-03, -4.2243e-03, -5.2627e-03, -1.9740e-02],\n",
      "        [ 2.3064e-02, -4.4883e-03, -2.1480e-02,  6.6986e-03, -1.3216e-02],\n",
      "        [-9.9881e-03,  1.0588e-02, -1.4943e-02, -2.4294e-03,  5.3813e-03],\n",
      "        [-1.3154e-02, -3.0548e-03, -3.3255e-02,  2.4739e-03,  1.5981e-05],\n",
      "        [-1.1049e-02,  5.3264e-03, -1.1814e-02, -1.7500e-02, -6.7650e-04]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0217,  0.0247, -0.0251, -0.0086,  0.0288],\n",
      "        [ 0.0067,  0.0219,  0.0272, -0.0301,  0.0060],\n",
      "        [ 0.0204, -0.0223,  0.0393, -0.0209, -0.0326],\n",
      "        [-0.0019, -0.0157, -0.0080,  0.0306,  0.0225],\n",
      "        [ 0.0253,  0.0089,  0.0233, -0.0021,  0.0092],\n",
      "        [ 0.0147, -0.0129, -0.0353,  0.0216,  0.0075],\n",
      "        [ 0.0138, -0.0213,  0.0316,  0.0033,  0.0091],\n",
      "        [-0.0189, -0.0285, -0.0143, -0.0169,  0.0041]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0120, grad_fn=<MinBackward1>), tensor(0.8412, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09711048752069473\n",
      "@sample 994: tensor([[ 0.0076,  0.0125, -0.0057,  0.0080, -0.0347],\n",
      "        [ 0.0170, -0.0292, -0.0008,  0.0272,  0.0072],\n",
      "        [-0.0151,  0.0142, -0.0067,  0.0159, -0.0047],\n",
      "        [-0.0029, -0.0069, -0.0142, -0.0007, -0.0170],\n",
      "        [-0.0056, -0.0262, -0.0150,  0.0209, -0.0192],\n",
      "        [-0.0146, -0.0131, -0.0010,  0.0177,  0.0034],\n",
      "        [ 0.0328, -0.0247, -0.0203,  0.0104, -0.0239],\n",
      "        [-0.0225,  0.0255, -0.0450,  0.0036,  0.0042]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0178,  0.0282, -0.0235,  0.0305,  0.0213],\n",
      "        [-0.0120, -0.0147,  0.0166, -0.0311, -0.0110],\n",
      "        [ 0.0147,  0.0134, -0.0072, -0.0074, -0.0123],\n",
      "        [ 0.0247, -0.0062,  0.0266, -0.0097,  0.0225],\n",
      "        [-0.0026,  0.0152,  0.0177, -0.0136, -0.0091],\n",
      "        [ 0.0232,  0.0126,  0.0022, -0.0175,  0.0079],\n",
      "        [-0.0053,  0.0050,  0.0057, -0.0050, -0.0212],\n",
      "        [ 0.0103, -0.0214,  0.0110,  0.0150,  0.0206]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0150, grad_fn=<MinBackward1>), tensor(0.8435, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1046370342373848\n",
      "@sample 995: tensor([[-0.0105,  0.0045, -0.0033,  0.0067,  0.0066],\n",
      "        [-0.0142,  0.0312, -0.0153,  0.0058, -0.0324],\n",
      "        [ 0.0072,  0.0132,  0.0146, -0.0235,  0.0062],\n",
      "        [ 0.0034, -0.0174,  0.0099, -0.0297,  0.0284],\n",
      "        [-0.0137,  0.0233, -0.0123, -0.0222,  0.0032],\n",
      "        [-0.0087,  0.0051,  0.0079,  0.0150,  0.0008],\n",
      "        [ 0.0069, -0.0269, -0.0051,  0.0345, -0.0288],\n",
      "        [-0.0035, -0.0081, -0.0040,  0.0235, -0.0125]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0105,  0.0003,  0.0065, -0.0209,  0.0063],\n",
      "        [-0.0099,  0.0102, -0.0100, -0.0092,  0.0039],\n",
      "        [-0.0570,  0.0081, -0.0457,  0.0310,  0.0038],\n",
      "        [ 0.0347,  0.0391,  0.0223,  0.0024, -0.0042],\n",
      "        [ 0.0147, -0.0284,  0.0091, -0.0104, -0.0014],\n",
      "        [-0.0091,  0.0134, -0.0027, -0.0012,  0.0061],\n",
      "        [-0.0005,  0.0224, -0.0102, -0.0073,  0.0223],\n",
      "        [ 0.0580,  0.0272,  0.0406, -0.0169,  0.0079]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0114, grad_fn=<MinBackward1>), tensor(0.9375, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10050158947706223\n",
      "@sample 996: tensor([[ 0.0161, -0.0032, -0.0099,  0.0098, -0.0020],\n",
      "        [ 0.0131,  0.0047,  0.0009,  0.0178, -0.0020],\n",
      "        [-0.0175,  0.0282, -0.0034, -0.0057,  0.0070],\n",
      "        [-0.0197, -0.0098, -0.0041, -0.0001, -0.0107],\n",
      "        [ 0.0090, -0.0028, -0.0147,  0.0369, -0.0053],\n",
      "        [-0.0287,  0.0048, -0.0087,  0.0181, -0.0194],\n",
      "        [ 0.0021,  0.0006, -0.0013,  0.0136, -0.0176],\n",
      "        [ 0.0208, -0.0130, -0.0135,  0.0088, -0.0108]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0062, -0.0006,  0.0055,  0.0082, -0.0197],\n",
      "        [ 0.0042,  0.0385,  0.0047,  0.0225,  0.0094],\n",
      "        [ 0.0158, -0.0239,  0.0286, -0.0031, -0.0125],\n",
      "        [-0.0032, -0.0079, -0.0462,  0.0024,  0.0099],\n",
      "        [ 0.0486, -0.0037,  0.0508, -0.0152, -0.0055],\n",
      "        [ 0.0184,  0.0079,  0.0295, -0.0462,  0.0141],\n",
      "        [-0.0163, -0.0037, -0.0111,  0.0010, -0.0329],\n",
      "        [ 0.0011, -0.0007,  0.0021, -0.0006,  0.0076]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0128, grad_fn=<MinBackward1>), tensor(0.8448, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09512560814619064\n",
      "@sample 997: tensor([[-0.0331,  0.0090,  0.0278, -0.0224,  0.0098],\n",
      "        [ 0.0034, -0.0069,  0.0368,  0.0140, -0.0015],\n",
      "        [-0.0110,  0.0241,  0.0062,  0.0051,  0.0057],\n",
      "        [ 0.0270,  0.0023, -0.0094,  0.0031,  0.0224],\n",
      "        [-0.0007, -0.0060,  0.0180, -0.0158,  0.0432],\n",
      "        [ 0.0287, -0.0163, -0.0052, -0.0009, -0.0295],\n",
      "        [ 0.0169, -0.0006, -0.0248,  0.0169, -0.0149],\n",
      "        [ 0.0002,  0.0211, -0.0219, -0.0033,  0.0003]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0194, -0.0343,  0.0187, -0.0203, -0.0076],\n",
      "        [ 0.0127,  0.0188,  0.0206,  0.0095, -0.0450],\n",
      "        [ 0.0024,  0.0038, -0.0063, -0.0176,  0.0010],\n",
      "        [-0.0285,  0.0074, -0.0347, -0.0061, -0.0201],\n",
      "        [-0.0028,  0.0279,  0.0478, -0.0329, -0.0199],\n",
      "        [ 0.0220,  0.0368,  0.0410, -0.0482,  0.0149],\n",
      "        [ 0.0177,  0.0160, -0.0144, -0.0008, -0.0075],\n",
      "        [ 0.0082,  0.0040,  0.0044, -0.0133,  0.0079]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0196, grad_fn=<MinBackward1>), tensor(0.8088, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10268999636173248\n",
      "@sample 998: tensor([[-0.0207, -0.0197, -0.0034, -0.0149,  0.0152],\n",
      "        [ 0.0131,  0.0278,  0.0220,  0.0053, -0.0120],\n",
      "        [ 0.0204, -0.0112, -0.0121, -0.0018, -0.0100],\n",
      "        [-0.0155, -0.0299, -0.0050, -0.0102,  0.0218],\n",
      "        [ 0.0121, -0.0056, -0.0250,  0.0207, -0.0046],\n",
      "        [ 0.0068,  0.0134,  0.0037, -0.0045,  0.0072],\n",
      "        [-0.0096,  0.0041,  0.0208, -0.0068,  0.0240],\n",
      "        [-0.0017,  0.0447, -0.0065, -0.0383,  0.0170]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0035, -0.0273,  0.0321, -0.0233, -0.0229],\n",
      "        [ 0.0005,  0.0212, -0.0824,  0.0450,  0.0231],\n",
      "        [ 0.0044,  0.0042,  0.0257, -0.0053,  0.0018],\n",
      "        [-0.0295, -0.0523,  0.0509, -0.0219, -0.0030],\n",
      "        [-0.0014,  0.0155, -0.0379, -0.0231, -0.0161],\n",
      "        [ 0.0002,  0.0135, -0.0558,  0.0109, -0.0172],\n",
      "        [-0.0147, -0.0034, -0.0096, -0.0023, -0.0305],\n",
      "        [-0.0363, -0.0217, -0.0569,  0.0386,  0.0132]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0138, grad_fn=<MinBackward1>), tensor(0.8687, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09437035769224167\n",
      "@sample 999: tensor([[ 0.0087, -0.0314,  0.0231, -0.0096, -0.0154],\n",
      "        [-0.0131, -0.0011,  0.0196, -0.0204,  0.0080],\n",
      "        [ 0.0077,  0.0140,  0.0253,  0.0146, -0.0305],\n",
      "        [-0.0205,  0.0158,  0.0281, -0.0037,  0.0139],\n",
      "        [ 0.0272, -0.0112,  0.0073,  0.0167,  0.0053],\n",
      "        [ 0.0109, -0.0035, -0.0047, -0.0030, -0.0091],\n",
      "        [-0.0045, -0.0212,  0.0068,  0.0121,  0.0013],\n",
      "        [ 0.0063, -0.0010,  0.0131, -0.0059,  0.0052]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0100, -0.0186,  0.0312, -0.0042,  0.0053],\n",
      "        [-0.0370, -0.0059, -0.0398, -0.0136, -0.0035],\n",
      "        [-0.0087,  0.0316,  0.0148,  0.0046,  0.0014],\n",
      "        [-0.0214, -0.0086, -0.0158,  0.0097,  0.0024],\n",
      "        [ 0.0121, -0.0034,  0.0204,  0.0039, -0.0135],\n",
      "        [ 0.0119, -0.0155,  0.0142, -0.0166,  0.0171],\n",
      "        [ 0.0039, -0.0062, -0.0204,  0.0176, -0.0048],\n",
      "        [-0.0208,  0.0291, -0.0540,  0.0473,  0.0231]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0097, grad_fn=<MinBackward1>), tensor(0.8503, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09573060274124146\n",
      "@sample 1000: tensor([[-0.0047, -0.0044,  0.0186, -0.0197,  0.0031],\n",
      "        [-0.0212,  0.0183,  0.0079, -0.0088,  0.0185],\n",
      "        [-0.0187, -0.0089,  0.0098,  0.0022,  0.0148],\n",
      "        [-0.0067, -0.0017,  0.0010, -0.0023,  0.0356],\n",
      "        [-0.0308,  0.0163,  0.0149, -0.0365,  0.0229],\n",
      "        [-0.0071, -0.0261, -0.0436,  0.0116,  0.0083],\n",
      "        [ 0.0108,  0.0120, -0.0042, -0.0121,  0.0067],\n",
      "        [ 0.0062, -0.0010,  0.0281,  0.0184, -0.0049]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0337,  0.0004, -0.0244,  0.0194, -0.0093],\n",
      "        [ 0.0074,  0.0008, -0.0269, -0.0018,  0.0134],\n",
      "        [ 0.0235, -0.0158,  0.0460, -0.0003,  0.0120],\n",
      "        [ 0.0208, -0.0266,  0.0241, -0.0018, -0.0200],\n",
      "        [-0.0100, -0.0195, -0.0330,  0.0105, -0.0199],\n",
      "        [ 0.0040, -0.0429,  0.0229,  0.0026,  0.0152],\n",
      "        [-0.0227,  0.0071, -0.0335, -0.0021,  0.0058],\n",
      "        [-0.0004, -0.0121, -0.0010, -0.0068,  0.0215]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0139, grad_fn=<MinBackward1>), tensor(0.8518, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1071239486336708\n",
      "@sample 1001: tensor([[ 0.0034,  0.0363,  0.0164, -0.0210,  0.0384],\n",
      "        [ 0.0055, -0.0076,  0.0126, -0.0015,  0.0124],\n",
      "        [ 0.0185, -0.0379, -0.0097,  0.0019, -0.0004],\n",
      "        [-0.0114, -0.0078,  0.0045, -0.0003, -0.0043],\n",
      "        [-0.0013,  0.0023, -0.0040, -0.0116, -0.0049],\n",
      "        [ 0.0107,  0.0212, -0.0067, -0.0128,  0.0156],\n",
      "        [-0.0084,  0.0024,  0.0270,  0.0090,  0.0328],\n",
      "        [-0.0139,  0.0234,  0.0119, -0.0381,  0.0071]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0055, -0.0038, -0.0377, -0.0112, -0.0377],\n",
      "        [-0.0212, -0.0068,  0.0047,  0.0071,  0.0133],\n",
      "        [-0.0157, -0.0060,  0.0156, -0.0001,  0.0015],\n",
      "        [-0.0039, -0.0074,  0.0309, -0.0440, -0.0387],\n",
      "        [-0.0042, -0.0153,  0.0297, -0.0296, -0.0042],\n",
      "        [ 0.0156, -0.0038,  0.0032, -0.0218, -0.0348],\n",
      "        [ 0.0010,  0.0239, -0.0150,  0.0177,  0.0106],\n",
      "        [-0.0270, -0.0011, -0.0170,  0.0324, -0.0022]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0177, grad_fn=<MinBackward1>), tensor(0.8259, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.0878194272518158\n",
      "@sample 1002: tensor([[-0.0109, -0.0003,  0.0350, -0.0116,  0.0281],\n",
      "        [ 0.0082, -0.0026,  0.0078, -0.0354, -0.0077],\n",
      "        [-0.0133,  0.0140,  0.0297,  0.0197, -0.0137],\n",
      "        [ 0.0039,  0.0291,  0.0130, -0.0105,  0.0005],\n",
      "        [ 0.0171,  0.0196,  0.0105,  0.0140, -0.0060],\n",
      "        [ 0.0045,  0.0257, -0.0105, -0.0170, -0.0152],\n",
      "        [ 0.0059, -0.0209, -0.0115,  0.0214,  0.0018],\n",
      "        [-0.0041,  0.0258,  0.0109, -0.0096,  0.0267]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0006,  0.0047,  0.0048, -0.0060, -0.0207],\n",
      "        [-0.0077, -0.0056, -0.0251,  0.0199, -0.0037],\n",
      "        [ 0.0205,  0.0305,  0.0006,  0.0018, -0.0050],\n",
      "        [ 0.0246, -0.0013, -0.0159,  0.0111,  0.0248],\n",
      "        [ 0.0134,  0.0106,  0.0079,  0.0111, -0.0100],\n",
      "        [-0.0152,  0.0101, -0.0157, -0.0092,  0.0055],\n",
      "        [ 0.0270, -0.0123,  0.0287, -0.0069, -0.0375],\n",
      "        [-0.0101,  0.0004,  0.0326,  0.0028, -0.0094]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0155, grad_fn=<MinBackward1>), tensor(0.8716, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.0968751385807991\n",
      "@sample 1003: tensor([[-0.0224, -0.0113,  0.0036,  0.0116,  0.0048],\n",
      "        [ 0.0104,  0.0108,  0.0351, -0.0212,  0.0235],\n",
      "        [-0.0074,  0.0060,  0.0044,  0.0063,  0.0059],\n",
      "        [ 0.0027, -0.0144,  0.0326, -0.0114,  0.0026],\n",
      "        [-0.0085,  0.0020,  0.0277, -0.0153, -0.0199],\n",
      "        [ 0.0133,  0.0090,  0.0243, -0.0160, -0.0068],\n",
      "        [ 0.0282, -0.0243, -0.0118,  0.0061, -0.0072],\n",
      "        [ 0.0230, -0.0002,  0.0142,  0.0062, -0.0016]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0007, -0.0047, -0.0022, -0.0038, -0.0224],\n",
      "        [-0.0364, -0.0090, -0.0388,  0.0061, -0.0257],\n",
      "        [-0.0031,  0.0078, -0.0115,  0.0209, -0.0106],\n",
      "        [-0.0123,  0.0242, -0.0220,  0.0138, -0.0066],\n",
      "        [-0.0171, -0.0708,  0.0068,  0.0145, -0.0262],\n",
      "        [ 0.0009,  0.0370,  0.0035,  0.0193,  0.0219],\n",
      "        [ 0.0108, -0.0110,  0.0428, -0.0191, -0.0338],\n",
      "        [-0.0122,  0.0284, -0.0102,  0.0108,  0.0040]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0165, grad_fn=<MinBackward1>), tensor(0.8934, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09990759938955307\n",
      "@sample 1004: tensor([[ 0.0124, -0.0253,  0.0102,  0.0169, -0.0140],\n",
      "        [ 0.0048,  0.0076, -0.0015, -0.0099,  0.0060],\n",
      "        [ 0.0150,  0.0178, -0.0015, -0.0153,  0.0134],\n",
      "        [-0.0145, -0.0126,  0.0062, -0.0067,  0.0072],\n",
      "        [ 0.0010,  0.0104, -0.0092,  0.0132, -0.0118],\n",
      "        [-0.0072, -0.0099,  0.0089, -0.0188,  0.0076],\n",
      "        [ 0.0131, -0.0118, -0.0008, -0.0196,  0.0295],\n",
      "        [-0.0095,  0.0044, -0.0119, -0.0052,  0.0096]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0085,  0.0015,  0.0168, -0.0070, -0.0226],\n",
      "        [ 0.0088, -0.0064, -0.0010, -0.0165, -0.0154],\n",
      "        [-0.0196,  0.0054, -0.0545,  0.0389, -0.0164],\n",
      "        [-0.0055, -0.0198, -0.0024,  0.0002,  0.0263],\n",
      "        [-0.0012, -0.0155,  0.0038,  0.0207, -0.0005],\n",
      "        [-0.0212, -0.0033, -0.0349,  0.0072, -0.0002],\n",
      "        [ 0.0191, -0.0437,  0.0222,  0.0140, -0.0355],\n",
      "        [ 0.0013, -0.0254,  0.0107, -0.0068, -0.0049]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0129, grad_fn=<MinBackward1>), tensor(0.8715, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09457991272211075\n",
      "@sample 1005: tensor([[ 0.0142,  0.0245, -0.0037, -0.0194,  0.0085],\n",
      "        [ 0.0325, -0.0129,  0.0552, -0.0459, -0.0037],\n",
      "        [ 0.0184, -0.0123,  0.0181,  0.0143,  0.0031],\n",
      "        [ 0.0123,  0.0023, -0.0450,  0.0223, -0.0099],\n",
      "        [ 0.0214, -0.0286,  0.0068,  0.0338, -0.0403],\n",
      "        [ 0.0260,  0.0195, -0.0167,  0.0097,  0.0203],\n",
      "        [-0.0338, -0.0204,  0.0213, -0.0258, -0.0166],\n",
      "        [ 0.0084, -0.0117, -0.0170,  0.0013, -0.0137]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0135, -0.0175, -0.0489,  0.0309, -0.0178],\n",
      "        [-0.0364,  0.0080,  0.0053,  0.0176, -0.0066],\n",
      "        [ 0.0015,  0.0184, -0.0296,  0.0116, -0.0345],\n",
      "        [-0.0066, -0.0125,  0.0029, -0.0321, -0.0115],\n",
      "        [ 0.0146, -0.0150, -0.0211,  0.0271,  0.0283],\n",
      "        [-0.0017,  0.0260, -0.0406,  0.0053, -0.0252],\n",
      "        [ 0.0743,  0.0481, -0.0119,  0.0443,  0.0208],\n",
      "        [ 0.0067, -0.0049,  0.0132, -0.0061, -0.0151]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0129, grad_fn=<MinBackward1>), tensor(0.8251, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10742326080799103\n",
      "@sample 1006: tensor([[ 0.0249,  0.0039,  0.0152, -0.0091,  0.0074],\n",
      "        [-0.0127,  0.0321, -0.0120,  0.0202, -0.0073],\n",
      "        [-0.0109,  0.0220, -0.0117, -0.0105,  0.0039],\n",
      "        [ 0.0123,  0.0127, -0.0251,  0.0212, -0.0010],\n",
      "        [-0.0143,  0.0226,  0.0164, -0.0071,  0.0139],\n",
      "        [-0.0001, -0.0534, -0.0018,  0.0127, -0.0149],\n",
      "        [ 0.0309, -0.0130,  0.0032, -0.0083, -0.0081],\n",
      "        [-0.0201, -0.0058, -0.0087,  0.0052,  0.0105]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0055,  0.0108, -0.0532, -0.0017, -0.0288],\n",
      "        [ 0.0084,  0.0257, -0.0149, -0.0041,  0.0072],\n",
      "        [-0.0121, -0.0179,  0.0181, -0.0075,  0.0147],\n",
      "        [ 0.0210,  0.0329,  0.0188, -0.0136,  0.0139],\n",
      "        [-0.0241, -0.0235,  0.0115, -0.0235, -0.0263],\n",
      "        [-0.0248,  0.0145,  0.0287, -0.0061, -0.0073],\n",
      "        [-0.0123,  0.0042,  0.0086,  0.0238,  0.0180],\n",
      "        [-0.0295,  0.0041, -0.0163, -0.0220, -0.0045]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0151, grad_fn=<MinBackward1>), tensor(0.8773, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10932230204343796\n",
      "@sample 1007: tensor([[ 0.0119,  0.0043, -0.0325,  0.0184,  0.0044],\n",
      "        [-0.0056, -0.0286,  0.0024, -0.0041,  0.0060],\n",
      "        [-0.0031,  0.0097, -0.0197,  0.0041,  0.0146],\n",
      "        [-0.0056,  0.0305,  0.0580, -0.0422,  0.0303],\n",
      "        [ 0.0036,  0.0422,  0.0202, -0.0487, -0.0055],\n",
      "        [-0.0111, -0.0024,  0.0050, -0.0136, -0.0069],\n",
      "        [-0.0126, -0.0017, -0.0111, -0.0071,  0.0255],\n",
      "        [ 0.0234, -0.0085, -0.0053, -0.0196,  0.0123]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0384,  0.0292,  0.0122, -0.0229,  0.0261],\n",
      "        [-0.0255,  0.0079, -0.0058, -0.0081,  0.0046],\n",
      "        [-0.0109, -0.0201,  0.0076, -0.0043,  0.0085],\n",
      "        [-0.0307, -0.0025, -0.0765,  0.0189, -0.0229],\n",
      "        [-0.0460,  0.0033,  0.0219, -0.0091, -0.0120],\n",
      "        [-0.0058, -0.0264,  0.0012, -0.0088, -0.0042],\n",
      "        [-0.0116, -0.0266, -0.0119, -0.0094, -0.0393],\n",
      "        [-0.0096,  0.0301, -0.0407,  0.0216,  0.0329]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0150, grad_fn=<MinBackward1>), tensor(0.8572, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10015776753425598\n",
      "@sample 1008: tensor([[ 0.0050, -0.0117, -0.0042,  0.0269,  0.0003],\n",
      "        [ 0.0123, -0.0127, -0.0401,  0.0178, -0.0058],\n",
      "        [-0.0304,  0.0104,  0.0134, -0.0105,  0.0218],\n",
      "        [ 0.0120,  0.0031,  0.0046,  0.0135,  0.0020],\n",
      "        [-0.0091,  0.0032, -0.0502,  0.0008, -0.0019],\n",
      "        [-0.0081,  0.0204, -0.0004, -0.0068, -0.0020],\n",
      "        [-0.0303,  0.0084, -0.0056,  0.0063,  0.0158],\n",
      "        [ 0.0206, -0.0040,  0.0028,  0.0182,  0.0012]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0188,  0.0250, -0.0088, -0.0018,  0.0077],\n",
      "        [ 0.0275, -0.0012,  0.0096, -0.0018,  0.0292],\n",
      "        [-0.0077, -0.0044, -0.0193,  0.0111, -0.0273],\n",
      "        [-0.0087,  0.0235, -0.0288,  0.0378,  0.0326],\n",
      "        [ 0.0373, -0.0131,  0.0192, -0.0084,  0.0164],\n",
      "        [ 0.0159,  0.0275, -0.0035, -0.0235,  0.0183],\n",
      "        [ 0.0193, -0.0145,  0.0082, -0.0446, -0.0015],\n",
      "        [ 0.0048,  0.0195, -0.0043, -0.0026, -0.0212]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0105, grad_fn=<MinBackward1>), tensor(0.8838, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10690644383430481\n",
      "@sample 1009: tensor([[ 0.0100, -0.0013,  0.0040,  0.0164, -0.0089],\n",
      "        [ 0.0140, -0.0028, -0.0149,  0.0002, -0.0055],\n",
      "        [ 0.0155,  0.0029,  0.0214,  0.0382, -0.0413],\n",
      "        [-0.0102,  0.0235, -0.0025, -0.0051, -0.0037],\n",
      "        [ 0.0045, -0.0050, -0.0037,  0.0076,  0.0196],\n",
      "        [ 0.0108,  0.0062, -0.0033,  0.0074, -0.0003],\n",
      "        [ 0.0114,  0.0223, -0.0119, -0.0229,  0.0149],\n",
      "        [ 0.0168,  0.0075,  0.0056,  0.0154,  0.0057]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0066,  0.0433, -0.0066,  0.0008,  0.0159],\n",
      "        [-0.0114, -0.0042, -0.0127,  0.0041,  0.0041],\n",
      "        [-0.0058,  0.0466, -0.0144,  0.0247,  0.0131],\n",
      "        [-0.0153, -0.0101, -0.0066, -0.0127, -0.0215],\n",
      "        [-0.0093,  0.0246, -0.0266, -0.0081, -0.0093],\n",
      "        [-0.0011,  0.0135, -0.0312, -0.0006,  0.0261],\n",
      "        [-0.0023, -0.0213, -0.0281,  0.0221, -0.0322],\n",
      "        [-0.0014,  0.0363, -0.0365,  0.0100,  0.0132]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0197, grad_fn=<MinBackward1>), tensor(0.8117, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.0836561769247055\n",
      "@sample 1010: tensor([[-1.4817e-02, -2.9396e-04, -5.0149e-03,  2.1541e-02, -3.2678e-03],\n",
      "        [-1.9573e-03,  3.5247e-02,  1.8177e-02, -5.3630e-03,  6.5587e-05],\n",
      "        [-3.9789e-03,  4.2963e-03, -1.3219e-02,  3.7477e-02, -9.4118e-03],\n",
      "        [ 1.0673e-02, -1.8586e-02, -1.9869e-02,  6.5488e-03,  1.9502e-02],\n",
      "        [ 6.9900e-03,  4.7513e-02,  2.9462e-02,  1.3431e-03,  1.1421e-02],\n",
      "        [-7.8351e-03,  6.8496e-03, -1.8611e-02,  2.3493e-02, -2.4897e-02],\n",
      "        [ 2.3902e-02,  4.7360e-03, -2.7439e-03, -1.0234e-04, -4.8061e-03],\n",
      "        [-9.3070e-04,  2.0094e-02,  1.9716e-03, -3.8006e-03, -2.8987e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0002,  0.0232,  0.0269, -0.0212, -0.0007],\n",
      "        [-0.0234,  0.0036, -0.0528,  0.0291, -0.0237],\n",
      "        [ 0.0154,  0.0123, -0.0169, -0.0018, -0.0172],\n",
      "        [-0.0066,  0.0185, -0.0259,  0.0053, -0.0088],\n",
      "        [-0.0098,  0.0104, -0.0285,  0.0499, -0.0132],\n",
      "        [ 0.0191, -0.0104, -0.0084, -0.0194,  0.0084],\n",
      "        [-0.0087,  0.0140, -0.0057,  0.0268,  0.0247],\n",
      "        [-0.0035,  0.0345,  0.0186, -0.0081, -0.0045]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0134, grad_fn=<MinBackward1>), tensor(0.8607, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1011008769273758\n",
      "@sample 1011: tensor([[-0.0026,  0.0537, -0.0159, -0.0412,  0.0166],\n",
      "        [ 0.0026, -0.0254, -0.0150,  0.0127, -0.0218],\n",
      "        [ 0.0252,  0.0115,  0.0169,  0.0386, -0.0334],\n",
      "        [ 0.0097, -0.0027, -0.0085,  0.0270,  0.0107],\n",
      "        [ 0.0124,  0.0165,  0.0197,  0.0191, -0.0326],\n",
      "        [ 0.0074, -0.0097, -0.0211,  0.0316, -0.0089],\n",
      "        [ 0.0316, -0.0206,  0.0144, -0.0025,  0.0013],\n",
      "        [-0.0234,  0.0010, -0.0141, -0.0067,  0.0098]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0357,  0.0032, -0.0366,  0.0033,  0.0324],\n",
      "        [-0.0223, -0.0165,  0.0141,  0.0294,  0.0042],\n",
      "        [ 0.0125,  0.0496, -0.0177, -0.0062,  0.0286],\n",
      "        [ 0.0086,  0.0105, -0.0245,  0.0032, -0.0084],\n",
      "        [ 0.0004,  0.0379,  0.0183,  0.0252,  0.0162],\n",
      "        [ 0.0043,  0.0220,  0.0225, -0.0226, -0.0033],\n",
      "        [ 0.0126,  0.0230, -0.0002, -0.0008,  0.0088],\n",
      "        [ 0.0084, -0.0481,  0.0075, -0.0036, -0.0402]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0138, grad_fn=<MinBackward1>), tensor(0.8822, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1038096621632576\n",
      "@sample 1012: tensor([[-1.0138e-03, -1.0433e-02,  7.2327e-03,  2.2812e-02, -3.0936e-02],\n",
      "        [ 6.9076e-03, -2.8719e-03, -5.6137e-03,  1.7279e-02, -5.3722e-04],\n",
      "        [-2.6108e-02, -6.2762e-03,  2.1370e-02, -1.7408e-02,  1.7300e-02],\n",
      "        [ 8.4273e-03,  7.1135e-03, -9.5478e-03,  5.9386e-03, -1.3850e-02],\n",
      "        [ 6.7606e-03,  7.6708e-03, -4.6545e-03,  4.6821e-03,  9.4920e-05],\n",
      "        [ 1.7075e-02, -1.2623e-02, -2.6227e-02,  1.1004e-02, -9.4187e-03],\n",
      "        [-6.3599e-03,  8.3427e-03, -1.7222e-02,  2.1396e-02, -1.5736e-02],\n",
      "        [-1.5690e-02,  8.0640e-03,  2.5661e-02,  1.1223e-02, -1.0100e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-2.6433e-02, -2.5998e-03,  1.2178e-02, -1.8734e-02,  1.6254e-02],\n",
      "        [ 1.7126e-02, -1.2760e-02, -1.0969e-02, -1.0931e-02,  3.3229e-03],\n",
      "        [ 6.6491e-03,  1.6821e-03, -8.6478e-03,  2.2188e-02, -7.7200e-03],\n",
      "        [ 2.1281e-02,  1.0644e-02, -1.2900e-03, -1.2520e-02, -1.5467e-03],\n",
      "        [ 7.7933e-05,  1.7679e-02, -4.8952e-02,  1.5844e-02, -3.1687e-03],\n",
      "        [-2.6873e-02,  1.0956e-02,  5.6527e-03,  7.1930e-03,  5.0488e-03],\n",
      "        [-3.6850e-02, -1.3618e-02, -4.1637e-02, -6.0837e-03,  3.9912e-03],\n",
      "        [ 1.0857e-02,  1.6567e-02,  2.0289e-02, -1.3876e-02, -2.3066e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0132, grad_fn=<MinBackward1>), tensor(0.8420, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09570074081420898\n",
      "@sample 1013: tensor([[-0.0095, -0.0193, -0.0135,  0.0001, -0.0012],\n",
      "        [-0.0125,  0.0054,  0.0099,  0.0014, -0.0109],\n",
      "        [ 0.0152,  0.0334, -0.0347,  0.0143, -0.0195],\n",
      "        [-0.0050,  0.0095, -0.0073, -0.0681,  0.0557],\n",
      "        [-0.0071,  0.0064, -0.0095, -0.0098, -0.0410],\n",
      "        [ 0.0069,  0.0078, -0.0016, -0.0231, -0.0271],\n",
      "        [-0.0143,  0.0054,  0.0112, -0.0195,  0.0020],\n",
      "        [ 0.0091,  0.0048, -0.0010, -0.0223,  0.0197]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0143, -0.0314,  0.0159, -0.0003, -0.0053],\n",
      "        [-0.0122,  0.0060, -0.0073,  0.0010,  0.0072],\n",
      "        [ 0.0005,  0.0148, -0.0036,  0.0271,  0.0010],\n",
      "        [-0.0163, -0.0106, -0.0235,  0.0310, -0.0151],\n",
      "        [-0.0197, -0.0074, -0.0046, -0.0034,  0.0121],\n",
      "        [-0.0154, -0.0318, -0.0805,  0.0393,  0.0013],\n",
      "        [-0.0154, -0.0252, -0.0135,  0.0027,  0.0079],\n",
      "        [-0.0392, -0.0213, -0.0324,  0.0316,  0.0019]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0120, grad_fn=<MinBackward1>), tensor(0.8204, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09529281407594681\n",
      "@sample 1014: tensor([[ 0.0022,  0.0116, -0.0276, -0.0135,  0.0116],\n",
      "        [ 0.0241,  0.0188,  0.0165,  0.0027,  0.0042],\n",
      "        [-0.0213,  0.0013, -0.0300, -0.0054, -0.0120],\n",
      "        [ 0.0086, -0.0126, -0.0166,  0.0153,  0.0285],\n",
      "        [ 0.0112,  0.0099,  0.0076,  0.0154, -0.0034],\n",
      "        [ 0.0033,  0.0017,  0.0028, -0.0145,  0.0024],\n",
      "        [ 0.0053,  0.0299,  0.0192, -0.0032,  0.0101],\n",
      "        [-0.0165, -0.0193, -0.0072,  0.0039,  0.0087]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-2.7773e-03, -3.8856e-04,  9.2613e-03, -1.4826e-02, -2.2946e-03],\n",
      "        [-4.5801e-03,  3.2735e-02,  2.0295e-03, -2.5830e-02, -7.7489e-03],\n",
      "        [-1.7864e-02, -2.7893e-02,  1.9615e-02, -4.7643e-02, -9.6289e-03],\n",
      "        [ 9.3189e-03,  6.8960e-04, -2.7200e-02, -3.8735e-03, -5.8818e-03],\n",
      "        [ 6.3569e-03,  2.0541e-02, -1.6472e-02,  2.0575e-02,  2.4703e-02],\n",
      "        [-2.2837e-03,  7.9075e-03,  1.8524e-02,  6.9710e-03, -6.0410e-03],\n",
      "        [-1.4700e-05,  2.1911e-02, -2.9032e-02,  1.0462e-02, -6.9278e-03],\n",
      "        [ 6.3144e-04, -9.9378e-03, -1.1333e-02, -1.8910e-02, -9.6590e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0125, grad_fn=<MinBackward1>), tensor(0.9012, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09666801244020462\n",
      "@sample 1015: tensor([[ 0.0008,  0.0065, -0.0179, -0.0104, -0.0048],\n",
      "        [-0.0028,  0.0224,  0.0288, -0.0157, -0.0098],\n",
      "        [-0.0177,  0.0223, -0.0112, -0.0198, -0.0106],\n",
      "        [-0.0289,  0.0146, -0.0038,  0.0168, -0.0380],\n",
      "        [ 0.0164,  0.0282, -0.0094, -0.0160,  0.0164],\n",
      "        [ 0.0272, -0.0064, -0.0138,  0.0065,  0.0028],\n",
      "        [-0.0352, -0.0185,  0.0020, -0.0300,  0.0363],\n",
      "        [-0.0094,  0.0047, -0.0018,  0.0245,  0.0038]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0102, -0.0323,  0.0247,  0.0065, -0.0191],\n",
      "        [-0.0408, -0.0082, -0.0599,  0.0658,  0.0318],\n",
      "        [-0.0132,  0.0036,  0.0056, -0.0284, -0.0059],\n",
      "        [ 0.0040, -0.0190,  0.0189, -0.0194, -0.0053],\n",
      "        [-0.0344, -0.0018, -0.0273,  0.0085,  0.0078],\n",
      "        [-0.0006,  0.0282, -0.0602,  0.0094,  0.0049],\n",
      "        [ 0.0129, -0.0723,  0.0165, -0.0057, -0.0206],\n",
      "        [ 0.0324,  0.0378, -0.0100,  0.0266, -0.0116]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0120, grad_fn=<MinBackward1>), tensor(0.7963, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10216197371482849\n",
      "@sample 1016: tensor([[ 0.0015, -0.0114,  0.0066, -0.0133,  0.0110],\n",
      "        [-0.0112, -0.0305, -0.0044,  0.0077,  0.0057],\n",
      "        [ 0.0070,  0.0025, -0.0337, -0.0136, -0.0125],\n",
      "        [-0.0089, -0.0086, -0.0195,  0.0080,  0.0035],\n",
      "        [ 0.0015, -0.0069,  0.0002, -0.0078, -0.0007],\n",
      "        [ 0.0030,  0.0214, -0.0021, -0.0198,  0.0109],\n",
      "        [ 0.0094,  0.0197,  0.0207, -0.0304, -0.0034],\n",
      "        [ 0.0367, -0.0206, -0.0025, -0.0005, -0.0096]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0080, -0.0182,  0.0390, -0.0154,  0.0006],\n",
      "        [-0.0022, -0.0110,  0.0140,  0.0038, -0.0133],\n",
      "        [-0.0302,  0.0036, -0.0097,  0.0049,  0.0035],\n",
      "        [-0.0057,  0.0054,  0.0081, -0.0166,  0.0031],\n",
      "        [ 0.0020,  0.0062, -0.0075,  0.0015, -0.0038],\n",
      "        [-0.0039,  0.0244, -0.0269,  0.0224, -0.0167],\n",
      "        [-0.0537, -0.0256, -0.0415,  0.0441, -0.0044],\n",
      "        [-0.0225, -0.0124,  0.0092, -0.0052, -0.0115]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0117, grad_fn=<MinBackward1>), tensor(0.8439, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09145592898130417\n",
      "@sample 1017: tensor([[ 0.0177, -0.0090,  0.0111, -0.0011, -0.0160],\n",
      "        [-0.0069, -0.0214,  0.0124, -0.0310,  0.0116],\n",
      "        [ 0.0073, -0.0327,  0.0054, -0.0310,  0.0241],\n",
      "        [-0.0123,  0.0039,  0.0009,  0.0026, -0.0058],\n",
      "        [ 0.0011,  0.0055,  0.0045, -0.0078,  0.0150],\n",
      "        [-0.0240, -0.0080,  0.0073, -0.0045,  0.0073],\n",
      "        [ 0.0213, -0.0302,  0.0119, -0.0040, -0.0004],\n",
      "        [-0.0355, -0.0158, -0.0213,  0.0014, -0.0005]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0112,  0.0442, -0.0127,  0.0072,  0.0074],\n",
      "        [-0.0021,  0.0099, -0.0062, -0.0111,  0.0188],\n",
      "        [ 0.0165, -0.0204,  0.0133, -0.0333, -0.0182],\n",
      "        [ 0.0268,  0.0126, -0.0015, -0.0041,  0.0087],\n",
      "        [-0.0229, -0.0016, -0.0204, -0.0076,  0.0012],\n",
      "        [ 0.0233, -0.0088,  0.0409, -0.0038,  0.0200],\n",
      "        [ 0.0027, -0.0174, -0.0140,  0.0179,  0.0324],\n",
      "        [ 0.0044, -0.0149,  0.0252, -0.0104, -0.0124]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0126, grad_fn=<MinBackward1>), tensor(0.8455, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.087179034948349\n",
      "@sample 1018: tensor([[-0.0113, -0.0073, -0.0201,  0.0056,  0.0068],\n",
      "        [ 0.0177,  0.0037, -0.0124,  0.0110,  0.0138],\n",
      "        [ 0.0018, -0.0245,  0.0068,  0.0151, -0.0075],\n",
      "        [ 0.0283, -0.0120, -0.0052, -0.0010, -0.0104],\n",
      "        [ 0.0384, -0.0161, -0.0083, -0.0056, -0.0162],\n",
      "        [ 0.0183,  0.0098,  0.0046, -0.0220,  0.0045],\n",
      "        [ 0.0079, -0.0169, -0.0084,  0.0013,  0.0214],\n",
      "        [ 0.0043,  0.0134,  0.0070, -0.0334,  0.0020]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0004, -0.0339, -0.0038, -0.0202, -0.0119],\n",
      "        [ 0.0050,  0.0374, -0.0573, -0.0114, -0.0008],\n",
      "        [ 0.0008,  0.0116,  0.0290,  0.0143,  0.0285],\n",
      "        [ 0.0031,  0.0130, -0.0620, -0.0005,  0.0192],\n",
      "        [-0.0025,  0.0047,  0.0135,  0.0166,  0.0352],\n",
      "        [-0.0308, -0.0070, -0.0237,  0.0020, -0.0140],\n",
      "        [ 0.0218, -0.0189,  0.0096, -0.0163, -0.0198],\n",
      "        [-0.0189, -0.0334, -0.0093,  0.0165,  0.0187]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0161, grad_fn=<MinBackward1>), tensor(0.8228, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08847736567258835\n",
      "@sample 1019: tensor([[-0.0002, -0.0009, -0.0124, -0.0147,  0.0105],\n",
      "        [-0.0186, -0.0060,  0.0132, -0.0315,  0.0093],\n",
      "        [ 0.0110, -0.0136,  0.0147, -0.0244, -0.0037],\n",
      "        [-0.0061, -0.0227, -0.0042, -0.0009,  0.0149],\n",
      "        [-0.0162, -0.0137,  0.0161, -0.0008, -0.0067],\n",
      "        [ 0.0132, -0.0279,  0.0128,  0.0049,  0.0077],\n",
      "        [ 0.0122,  0.0349,  0.0294, -0.0448,  0.0177],\n",
      "        [-0.0105,  0.0151, -0.0076,  0.0077, -0.0090]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0065,  0.0023, -0.0109,  0.0155, -0.0083],\n",
      "        [-0.0160, -0.0439,  0.0324, -0.0212,  0.0144],\n",
      "        [-0.0167,  0.0100,  0.0152, -0.0077,  0.0104],\n",
      "        [-0.0290, -0.0202, -0.0105,  0.0477,  0.0199],\n",
      "        [ 0.0017, -0.0002,  0.0297,  0.0027, -0.0018],\n",
      "        [-0.0063, -0.0158,  0.0138, -0.0043, -0.0009],\n",
      "        [-0.0331, -0.0320, -0.0939,  0.0250,  0.0219],\n",
      "        [ 0.0003,  0.0003,  0.0289, -0.0261, -0.0008]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0060, grad_fn=<MinBackward1>), tensor(0.8047, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09079623222351074\n",
      "@sample 1020: tensor([[ 2.3116e-02, -1.0830e-02,  1.2447e-02,  2.3535e-04,  1.2934e-02],\n",
      "        [-3.9485e-03, -1.4410e-02,  1.4801e-02, -4.0568e-03,  5.5969e-05],\n",
      "        [ 1.7510e-02,  2.2182e-02,  8.8031e-03, -7.2087e-02,  1.9629e-04],\n",
      "        [-9.5374e-03,  5.5778e-02,  3.2767e-02, -6.0781e-02,  4.5525e-02],\n",
      "        [ 1.2947e-02,  2.7516e-03,  1.8375e-03, -2.6774e-03,  1.5529e-02],\n",
      "        [ 3.8756e-03,  4.0713e-02, -1.9982e-02, -4.4269e-02,  2.0215e-02],\n",
      "        [-3.6161e-03,  4.0185e-02,  7.3074e-04, -4.6939e-02,  3.0615e-02],\n",
      "        [-6.0741e-03, -1.6953e-02, -1.2881e-02,  1.3075e-02,  8.2988e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 1.6757e-03, -5.9243e-04,  7.1891e-03,  2.1330e-03,  6.9041e-03],\n",
      "        [-1.5767e-02, -5.1720e-03, -2.0812e-02,  3.6784e-02,  1.3967e-02],\n",
      "        [-3.5653e-02, -3.4285e-02, -3.5158e-02,  1.7332e-02, -4.5068e-03],\n",
      "        [-4.6326e-02, -6.1693e-03, -4.0744e-02,  2.5545e-02, -1.2449e-02],\n",
      "        [-9.2702e-04,  1.1181e-03,  4.2502e-02, -2.8516e-02, -1.2843e-02],\n",
      "        [ 3.8653e-02,  5.8034e-03, -3.9386e-02,  2.5155e-02,  1.4594e-02],\n",
      "        [-1.1877e-02, -2.8842e-02, -5.9994e-02,  1.7366e-02, -2.7200e-03],\n",
      "        [-5.3011e-05,  3.5696e-04,  3.5705e-03,  9.7618e-03, -2.8913e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0128, grad_fn=<MinBackward1>), tensor(0.8797, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10258924216032028\n",
      "@sample 1021: tensor([[ 7.7017e-05,  3.8892e-03, -8.9939e-03,  1.3023e-02, -1.5526e-02],\n",
      "        [ 2.4184e-02,  2.8556e-03,  1.0182e-02,  2.6082e-02, -3.7563e-02],\n",
      "        [ 4.2354e-04,  5.6344e-03,  2.2317e-02, -1.9983e-02,  8.9607e-03],\n",
      "        [-1.1285e-02,  1.4638e-02,  3.0761e-02, -1.4458e-02,  2.6033e-02],\n",
      "        [ 2.5619e-03,  5.5388e-05,  1.0003e-02, -5.7972e-03,  1.7102e-02],\n",
      "        [ 7.8679e-03, -1.4596e-02,  9.6996e-03,  1.7729e-02, -1.0259e-02],\n",
      "        [-7.1936e-03,  2.4319e-02,  1.0373e-02, -5.5226e-03, -9.8831e-03],\n",
      "        [-1.1722e-02,  9.2346e-03,  1.2180e-02, -3.7832e-02,  2.0193e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0340, -0.0124,  0.0237, -0.0143,  0.0076],\n",
      "        [ 0.0239, -0.0210,  0.0032, -0.0291, -0.0293],\n",
      "        [-0.0028,  0.0150,  0.0307, -0.0080, -0.0075],\n",
      "        [ 0.0327,  0.0004,  0.0146,  0.0115, -0.0127],\n",
      "        [ 0.0018, -0.0015,  0.0005, -0.0090,  0.0025],\n",
      "        [ 0.0174,  0.0207,  0.0212, -0.0129,  0.0330],\n",
      "        [-0.0240, -0.0068, -0.0279,  0.0163,  0.0217],\n",
      "        [-0.0455,  0.0100, -0.0106,  0.0042, -0.0134]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0120, grad_fn=<MinBackward1>), tensor(0.8419, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09897283464670181\n",
      "@sample 1022: tensor([[ 1.6622e-02, -4.4153e-02, -3.4602e-03, -1.9826e-02,  1.2012e-02],\n",
      "        [-1.5211e-02, -4.4075e-03, -4.0534e-03,  4.9684e-03,  8.7135e-03],\n",
      "        [ 5.7238e-03, -2.1182e-02, -1.9263e-03,  5.6321e-03, -5.9680e-03],\n",
      "        [ 1.0911e-02, -2.5797e-02, -1.9456e-02,  2.2393e-02,  3.0362e-03],\n",
      "        [-1.2164e-02, -2.3216e-02,  1.7756e-02,  3.7795e-03, -1.7359e-02],\n",
      "        [-7.9830e-03,  1.3859e-02,  1.6648e-02,  8.8881e-03,  1.2968e-02],\n",
      "        [ 2.6522e-02, -1.2991e-02, -1.5751e-02,  1.8153e-02, -2.5310e-02],\n",
      "        [ 2.2912e-02,  2.2115e-02, -3.7551e-06, -5.7373e-03, -9.5350e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0021, -0.0231,  0.0122,  0.0168,  0.0098],\n",
      "        [ 0.0253, -0.0167,  0.0149, -0.0287, -0.0150],\n",
      "        [ 0.0089, -0.0069,  0.0221,  0.0006,  0.0075],\n",
      "        [ 0.0271, -0.0009,  0.0239, -0.0061,  0.0030],\n",
      "        [ 0.0115, -0.0157,  0.0322,  0.0180, -0.0003],\n",
      "        [ 0.0037,  0.0175, -0.0048,  0.0020,  0.0056],\n",
      "        [-0.0003,  0.0079,  0.0079,  0.0093,  0.0155],\n",
      "        [-0.0185,  0.0247, -0.0057,  0.0027, -0.0167]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0078, grad_fn=<MinBackward1>), tensor(0.8150, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.0918092280626297\n",
      "@sample 1023: tensor([[-0.0065, -0.0009,  0.0150,  0.0103, -0.0002],\n",
      "        [-0.0362, -0.0050,  0.0209, -0.0026,  0.0083],\n",
      "        [ 0.0134, -0.0128,  0.0136, -0.0158, -0.0092],\n",
      "        [-0.0025,  0.0136,  0.0139, -0.0261,  0.0182],\n",
      "        [ 0.0067, -0.0319, -0.0024,  0.0313, -0.0059],\n",
      "        [ 0.0042, -0.0071,  0.0060,  0.0167, -0.0076],\n",
      "        [-0.0031, -0.0316,  0.0018,  0.0166,  0.0085],\n",
      "        [-0.0073,  0.0010, -0.0170,  0.0019, -0.0208]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0124,  0.0188, -0.0030,  0.0171,  0.0224],\n",
      "        [ 0.0151, -0.0153,  0.0402, -0.0207, -0.0164],\n",
      "        [-0.0101,  0.0113, -0.0036, -0.0043, -0.0210],\n",
      "        [-0.0120,  0.0054, -0.0014,  0.0195, -0.0110],\n",
      "        [-0.0153,  0.0269, -0.0038,  0.0203, -0.0136],\n",
      "        [-0.0153,  0.0132, -0.0005,  0.0423,  0.0069],\n",
      "        [ 0.0179, -0.0037,  0.0142, -0.0151,  0.0021],\n",
      "        [-0.0156, -0.0013,  0.0029, -0.0393,  0.0043]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0144, grad_fn=<MinBackward1>), tensor(0.8448, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09150397032499313\n",
      "@sample 1024: tensor([[ 7.6156e-03,  9.0291e-03,  2.8741e-02, -7.7585e-03,  2.3523e-02],\n",
      "        [ 1.8972e-02,  4.5019e-03,  9.5670e-03, -8.9289e-03,  1.4621e-03],\n",
      "        [-8.3651e-03, -4.6920e-03, -1.3460e-02, -2.8529e-02,  2.9894e-03],\n",
      "        [ 8.9245e-03, -3.6966e-02, -1.2766e-02, -5.7726e-03,  1.7876e-02],\n",
      "        [-2.9269e-02, -2.6603e-03, -5.6586e-03,  1.1367e-02, -2.7716e-05],\n",
      "        [-2.2636e-02, -7.5440e-03, -8.8099e-03,  1.8235e-02, -3.3818e-03],\n",
      "        [-1.0141e-02,  3.9003e-03,  1.1344e-02, -1.4456e-02,  9.3003e-03],\n",
      "        [ 3.8002e-02,  2.5472e-02, -1.7111e-02, -1.7401e-02, -5.9248e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-8.9156e-03,  1.4644e-02, -4.1242e-02,  4.9015e-03, -1.4393e-02],\n",
      "        [-8.9768e-03,  3.0867e-02,  3.6309e-03, -7.3318e-03, -1.1622e-02],\n",
      "        [ 1.5925e-02, -6.0043e-03, -3.0089e-02,  9.5028e-03,  1.3321e-02],\n",
      "        [-1.4602e-02, -1.7309e-02,  7.7837e-03,  6.4597e-05, -9.5039e-03],\n",
      "        [ 2.3113e-02, -8.1903e-03,  2.4984e-02,  2.6596e-03, -2.2800e-03],\n",
      "        [ 2.0708e-02,  8.0449e-03,  6.3908e-02, -2.1736e-03, -1.9991e-02],\n",
      "        [ 8.6774e-03,  1.3246e-02,  3.5780e-03,  1.6693e-02,  3.0722e-02],\n",
      "        [-2.8778e-02, -1.6344e-02, -3.0735e-02,  2.8360e-02,  1.4359e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0196, grad_fn=<MinBackward1>), tensor(0.8605, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09202124178409576\n",
      "@sample 1025: tensor([[-0.0282, -0.0013,  0.0084,  0.0011,  0.0118],\n",
      "        [ 0.0110,  0.0132,  0.0023, -0.0235, -0.0275],\n",
      "        [-0.0092,  0.0009,  0.0082,  0.0218,  0.0045],\n",
      "        [-0.0081, -0.0018, -0.0042,  0.0211,  0.0002],\n",
      "        [ 0.0229,  0.0088,  0.0252,  0.0136, -0.0065],\n",
      "        [ 0.0081,  0.0078,  0.0160, -0.0142, -0.0031],\n",
      "        [-0.0029, -0.0195, -0.0084,  0.0141,  0.0217],\n",
      "        [ 0.0101, -0.0219,  0.0010,  0.0040,  0.0181]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0239,  0.0111,  0.0291, -0.0225,  0.0016],\n",
      "        [-0.0101, -0.0171,  0.0318,  0.0154,  0.0098],\n",
      "        [-0.0071,  0.0161, -0.0589, -0.0013, -0.0171],\n",
      "        [ 0.0205,  0.0139,  0.0025, -0.0073,  0.0107],\n",
      "        [-0.0018, -0.0023, -0.0068, -0.0132,  0.0080],\n",
      "        [-0.0091,  0.0010,  0.0141, -0.0078, -0.0090],\n",
      "        [ 0.0219,  0.0110,  0.0213,  0.0047,  0.0080],\n",
      "        [ 0.0272,  0.0189,  0.0136, -0.0189, -0.0172]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0180, grad_fn=<MinBackward1>), tensor(0.8408, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09358443319797516\n",
      "@sample 1026: tensor([[ 2.9054e-03, -2.3093e-02,  1.8860e-02,  3.1931e-02, -2.7860e-02],\n",
      "        [ 6.3575e-03,  1.1374e-03, -5.3318e-03,  6.7198e-03, -1.6351e-02],\n",
      "        [-7.9893e-03,  1.6258e-02, -2.0671e-02, -8.9923e-04,  7.0095e-03],\n",
      "        [ 2.8427e-05,  3.3367e-03,  3.0990e-03,  4.7184e-03,  1.4202e-03],\n",
      "        [-4.9021e-03,  3.2229e-02,  1.9666e-02, -6.8744e-03,  1.6867e-02],\n",
      "        [ 1.4698e-02, -3.0793e-02,  4.9430e-03,  4.0841e-02, -2.9248e-02],\n",
      "        [ 2.4125e-02,  1.3367e-02, -1.4363e-02,  1.7988e-02, -3.5816e-03],\n",
      "        [-8.3388e-04,  2.1194e-02,  1.4157e-02, -9.0008e-03,  7.6842e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0110,  0.0244,  0.0495, -0.0328,  0.0143],\n",
      "        [ 0.0090,  0.0172,  0.0396, -0.0278, -0.0191],\n",
      "        [ 0.0090,  0.0276, -0.0279,  0.0159,  0.0124],\n",
      "        [ 0.0020,  0.0071, -0.0098,  0.0184,  0.0033],\n",
      "        [ 0.0048,  0.0345, -0.0006,  0.0177,  0.0066],\n",
      "        [ 0.0211,  0.0308,  0.0341,  0.0243,  0.0199],\n",
      "        [-0.0035, -0.0136, -0.0883,  0.0265,  0.0197],\n",
      "        [-0.0127, -0.0150, -0.0117, -0.0146, -0.0302]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0082, grad_fn=<MinBackward1>), tensor(0.8799, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09191380441188812\n",
      "@sample 1027: tensor([[ 0.0070,  0.0083, -0.0108,  0.0027, -0.0070],\n",
      "        [-0.0046,  0.0209,  0.0002, -0.0057,  0.0098],\n",
      "        [ 0.0105,  0.0045,  0.0042, -0.0103,  0.0088],\n",
      "        [ 0.0040,  0.0091,  0.0186, -0.0098, -0.0358],\n",
      "        [-0.0123, -0.0056, -0.0037,  0.0041,  0.0016],\n",
      "        [-0.0158,  0.0180,  0.0138,  0.0160, -0.0045],\n",
      "        [-0.0054, -0.0069,  0.0061, -0.0002, -0.0194],\n",
      "        [-0.0114,  0.0091, -0.0198,  0.0061,  0.0219]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0080, -0.0087, -0.0129,  0.0126,  0.0155],\n",
      "        [ 0.0180,  0.0104, -0.0082, -0.0060,  0.0113],\n",
      "        [-0.0110, -0.0409,  0.0421,  0.0028, -0.0604],\n",
      "        [-0.0051, -0.0095,  0.0310, -0.0311, -0.0103],\n",
      "        [ 0.0042,  0.0154,  0.0182, -0.0329, -0.0046],\n",
      "        [ 0.0106,  0.0105, -0.0046, -0.0212, -0.0215],\n",
      "        [ 0.0024, -0.0120,  0.0407,  0.0084,  0.0078],\n",
      "        [ 0.0286,  0.0046,  0.0070, -0.0277,  0.0027]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0149, grad_fn=<MinBackward1>), tensor(0.8614, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09589967876672745\n",
      "@sample 1028: tensor([[ 0.0030,  0.0189,  0.0060,  0.0295,  0.0028],\n",
      "        [ 0.0046,  0.0007, -0.0227,  0.0097, -0.0049],\n",
      "        [-0.0225, -0.0143, -0.0254,  0.0248, -0.0205],\n",
      "        [ 0.0156, -0.0126,  0.0090,  0.0193, -0.0161],\n",
      "        [ 0.0013, -0.0033,  0.0185, -0.0011, -0.0046],\n",
      "        [-0.0391, -0.0038,  0.0032, -0.0401,  0.0219],\n",
      "        [ 0.0091,  0.0063,  0.0139, -0.0016,  0.0126],\n",
      "        [-0.0251, -0.0115, -0.0083,  0.0269,  0.0029]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0049,  0.0313, -0.0013, -0.0148, -0.0081],\n",
      "        [-0.0024, -0.0021, -0.0173,  0.0102,  0.0008],\n",
      "        [ 0.0019, -0.0580,  0.0016,  0.0009,  0.0121],\n",
      "        [ 0.0116,  0.0244,  0.0103, -0.0100,  0.0030],\n",
      "        [-0.0059, -0.0118, -0.0114,  0.0011, -0.0075],\n",
      "        [-0.0051, -0.1112, -0.0095,  0.0109, -0.0338],\n",
      "        [-0.0192, -0.0137,  0.0247,  0.0502,  0.0084],\n",
      "        [ 0.0441, -0.0319,  0.0170, -0.0143,  0.0054]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0165, grad_fn=<MinBackward1>), tensor(0.9123, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11484751105308533\n",
      "@sample 1029: tensor([[-2.3112e-02,  3.2880e-02,  3.0421e-02, -1.6786e-02, -2.0230e-03],\n",
      "        [-6.0712e-05,  1.3147e-02, -3.5590e-03, -3.0243e-03,  3.5238e-04],\n",
      "        [ 2.9884e-03,  1.8504e-02, -1.0586e-02, -2.6540e-02,  1.6801e-02],\n",
      "        [-6.6436e-03,  7.2596e-03, -1.7393e-02,  1.9256e-02, -1.8243e-02],\n",
      "        [-4.0078e-03, -9.1423e-03, -5.3132e-03,  8.2991e-04,  9.4969e-03],\n",
      "        [-1.2523e-02,  1.6860e-02,  2.6995e-02, -4.3949e-02,  2.1041e-02],\n",
      "        [ 3.0308e-03,  5.8738e-03,  2.7391e-02, -5.9462e-03,  1.4587e-03],\n",
      "        [ 3.0986e-02,  4.7909e-02,  4.9953e-02, -2.6667e-02,  3.3768e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-3.6872e-02,  1.7987e-02, -3.9664e-02,  1.1489e-02,  1.9744e-06],\n",
      "        [-5.6006e-05,  9.9121e-03, -1.7459e-03, -4.1031e-03, -6.0524e-04],\n",
      "        [ 1.4103e-02, -1.2027e-03, -5.5991e-02,  5.7266e-02,  4.8434e-03],\n",
      "        [ 1.4963e-02, -1.6248e-02, -4.4299e-03, -1.0845e-02, -1.0029e-02],\n",
      "        [ 5.1468e-03,  7.2441e-03,  6.4698e-03, -1.3455e-02, -1.7101e-03],\n",
      "        [-5.4232e-02, -2.1119e-02, -3.7188e-02, -5.8460e-04, -1.5486e-02],\n",
      "        [-1.6217e-02, -2.8668e-03, -5.0849e-03, -5.5160e-03, -2.7009e-03],\n",
      "        [-4.8719e-02, -3.7552e-02, -8.8485e-02,  2.4527e-02,  8.4223e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0128, grad_fn=<MinBackward1>), tensor(0.8639, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09373853355646133\n",
      "@sample 1030: tensor([[ 0.0025, -0.0179, -0.0117,  0.0213, -0.0034],\n",
      "        [-0.0164,  0.0202, -0.0124, -0.0074,  0.0151],\n",
      "        [ 0.0272, -0.0009, -0.0161,  0.0138, -0.0169],\n",
      "        [ 0.0306, -0.0258,  0.0159, -0.0002, -0.0002],\n",
      "        [-0.0076,  0.0422,  0.0078, -0.0410,  0.0103],\n",
      "        [ 0.0013, -0.0076,  0.0055,  0.0214, -0.0176],\n",
      "        [ 0.0041,  0.0404,  0.0070, -0.0021, -0.0111],\n",
      "        [ 0.0162, -0.0187, -0.0365, -0.0071, -0.0264]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-2.5151e-02,  1.9024e-02, -2.3456e-03, -1.3842e-02,  9.4200e-03],\n",
      "        [-8.6319e-03,  2.5101e-03, -4.5247e-03,  6.2509e-03,  6.3897e-03],\n",
      "        [ 4.2055e-03, -2.1203e-02, -5.4834e-02, -3.8560e-03,  2.1172e-02],\n",
      "        [-2.8167e-02, -4.0989e-03,  4.4523e-03,  3.7764e-02,  1.5111e-02],\n",
      "        [-4.0248e-03,  5.6554e-03, -4.3652e-02,  5.0642e-02,  1.0387e-02],\n",
      "        [ 4.6825e-04,  4.9547e-03, -6.6709e-03, -9.9987e-06, -1.4772e-02],\n",
      "        [-1.8230e-02,  2.9981e-02, -2.8884e-02, -3.0338e-02, -1.1731e-02],\n",
      "        [-7.9475e-03, -3.0158e-02,  1.1306e-02, -4.2898e-02, -2.0777e-04]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0143, grad_fn=<MinBackward1>), tensor(0.8491, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09226209670305252\n",
      "@sample 1031: tensor([[-2.0234e-02,  5.7541e-03, -3.6670e-04,  6.5015e-03, -8.3518e-04],\n",
      "        [-7.7718e-03, -1.8375e-03,  5.0020e-03,  3.6947e-02,  7.0627e-03],\n",
      "        [ 1.5580e-02,  2.8182e-02,  1.8952e-02, -1.7670e-02, -2.4496e-02],\n",
      "        [-1.3306e-02, -1.3948e-03,  5.4887e-03, -1.6740e-02,  1.3833e-02],\n",
      "        [-2.4699e-02, -2.4083e-02, -2.6951e-02,  8.9789e-03, -1.6339e-02],\n",
      "        [ 1.7277e-02, -8.3340e-03,  1.4155e-02,  3.3487e-02, -2.9063e-02],\n",
      "        [ 2.3610e-05,  3.2258e-02,  1.5351e-02, -1.9635e-02,  1.4572e-02],\n",
      "        [ 5.3839e-03, -7.8949e-03, -1.6652e-02,  1.4537e-02, -1.3941e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0116,  0.0233, -0.0278,  0.0034,  0.0046],\n",
      "        [ 0.0162,  0.0376,  0.0051, -0.0125,  0.0003],\n",
      "        [ 0.0008,  0.0271,  0.0100,  0.0144,  0.0159],\n",
      "        [-0.0045, -0.0411, -0.0058, -0.0007, -0.0323],\n",
      "        [ 0.0072, -0.0440, -0.0008, -0.0034,  0.0127],\n",
      "        [-0.0095,  0.0483, -0.0117, -0.0004,  0.0054],\n",
      "        [ 0.0451,  0.0037, -0.0476,  0.0134, -0.0025],\n",
      "        [ 0.0143, -0.0013,  0.0021, -0.0083, -0.0166]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0152, grad_fn=<MinBackward1>), tensor(0.8403, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08660679310560226\n",
      "@sample 1032: tensor([[-0.0065,  0.0076, -0.0024,  0.0048,  0.0018],\n",
      "        [-0.0025,  0.0305, -0.0252,  0.0130, -0.0011],\n",
      "        [-0.0188,  0.0010, -0.0110,  0.0177, -0.0015],\n",
      "        [-0.0025, -0.0269, -0.0027,  0.0008,  0.0294],\n",
      "        [ 0.0039, -0.0133, -0.0013, -0.0041, -0.0020],\n",
      "        [-0.0048,  0.0123,  0.0020,  0.0055, -0.0118],\n",
      "        [ 0.0105,  0.0052,  0.0066,  0.0327, -0.0202],\n",
      "        [ 0.0181,  0.0202, -0.0072,  0.0374, -0.0358]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0037,  0.0131,  0.0038, -0.0261, -0.0162],\n",
      "        [ 0.0068,  0.0183, -0.0429, -0.0146, -0.0128],\n",
      "        [-0.0116,  0.0219, -0.0241,  0.0071,  0.0042],\n",
      "        [ 0.0153, -0.0179,  0.0101, -0.0075, -0.0151],\n",
      "        [-0.0063,  0.0063,  0.0288,  0.0197,  0.0182],\n",
      "        [ 0.0123, -0.0011,  0.0029, -0.0032, -0.0318],\n",
      "        [ 0.0436,  0.0294,  0.0534, -0.0157,  0.0106],\n",
      "        [ 0.0268,  0.0484,  0.0036,  0.0029, -0.0195]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0188, grad_fn=<MinBackward1>), tensor(0.8402, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09307374805212021\n",
      "@sample 1033: tensor([[ 0.0070,  0.0107, -0.0120,  0.0086, -0.0119],\n",
      "        [ 0.0165,  0.0375,  0.0050, -0.0398, -0.0024],\n",
      "        [ 0.0143,  0.0006,  0.0180, -0.0230, -0.0114],\n",
      "        [ 0.0094, -0.0116, -0.0245, -0.0331,  0.0052],\n",
      "        [-0.0062,  0.0277,  0.0141, -0.0361,  0.0087],\n",
      "        [ 0.0159, -0.0039,  0.0090,  0.0023, -0.0335],\n",
      "        [ 0.0026, -0.0060, -0.0250,  0.0218,  0.0020],\n",
      "        [ 0.0129, -0.0246, -0.0448,  0.0265, -0.0301]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0048, -0.0094, -0.0220,  0.0016, -0.0076],\n",
      "        [-0.0230,  0.0029, -0.0383,  0.0045,  0.0091],\n",
      "        [-0.0259,  0.0206, -0.0551,  0.0293,  0.0224],\n",
      "        [-0.0309, -0.0274, -0.0173,  0.0080,  0.0050],\n",
      "        [-0.0113, -0.0088, -0.0273,  0.0236,  0.0095],\n",
      "        [ 0.0082,  0.0046,  0.0175,  0.0086,  0.0172],\n",
      "        [-0.0060,  0.0146, -0.0094,  0.0043, -0.0022],\n",
      "        [ 0.0154, -0.0055,  0.0059, -0.0184,  0.0320]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0155, grad_fn=<MinBackward1>), tensor(0.7954, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.0942738726735115\n",
      "@sample 1034: tensor([[ 0.0050,  0.0041,  0.0036,  0.0046, -0.0016],\n",
      "        [ 0.0222,  0.0222,  0.0100, -0.0162,  0.0113],\n",
      "        [ 0.0096, -0.0101,  0.0109,  0.0114, -0.0104],\n",
      "        [-0.0014, -0.0032,  0.0156, -0.0236, -0.0061],\n",
      "        [ 0.0071, -0.0159, -0.0049,  0.0115, -0.0064],\n",
      "        [-0.0082,  0.0223, -0.0068, -0.0082,  0.0082],\n",
      "        [-0.0037, -0.0154, -0.0023,  0.0056, -0.0009],\n",
      "        [ 0.0042, -0.0236, -0.0256,  0.0199, -0.0060]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0062,  0.0202,  0.0036, -0.0021,  0.0025],\n",
      "        [-0.0315, -0.0022, -0.0425,  0.0148, -0.0127],\n",
      "        [ 0.0089,  0.0006, -0.0254, -0.0093, -0.0055],\n",
      "        [-0.0042, -0.0016,  0.0039,  0.0019, -0.0191],\n",
      "        [ 0.0085,  0.0052,  0.0243, -0.0326,  0.0018],\n",
      "        [-0.0036,  0.0214, -0.0146, -0.0044,  0.0066],\n",
      "        [ 0.0277, -0.0228,  0.0203, -0.0069, -0.0004],\n",
      "        [ 0.0020,  0.0162, -0.0281,  0.0191, -0.0001]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0149, grad_fn=<MinBackward1>), tensor(0.8853, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08647466450929642\n",
      "@sample 1035: tensor([[-0.0029,  0.0040,  0.0048, -0.0022, -0.0133],\n",
      "        [ 0.0232, -0.0539, -0.0126,  0.0020,  0.0241],\n",
      "        [-0.0268, -0.0084, -0.0159,  0.0028, -0.0125],\n",
      "        [ 0.0154, -0.0143, -0.0016,  0.0053,  0.0018],\n",
      "        [-0.0381, -0.0059,  0.0049,  0.0050, -0.0173],\n",
      "        [-0.0063,  0.0170, -0.0012, -0.0129, -0.0049],\n",
      "        [ 0.0011, -0.0086,  0.0058,  0.0188,  0.0084],\n",
      "        [ 0.0150, -0.0111, -0.0109,  0.0144, -0.0387]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0236, -0.0059,  0.0022, -0.0152,  0.0196],\n",
      "        [-0.0036, -0.0266,  0.0310,  0.0069, -0.0169],\n",
      "        [-0.0007, -0.0140,  0.0484, -0.0469, -0.0049],\n",
      "        [-0.0167,  0.0115,  0.0085,  0.0137,  0.0267],\n",
      "        [ 0.0294, -0.0205,  0.0342, -0.0044,  0.0029],\n",
      "        [-0.0014,  0.0057,  0.0222,  0.0024,  0.0096],\n",
      "        [ 0.0011,  0.0060,  0.0209, -0.0049,  0.0219],\n",
      "        [ 0.0255, -0.0286,  0.0044,  0.0298,  0.0384]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0171, grad_fn=<MinBackward1>), tensor(0.8557, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10831914097070694\n",
      "@sample 1036: tensor([[-3.1176e-03, -7.7626e-04, -2.1041e-02,  2.8534e-03,  9.9679e-03],\n",
      "        [-8.8067e-03, -8.7701e-03, -1.2560e-02,  1.7857e-02, -8.3980e-03],\n",
      "        [ 1.3598e-02,  4.8600e-03, -3.8768e-02, -4.9606e-03,  2.3576e-02],\n",
      "        [-2.2687e-02, -5.2578e-03, -2.8626e-02,  8.9771e-03, -8.2543e-03],\n",
      "        [ 1.4448e-02, -4.6736e-02,  1.0602e-02,  1.5618e-02, -8.5370e-03],\n",
      "        [-6.0274e-03, -4.4167e-05, -2.1012e-02,  1.0116e-02,  2.4801e-03],\n",
      "        [ 7.4195e-03,  3.8001e-02,  2.1850e-02, -1.1994e-02,  2.8691e-02],\n",
      "        [-5.0974e-03,  1.5782e-02,  9.7600e-03,  8.2806e-03, -1.6481e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0074, -0.0180,  0.0027, -0.0048,  0.0056],\n",
      "        [ 0.0308,  0.0110, -0.0018,  0.0106,  0.0039],\n",
      "        [ 0.0349, -0.0315, -0.0324, -0.0238, -0.0123],\n",
      "        [-0.0060, -0.0340,  0.0342, -0.0028,  0.0046],\n",
      "        [-0.0212,  0.0052, -0.0096,  0.0137,  0.0010],\n",
      "        [-0.0135,  0.0178, -0.0295,  0.0259,  0.0271],\n",
      "        [ 0.0053, -0.0102, -0.0325, -0.0008, -0.0100],\n",
      "        [-0.0075, -0.0066, -0.0432,  0.0164,  0.0143]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0153, grad_fn=<MinBackward1>), tensor(0.9014, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09474507719278336\n",
      "@sample 1037: tensor([[ 0.0108,  0.0022, -0.0094,  0.0005,  0.0083],\n",
      "        [-0.0005, -0.0079, -0.0049, -0.0196, -0.0124],\n",
      "        [-0.0080,  0.0180, -0.0209,  0.0342, -0.0133],\n",
      "        [-0.0109, -0.0160, -0.0026,  0.0478, -0.0197],\n",
      "        [-0.0084, -0.0033, -0.0237,  0.0296, -0.0094],\n",
      "        [ 0.0081, -0.0193,  0.0023,  0.0064,  0.0017],\n",
      "        [-0.0105,  0.0006, -0.0007, -0.0189,  0.0268],\n",
      "        [-0.0106, -0.0025,  0.0151, -0.0271,  0.0085]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0151,  0.0059,  0.0158, -0.0180, -0.0105],\n",
      "        [-0.0216, -0.0340,  0.0344, -0.0272, -0.0011],\n",
      "        [ 0.0286,  0.0042,  0.0055, -0.0378, -0.0049],\n",
      "        [ 0.0085,  0.0277, -0.0246, -0.0223, -0.0033],\n",
      "        [ 0.0235, -0.0090,  0.0150, -0.0389, -0.0313],\n",
      "        [ 0.0114, -0.0152, -0.0244,  0.0027, -0.0159],\n",
      "        [-0.0277, -0.0755,  0.0221, -0.0248, -0.0398],\n",
      "        [-0.0289,  0.0106,  0.0093,  0.0078, -0.0176]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0153, grad_fn=<MinBackward1>), tensor(0.8738, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1119321882724762\n",
      "@sample 1038: tensor([[-0.0018,  0.0098, -0.0068, -0.0141,  0.0013],\n",
      "        [-0.0001,  0.0308,  0.0371, -0.0209,  0.0185],\n",
      "        [ 0.0009, -0.0140, -0.0067,  0.0222, -0.0098],\n",
      "        [-0.0110, -0.0075,  0.0056,  0.0193,  0.0169],\n",
      "        [-0.0198,  0.0122,  0.0262, -0.0196, -0.0073],\n",
      "        [ 0.0007, -0.0017,  0.0040, -0.0029, -0.0204],\n",
      "        [ 0.0048, -0.0245, -0.0133, -0.0012,  0.0017],\n",
      "        [-0.0090, -0.0016, -0.0058,  0.0085,  0.0143]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0218, -0.0157, -0.0047, -0.0066, -0.0125],\n",
      "        [-0.0018,  0.0177, -0.0683,  0.0352,  0.0149],\n",
      "        [ 0.0084,  0.0156, -0.0230,  0.0016, -0.0018],\n",
      "        [ 0.0086,  0.0144, -0.0188,  0.0053,  0.0070],\n",
      "        [-0.0351,  0.0029,  0.0103, -0.0014, -0.0180],\n",
      "        [-0.0208,  0.0298,  0.0251, -0.0061,  0.0229],\n",
      "        [ 0.0181, -0.0257,  0.0110, -0.0328, -0.0119],\n",
      "        [ 0.0121,  0.0258, -0.0310,  0.0117, -0.0102]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0108, grad_fn=<MinBackward1>), tensor(0.7956, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09069129824638367\n",
      "@sample 1039: tensor([[ 0.0056,  0.0246,  0.0031, -0.0103,  0.0128],\n",
      "        [-0.0200,  0.0009, -0.0188,  0.0091,  0.0046],\n",
      "        [-0.0206,  0.0128,  0.0033,  0.0344, -0.0161],\n",
      "        [-0.0221, -0.0171, -0.0189,  0.0257, -0.0158],\n",
      "        [-0.0071, -0.0070, -0.0210,  0.0138, -0.0004],\n",
      "        [-0.0010, -0.0233, -0.0024,  0.0165, -0.0103],\n",
      "        [-0.0262,  0.0026, -0.0140, -0.0074,  0.0084],\n",
      "        [-0.0262,  0.0431,  0.0029, -0.0074,  0.0063]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0044,  0.0122, -0.0073,  0.0136, -0.0062],\n",
      "        [ 0.0110,  0.0171, -0.0074, -0.0205,  0.0075],\n",
      "        [-0.0037,  0.0089, -0.0280,  0.0027,  0.0038],\n",
      "        [ 0.0293, -0.0252, -0.0067, -0.0146,  0.0047],\n",
      "        [-0.0259,  0.0186,  0.0092,  0.0254,  0.0025],\n",
      "        [ 0.0259,  0.0161,  0.0502, -0.0218, -0.0010],\n",
      "        [ 0.0202, -0.0299,  0.0275,  0.0124,  0.0105],\n",
      "        [ 0.0374,  0.0281,  0.0138, -0.0350, -0.0093]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0111, grad_fn=<MinBackward1>), tensor(0.8779, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10003858804702759\n",
      "@sample 1040: tensor([[-0.0364, -0.0064, -0.0043, -0.0033,  0.0038],\n",
      "        [-0.0109,  0.0076,  0.0039,  0.0104,  0.0055],\n",
      "        [-0.0165,  0.0085,  0.0249, -0.0063,  0.0107],\n",
      "        [ 0.0063, -0.0047,  0.0017,  0.0061,  0.0069],\n",
      "        [ 0.0134, -0.0200, -0.0153,  0.0063,  0.0118],\n",
      "        [ 0.0081,  0.0431, -0.0090, -0.0503,  0.0176],\n",
      "        [-0.0132, -0.0220,  0.0266, -0.0194,  0.0059],\n",
      "        [ 0.0156, -0.0204,  0.0016,  0.0007, -0.0017]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0002, -0.0342, -0.0006,  0.0025,  0.0069],\n",
      "        [ 0.0220,  0.0204,  0.0072,  0.0054,  0.0122],\n",
      "        [ 0.0063,  0.0206,  0.0009,  0.0273, -0.0107],\n",
      "        [ 0.0307, -0.0142, -0.0372,  0.0196,  0.0126],\n",
      "        [ 0.0037,  0.0046, -0.0286,  0.0102, -0.0025],\n",
      "        [ 0.0052, -0.0584, -0.0558,  0.0469,  0.0161],\n",
      "        [-0.0172, -0.0250, -0.0013, -0.0040,  0.0061],\n",
      "        [-0.0032, -0.0204,  0.0328, -0.0040, -0.0204]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0135, grad_fn=<MinBackward1>), tensor(0.8429, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.0957871600985527\n",
      "@sample 1041: tensor([[-0.0055, -0.0027,  0.0251, -0.0326,  0.0259],\n",
      "        [-0.0158, -0.0435,  0.0151, -0.0082,  0.0220],\n",
      "        [-0.0113, -0.0144,  0.0067,  0.0148,  0.0074],\n",
      "        [ 0.0099,  0.0089, -0.0046,  0.0086, -0.0001],\n",
      "        [ 0.0140, -0.0033, -0.0013,  0.0136, -0.0013],\n",
      "        [-0.0098, -0.0109, -0.0045, -0.0051,  0.0096],\n",
      "        [-0.0035,  0.0132, -0.0084, -0.0134,  0.0220],\n",
      "        [-0.0103,  0.0338,  0.0281, -0.0550,  0.0077]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0242, -0.0339,  0.0072,  0.0058, -0.0244],\n",
      "        [-0.0132, -0.0110, -0.0141,  0.0042,  0.0141],\n",
      "        [ 0.0121,  0.0023,  0.0249, -0.0374,  0.0139],\n",
      "        [ 0.0044,  0.0131, -0.0002, -0.0161, -0.0311],\n",
      "        [ 0.0081,  0.0480,  0.0273,  0.0155,  0.0175],\n",
      "        [ 0.0054, -0.0022,  0.0177, -0.0150, -0.0126],\n",
      "        [-0.0190, -0.0080,  0.0455, -0.0359, -0.0414],\n",
      "        [-0.0775, -0.0505, -0.0252,  0.0165, -0.0169]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0144, grad_fn=<MinBackward1>), tensor(0.8226, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09394937753677368\n",
      "@sample 1042: tensor([[ 0.0039,  0.0106, -0.0095,  0.0077,  0.0073],\n",
      "        [-0.0169,  0.0248,  0.0170, -0.0459,  0.0281],\n",
      "        [ 0.0135,  0.0108, -0.0128, -0.0186, -0.0193],\n",
      "        [ 0.0068,  0.0156,  0.0324, -0.0099, -0.0011],\n",
      "        [-0.0204,  0.0089,  0.0014,  0.0158, -0.0034],\n",
      "        [ 0.0118,  0.0053,  0.0129, -0.0198,  0.0152],\n",
      "        [ 0.0036, -0.0035, -0.0045,  0.0071,  0.0095],\n",
      "        [ 0.0272,  0.0300, -0.0024,  0.0303, -0.0366]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0024,  0.0050, -0.0123,  0.0058,  0.0101],\n",
      "        [-0.0111, -0.0108, -0.0526, -0.0073, -0.0072],\n",
      "        [ 0.0128,  0.0180, -0.0178,  0.0108,  0.0027],\n",
      "        [-0.0263,  0.0151, -0.0207,  0.0045,  0.0060],\n",
      "        [ 0.0224,  0.0194,  0.0216, -0.0254, -0.0041],\n",
      "        [-0.0071, -0.0072, -0.0120,  0.0147,  0.0272],\n",
      "        [ 0.0171, -0.0028,  0.0043, -0.0093, -0.0193],\n",
      "        [-0.0082,  0.0585,  0.0376, -0.0017, -0.0096]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0156, grad_fn=<MinBackward1>), tensor(0.8745, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.07965227961540222\n",
      "@sample 1043: tensor([[-5.7028e-03, -7.8757e-03, -1.7357e-03,  4.3162e-03, -3.3537e-03],\n",
      "        [-3.3822e-04, -2.2564e-03,  8.1474e-03, -3.2649e-02,  3.6985e-02],\n",
      "        [-1.1553e-03, -3.3197e-02, -8.8588e-03,  4.6296e-03,  5.6775e-03],\n",
      "        [-3.6657e-02, -8.3555e-03,  1.9946e-02, -2.0354e-02,  1.6880e-02],\n",
      "        [-2.6643e-03, -7.1883e-05,  6.0115e-03,  3.5791e-03,  8.0177e-03],\n",
      "        [-2.1766e-02,  3.8265e-04,  5.5492e-05, -2.0860e-03,  1.2398e-02],\n",
      "        [-1.3355e-02, -2.7560e-02,  1.1000e-02, -1.5057e-02,  1.5941e-02],\n",
      "        [ 2.2141e-02,  3.7721e-02,  2.5532e-02, -3.5472e-02, -1.1884e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0155,  0.0002,  0.0060, -0.0062, -0.0062],\n",
      "        [-0.0070, -0.0022, -0.0006,  0.0305, -0.0043],\n",
      "        [ 0.0058, -0.0149,  0.0335, -0.0119, -0.0350],\n",
      "        [-0.0263,  0.0318, -0.0289,  0.0244,  0.0183],\n",
      "        [ 0.0030, -0.0001, -0.0086, -0.0154, -0.0040],\n",
      "        [ 0.0018, -0.0019, -0.0096, -0.0244,  0.0083],\n",
      "        [ 0.0022, -0.0380,  0.0391,  0.0294, -0.0154],\n",
      "        [-0.0287,  0.0155, -0.0281,  0.0259,  0.0247]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0180, grad_fn=<MinBackward1>), tensor(0.8295, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09745785593986511\n",
      "@sample 1044: tensor([[ 1.0389e-02,  3.6055e-03, -3.2168e-02,  9.6941e-03, -8.0035e-03],\n",
      "        [ 2.2567e-02, -3.2400e-02,  3.5980e-03, -4.4231e-02,  2.3731e-02],\n",
      "        [-1.3345e-02,  1.2013e-02, -2.0076e-02, -2.6320e-02, -1.1066e-02],\n",
      "        [-2.6830e-02,  1.4217e-02,  1.8999e-02, -6.8450e-03, -9.0742e-03],\n",
      "        [-1.7378e-02,  8.1900e-03,  1.3435e-02, -1.7628e-02,  1.6646e-02],\n",
      "        [-2.3060e-02,  1.3611e-02,  1.3437e-02, -1.3173e-02,  1.7802e-03],\n",
      "        [-3.2070e-02, -1.7554e-02,  1.3282e-02,  2.4989e-05,  1.6214e-02],\n",
      "        [-2.7390e-02, -5.4649e-03,  4.9406e-04, -1.1591e-02,  1.6289e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0128,  0.0436,  0.0160,  0.0018,  0.0155],\n",
      "        [-0.0287, -0.0378,  0.0310, -0.0343, -0.0299],\n",
      "        [-0.0124, -0.0202, -0.0135,  0.0109, -0.0024],\n",
      "        [-0.0150,  0.0043,  0.0057,  0.0056, -0.0057],\n",
      "        [ 0.0087,  0.0020,  0.0096, -0.0130, -0.0089],\n",
      "        [ 0.0084, -0.0038, -0.0209,  0.0320,  0.0055],\n",
      "        [ 0.0103, -0.0242,  0.0379, -0.0247, -0.0094],\n",
      "        [-0.0103,  0.0073, -0.0066,  0.0020,  0.0065]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0116, grad_fn=<MinBackward1>), tensor(0.8374, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08990108221769333\n",
      "@sample 1045: tensor([[-5.9894e-03, -1.0970e-02,  3.2054e-02, -2.8584e-02,  1.9595e-02],\n",
      "        [-3.4902e-03, -7.5743e-03, -2.7842e-02,  1.1021e-02, -1.3661e-03],\n",
      "        [ 2.3451e-03, -2.1308e-02,  4.1560e-03, -2.5606e-02,  3.6703e-03],\n",
      "        [-8.5931e-05, -2.7707e-02, -4.2423e-03,  1.8531e-02, -5.8585e-03],\n",
      "        [ 1.0118e-02, -2.9211e-03, -1.0257e-02,  1.0381e-02, -1.5005e-02],\n",
      "        [ 1.8868e-02, -3.6649e-02,  1.0046e-02, -1.1290e-03,  1.2120e-02],\n",
      "        [ 9.3159e-03, -1.9119e-02,  7.7787e-03, -1.1547e-02,  1.8421e-02],\n",
      "        [ 2.1195e-02, -9.2845e-03,  1.1914e-02, -3.2906e-02,  4.0374e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0236, -0.0002,  0.0264, -0.0089, -0.0173],\n",
      "        [ 0.0306, -0.0122, -0.0115, -0.0169,  0.0060],\n",
      "        [-0.0135, -0.0387,  0.0508, -0.0157,  0.0083],\n",
      "        [ 0.0078,  0.0095,  0.0023, -0.0058, -0.0115],\n",
      "        [-0.0029, -0.0035,  0.0040, -0.0169, -0.0113],\n",
      "        [ 0.0008, -0.0177,  0.0092,  0.0038,  0.0191],\n",
      "        [ 0.0113,  0.0108, -0.0334,  0.0164, -0.0247],\n",
      "        [ 0.0138,  0.0025,  0.0376,  0.0526,  0.0393]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0123, grad_fn=<MinBackward1>), tensor(0.8557, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09163784980773926\n",
      "@sample 1046: tensor([[-0.0163, -0.0018,  0.0023, -0.0023,  0.0097],\n",
      "        [ 0.0003, -0.0191, -0.0191, -0.0059,  0.0183],\n",
      "        [ 0.0194,  0.0080, -0.0077,  0.0082, -0.0068],\n",
      "        [ 0.0062,  0.0227,  0.0219, -0.0353, -0.0260],\n",
      "        [ 0.0135,  0.0175,  0.0106, -0.0188,  0.0041],\n",
      "        [ 0.0176,  0.0017, -0.0013,  0.0132, -0.0407],\n",
      "        [-0.0074, -0.0167,  0.0095, -0.0129,  0.0235],\n",
      "        [-0.0164, -0.0075,  0.0060,  0.0267,  0.0139]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0026, -0.0116,  0.0223,  0.0065,  0.0059],\n",
      "        [-0.0240, -0.0039,  0.0298, -0.0199, -0.0162],\n",
      "        [-0.0070, -0.0080, -0.0007,  0.0295,  0.0233],\n",
      "        [-0.0364, -0.0216, -0.0211,  0.0335,  0.0243],\n",
      "        [-0.0179, -0.0245,  0.0037,  0.0074, -0.0242],\n",
      "        [ 0.0425,  0.0374, -0.0051, -0.0093,  0.0241],\n",
      "        [-0.0080,  0.0311,  0.0446, -0.0269, -0.0264],\n",
      "        [ 0.0161,  0.0332, -0.0108, -0.0117,  0.0015]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0101, grad_fn=<MinBackward1>), tensor(0.8389, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10204675048589706\n",
      "@sample 1047: tensor([[-0.0089, -0.0099,  0.0281, -0.0068,  0.0140],\n",
      "        [ 0.0247, -0.0110,  0.0088,  0.0146, -0.0176],\n",
      "        [ 0.0126, -0.0052,  0.0218, -0.0182,  0.0031],\n",
      "        [-0.0008,  0.0207,  0.0172,  0.0110, -0.0044],\n",
      "        [-0.0093,  0.0179, -0.0079,  0.0087, -0.0130],\n",
      "        [-0.0026, -0.0144,  0.0079,  0.0176,  0.0086],\n",
      "        [-0.0119,  0.0315,  0.0235, -0.0149, -0.0142],\n",
      "        [ 0.0005,  0.0022,  0.0130,  0.0109,  0.0025]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0257, -0.0014,  0.0333,  0.0141, -0.0163],\n",
      "        [ 0.0046, -0.0053,  0.0141,  0.0059,  0.0108],\n",
      "        [-0.0110,  0.0187,  0.0110, -0.0140, -0.0144],\n",
      "        [ 0.0196,  0.0118,  0.0348, -0.0003, -0.0154],\n",
      "        [ 0.0087,  0.0133, -0.0098, -0.0100, -0.0057],\n",
      "        [ 0.0236,  0.0347,  0.0028, -0.0278,  0.0042],\n",
      "        [-0.0033,  0.0101, -0.0158, -0.0044, -0.0118],\n",
      "        [ 0.0048,  0.0113,  0.0167, -0.0095,  0.0081]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0118, grad_fn=<MinBackward1>), tensor(0.8673, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.0919935405254364\n",
      "@sample 1048: tensor([[-0.0133,  0.0014,  0.0144, -0.0091,  0.0254],\n",
      "        [-0.0054,  0.0235, -0.0004,  0.0070, -0.0028],\n",
      "        [ 0.0309,  0.0111,  0.0081,  0.0099, -0.0092],\n",
      "        [-0.0029,  0.0073,  0.0100,  0.0025,  0.0148],\n",
      "        [-0.0066, -0.0172,  0.0097, -0.0082,  0.0177],\n",
      "        [ 0.0136,  0.0192,  0.0166, -0.0034, -0.0122],\n",
      "        [-0.0224,  0.0201, -0.0160,  0.0169, -0.0054],\n",
      "        [-0.0098, -0.0016, -0.0122,  0.0054, -0.0053]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0105, -0.0136,  0.0239,  0.0008,  0.0055],\n",
      "        [-0.0080,  0.0266, -0.0279,  0.0071,  0.0034],\n",
      "        [ 0.0277,  0.0234,  0.0064,  0.0062,  0.0043],\n",
      "        [ 0.0336,  0.0030,  0.0160, -0.0002, -0.0148],\n",
      "        [ 0.0287,  0.0152, -0.0329, -0.0066, -0.0183],\n",
      "        [ 0.0045,  0.0145,  0.0184, -0.0169, -0.0038],\n",
      "        [ 0.0338,  0.0113, -0.0264,  0.0039,  0.0001],\n",
      "        [-0.0035, -0.0030,  0.0068, -0.0159,  0.0019]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0136, grad_fn=<MinBackward1>), tensor(0.8565, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10345544666051865\n",
      "@sample 1049: tensor([[-0.0211,  0.0214,  0.0152, -0.0272,  0.0228],\n",
      "        [-0.0117,  0.0024, -0.0102,  0.0090, -0.0120],\n",
      "        [ 0.0060,  0.0286, -0.0128, -0.0087,  0.0005],\n",
      "        [ 0.0025,  0.0122,  0.0025,  0.0304,  0.0047],\n",
      "        [ 0.0044,  0.0064,  0.0109, -0.0113,  0.0171],\n",
      "        [-0.0005,  0.0024,  0.0450,  0.0022, -0.0205],\n",
      "        [ 0.0054, -0.0242, -0.0131,  0.0196, -0.0053],\n",
      "        [-0.0072, -0.0040, -0.0025,  0.0057, -0.0018]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0059, -0.0157, -0.0273,  0.0188, -0.0062],\n",
      "        [ 0.0294,  0.0185,  0.0260, -0.0077,  0.0174],\n",
      "        [ 0.0061,  0.0072,  0.0010, -0.0229, -0.0046],\n",
      "        [ 0.0262,  0.0332, -0.0165,  0.0067, -0.0076],\n",
      "        [ 0.0093, -0.0129, -0.0051, -0.0120,  0.0119],\n",
      "        [ 0.0261,  0.0406, -0.0002,  0.0263,  0.0009],\n",
      "        [-0.0035,  0.0053,  0.0100, -0.0044, -0.0052],\n",
      "        [-0.0039,  0.0104,  0.0327, -0.0418, -0.0272]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0123, grad_fn=<MinBackward1>), tensor(0.8522, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09718834608793259\n",
      "@sample 1050: tensor([[-0.0144,  0.0111,  0.0024, -0.0184,  0.0013],\n",
      "        [-0.0052, -0.0050,  0.0115, -0.0279,  0.0252],\n",
      "        [ 0.0251, -0.0070, -0.0067, -0.0176,  0.0220],\n",
      "        [-0.0331,  0.0137,  0.0066, -0.0195, -0.0094],\n",
      "        [ 0.0129, -0.0033, -0.0020,  0.0131, -0.0066],\n",
      "        [ 0.0209,  0.0404,  0.0337, -0.0237,  0.0091],\n",
      "        [-0.0056,  0.0017, -0.0032, -0.0050,  0.0051],\n",
      "        [ 0.0082,  0.0217,  0.0129, -0.0014,  0.0039]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0039, -0.0318,  0.0276,  0.0156, -0.0146],\n",
      "        [-0.0275, -0.0135, -0.0213,  0.0098, -0.0059],\n",
      "        [-0.0068, -0.0355, -0.0028,  0.0145, -0.0230],\n",
      "        [-0.0146,  0.0060, -0.0698,  0.0326,  0.0115],\n",
      "        [-0.0056, -0.0046, -0.0083,  0.0066,  0.0231],\n",
      "        [-0.0362, -0.0259,  0.0064,  0.0145, -0.0126],\n",
      "        [-0.0038, -0.0278, -0.0301,  0.0131, -0.0095],\n",
      "        [ 0.0004,  0.0123, -0.0554,  0.0212, -0.0016]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0194, grad_fn=<MinBackward1>), tensor(0.8695, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10522318631410599\n",
      "@sample 1051: tensor([[-0.0300,  0.0086,  0.0037,  0.0155, -0.0263],\n",
      "        [-0.0015, -0.0026, -0.0139,  0.0222,  0.0080],\n",
      "        [ 0.0038, -0.0201, -0.0228,  0.0046,  0.0041],\n",
      "        [ 0.0074, -0.0147, -0.0046,  0.0303, -0.0014],\n",
      "        [ 0.0118, -0.0187, -0.0055, -0.0090, -0.0090],\n",
      "        [-0.0059, -0.0144, -0.0115,  0.0159,  0.0218],\n",
      "        [ 0.0252, -0.0088, -0.0245,  0.0020, -0.0099],\n",
      "        [ 0.0051, -0.0086,  0.0006, -0.0157, -0.0071]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0120,  0.0213, -0.0033, -0.0463, -0.0087],\n",
      "        [ 0.0173, -0.0136,  0.0035,  0.0020, -0.0371],\n",
      "        [ 0.0021, -0.0245, -0.0014, -0.0026, -0.0192],\n",
      "        [ 0.0196,  0.0142, -0.0058,  0.0279,  0.0153],\n",
      "        [-0.0306,  0.0043, -0.0214,  0.0248, -0.0107],\n",
      "        [-0.0003, -0.0190, -0.0088, -0.0112, -0.0059],\n",
      "        [ 0.0033,  0.0055, -0.0034,  0.0061,  0.0003],\n",
      "        [-0.0076, -0.0205,  0.0260, -0.0335, -0.0210]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0153, grad_fn=<MinBackward1>), tensor(0.8542, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10254240781068802\n",
      "@sample 1052: tensor([[ 0.0054,  0.0040,  0.0002,  0.0080,  0.0068],\n",
      "        [ 0.0040,  0.0291, -0.0136, -0.0099,  0.0026],\n",
      "        [-0.0176,  0.0376,  0.0044, -0.0087, -0.0036],\n",
      "        [ 0.0160,  0.0031,  0.0109, -0.0154,  0.0075],\n",
      "        [ 0.0139,  0.0299,  0.0210, -0.0344,  0.0461],\n",
      "        [ 0.0197, -0.0080, -0.0090,  0.0335, -0.0273],\n",
      "        [-0.0079,  0.0008, -0.0093, -0.0096, -0.0153],\n",
      "        [ 0.0034,  0.0205, -0.0099, -0.0267, -0.0096]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0250,  0.0232,  0.0065, -0.0064, -0.0039],\n",
      "        [-0.0100,  0.0127, -0.0198,  0.0026,  0.0030],\n",
      "        [ 0.0016, -0.0117, -0.0121,  0.0054, -0.0114],\n",
      "        [-0.0164,  0.0402, -0.0178, -0.0162,  0.0053],\n",
      "        [ 0.0250, -0.0366, -0.0234, -0.0039, -0.0223],\n",
      "        [ 0.0231,  0.0178, -0.0077, -0.0176,  0.0134],\n",
      "        [ 0.0011, -0.0121,  0.0031, -0.0031,  0.0052],\n",
      "        [-0.0020, -0.0320, -0.0028,  0.0047,  0.0098]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0110, grad_fn=<MinBackward1>), tensor(0.9004, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09487979859113693\n",
      "@sample 1053: tensor([[ 0.0068, -0.0205, -0.0050,  0.0022, -0.0042],\n",
      "        [-0.0076,  0.0048, -0.0022,  0.0052, -0.0089],\n",
      "        [ 0.0052, -0.0057, -0.0052,  0.0319, -0.0265],\n",
      "        [-0.0121,  0.0099, -0.0373, -0.0218,  0.0162],\n",
      "        [ 0.0189, -0.0359,  0.0010,  0.0300, -0.0230],\n",
      "        [-0.0068, -0.0043,  0.0055, -0.0032, -0.0137],\n",
      "        [ 0.0176, -0.0180, -0.0151, -0.0046,  0.0149],\n",
      "        [-0.0061,  0.0151,  0.0084, -0.0372,  0.0113]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0019,  0.0129,  0.0335, -0.0211,  0.0092],\n",
      "        [-0.0111, -0.0344,  0.0023, -0.0119,  0.0022],\n",
      "        [-0.0094,  0.0031, -0.0199, -0.0213, -0.0116],\n",
      "        [-0.0066, -0.0304, -0.0049,  0.0069,  0.0314],\n",
      "        [-0.0066,  0.0245, -0.0101,  0.0182,  0.0145],\n",
      "        [ 0.0086,  0.0081,  0.0026,  0.0080,  0.0213],\n",
      "        [-0.0082, -0.0205, -0.0021,  0.0387, -0.0126],\n",
      "        [-0.0359, -0.0281,  0.0027,  0.0121, -0.0089]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0142, grad_fn=<MinBackward1>), tensor(0.9018, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09738106280565262\n",
      "@sample 1054: tensor([[-0.0152, -0.0196, -0.0024,  0.0083,  0.0073],\n",
      "        [ 0.0162,  0.0039,  0.0043, -0.0292,  0.0070],\n",
      "        [-0.0039,  0.0179, -0.0157, -0.0071,  0.0035],\n",
      "        [ 0.0218, -0.0221,  0.0122,  0.0012,  0.0111],\n",
      "        [-0.0145, -0.0101, -0.0115,  0.0261, -0.0127],\n",
      "        [ 0.0104, -0.0171,  0.0013, -0.0093, -0.0172],\n",
      "        [-0.0011, -0.0079,  0.0065, -0.0086,  0.0240],\n",
      "        [-0.0097, -0.0153,  0.0126,  0.0053, -0.0198]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0075,  0.0141,  0.0069, -0.0059, -0.0020],\n",
      "        [-0.0110, -0.0121, -0.0612,  0.0284, -0.0039],\n",
      "        [ 0.0061, -0.0056, -0.0188, -0.0231,  0.0161],\n",
      "        [-0.0228,  0.0220, -0.0045,  0.0182,  0.0183],\n",
      "        [-0.0240,  0.0230, -0.0058,  0.0073,  0.0005],\n",
      "        [-0.0162, -0.0152,  0.0017, -0.0260,  0.0020],\n",
      "        [ 0.0036,  0.0093, -0.0127,  0.0019,  0.0046],\n",
      "        [ 0.0111,  0.0053, -0.0016,  0.0062,  0.0271]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0136, grad_fn=<MinBackward1>), tensor(0.8431, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08238963782787323\n",
      "@sample 1055: tensor([[ 0.0065, -0.0048, -0.0066,  0.0248, -0.0356],\n",
      "        [ 0.0173,  0.0240, -0.0196, -0.0014, -0.0487],\n",
      "        [ 0.0177, -0.0282, -0.0040,  0.0046, -0.0076],\n",
      "        [-0.0126,  0.0189, -0.0091,  0.0071,  0.0026],\n",
      "        [ 0.0179,  0.0064, -0.0234,  0.0455, -0.0167],\n",
      "        [ 0.0329,  0.0034, -0.0147,  0.0035, -0.0171],\n",
      "        [ 0.0091,  0.0060,  0.0239, -0.0085,  0.0019],\n",
      "        [-0.0220, -0.0010,  0.0049,  0.0248, -0.0139]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0110,  0.0550, -0.0188, -0.0306,  0.0058],\n",
      "        [-0.0170,  0.0126, -0.0103, -0.0094,  0.0086],\n",
      "        [ 0.0034, -0.0104,  0.0287, -0.0025,  0.0108],\n",
      "        [-0.0089, -0.0046, -0.0170,  0.0093, -0.0068],\n",
      "        [ 0.0405,  0.0359, -0.0245, -0.0065, -0.0255],\n",
      "        [-0.0008,  0.0120, -0.0096,  0.0262,  0.0208],\n",
      "        [-0.0340, -0.0032, -0.0265,  0.0174, -0.0094],\n",
      "        [ 0.0173,  0.0018,  0.0106, -0.0015, -0.0011]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0099, grad_fn=<MinBackward1>), tensor(0.8551, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09526482224464417\n",
      "@sample 1056: tensor([[ 0.0055, -0.0048,  0.0023, -0.0225, -0.0111],\n",
      "        [ 0.0054,  0.0187, -0.0168,  0.0443, -0.0075],\n",
      "        [ 0.0194,  0.0002, -0.0168,  0.0140, -0.0249],\n",
      "        [ 0.0157, -0.0185,  0.0185, -0.0275, -0.0123],\n",
      "        [-0.0126,  0.0054, -0.0044,  0.0090,  0.0104],\n",
      "        [ 0.0292, -0.0150, -0.0279,  0.0550, -0.0376],\n",
      "        [ 0.0148, -0.0034, -0.0320,  0.0192, -0.0299],\n",
      "        [-0.0021, -0.0208, -0.0247,  0.0188, -0.0286]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0342,  0.0170,  0.0191,  0.0272,  0.0239],\n",
      "        [ 0.0343,  0.0279, -0.0186, -0.0211, -0.0023],\n",
      "        [-0.0113, -0.0041, -0.0025, -0.0091,  0.0099],\n",
      "        [-0.0419, -0.0101, -0.0349,  0.0187, -0.0022],\n",
      "        [ 0.0446,  0.0151,  0.0128, -0.0149,  0.0100],\n",
      "        [ 0.0154,  0.0384, -0.0100,  0.0070,  0.0248],\n",
      "        [-0.0099, -0.0183, -0.0145, -0.0080,  0.0295],\n",
      "        [ 0.0042, -0.0291,  0.0029,  0.0027, -0.0344]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0102, grad_fn=<MinBackward1>), tensor(0.8733, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1120290458202362\n",
      "@sample 1057: tensor([[-0.0021,  0.0396,  0.0202, -0.0313,  0.0243],\n",
      "        [-0.0068, -0.0050,  0.0015, -0.0021,  0.0162],\n",
      "        [ 0.0053, -0.0109,  0.0012, -0.0042, -0.0076],\n",
      "        [ 0.0087,  0.0285, -0.0146, -0.0056, -0.0274],\n",
      "        [ 0.0116,  0.0084, -0.0274,  0.0070, -0.0158],\n",
      "        [-0.0022, -0.0049,  0.0190,  0.0072,  0.0226],\n",
      "        [ 0.0138,  0.0554, -0.0061, -0.0196, -0.0072],\n",
      "        [-0.0026,  0.0027, -0.0024, -0.0149,  0.0049]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0132, -0.0428, -0.0519,  0.0106, -0.0041],\n",
      "        [-0.0080,  0.0074, -0.0240, -0.0353,  0.0196],\n",
      "        [ 0.0050, -0.0024,  0.0013,  0.0035,  0.0102],\n",
      "        [-0.0066,  0.0105,  0.0022,  0.0067, -0.0225],\n",
      "        [-0.0092, -0.0012, -0.0002, -0.0118, -0.0066],\n",
      "        [-0.0544, -0.0231, -0.0362,  0.0128, -0.0185],\n",
      "        [-0.0017,  0.0230,  0.0132,  0.0011,  0.0267],\n",
      "        [-0.0339,  0.0229, -0.0372,  0.0039,  0.0163]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0137, grad_fn=<MinBackward1>), tensor(0.8478, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09588851034641266\n",
      "@sample 1058: tensor([[-0.0186, -0.0252, -0.0291,  0.0187, -0.0242],\n",
      "        [ 0.0019,  0.0094,  0.0178, -0.0055,  0.0227],\n",
      "        [-0.0109, -0.0143,  0.0092, -0.0221,  0.0102],\n",
      "        [ 0.0225, -0.0197,  0.0051,  0.0200, -0.0085],\n",
      "        [-0.0171,  0.0120, -0.0297,  0.0123, -0.0093],\n",
      "        [ 0.0009,  0.0126, -0.0246, -0.0064,  0.0009],\n",
      "        [-0.0145, -0.0204, -0.0069, -0.0062, -0.0026],\n",
      "        [ 0.0023, -0.0110,  0.0086,  0.0008, -0.0069]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0074, -0.0103,  0.0345, -0.0180, -0.0038],\n",
      "        [ 0.0196, -0.0102,  0.0043, -0.0171, -0.0112],\n",
      "        [-0.0263,  0.0215, -0.0003,  0.0084,  0.0201],\n",
      "        [ 0.0048, -0.0011, -0.0054, -0.0032,  0.0063],\n",
      "        [-0.0028,  0.0268, -0.0412,  0.0044,  0.0166],\n",
      "        [ 0.0077, -0.0256, -0.0600,  0.0054, -0.0145],\n",
      "        [-0.0075, -0.0077,  0.0140,  0.0065,  0.0214],\n",
      "        [-0.0135, -0.0027, -0.0089,  0.0005,  0.0101]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0142, grad_fn=<MinBackward1>), tensor(0.8479, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08892002701759338\n",
      "@sample 1059: tensor([[ 0.0019, -0.0267,  0.0021,  0.0050, -0.0054],\n",
      "        [ 0.0312,  0.0053, -0.0022, -0.0085,  0.0021],\n",
      "        [ 0.0061, -0.0011, -0.0094, -0.0225, -0.0135],\n",
      "        [-0.0013, -0.0069, -0.0290,  0.0144, -0.0066],\n",
      "        [-0.0011,  0.0073,  0.0271,  0.0066,  0.0021],\n",
      "        [-0.0159, -0.0069, -0.0039,  0.0110, -0.0077],\n",
      "        [-0.0142, -0.0011, -0.0219,  0.0023, -0.0160],\n",
      "        [-0.0176,  0.0076, -0.0149,  0.0204, -0.0059]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 9.8046e-03, -2.7495e-02,  3.3421e-02,  1.0228e-03,  5.6319e-04],\n",
      "        [-2.0181e-02,  5.1964e-03, -2.4777e-02, -1.3429e-02, -1.5487e-02],\n",
      "        [-1.5370e-04, -1.6665e-02,  6.5142e-03, -5.7224e-03,  2.8575e-02],\n",
      "        [ 2.9604e-02, -1.5122e-02, -1.4514e-05, -5.4163e-03,  9.5860e-03],\n",
      "        [ 5.9675e-03,  2.3983e-02, -4.9813e-03,  8.4336e-03,  7.2165e-03],\n",
      "        [-1.0610e-02, -1.0930e-02,  8.5469e-03, -6.7144e-03,  2.7969e-04],\n",
      "        [ 1.2749e-02, -3.2941e-02,  1.3932e-02, -1.8874e-02, -1.1036e-02],\n",
      "        [ 2.0218e-02,  3.6141e-03, -4.2377e-02, -1.8798e-02,  1.5749e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0124, grad_fn=<MinBackward1>), tensor(0.8446, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09290964901447296\n",
      "@sample 1060: tensor([[ 0.0141, -0.0051,  0.0142,  0.0027,  0.0019],\n",
      "        [-0.0201, -0.0226, -0.0025,  0.0267,  0.0050],\n",
      "        [ 0.0235,  0.0116,  0.0053, -0.0210,  0.0047],\n",
      "        [-0.0005, -0.0078, -0.0288,  0.0026, -0.0146],\n",
      "        [ 0.0152,  0.0027, -0.0233,  0.0042, -0.0033],\n",
      "        [ 0.0242, -0.0326, -0.0109,  0.0045, -0.0168],\n",
      "        [ 0.0120,  0.0255, -0.0078, -0.0052, -0.0048],\n",
      "        [-0.0087,  0.0106,  0.0143, -0.0182,  0.0100]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0019,  0.0069, -0.0064,  0.0114,  0.0233],\n",
      "        [ 0.0079, -0.0228,  0.0041, -0.0191, -0.0093],\n",
      "        [-0.0196, -0.0171, -0.0210,  0.0253,  0.0155],\n",
      "        [-0.0238, -0.0162,  0.0147,  0.0213, -0.0216],\n",
      "        [ 0.0195, -0.0125,  0.0133, -0.0332,  0.0055],\n",
      "        [ 0.0104, -0.0124,  0.0082,  0.0248,  0.0046],\n",
      "        [ 0.0092, -0.0137, -0.0012, -0.0264, -0.0267],\n",
      "        [-0.0288,  0.0028, -0.0089, -0.0087, -0.0151]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0150, grad_fn=<MinBackward1>), tensor(0.8466, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09476073086261749\n",
      "@sample 1061: tensor([[-0.0121, -0.0275,  0.0002,  0.0002,  0.0240],\n",
      "        [ 0.0019, -0.0061, -0.0148, -0.0062, -0.0047],\n",
      "        [ 0.0014, -0.0199,  0.0145,  0.0039, -0.0023],\n",
      "        [ 0.0325, -0.0096, -0.0217, -0.0101,  0.0036],\n",
      "        [-0.0063,  0.0044, -0.0091, -0.0094, -0.0004],\n",
      "        [ 0.0128,  0.0121, -0.0015, -0.0213, -0.0229],\n",
      "        [-0.0245,  0.0208,  0.0200, -0.0101,  0.0299],\n",
      "        [ 0.0114,  0.0077,  0.0273, -0.0133, -0.0079]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0370, -0.0223,  0.0465,  0.0129, -0.0016],\n",
      "        [-0.0090, -0.0103, -0.0190,  0.0158, -0.0093],\n",
      "        [-0.0142,  0.0053, -0.0048,  0.0073,  0.0091],\n",
      "        [-0.0286,  0.0250, -0.0103,  0.0258,  0.0454],\n",
      "        [ 0.0081, -0.0330,  0.0374, -0.0280, -0.0202],\n",
      "        [-0.0066, -0.0091,  0.0093, -0.0205, -0.0050],\n",
      "        [ 0.0194, -0.0257,  0.0165, -0.0098, -0.0256],\n",
      "        [-0.0196, -0.0222,  0.0154,  0.0047, -0.0158]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0152, grad_fn=<MinBackward1>), tensor(0.8834, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09860025346279144\n",
      "@sample 1062: tensor([[ 0.0029,  0.0003,  0.0241,  0.0160,  0.0035],\n",
      "        [-0.0022,  0.0075,  0.0227,  0.0100,  0.0056],\n",
      "        [ 0.0137,  0.0499,  0.0275, -0.0233,  0.0259],\n",
      "        [-0.0140,  0.0221,  0.0042,  0.0070,  0.0097],\n",
      "        [ 0.0104,  0.0025,  0.0057,  0.0046,  0.0091],\n",
      "        [-0.0134, -0.0195,  0.0127, -0.0208,  0.0216],\n",
      "        [-0.0204, -0.0047,  0.0031, -0.0248,  0.0235],\n",
      "        [ 0.0116, -0.0020, -0.0213, -0.0023,  0.0279]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0149,  0.0170, -0.0523, -0.0069, -0.0058],\n",
      "        [-0.0225,  0.0411, -0.0111,  0.0217,  0.0156],\n",
      "        [ 0.0002, -0.0045, -0.0299,  0.0292, -0.0474],\n",
      "        [ 0.0283,  0.0319, -0.0069,  0.0177,  0.0226],\n",
      "        [-0.0110,  0.0027, -0.0289,  0.0058, -0.0130],\n",
      "        [-0.0203, -0.0157,  0.0087, -0.0054, -0.0082],\n",
      "        [-0.0185, -0.0516,  0.0427, -0.0112, -0.0280],\n",
      "        [-0.0052,  0.0125,  0.0380, -0.0342, -0.0328]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0168, grad_fn=<MinBackward1>), tensor(0.8426, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10696373134851456\n",
      "@sample 1063: tensor([[-0.0070, -0.0012,  0.0094, -0.0277,  0.0077],\n",
      "        [ 0.0057, -0.0032,  0.0326, -0.0108,  0.0149],\n",
      "        [ 0.0022,  0.0078,  0.0224, -0.0356, -0.0138],\n",
      "        [-0.0044,  0.0129,  0.0190, -0.0152,  0.0089],\n",
      "        [ 0.0030,  0.0213,  0.0102, -0.0194,  0.0145],\n",
      "        [ 0.0026, -0.0046, -0.0256, -0.0355,  0.0527],\n",
      "        [-0.0193,  0.0127,  0.0292, -0.0377,  0.0063],\n",
      "        [ 0.0022, -0.0159, -0.0061,  0.0039, -0.0006]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0086,  0.0066,  0.0136,  0.0059, -0.0039],\n",
      "        [-0.0098, -0.0072, -0.0016,  0.0250, -0.0158],\n",
      "        [-0.0094, -0.0028,  0.0346,  0.0092,  0.0047],\n",
      "        [-0.0198,  0.0050, -0.0158, -0.0141, -0.0202],\n",
      "        [-0.0253, -0.0016, -0.0365,  0.0118, -0.0052],\n",
      "        [ 0.0048,  0.0059, -0.0061, -0.0065, -0.0036],\n",
      "        [ 0.0029, -0.0101, -0.0148,  0.0071, -0.0305],\n",
      "        [-0.0069,  0.0006, -0.0235, -0.0029,  0.0078]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0146, grad_fn=<MinBackward1>), tensor(0.8984, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09383094310760498\n",
      "@sample 1064: tensor([[-0.0057,  0.0094,  0.0104, -0.0229, -0.0085],\n",
      "        [-0.0311, -0.0015,  0.0156, -0.0276,  0.0285],\n",
      "        [ 0.0114,  0.0404,  0.0163, -0.0250,  0.0053],\n",
      "        [ 0.0106,  0.0151,  0.0313, -0.0318,  0.0179],\n",
      "        [ 0.0192, -0.0102, -0.0069, -0.0236,  0.0221],\n",
      "        [ 0.0064,  0.0045,  0.0306, -0.0080,  0.0031],\n",
      "        [-0.0039,  0.0143,  0.0195, -0.0179,  0.0104],\n",
      "        [-0.0109, -0.0083,  0.0067, -0.0232,  0.0159]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0348, -0.0246,  0.0195,  0.0056, -0.0063],\n",
      "        [ 0.0024, -0.0598,  0.0330,  0.0047, -0.0189],\n",
      "        [-0.0152, -0.0151, -0.0515,  0.0184, -0.0143],\n",
      "        [-0.0374, -0.0125, -0.0296,  0.0092, -0.0288],\n",
      "        [ 0.0069,  0.0031, -0.0010,  0.0122,  0.0084],\n",
      "        [-0.0089,  0.0014,  0.0136,  0.0269,  0.0126],\n",
      "        [-0.0115, -0.0202, -0.0209,  0.0084, -0.0322],\n",
      "        [-0.0035, -0.0215,  0.0013,  0.0261,  0.0093]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0107, grad_fn=<MinBackward1>), tensor(0.8062, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10220345109701157\n",
      "@sample 1065: tensor([[ 0.0171,  0.0266,  0.0569, -0.0164,  0.0033],\n",
      "        [ 0.0209,  0.0269,  0.0108, -0.0009,  0.0038],\n",
      "        [ 0.0152,  0.0094,  0.0254,  0.0278, -0.0252],\n",
      "        [ 0.0088, -0.0146,  0.0048,  0.0216,  0.0014],\n",
      "        [ 0.0047,  0.0176,  0.0059, -0.0193,  0.0029],\n",
      "        [ 0.0178,  0.0020,  0.0201, -0.0148, -0.0003],\n",
      "        [ 0.0067, -0.0190, -0.0018,  0.0027,  0.0050],\n",
      "        [-0.0071,  0.0254,  0.0128,  0.0111,  0.0256]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0217,  0.0166, -0.0539,  0.0345, -0.0249],\n",
      "        [ 0.0180,  0.0291, -0.0110,  0.0129,  0.0090],\n",
      "        [-0.0276,  0.0045,  0.0041, -0.0222, -0.0308],\n",
      "        [ 0.0019,  0.0107,  0.0183, -0.0032,  0.0092],\n",
      "        [-0.0366, -0.0245, -0.0352,  0.0355,  0.0068],\n",
      "        [-0.0258,  0.0284, -0.0076,  0.0451,  0.0127],\n",
      "        [ 0.0100, -0.0016,  0.0167,  0.0019, -0.0104],\n",
      "        [ 0.0355,  0.0224, -0.0013, -0.0362, -0.0221]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0081, grad_fn=<MinBackward1>), tensor(0.8587, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09047175198793411\n",
      "@sample 1066: tensor([[-0.0282,  0.0174,  0.0005, -0.0024, -0.0050],\n",
      "        [ 0.0075, -0.0138, -0.0089,  0.0160, -0.0026],\n",
      "        [ 0.0074, -0.0095,  0.0118, -0.0240,  0.0089],\n",
      "        [-0.0042, -0.0047, -0.0050,  0.0106, -0.0034],\n",
      "        [-0.0035,  0.0128,  0.0080, -0.0050, -0.0197],\n",
      "        [-0.0018,  0.0278,  0.0213, -0.0088, -0.0125],\n",
      "        [ 0.0199,  0.0130,  0.0151, -0.0254, -0.0074],\n",
      "        [-0.0095, -0.0029, -0.0084,  0.0094, -0.0028]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0072,  0.0146, -0.0094,  0.0079, -0.0157],\n",
      "        [ 0.0254, -0.0024,  0.0216,  0.0262,  0.0094],\n",
      "        [-0.0336,  0.0215,  0.0094,  0.0088, -0.0097],\n",
      "        [ 0.0026,  0.0119, -0.0139,  0.0126, -0.0205],\n",
      "        [-0.0039,  0.0117,  0.0104, -0.0255, -0.0031],\n",
      "        [ 0.0009,  0.0214,  0.0150, -0.0336, -0.0241],\n",
      "        [-0.0182, -0.0305, -0.0208,  0.0179,  0.0125],\n",
      "        [-0.0020,  0.0101,  0.0305, -0.0306, -0.0100]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0158, grad_fn=<MinBackward1>), tensor(0.8755, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08047996461391449\n",
      "@sample 1067: tensor([[ 1.0597e-02, -2.0462e-02, -1.7421e-02,  5.4458e-03, -9.9424e-04],\n",
      "        [ 1.2009e-02, -2.0222e-02, -9.7050e-03,  5.3118e-03, -1.4163e-02],\n",
      "        [ 1.4437e-02, -1.4371e-02, -1.3785e-02,  2.0097e-02, -4.1850e-05],\n",
      "        [ 6.3179e-03,  3.8957e-02,  1.7611e-02, -3.6304e-02,  1.7437e-02],\n",
      "        [ 1.1308e-02, -5.0737e-03, -3.3225e-03,  2.9600e-02, -3.9794e-03],\n",
      "        [-2.1290e-02,  4.3532e-03,  3.0991e-02, -4.1634e-03, -1.3199e-02],\n",
      "        [-2.8716e-04,  4.4858e-02,  4.4625e-03, -5.4474e-02,  2.0287e-02],\n",
      "        [ 1.1791e-02,  4.3885e-02,  1.9851e-02, -1.8695e-02,  2.4809e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0365, -0.0041, -0.0071,  0.0171,  0.0148],\n",
      "        [ 0.0332,  0.0044,  0.0359, -0.0166, -0.0094],\n",
      "        [-0.0020, -0.0225, -0.0046, -0.0049, -0.0275],\n",
      "        [-0.0304, -0.0378, -0.0388,  0.0398,  0.0082],\n",
      "        [ 0.0130,  0.0137, -0.0074, -0.0054,  0.0008],\n",
      "        [ 0.0011, -0.0375,  0.0031,  0.0194,  0.0064],\n",
      "        [-0.0009, -0.0327, -0.0452,  0.0490,  0.0126],\n",
      "        [-0.0166, -0.0132, -0.0529,  0.0426, -0.0148]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0129, grad_fn=<MinBackward1>), tensor(0.8627, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.0998271107673645\n",
      "@sample 1068: tensor([[ 0.0309, -0.0208, -0.0147,  0.0183, -0.0043],\n",
      "        [ 0.0139, -0.0139, -0.0079,  0.0111, -0.0009],\n",
      "        [ 0.0087, -0.0176, -0.0133,  0.0234,  0.0153],\n",
      "        [-0.0046, -0.0120,  0.0073, -0.0062, -0.0086],\n",
      "        [ 0.0081, -0.0008,  0.0218,  0.0307, -0.0196],\n",
      "        [ 0.0024,  0.0012,  0.0022,  0.0082, -0.0120],\n",
      "        [ 0.0123, -0.0055, -0.0183,  0.0026, -0.0151],\n",
      "        [ 0.0062, -0.0058, -0.0405,  0.0346, -0.0173]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0161,  0.0092, -0.0146, -0.0055, -0.0105],\n",
      "        [ 0.0224,  0.0086,  0.0153,  0.0337,  0.0071],\n",
      "        [ 0.0333,  0.0166, -0.0033,  0.0113,  0.0480],\n",
      "        [ 0.0014,  0.0011,  0.0025, -0.0341, -0.0062],\n",
      "        [-0.0025,  0.0329,  0.0087, -0.0139, -0.0081],\n",
      "        [ 0.0173,  0.0172,  0.0100, -0.0057,  0.0118],\n",
      "        [ 0.0017,  0.0226,  0.0183, -0.0047,  0.0084],\n",
      "        [ 0.0379, -0.0030,  0.0327,  0.0057, -0.0299]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0127, grad_fn=<MinBackward1>), tensor(0.8342, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09215359389781952\n",
      "@sample 1069: tensor([[ 0.0036,  0.0141,  0.0048, -0.0211, -0.0149],\n",
      "        [ 0.0052,  0.0059,  0.0050, -0.0055, -0.0098],\n",
      "        [-0.0219,  0.0242, -0.0171, -0.0179,  0.0186],\n",
      "        [-0.0125,  0.0088, -0.0143,  0.0006,  0.0018],\n",
      "        [-0.0059,  0.0029, -0.0069,  0.0183,  0.0168],\n",
      "        [ 0.0196,  0.0180, -0.0112,  0.0009,  0.0005],\n",
      "        [-0.0059,  0.0193, -0.0161,  0.0092, -0.0018],\n",
      "        [ 0.0065,  0.0185,  0.0080, -0.0036,  0.0232]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0308, -0.0107, -0.0279,  0.0279,  0.0200],\n",
      "        [ 0.0058, -0.0035,  0.0091,  0.0096,  0.0201],\n",
      "        [ 0.0042,  0.0171, -0.0104, -0.0020,  0.0273],\n",
      "        [-0.0085, -0.0168,  0.0051, -0.0262, -0.0067],\n",
      "        [ 0.0099, -0.0033, -0.0343,  0.0026, -0.0185],\n",
      "        [ 0.0034,  0.0386,  0.0067, -0.0271, -0.0195],\n",
      "        [-0.0056, -0.0252, -0.0038, -0.0207, -0.0169],\n",
      "        [ 0.0072,  0.0112, -0.0434,  0.0102, -0.0071]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0161, grad_fn=<MinBackward1>), tensor(0.8340, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10220415890216827\n",
      "@sample 1070: tensor([[-0.0044, -0.0047,  0.0069,  0.0018, -0.0070],\n",
      "        [ 0.0034,  0.0063, -0.0024,  0.0108,  0.0217],\n",
      "        [ 0.0044, -0.0014, -0.0161,  0.0209, -0.0346],\n",
      "        [ 0.0139,  0.0322,  0.0285,  0.0040, -0.0024],\n",
      "        [-0.0200, -0.0068,  0.0020,  0.0282, -0.0118],\n",
      "        [ 0.0068, -0.0261, -0.0098,  0.0119, -0.0057],\n",
      "        [-0.0123,  0.0033, -0.0270,  0.0100, -0.0025],\n",
      "        [-0.0290, -0.0021, -0.0121,  0.0190,  0.0045]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 9.0127e-03,  3.1180e-02,  1.6124e-02,  4.5542e-03,  1.9642e-02],\n",
      "        [ 1.3176e-02,  1.1675e-02,  6.3142e-04, -6.4029e-03, -1.3424e-02],\n",
      "        [ 1.8973e-02,  2.4370e-02, -3.8237e-03, -4.9858e-03,  3.9419e-03],\n",
      "        [-8.5498e-03, -1.1505e-02, -3.7818e-02, -1.5733e-03,  1.0052e-02],\n",
      "        [-7.2069e-05,  1.6557e-02,  3.8849e-03, -1.2498e-02,  2.6264e-03],\n",
      "        [-1.5868e-02,  7.6727e-03,  5.8925e-03, -2.5177e-02,  1.7126e-02],\n",
      "        [ 1.6821e-02,  1.7674e-02,  4.9518e-03,  2.6872e-02,  2.9196e-02],\n",
      "        [ 3.4165e-02, -2.5228e-02,  9.3857e-03, -2.5857e-02, -1.2227e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0154, grad_fn=<MinBackward1>), tensor(0.8578, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08948735892772675\n",
      "@sample 1071: tensor([[-0.0013, -0.0144, -0.0038,  0.0063,  0.0210],\n",
      "        [-0.0177,  0.0089, -0.0042,  0.0004,  0.0028],\n",
      "        [-0.0133, -0.0251, -0.0211,  0.0118, -0.0090],\n",
      "        [-0.0053,  0.0074, -0.0003, -0.0174, -0.0113],\n",
      "        [ 0.0185, -0.0253, -0.0101, -0.0026,  0.0155],\n",
      "        [-0.0033, -0.0041, -0.0012,  0.0096, -0.0002],\n",
      "        [ 0.0023,  0.0057,  0.0118,  0.0044,  0.0103],\n",
      "        [ 0.0220,  0.0111,  0.0277,  0.0052, -0.0099]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0366,  0.0091,  0.0367, -0.0022,  0.0124],\n",
      "        [-0.0110, -0.0450,  0.0127, -0.0158,  0.0006],\n",
      "        [-0.0018, -0.0277,  0.0115, -0.0212,  0.0236],\n",
      "        [-0.0066,  0.0015,  0.0020,  0.0003, -0.0128],\n",
      "        [-0.0137, -0.0118, -0.0011,  0.0304, -0.0017],\n",
      "        [-0.0068, -0.0062, -0.0169,  0.0112,  0.0121],\n",
      "        [-0.0044, -0.0193, -0.0483, -0.0165, -0.0022],\n",
      "        [-0.0072,  0.0387, -0.0066,  0.0090,  0.0215]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0180, grad_fn=<MinBackward1>), tensor(0.8330, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09620910882949829\n",
      "@sample 1072: tensor([[-0.0202, -0.0102,  0.0255, -0.0162,  0.0060],\n",
      "        [-0.0148,  0.0166,  0.0210, -0.0177,  0.0090],\n",
      "        [-0.0053, -0.0017,  0.0134,  0.0093,  0.0291],\n",
      "        [-0.0036, -0.0074, -0.0193, -0.0153, -0.0015],\n",
      "        [-0.0196, -0.0062, -0.0263, -0.0036,  0.0256],\n",
      "        [-0.0208,  0.0460,  0.0468, -0.1083,  0.0593],\n",
      "        [ 0.0132, -0.0227, -0.0105,  0.0084, -0.0092],\n",
      "        [-0.0264,  0.0026, -0.0094,  0.0018,  0.0038]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0014, -0.0120,  0.0111,  0.0189, -0.0168],\n",
      "        [-0.0075,  0.0050,  0.0161,  0.0194,  0.0058],\n",
      "        [ 0.0002,  0.0306, -0.0258,  0.0268, -0.0235],\n",
      "        [ 0.0111, -0.0245, -0.0207,  0.0169,  0.0030],\n",
      "        [-0.0187, -0.0333, -0.0031,  0.0074, -0.0290],\n",
      "        [-0.0243, -0.0638, -0.0786,  0.0649, -0.0008],\n",
      "        [ 0.0094, -0.0194, -0.0190,  0.0124,  0.0118],\n",
      "        [-0.0172, -0.0353,  0.0302, -0.0048, -0.0044]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0133, grad_fn=<MinBackward1>), tensor(0.8626, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11455289274454117\n",
      "@sample 1073: tensor([[ 0.0166,  0.0014, -0.0047,  0.0024,  0.0132],\n",
      "        [-0.0075,  0.0221,  0.0023, -0.0152,  0.0040],\n",
      "        [-0.0194, -0.0057, -0.0135,  0.0133, -0.0283],\n",
      "        [-0.0026, -0.0185, -0.0080,  0.0188, -0.0028],\n",
      "        [-0.0151, -0.0081, -0.0184,  0.0241, -0.0024],\n",
      "        [ 0.0259, -0.0183,  0.0007, -0.0262,  0.0289],\n",
      "        [-0.0183,  0.0022, -0.0141,  0.0285, -0.0128],\n",
      "        [-0.0117,  0.0022,  0.0069, -0.0033, -0.0041]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0157,  0.0140, -0.0273,  0.0392,  0.0386],\n",
      "        [-0.0203, -0.0103, -0.0118, -0.0042, -0.0121],\n",
      "        [ 0.0140, -0.0109,  0.0156,  0.0080,  0.0196],\n",
      "        [ 0.0106, -0.0016,  0.0117, -0.0066,  0.0198],\n",
      "        [ 0.0078, -0.0162, -0.0182,  0.0161,  0.0039],\n",
      "        [-0.0114,  0.0110, -0.0164,  0.0119,  0.0003],\n",
      "        [ 0.0190, -0.0025,  0.0101, -0.0052, -0.0154],\n",
      "        [ 0.0099, -0.0096,  0.0234, -0.0355, -0.0370]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0102, grad_fn=<MinBackward1>), tensor(0.8473, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09639732539653778\n",
      "@sample 1074: tensor([[-0.0052, -0.0187,  0.0198,  0.0061, -0.0195],\n",
      "        [ 0.0123, -0.0007,  0.0012,  0.0164, -0.0168],\n",
      "        [ 0.0020,  0.0001,  0.0143,  0.0095,  0.0033],\n",
      "        [-0.0033,  0.0045, -0.0014,  0.0076,  0.0045],\n",
      "        [ 0.0092,  0.0114,  0.0185,  0.0267, -0.0084],\n",
      "        [ 0.0132, -0.0272,  0.0061,  0.0256, -0.0068],\n",
      "        [ 0.0083,  0.0313,  0.0309, -0.0215,  0.0349],\n",
      "        [-0.0143, -0.0091, -0.0070,  0.0226, -0.0064]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0297, -0.0047,  0.0034,  0.0385,  0.0381],\n",
      "        [-0.0103,  0.0185, -0.0130,  0.0052,  0.0163],\n",
      "        [ 0.0067, -0.0025,  0.0177, -0.0224, -0.0514],\n",
      "        [-0.0110,  0.0151, -0.0288,  0.0197,  0.0240],\n",
      "        [ 0.0338,  0.0424, -0.0279,  0.0272,  0.0024],\n",
      "        [ 0.0151, -0.0018,  0.0238, -0.0069, -0.0237],\n",
      "        [ 0.0186, -0.0146, -0.0885,  0.0270,  0.0246],\n",
      "        [ 0.0072,  0.0231,  0.0146, -0.0401, -0.0017]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0125, grad_fn=<MinBackward1>), tensor(0.8402, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10284894704818726\n",
      "@sample 1075: tensor([[-0.0238,  0.0148,  0.0163, -0.0163,  0.0137],\n",
      "        [ 0.0251, -0.0193, -0.0017,  0.0151, -0.0121],\n",
      "        [-0.0050, -0.0212, -0.0138,  0.0068,  0.0035],\n",
      "        [-0.0011,  0.0045,  0.0028,  0.0015, -0.0076],\n",
      "        [-0.0082, -0.0004,  0.0022, -0.0079,  0.0081],\n",
      "        [-0.0084, -0.0097, -0.0027, -0.0138,  0.0038],\n",
      "        [-0.0020, -0.0129, -0.0090,  0.0115, -0.0066],\n",
      "        [ 0.0108,  0.0004,  0.0152, -0.0094,  0.0080]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0040,  0.0137, -0.0274,  0.0136,  0.0225],\n",
      "        [ 0.0027,  0.0057, -0.0159, -0.0058,  0.0084],\n",
      "        [ 0.0117,  0.0106,  0.0260, -0.0258,  0.0239],\n",
      "        [ 0.0007,  0.0232,  0.0147, -0.0425, -0.0072],\n",
      "        [ 0.0055,  0.0083, -0.0087,  0.0003, -0.0011],\n",
      "        [ 0.0003, -0.0273,  0.0180, -0.0217,  0.0069],\n",
      "        [ 0.0002, -0.0039,  0.0340, -0.0117, -0.0094],\n",
      "        [-0.0078,  0.0104, -0.0008,  0.0123,  0.0204]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0098, grad_fn=<MinBackward1>), tensor(0.8744, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09896820783615112\n",
      "@sample 1076: tensor([[ 0.0014,  0.0047,  0.0383, -0.0281, -0.0118],\n",
      "        [ 0.0131, -0.0142,  0.0089, -0.0016, -0.0068],\n",
      "        [-0.0043,  0.0097,  0.0054, -0.0151, -0.0100],\n",
      "        [-0.0043, -0.0291, -0.0148, -0.0098,  0.0217],\n",
      "        [ 0.0063,  0.0232, -0.0198, -0.0397, -0.0030],\n",
      "        [ 0.0102, -0.0124, -0.0126, -0.0039,  0.0105],\n",
      "        [ 0.0135,  0.0073,  0.0247, -0.0230,  0.0047],\n",
      "        [-0.0058, -0.0109, -0.0006, -0.0164,  0.0178]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-4.7368e-02,  5.2264e-02, -1.4942e-02,  8.9018e-03,  2.2375e-03],\n",
      "        [ 2.8731e-02, -2.4737e-02, -6.8434e-03, -6.0573e-05, -2.7048e-03],\n",
      "        [ 7.8457e-03,  1.0811e-03, -2.1690e-02,  1.9605e-02, -2.0731e-02],\n",
      "        [-6.9215e-03, -2.8975e-02, -5.9867e-03,  8.1813e-03,  4.4902e-03],\n",
      "        [-4.0456e-02, -2.7287e-02, -6.1728e-03, -8.4507e-03, -4.6385e-02],\n",
      "        [-3.2372e-04,  2.3347e-02,  1.6345e-02,  1.7460e-02,  6.2352e-03],\n",
      "        [-2.6631e-02,  9.2494e-03, -3.3408e-02,  1.7125e-02, -1.8135e-02],\n",
      "        [-1.8246e-03, -2.4124e-02, -2.0639e-02,  3.3405e-02, -7.1206e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0185, grad_fn=<MinBackward1>), tensor(0.8215, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.0839095488190651\n",
      "@sample 1077: tensor([[-0.0264,  0.0023, -0.0090, -0.0204,  0.0228],\n",
      "        [-0.0013,  0.0100,  0.0148,  0.0046,  0.0107],\n",
      "        [-0.0058, -0.0098, -0.0056,  0.0219,  0.0194],\n",
      "        [ 0.0163, -0.0025,  0.0141, -0.0147, -0.0198],\n",
      "        [ 0.0024, -0.0128, -0.0136,  0.0193, -0.0042],\n",
      "        [-0.0005, -0.0208,  0.0040,  0.0096, -0.0009],\n",
      "        [ 0.0086, -0.0120, -0.0009, -0.0062, -0.0064],\n",
      "        [ 0.0334, -0.0122, -0.0114, -0.0044,  0.0093]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0289, -0.0363,  0.0296, -0.0487,  0.0038],\n",
      "        [ 0.0067,  0.0253, -0.0031,  0.0115,  0.0211],\n",
      "        [ 0.0020,  0.0171, -0.0261,  0.0194, -0.0101],\n",
      "        [ 0.0084, -0.0027,  0.0054, -0.0007,  0.0033],\n",
      "        [ 0.0145,  0.0288,  0.0045, -0.0103, -0.0141],\n",
      "        [-0.0178, -0.0065,  0.0175, -0.0053, -0.0038],\n",
      "        [-0.0172, -0.0035,  0.0248, -0.0312, -0.0055],\n",
      "        [-0.0062,  0.0109,  0.0255,  0.0140,  0.0464]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0123, grad_fn=<MinBackward1>), tensor(0.8511, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08973775058984756\n",
      "@sample 1078: tensor([[ 0.0100, -0.0100, -0.0004,  0.0092,  0.0212],\n",
      "        [ 0.0224,  0.0193, -0.0071,  0.0276, -0.0216],\n",
      "        [-0.0056,  0.0065, -0.0050, -0.0279,  0.0082],\n",
      "        [ 0.0037,  0.0126,  0.0008, -0.0035, -0.0155],\n",
      "        [-0.0087, -0.0225, -0.0085, -0.0289,  0.0641],\n",
      "        [ 0.0116, -0.0150,  0.0173, -0.0202,  0.0170],\n",
      "        [-0.0314, -0.0184,  0.0031, -0.0024,  0.0039],\n",
      "        [ 0.0143, -0.0151,  0.0031,  0.0146,  0.0032]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0061,  0.0104, -0.0264,  0.0196,  0.0151],\n",
      "        [ 0.0258,  0.0252,  0.0037,  0.0023,  0.0025],\n",
      "        [-0.0071, -0.0106, -0.0135, -0.0109,  0.0169],\n",
      "        [ 0.0198, -0.0049,  0.0201, -0.0267, -0.0206],\n",
      "        [ 0.0025, -0.0051,  0.0232,  0.0129,  0.0245],\n",
      "        [-0.0039, -0.0094,  0.0070,  0.0050, -0.0037],\n",
      "        [ 0.0022, -0.0290, -0.0176,  0.0198,  0.0168],\n",
      "        [ 0.0144, -0.0020,  0.0186, -0.0192, -0.0010]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0114, grad_fn=<MinBackward1>), tensor(0.8378, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09723018109798431\n",
      "@sample 1079: tensor([[-0.0086, -0.0077,  0.0154, -0.0186, -0.0035],\n",
      "        [-0.0061,  0.0010,  0.0021,  0.0050, -0.0102],\n",
      "        [-0.0024,  0.0273,  0.0151, -0.0678,  0.0323],\n",
      "        [-0.0012,  0.0180,  0.0175, -0.0328, -0.0133],\n",
      "        [-0.0004,  0.0026,  0.0009, -0.0089,  0.0013],\n",
      "        [ 0.0197, -0.0192,  0.0012, -0.0289,  0.0301],\n",
      "        [ 0.0278,  0.0063, -0.0107, -0.0154, -0.0027],\n",
      "        [-0.0025, -0.0197, -0.0070, -0.0024, -0.0076]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0074, -0.0270,  0.0287,  0.0036, -0.0041],\n",
      "        [ 0.0214,  0.0081,  0.0327, -0.0053,  0.0066],\n",
      "        [-0.0465, -0.0190, -0.0706,  0.0413, -0.0039],\n",
      "        [ 0.0031, -0.0174,  0.0380, -0.0444, -0.0115],\n",
      "        [-0.0132,  0.0057, -0.0048, -0.0071,  0.0105],\n",
      "        [-0.0177, -0.0262, -0.0198,  0.0063, -0.0172],\n",
      "        [-0.0094,  0.0132, -0.0169, -0.0026, -0.0073],\n",
      "        [-0.0394, -0.0047, -0.0161,  0.0176,  0.0084]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0119, grad_fn=<MinBackward1>), tensor(0.8429, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08921126276254654\n",
      "@sample 1080: tensor([[ 2.3254e-02,  4.6649e-02,  1.2919e-02, -2.5823e-02,  1.3508e-02],\n",
      "        [-2.7609e-02, -9.2328e-03,  1.1692e-03, -3.5406e-03,  9.4038e-03],\n",
      "        [ 1.1369e-02, -1.3252e-02, -1.3309e-02, -1.1464e-02, -1.3172e-03],\n",
      "        [ 3.9894e-03,  1.3007e-02,  9.7024e-03, -1.2165e-02,  3.5326e-02],\n",
      "        [ 1.6473e-02, -1.9920e-02, -1.4327e-02,  1.5717e-02,  1.0094e-03],\n",
      "        [ 3.7148e-03, -1.0871e-02,  5.4553e-03,  1.2283e-02,  4.8996e-03],\n",
      "        [ 2.8483e-02,  1.2957e-02,  3.0726e-03, -8.4090e-03, -9.4920e-06],\n",
      "        [-8.9937e-03, -4.8935e-04,  8.4603e-03,  7.7765e-03, -7.9980e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-3.5815e-02, -1.1693e-02, -4.7156e-02,  2.5473e-02, -3.5277e-03],\n",
      "        [-1.2048e-02,  7.3156e-03,  3.1736e-04, -5.4136e-03, -3.4024e-02],\n",
      "        [ 7.5234e-04,  2.2339e-02, -2.6173e-02, -9.7523e-03, -1.1777e-02],\n",
      "        [-1.7964e-02, -1.0565e-02, -7.8489e-02,  4.7838e-02, -2.4886e-02],\n",
      "        [ 2.3178e-02, -8.4187e-03, -2.9514e-02,  1.0654e-02, -4.4640e-03],\n",
      "        [ 3.4302e-05,  2.1539e-02,  8.5473e-03, -2.9870e-02, -8.8910e-03],\n",
      "        [ 4.1550e-04,  1.3394e-02, -1.2257e-03, -9.4932e-03, -5.8293e-03],\n",
      "        [ 2.2886e-02,  1.1750e-02,  1.4313e-02, -7.1651e-03, -7.9709e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0121, grad_fn=<MinBackward1>), tensor(0.8164, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.0905657559633255\n",
      "@sample 1081: tensor([[-1.8367e-02, -5.1102e-03, -1.2855e-03, -3.4790e-02,  1.3942e-02],\n",
      "        [-5.9561e-03,  1.1109e-02, -5.7598e-03, -3.3820e-02,  3.8059e-02],\n",
      "        [-4.4323e-05,  8.3157e-03,  9.5549e-03, -2.4203e-02,  9.3598e-03],\n",
      "        [-6.9640e-04,  1.8195e-03,  6.8770e-03, -1.3515e-02,  1.3299e-02],\n",
      "        [ 4.4784e-03, -1.2683e-02, -9.3821e-04,  2.8173e-02, -1.5743e-02],\n",
      "        [ 2.1582e-03, -1.9452e-02,  6.2815e-03,  1.9967e-02, -1.9845e-02],\n",
      "        [-5.2347e-03, -3.5063e-03, -1.0977e-02, -7.5504e-03, -1.3609e-02],\n",
      "        [ 6.2971e-03,  4.6024e-04, -6.3649e-03, -1.2860e-02, -2.6810e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-1.3254e-02, -4.5342e-02,  4.7017e-02, -2.4923e-02, -2.2388e-02],\n",
      "        [-1.0466e-02, -4.8193e-03,  2.3433e-02,  2.6863e-02,  1.5340e-02],\n",
      "        [-7.1637e-03, -3.5972e-04, -5.9249e-03,  3.6070e-03,  9.9487e-03],\n",
      "        [ 8.7350e-03, -1.9835e-02,  9.0308e-03,  1.1348e-02,  9.1203e-05],\n",
      "        [ 1.1594e-02,  8.7685e-03,  1.4388e-02, -1.8352e-02, -5.1543e-05],\n",
      "        [ 1.1045e-02,  7.4197e-03,  1.2389e-02, -3.1329e-02,  9.5655e-03],\n",
      "        [-2.5445e-03,  3.0191e-03,  2.4740e-02, -2.4722e-02, -3.3723e-02],\n",
      "        [ 3.0443e-04,  6.1860e-03, -2.7664e-02, -5.6264e-03,  2.6929e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0134, grad_fn=<MinBackward1>), tensor(0.8863, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09055207669734955\n",
      "@sample 1082: tensor([[-0.0094,  0.0193, -0.0108, -0.0038, -0.0190],\n",
      "        [ 0.0046, -0.0192, -0.0001,  0.0179,  0.0070],\n",
      "        [-0.0104,  0.0162,  0.0055, -0.0244,  0.0082],\n",
      "        [ 0.0056,  0.0048,  0.0006,  0.0079, -0.0028],\n",
      "        [ 0.0044,  0.0258,  0.0138, -0.0193,  0.0250],\n",
      "        [ 0.0038, -0.0330,  0.0146, -0.0114, -0.0165],\n",
      "        [-0.0018, -0.0037, -0.0153,  0.0037, -0.0052],\n",
      "        [-0.0300,  0.0044, -0.0303,  0.0136, -0.0160]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0312, -0.0064, -0.0348,  0.0327,  0.0130],\n",
      "        [-0.0080, -0.0126, -0.0211, -0.0076, -0.0267],\n",
      "        [-0.0146, -0.0036, -0.0079,  0.0127, -0.0046],\n",
      "        [ 0.0017,  0.0353,  0.0020, -0.0067,  0.0178],\n",
      "        [ 0.0097,  0.0018, -0.0440,  0.0217,  0.0048],\n",
      "        [-0.0124,  0.0092,  0.0199,  0.0107, -0.0179],\n",
      "        [ 0.0176, -0.0026, -0.0255,  0.0130,  0.0007],\n",
      "        [ 0.0086,  0.0454,  0.0111, -0.0054,  0.0114]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0154, grad_fn=<MinBackward1>), tensor(0.9166, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09051521122455597\n",
      "@sample 1083: tensor([[ 0.0141,  0.0071,  0.0179,  0.0040,  0.0060],\n",
      "        [ 0.0034,  0.0214, -0.0131,  0.0288, -0.0370],\n",
      "        [ 0.0055,  0.0116, -0.0348,  0.0267, -0.0500],\n",
      "        [-0.0083,  0.0041, -0.0179,  0.0058, -0.0105],\n",
      "        [ 0.0084, -0.0196, -0.0094, -0.0026,  0.0151],\n",
      "        [-0.0002,  0.0383,  0.0061, -0.0394,  0.0289],\n",
      "        [ 0.0022, -0.0090,  0.0048,  0.0074, -0.0047],\n",
      "        [-0.0059, -0.0041,  0.0165, -0.0094,  0.0041]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0070,  0.0005, -0.0089, -0.0079,  0.0077],\n",
      "        [ 0.0071,  0.0093, -0.0259, -0.0036,  0.0208],\n",
      "        [ 0.0070,  0.0246,  0.0452,  0.0132, -0.0021],\n",
      "        [-0.0188,  0.0195, -0.0119, -0.0174,  0.0073],\n",
      "        [-0.0181,  0.0030, -0.0188, -0.0323,  0.0060],\n",
      "        [-0.0243, -0.0191, -0.0858,  0.0274, -0.0085],\n",
      "        [ 0.0062,  0.0135,  0.0009, -0.0172,  0.0205],\n",
      "        [-0.0102,  0.0082,  0.0299, -0.0096, -0.0157]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0212, grad_fn=<MinBackward1>), tensor(0.8398, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10120189934968948\n",
      "@sample 1084: tensor([[-0.0143,  0.0257, -0.0189,  0.0182, -0.0266],\n",
      "        [-0.0095, -0.0087, -0.0054,  0.0035,  0.0077],\n",
      "        [ 0.0155,  0.0023, -0.0064, -0.0102, -0.0037],\n",
      "        [ 0.0113,  0.0133,  0.0050,  0.0059, -0.0075],\n",
      "        [-0.0055,  0.0185,  0.0218, -0.0180, -0.0055],\n",
      "        [ 0.0018,  0.0116, -0.0138,  0.0078, -0.0235],\n",
      "        [ 0.0032,  0.0036, -0.0234,  0.0060, -0.0170],\n",
      "        [-0.0141, -0.0048, -0.0159,  0.0022,  0.0014]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0037,  0.0112,  0.0264, -0.0301, -0.0199],\n",
      "        [-0.0062, -0.0052,  0.0072, -0.0227, -0.0159],\n",
      "        [-0.0068,  0.0370, -0.0071, -0.0083, -0.0268],\n",
      "        [ 0.0106,  0.0348, -0.0145,  0.0064,  0.0052],\n",
      "        [-0.0124,  0.0033, -0.0005,  0.0088,  0.0053],\n",
      "        [ 0.0121, -0.0003, -0.0107, -0.0006, -0.0011],\n",
      "        [-0.0078, -0.0085, -0.0073, -0.0182, -0.0072],\n",
      "        [ 0.0150,  0.0049,  0.0169, -0.0377, -0.0034]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0142, grad_fn=<MinBackward1>), tensor(0.8660, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.0868036150932312\n",
      "@sample 1085: tensor([[-0.0024,  0.0022, -0.0014,  0.0106, -0.0210],\n",
      "        [-0.0104,  0.0029, -0.0241,  0.0151, -0.0220],\n",
      "        [ 0.0046,  0.0224, -0.0075, -0.0033, -0.0138],\n",
      "        [-0.0106,  0.0267,  0.0051, -0.0040,  0.0079],\n",
      "        [-0.0113,  0.0133,  0.0226, -0.0216,  0.0082],\n",
      "        [ 0.0169,  0.0111, -0.0080,  0.0172,  0.0079],\n",
      "        [-0.0122,  0.0044, -0.0042,  0.0138,  0.0101],\n",
      "        [-0.0138,  0.0055,  0.0053,  0.0131, -0.0246]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0086,  0.0203,  0.0509, -0.0183, -0.0035],\n",
      "        [ 0.0029, -0.0022, -0.0121, -0.0112,  0.0088],\n",
      "        [-0.0265,  0.0052, -0.0122,  0.0068,  0.0095],\n",
      "        [ 0.0127,  0.0056,  0.0109, -0.0239, -0.0328],\n",
      "        [-0.0215,  0.0239, -0.0107,  0.0088, -0.0368],\n",
      "        [ 0.0158,  0.0106, -0.0238,  0.0053, -0.0156],\n",
      "        [ 0.0056,  0.0224,  0.0425, -0.0151, -0.0285],\n",
      "        [ 0.0181, -0.0123, -0.0208, -0.0087, -0.0005]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0178, grad_fn=<MinBackward1>), tensor(0.8671, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09239893406629562\n",
      "@sample 1086: tensor([[-0.0014,  0.0001, -0.0061,  0.0139, -0.0236],\n",
      "        [-0.0113, -0.0186, -0.0022,  0.0132,  0.0028],\n",
      "        [-0.0329,  0.0116,  0.0079,  0.0094, -0.0009],\n",
      "        [-0.0022, -0.0167,  0.0084,  0.0072, -0.0094],\n",
      "        [-0.0041, -0.0030, -0.0049,  0.0124, -0.0064],\n",
      "        [-0.0242, -0.0077, -0.0173,  0.0097,  0.0034],\n",
      "        [-0.0097,  0.0026,  0.0025,  0.0215, -0.0213],\n",
      "        [-0.0163,  0.0154, -0.0059,  0.0176, -0.0124]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0114, -0.0074, -0.0017, -0.0246,  0.0072],\n",
      "        [-0.0074,  0.0184,  0.0197, -0.0115, -0.0078],\n",
      "        [-0.0132,  0.0311, -0.0453,  0.0023, -0.0115],\n",
      "        [-0.0354,  0.0323,  0.0300, -0.0191, -0.0166],\n",
      "        [ 0.0121,  0.0076,  0.0310, -0.0022,  0.0281],\n",
      "        [ 0.0286, -0.0076,  0.0263, -0.0015, -0.0225],\n",
      "        [ 0.0278,  0.0034,  0.0021, -0.0002,  0.0130],\n",
      "        [ 0.0008,  0.0030,  0.0194, -0.0015,  0.0020]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0160, grad_fn=<MinBackward1>), tensor(0.8616, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09290242940187454\n",
      "@sample 1087: tensor([[-0.0148,  0.0125, -0.0028, -0.0055,  0.0066],\n",
      "        [ 0.0038,  0.0005,  0.0146,  0.0082, -0.0039],\n",
      "        [-0.0083,  0.0051, -0.0166,  0.0202, -0.0206],\n",
      "        [ 0.0048,  0.0095,  0.0149, -0.0088, -0.0057],\n",
      "        [-0.0159,  0.0078,  0.0079, -0.0202, -0.0098],\n",
      "        [ 0.0016, -0.0073, -0.0041,  0.0028,  0.0029],\n",
      "        [ 0.0019,  0.0048,  0.0029, -0.0129, -0.0051],\n",
      "        [ 0.0080,  0.0161, -0.0103,  0.0114,  0.0023]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0085,  0.0121,  0.0126, -0.0088,  0.0187],\n",
      "        [-0.0072,  0.0072, -0.0131, -0.0106, -0.0042],\n",
      "        [-0.0391,  0.0071, -0.0336, -0.0204, -0.0097],\n",
      "        [-0.0049, -0.0073, -0.0313, -0.0208, -0.0158],\n",
      "        [ 0.0096, -0.0159,  0.0217,  0.0097, -0.0059],\n",
      "        [-0.0061, -0.0009, -0.0151,  0.0003, -0.0016],\n",
      "        [-0.0184, -0.0062, -0.0126,  0.0103, -0.0062],\n",
      "        [ 0.0235,  0.0231, -0.0059,  0.0083,  0.0206]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0121, grad_fn=<MinBackward1>), tensor(0.8571, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08340132981538773\n",
      "@sample 1088: tensor([[ 0.0120, -0.0107,  0.0040,  0.0030, -0.0063],\n",
      "        [ 0.0071,  0.0168,  0.0254, -0.0155, -0.0019],\n",
      "        [-0.0047, -0.0039, -0.0075,  0.0171, -0.0015],\n",
      "        [-0.0003, -0.0045,  0.0009,  0.0093, -0.0068],\n",
      "        [ 0.0081,  0.0131, -0.0177, -0.0133,  0.0081],\n",
      "        [ 0.0182,  0.0009, -0.0017,  0.0066, -0.0146],\n",
      "        [-0.0097,  0.0070, -0.0135,  0.0011, -0.0218],\n",
      "        [-0.0045, -0.0061,  0.0045,  0.0125, -0.0232]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0119,  0.0106, -0.0061, -0.0108, -0.0020],\n",
      "        [-0.0295,  0.0109, -0.0247,  0.0147, -0.0059],\n",
      "        [ 0.0174,  0.0006,  0.0087, -0.0237,  0.0048],\n",
      "        [ 0.0073,  0.0048,  0.0099,  0.0038,  0.0121],\n",
      "        [-0.0094, -0.0055, -0.0224, -0.0109,  0.0046],\n",
      "        [-0.0173, -0.0163,  0.0081,  0.0078,  0.0096],\n",
      "        [ 0.0160, -0.0016,  0.0366, -0.0112,  0.0099],\n",
      "        [ 0.0069, -0.0108,  0.0213, -0.0095,  0.0072]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0060, grad_fn=<MinBackward1>), tensor(0.8729, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09611407667398453\n",
      "@sample 1089: tensor([[-1.5408e-02,  3.1636e-02,  3.1021e-02, -8.1273e-03, -2.3007e-03],\n",
      "        [ 1.1422e-02, -9.7092e-03,  7.1645e-05,  7.7360e-03, -8.6277e-03],\n",
      "        [-1.6190e-04, -9.0386e-03, -9.5513e-03,  1.5410e-02, -1.8805e-03],\n",
      "        [ 1.4268e-02, -4.0317e-03, -7.5556e-04,  3.0178e-02, -2.1854e-02],\n",
      "        [-9.8371e-03,  6.0956e-03,  1.3012e-02, -2.7150e-02,  5.1167e-03],\n",
      "        [ 3.7633e-02, -2.6856e-02, -1.5795e-02, -3.2095e-02,  3.3859e-02],\n",
      "        [ 2.0939e-02,  2.7983e-03, -1.1206e-02, -2.6819e-03, -1.2919e-03],\n",
      "        [-1.2297e-02,  1.3488e-02, -3.0249e-06,  1.6998e-02,  1.1337e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0143, -0.0016,  0.0200,  0.0073, -0.0404],\n",
      "        [-0.0098,  0.0050, -0.0036, -0.0224, -0.0035],\n",
      "        [-0.0125,  0.0166,  0.0112,  0.0021, -0.0275],\n",
      "        [ 0.0295,  0.0314, -0.0165,  0.0106,  0.0155],\n",
      "        [-0.0074,  0.0231, -0.0122,  0.0145, -0.0051],\n",
      "        [-0.0105,  0.0044, -0.0236,  0.0101,  0.0118],\n",
      "        [-0.0166,  0.0124, -0.0051, -0.0015,  0.0050],\n",
      "        [ 0.0145, -0.0073, -0.0319,  0.0423,  0.0295]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0176, grad_fn=<MinBackward1>), tensor(0.8744, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09391023218631744\n",
      "@sample 1090: tensor([[ 1.3597e-03,  2.5634e-03, -1.8853e-02,  5.9453e-03, -5.2587e-03],\n",
      "        [ 7.7864e-03,  2.9978e-02,  7.0915e-03, -5.5693e-03, -8.7548e-03],\n",
      "        [ 1.5746e-02,  4.1073e-03, -5.6659e-03,  4.3974e-03,  1.4370e-02],\n",
      "        [-5.3367e-03, -6.7783e-03,  1.4718e-02, -1.2869e-02, -1.1974e-02],\n",
      "        [-1.5586e-03, -2.2792e-02, -6.7850e-03, -5.7644e-03,  3.8963e-03],\n",
      "        [-7.5824e-03, -1.2925e-02,  9.8564e-03,  1.2768e-02,  1.1692e-04],\n",
      "        [ 3.7586e-03,  1.9246e-03,  4.1352e-03,  5.0276e-03,  2.0728e-03],\n",
      "        [ 4.9323e-06,  3.6450e-02,  1.2208e-02, -3.9343e-02,  1.0716e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0149, -0.0346, -0.0188, -0.0150,  0.0058],\n",
      "        [-0.0177,  0.0029, -0.0160,  0.0162,  0.0046],\n",
      "        [-0.0025,  0.0069, -0.0751,  0.0285, -0.0100],\n",
      "        [-0.0444, -0.0253,  0.0005, -0.0142, -0.0184],\n",
      "        [-0.0095, -0.0168,  0.0043, -0.0127, -0.0073],\n",
      "        [ 0.0052,  0.0003,  0.0188, -0.0101,  0.0134],\n",
      "        [-0.0072, -0.0067,  0.0063,  0.0186,  0.0177],\n",
      "        [-0.0276,  0.0234, -0.0150, -0.0138, -0.0288]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0150, grad_fn=<MinBackward1>), tensor(0.8566, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09403213113546371\n",
      "@sample 1091: tensor([[ 0.0078, -0.0245, -0.0019,  0.0046,  0.0112],\n",
      "        [ 0.0002, -0.0052,  0.0062, -0.0344,  0.0244],\n",
      "        [-0.0075, -0.0068,  0.0065,  0.0155, -0.0222],\n",
      "        [ 0.0008, -0.0208,  0.0169, -0.0214,  0.0181],\n",
      "        [-0.0024, -0.0081,  0.0127, -0.0240,  0.0144],\n",
      "        [ 0.0068,  0.0307,  0.0130, -0.0261,  0.0221],\n",
      "        [-0.0057,  0.0033,  0.0160, -0.0103,  0.0186],\n",
      "        [-0.0032, -0.0010, -0.0078, -0.0135,  0.0094]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-1.9997e-02, -1.6265e-02, -2.4101e-02,  2.6687e-02, -8.1622e-03],\n",
      "        [-2.5663e-02, -2.4723e-02, -8.7462e-03, -7.5626e-03, -2.2108e-02],\n",
      "        [ 6.6813e-03, -8.8768e-04, -4.9666e-05, -1.2104e-02,  4.9024e-03],\n",
      "        [-3.2974e-03, -3.7454e-02, -3.5582e-02, -1.6254e-02, -4.3370e-02],\n",
      "        [ 4.9985e-03, -3.3123e-02,  1.7545e-02,  8.0846e-03,  4.9655e-03],\n",
      "        [ 7.9168e-03, -1.7698e-02, -2.9218e-02,  4.1955e-02,  1.3922e-02],\n",
      "        [-1.8609e-02,  2.7198e-02, -4.9306e-03, -1.4686e-02,  3.4115e-03],\n",
      "        [-2.7050e-03, -6.6560e-03, -6.0589e-02,  4.8117e-02,  3.4294e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0131, grad_fn=<MinBackward1>), tensor(0.8556, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08679698407649994\n",
      "@sample 1092: tensor([[ 3.8635e-03, -1.6767e-02, -5.4218e-05,  1.9500e-02, -6.1256e-03],\n",
      "        [-6.6669e-03, -1.5374e-02, -8.5150e-03,  7.0462e-03,  4.1319e-03],\n",
      "        [ 1.3780e-02, -2.4620e-02, -2.7634e-03, -1.6226e-02,  1.9402e-02],\n",
      "        [ 1.5791e-02, -9.2480e-04, -6.5696e-03,  3.0173e-02,  1.8779e-02],\n",
      "        [ 1.1025e-02, -1.5367e-02,  1.3926e-02, -3.1445e-03,  2.0753e-03],\n",
      "        [ 1.0158e-02, -2.6527e-02,  1.0928e-02, -8.0936e-03, -1.4852e-02],\n",
      "        [ 1.0271e-02, -2.9816e-03,  3.5220e-02, -6.5646e-03, -1.0304e-02],\n",
      "        [-1.2505e-02, -1.1077e-02, -1.1594e-02, -5.8481e-03,  4.6722e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 3.8075e-04,  3.2088e-02, -1.8869e-02,  1.8072e-02,  2.9796e-02],\n",
      "        [ 4.0459e-03,  4.7602e-05,  2.0774e-02, -3.2098e-02, -1.2599e-02],\n",
      "        [-3.3949e-02, -2.4379e-02,  6.9419e-03,  1.3279e-02,  2.3279e-02],\n",
      "        [-2.5324e-03,  2.0084e-02, -1.7114e-02, -2.0206e-02, -2.0511e-02],\n",
      "        [-4.2807e-03, -1.9557e-02, -2.0745e-02,  1.3948e-02, -7.7909e-03],\n",
      "        [ 1.3913e-02,  1.7783e-03, -4.3590e-02,  5.4895e-02,  3.5982e-02],\n",
      "        [ 7.9864e-03, -4.1239e-03,  4.2156e-02, -8.8172e-03,  6.3134e-03],\n",
      "        [-2.2121e-02, -9.2656e-04, -1.0965e-02,  3.6324e-02,  1.8018e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0156, grad_fn=<MinBackward1>), tensor(0.8399, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09196964651346207\n",
      "@sample 1093: tensor([[-0.0019, -0.0226, -0.0225, -0.0245,  0.0029],\n",
      "        [-0.0238, -0.0132,  0.0059, -0.0263,  0.0160],\n",
      "        [ 0.0089, -0.0071, -0.0024, -0.0097, -0.0111],\n",
      "        [-0.0168, -0.0190,  0.0216, -0.0251,  0.0096],\n",
      "        [ 0.0123,  0.0084,  0.0041,  0.0065, -0.0028],\n",
      "        [ 0.0182, -0.0050, -0.0300,  0.0106,  0.0059],\n",
      "        [ 0.0058, -0.0065,  0.0070,  0.0057,  0.0073],\n",
      "        [ 0.0145, -0.0187,  0.0018, -0.0128,  0.0162]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0180, -0.0972,  0.0156,  0.0424, -0.0055],\n",
      "        [-0.0188, -0.0437,  0.0049,  0.0113,  0.0145],\n",
      "        [ 0.0115,  0.0037,  0.0172, -0.0089, -0.0008],\n",
      "        [-0.0125, -0.0366,  0.0350,  0.0006,  0.0159],\n",
      "        [ 0.0357,  0.0259,  0.0244, -0.0256, -0.0004],\n",
      "        [ 0.0025,  0.0343, -0.0119, -0.0010,  0.0063],\n",
      "        [ 0.0210, -0.0067,  0.0050, -0.0194, -0.0190],\n",
      "        [-0.0125,  0.0060, -0.0064, -0.0012,  0.0196]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0173, grad_fn=<MinBackward1>), tensor(0.7904, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09281817078590393\n",
      "@sample 1094: tensor([[-0.0040, -0.0106, -0.0144,  0.0039, -0.0132],\n",
      "        [-0.0263, -0.0013,  0.0231, -0.0044, -0.0011],\n",
      "        [ 0.0027,  0.0346,  0.0024, -0.0332,  0.0184],\n",
      "        [ 0.0173, -0.0115, -0.0216, -0.0138, -0.0139],\n",
      "        [ 0.0103, -0.0030, -0.0111,  0.0080,  0.0071],\n",
      "        [ 0.0016,  0.0327,  0.0033, -0.0313,  0.0036],\n",
      "        [ 0.0184,  0.0126,  0.0245,  0.0091, -0.0163],\n",
      "        [-0.0052, -0.0054,  0.0117,  0.0022, -0.0045]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 1.8942e-03, -4.4526e-03,  3.0077e-02, -6.5828e-03, -2.4734e-02],\n",
      "        [ 7.4991e-03, -5.3501e-02,  4.6565e-02, -2.6787e-02, -2.5414e-02],\n",
      "        [-7.4730e-03, -3.1086e-02, -2.8306e-02,  2.9436e-02,  1.2379e-02],\n",
      "        [-2.4776e-02, -2.4819e-02, -1.4225e-02,  3.1885e-02, -1.7582e-02],\n",
      "        [ 5.1356e-03,  1.1262e-02,  6.2613e-03, -1.8529e-02, -3.1489e-02],\n",
      "        [-2.5038e-02, -1.0713e-02, -2.8339e-02,  1.9119e-02,  1.6135e-02],\n",
      "        [-1.1426e-02, -8.7220e-03,  9.7025e-03,  3.3253e-02,  2.1314e-02],\n",
      "        [-1.5735e-02,  1.5371e-02,  5.6481e-04, -1.1001e-02,  7.7292e-05]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0124, grad_fn=<MinBackward1>), tensor(0.8408, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08355781435966492\n",
      "@sample 1095: tensor([[-0.0192,  0.0327,  0.0342, -0.0081, -0.0150],\n",
      "        [-0.0025,  0.0235, -0.0039, -0.0205,  0.0038],\n",
      "        [ 0.0074,  0.0473,  0.0114, -0.0538,  0.0212],\n",
      "        [ 0.0230, -0.0183, -0.0020,  0.0143, -0.0118],\n",
      "        [ 0.0108, -0.0038,  0.0191, -0.0291,  0.0102],\n",
      "        [-0.0133, -0.0013, -0.0069, -0.0037,  0.0078],\n",
      "        [ 0.0216, -0.0090, -0.0005,  0.0040, -0.0051],\n",
      "        [ 0.0048, -0.0375, -0.0281, -0.0224, -0.0104]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-9.5703e-05, -7.6811e-03, -4.2712e-02,  1.6256e-02,  2.4018e-02],\n",
      "        [-3.1080e-02,  2.6002e-02, -3.6574e-03, -2.0533e-02, -1.2063e-02],\n",
      "        [-4.7527e-02, -5.0755e-02, -6.4124e-02,  6.7046e-02,  1.5237e-03],\n",
      "        [ 4.6834e-03,  2.3663e-02,  1.6157e-02, -1.2393e-02,  5.9284e-03],\n",
      "        [ 5.0627e-03, -1.2892e-02, -2.9667e-03, -1.3112e-02,  1.2010e-02],\n",
      "        [ 6.2132e-03, -2.2434e-03,  8.4874e-03, -2.9107e-02,  2.8797e-02],\n",
      "        [-1.4704e-04, -8.3340e-03,  1.7130e-02,  1.7564e-02,  3.1609e-02],\n",
      "        [-4.1933e-02, -3.2507e-02,  2.4434e-02, -5.5381e-03,  8.1492e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0137, grad_fn=<MinBackward1>), tensor(0.9028, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09458007663488388\n",
      "@sample 1096: tensor([[ 0.0104, -0.0115, -0.0132,  0.0264, -0.0009],\n",
      "        [ 0.0011, -0.0093,  0.0003, -0.0013,  0.0027],\n",
      "        [-0.0169,  0.0073,  0.0012, -0.0109, -0.0072],\n",
      "        [ 0.0107, -0.0128,  0.0161, -0.0047,  0.0039],\n",
      "        [ 0.0060,  0.0422,  0.0002, -0.0209,  0.0164],\n",
      "        [-0.0136,  0.0244,  0.0146, -0.0403,  0.0128],\n",
      "        [-0.0050,  0.0017, -0.0076, -0.0073,  0.0090],\n",
      "        [-0.0067,  0.0058, -0.0011, -0.0185,  0.0021]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0101,  0.0356,  0.0021,  0.0113,  0.0186],\n",
      "        [-0.0100,  0.0074, -0.0022,  0.0011,  0.0094],\n",
      "        [-0.0172,  0.0110,  0.0029,  0.0113,  0.0216],\n",
      "        [-0.0089, -0.0185, -0.0029, -0.0015, -0.0218],\n",
      "        [ 0.0119, -0.0250, -0.0313,  0.0303,  0.0119],\n",
      "        [-0.0444, -0.0132, -0.0489,  0.0243, -0.0156],\n",
      "        [-0.0037, -0.0069,  0.0396, -0.0276, -0.0075],\n",
      "        [ 0.0030,  0.0120,  0.0194, -0.0332,  0.0029]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0131, grad_fn=<MinBackward1>), tensor(0.8279, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09044184535741806\n",
      "@sample 1097: tensor([[ 0.0117,  0.0043, -0.0184, -0.0062,  0.0033],\n",
      "        [ 0.0077,  0.0111, -0.0145, -0.0351,  0.0196],\n",
      "        [ 0.0102, -0.0052, -0.0051,  0.0001, -0.0080],\n",
      "        [ 0.0189, -0.0033,  0.0105,  0.0021,  0.0013],\n",
      "        [-0.0011,  0.0192,  0.0187, -0.0045, -0.0020],\n",
      "        [ 0.0156, -0.0074, -0.0019,  0.0291, -0.0009],\n",
      "        [-0.0127, -0.0071, -0.0038, -0.0039,  0.0139],\n",
      "        [-0.0244,  0.0278,  0.0057,  0.0081, -0.0084]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0156, -0.0082,  0.0086, -0.0090, -0.0106],\n",
      "        [-0.0086, -0.0016, -0.0243, -0.0006, -0.0246],\n",
      "        [ 0.0028, -0.0187,  0.0145, -0.0156, -0.0223],\n",
      "        [ 0.0143, -0.0067, -0.0028, -0.0071, -0.0181],\n",
      "        [ 0.0319,  0.0244,  0.0069, -0.0204, -0.0146],\n",
      "        [ 0.0148,  0.0159,  0.0131, -0.0104, -0.0105],\n",
      "        [ 0.0171, -0.0173, -0.0158,  0.0142,  0.0191],\n",
      "        [ 0.0349,  0.0175,  0.0598, -0.0609, -0.0343]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0136, grad_fn=<MinBackward1>), tensor(0.8878, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09892019629478455\n",
      "@sample 1098: tensor([[-0.0143, -0.0059,  0.0130,  0.0032,  0.0182],\n",
      "        [ 0.0298,  0.0069, -0.0051, -0.0026, -0.0249],\n",
      "        [ 0.0076,  0.0204,  0.0019, -0.0131, -0.0107],\n",
      "        [-0.0058, -0.0018,  0.0030,  0.0182, -0.0108],\n",
      "        [-0.0086, -0.0070,  0.0046, -0.0011,  0.0060],\n",
      "        [-0.0035,  0.0021, -0.0100,  0.0132, -0.0152],\n",
      "        [ 0.0150,  0.0090,  0.0020,  0.0145,  0.0109],\n",
      "        [ 0.0109, -0.0200,  0.0010,  0.0198,  0.0056]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0449, -0.0203,  0.0232, -0.0048, -0.0029],\n",
      "        [-0.0069,  0.0196, -0.0381, -0.0298, -0.0056],\n",
      "        [-0.0185,  0.0299, -0.0034,  0.0138, -0.0213],\n",
      "        [ 0.0100,  0.0172,  0.0166,  0.0029,  0.0227],\n",
      "        [ 0.0219,  0.0030,  0.0338, -0.0286, -0.0046],\n",
      "        [ 0.0214,  0.0112,  0.0350, -0.0363, -0.0164],\n",
      "        [ 0.0158,  0.0223,  0.0017, -0.0180, -0.0095],\n",
      "        [ 0.0077,  0.0290,  0.0264,  0.0105,  0.0249]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0142, grad_fn=<MinBackward1>), tensor(0.8339, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.0928565114736557\n",
      "@sample 1099: tensor([[-0.0019, -0.0116, -0.0012,  0.0142, -0.0081],\n",
      "        [-0.0058,  0.0322,  0.0113, -0.0113, -0.0009],\n",
      "        [-0.0053, -0.0206, -0.0184,  0.0125, -0.0145],\n",
      "        [-0.0200, -0.0009, -0.0115,  0.0142,  0.0027],\n",
      "        [-0.0145, -0.0066, -0.0107,  0.0252,  0.0139],\n",
      "        [-0.0039, -0.0149, -0.0129,  0.0138,  0.0204],\n",
      "        [-0.0051, -0.0050, -0.0012,  0.0172,  0.0039],\n",
      "        [ 0.0091, -0.0129, -0.0143,  0.0111, -0.0128]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0296,  0.0357,  0.0077, -0.0072,  0.0151],\n",
      "        [ 0.0197, -0.0049, -0.0303,  0.0230,  0.0043],\n",
      "        [ 0.0265, -0.0226,  0.0003, -0.0045,  0.0040],\n",
      "        [ 0.0355, -0.0334, -0.0027,  0.0073,  0.0070],\n",
      "        [ 0.0001,  0.0248,  0.0089, -0.0262, -0.0020],\n",
      "        [ 0.0131, -0.0195,  0.0116, -0.0161,  0.0224],\n",
      "        [ 0.0286,  0.0185,  0.0187, -0.0101, -0.0002],\n",
      "        [ 0.0226,  0.0177, -0.0253, -0.0042,  0.0203]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0103, grad_fn=<MinBackward1>), tensor(0.8403, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09997503459453583\n",
      "@sample 1100: tensor([[-0.0105, -0.0065, -0.0157,  0.0375, -0.0160],\n",
      "        [ 0.0221, -0.0047, -0.0119,  0.0456,  0.0044],\n",
      "        [ 0.0068, -0.0036, -0.0015,  0.0017,  0.0038],\n",
      "        [ 0.0220, -0.0381, -0.0216,  0.0500,  0.0143],\n",
      "        [-0.0140, -0.0003,  0.0132, -0.0151, -0.0048],\n",
      "        [-0.0228,  0.0165,  0.0032, -0.0243,  0.0089],\n",
      "        [ 0.0222, -0.0201, -0.0095, -0.0036,  0.0170],\n",
      "        [-0.0179, -0.0025,  0.0007, -0.0038, -0.0028]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0130,  0.0064, -0.0099, -0.0056,  0.0195],\n",
      "        [ 0.0322,  0.0507, -0.0305,  0.0131,  0.0293],\n",
      "        [ 0.0078,  0.0254, -0.0106,  0.0220,  0.0066],\n",
      "        [ 0.0315,  0.0222,  0.0018, -0.0079,  0.0015],\n",
      "        [-0.0084,  0.0030, -0.0070,  0.0022,  0.0189],\n",
      "        [-0.0125, -0.0105, -0.0419,  0.0172, -0.0088],\n",
      "        [-0.0167, -0.0009,  0.0214, -0.0149,  0.0047],\n",
      "        [ 0.0194, -0.0318, -0.0007,  0.0014,  0.0278]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0149, grad_fn=<MinBackward1>), tensor(0.8129, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09559022635221481\n",
      "@sample 1101: tensor([[ 1.0963e-02,  2.2850e-02, -4.1097e-03, -3.2078e-03,  2.0157e-03],\n",
      "        [ 7.7375e-03, -7.0855e-05, -1.5648e-02,  2.4494e-02, -2.5592e-03],\n",
      "        [ 2.5495e-03, -6.4532e-03, -8.6683e-03,  2.3766e-02, -1.3306e-03],\n",
      "        [ 1.2834e-02, -1.4893e-02,  8.9693e-03,  2.0709e-02, -1.6533e-03],\n",
      "        [ 1.1889e-02,  1.0380e-02,  9.7959e-03, -1.0805e-02,  1.0376e-02],\n",
      "        [-2.1314e-02, -2.6302e-04,  1.3673e-02,  1.5777e-02,  7.8884e-03],\n",
      "        [ 2.3991e-02, -3.1291e-02, -7.6380e-03,  1.8072e-02, -1.9015e-02],\n",
      "        [ 4.0870e-03, -8.4529e-03,  4.5072e-03, -6.6315e-03,  2.1430e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0416,  0.0114, -0.0546,  0.0237, -0.0124],\n",
      "        [-0.0232, -0.0051,  0.0013, -0.0102,  0.0156],\n",
      "        [ 0.0323,  0.0035,  0.0528, -0.0081, -0.0202],\n",
      "        [ 0.0097,  0.0152,  0.0097,  0.0393,  0.0189],\n",
      "        [-0.0018,  0.0089, -0.0067,  0.0102,  0.0123],\n",
      "        [ 0.0368,  0.0109, -0.0362, -0.0020,  0.0186],\n",
      "        [-0.0019,  0.0444, -0.0051, -0.0115, -0.0101],\n",
      "        [ 0.0021, -0.0034, -0.0500,  0.0178, -0.0035]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0109, grad_fn=<MinBackward1>), tensor(0.8363, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09190234541893005\n",
      "@sample 1102: tensor([[ 0.0211,  0.0007,  0.0160,  0.0027,  0.0014],\n",
      "        [ 0.0270,  0.0138,  0.0033,  0.0117,  0.0030],\n",
      "        [ 0.0190, -0.0106, -0.0232, -0.0039,  0.0082],\n",
      "        [-0.0090,  0.0470,  0.0183, -0.0199, -0.0008],\n",
      "        [-0.0051, -0.0107, -0.0086,  0.0294, -0.0098],\n",
      "        [ 0.0079, -0.0333, -0.0077,  0.0368, -0.0229],\n",
      "        [-0.0100,  0.0170, -0.0096, -0.0047,  0.0032],\n",
      "        [ 0.0190, -0.0007, -0.0164,  0.0388, -0.0221]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0158,  0.0081, -0.0060,  0.0016, -0.0178],\n",
      "        [-0.0070,  0.0059, -0.0557,  0.0010,  0.0117],\n",
      "        [-0.0131, -0.0031,  0.0490, -0.0123, -0.0029],\n",
      "        [-0.0071,  0.0132, -0.0157, -0.0013,  0.0012],\n",
      "        [ 0.0235,  0.0038, -0.0006, -0.0049, -0.0151],\n",
      "        [-0.0066,  0.0081, -0.0153,  0.0178,  0.0062],\n",
      "        [-0.0058,  0.0101, -0.0070, -0.0139, -0.0082],\n",
      "        [ 0.0071,  0.0242,  0.0037, -0.0094,  0.0279]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0131, grad_fn=<MinBackward1>), tensor(0.8099, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08597198873758316\n",
      "@sample 1103: tensor([[ 0.0316, -0.0342,  0.0043,  0.0331, -0.0394],\n",
      "        [ 0.0011,  0.0061, -0.0078,  0.0200,  0.0072],\n",
      "        [-0.0022, -0.0039,  0.0008,  0.0010,  0.0080],\n",
      "        [-0.0075,  0.0032, -0.0045,  0.0091, -0.0149],\n",
      "        [ 0.0133, -0.0156, -0.0031,  0.0157, -0.0068],\n",
      "        [-0.0079, -0.0251, -0.0094,  0.0136,  0.0123],\n",
      "        [-0.0113,  0.0052, -0.0104, -0.0057,  0.0011],\n",
      "        [-0.0203,  0.0091,  0.0027, -0.0133, -0.0173]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0065, -0.0075,  0.0383, -0.0292, -0.0011],\n",
      "        [-0.0186, -0.0140, -0.0253,  0.0004, -0.0229],\n",
      "        [ 0.0080, -0.0146, -0.0365, -0.0091,  0.0008],\n",
      "        [ 0.0020,  0.0041, -0.0036, -0.0147,  0.0099],\n",
      "        [-0.0013,  0.0261,  0.0338, -0.0106, -0.0063],\n",
      "        [ 0.0266, -0.0146,  0.0311, -0.0168,  0.0097],\n",
      "        [ 0.0002, -0.0011, -0.0098,  0.0044,  0.0162],\n",
      "        [ 0.0014, -0.0159, -0.0118,  0.0101,  0.0193]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0129, grad_fn=<MinBackward1>), tensor(0.8426, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10986072570085526\n",
      "@sample 1104: tensor([[-1.0400e-02, -2.3626e-02, -9.5263e-03, -1.0556e-02,  2.7473e-03],\n",
      "        [ 8.2662e-04,  1.4018e-02,  4.7201e-03, -9.6553e-03,  1.3085e-02],\n",
      "        [ 1.9885e-02, -8.5796e-03, -2.0349e-02,  1.2220e-02,  3.2790e-05],\n",
      "        [ 3.1968e-03,  1.1204e-02,  1.0717e-02, -2.0566e-02,  7.3640e-03],\n",
      "        [ 1.4560e-02,  3.9937e-03,  5.4580e-03,  5.4327e-04,  1.2713e-02],\n",
      "        [-1.2842e-02, -2.1553e-03,  3.5908e-03, -1.9976e-03,  1.2861e-02],\n",
      "        [ 1.7607e-03,  2.0314e-02,  3.7239e-03,  1.3346e-03,  3.4968e-03],\n",
      "        [ 2.1577e-04,  7.0491e-03,  1.0690e-02,  2.8900e-02,  3.0283e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-1.8915e-02, -2.4362e-02,  2.9714e-02, -3.4364e-03, -3.3159e-03],\n",
      "        [-2.2569e-02, -1.6532e-02, -3.0144e-02,  3.0256e-02,  1.1697e-02],\n",
      "        [ 1.5492e-02,  5.7559e-03, -4.9150e-02,  3.2007e-02,  9.0442e-03],\n",
      "        [-4.8056e-02,  7.5598e-03,  8.5371e-03,  3.8755e-03, -1.7435e-02],\n",
      "        [-1.5657e-04,  1.0496e-02, -7.1526e-06, -1.5509e-02, -2.0519e-03],\n",
      "        [ 9.6528e-03, -1.1320e-02,  1.6971e-02,  5.7400e-03,  8.5835e-03],\n",
      "        [-1.8344e-03, -8.9126e-03,  8.0286e-03, -3.3394e-02, -1.4566e-02],\n",
      "        [ 9.9916e-03,  9.9407e-03, -2.4214e-03,  1.2933e-02, -3.7807e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0112, grad_fn=<MinBackward1>), tensor(0.8238, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.0854889526963234\n",
      "@sample 1105: tensor([[-0.0135, -0.0094,  0.0229, -0.0013,  0.0097],\n",
      "        [ 0.0057,  0.0134,  0.0115, -0.0169, -0.0186],\n",
      "        [-0.0048, -0.0074, -0.0164, -0.0135,  0.0118],\n",
      "        [ 0.0045,  0.0004,  0.0048,  0.0053, -0.0202],\n",
      "        [ 0.0058,  0.0137, -0.0132, -0.0112,  0.0089],\n",
      "        [-0.0064, -0.0095, -0.0239, -0.0033,  0.0116],\n",
      "        [-0.0086, -0.0062, -0.0035, -0.0066,  0.0082],\n",
      "        [-0.0014,  0.0099,  0.0221, -0.0150, -0.0066]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0072,  0.0118, -0.0210,  0.0184,  0.0152],\n",
      "        [ 0.0012, -0.0209, -0.0292, -0.0159,  0.0200],\n",
      "        [ 0.0051, -0.0105, -0.0228,  0.0137, -0.0196],\n",
      "        [ 0.0203,  0.0071,  0.0393, -0.0096,  0.0089],\n",
      "        [ 0.0286, -0.0063,  0.0185, -0.0108,  0.0166],\n",
      "        [ 0.0029,  0.0039,  0.0010,  0.0025,  0.0094],\n",
      "        [ 0.0072, -0.0176,  0.0357, -0.0358, -0.0115],\n",
      "        [ 0.0012, -0.0120,  0.0193, -0.0046,  0.0084]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0134, grad_fn=<MinBackward1>), tensor(0.8461, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.07943206280469894\n",
      "@sample 1106: tensor([[-1.7107e-02,  1.0580e-02,  2.2120e-02, -2.1196e-02,  3.0160e-03],\n",
      "        [-5.5029e-03,  3.0712e-02,  1.2935e-02,  4.6286e-03,  4.2174e-03],\n",
      "        [-2.2544e-02, -8.9496e-03, -4.4040e-05, -1.4542e-02,  4.3988e-03],\n",
      "        [-9.1508e-03,  2.2397e-02, -2.4109e-03, -1.4665e-02,  1.0476e-02],\n",
      "        [-3.6897e-03,  1.1392e-02, -1.5679e-02,  1.6715e-02, -7.7529e-03],\n",
      "        [ 1.1105e-03,  1.3233e-02,  1.9455e-02, -4.0725e-02,  2.4645e-02],\n",
      "        [-7.8730e-03,  4.7038e-03,  4.4834e-03, -4.2179e-03, -7.4089e-05],\n",
      "        [ 1.7152e-02,  8.5773e-03, -7.5858e-03, -7.7469e-03, -1.4525e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0193, -0.0131,  0.0552, -0.0334, -0.0050],\n",
      "        [-0.0048,  0.0038, -0.0370,  0.0418,  0.0049],\n",
      "        [-0.0227,  0.0007,  0.0095, -0.0033, -0.0193],\n",
      "        [-0.0255, -0.0143, -0.0530,  0.0322, -0.0144],\n",
      "        [ 0.0153,  0.0159, -0.0016, -0.0256, -0.0051],\n",
      "        [-0.0198,  0.0004, -0.0304,  0.0086, -0.0142],\n",
      "        [ 0.0059,  0.0214,  0.0096, -0.0025, -0.0003],\n",
      "        [-0.0102,  0.0310, -0.0089,  0.0160,  0.0028]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0140, grad_fn=<MinBackward1>), tensor(0.8824, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08596565574407578\n",
      "@sample 1107: tensor([[ 0.0015, -0.0032,  0.0073, -0.0205,  0.0162],\n",
      "        [ 0.0103,  0.0249, -0.0171, -0.0170, -0.0114],\n",
      "        [-0.0144, -0.0112, -0.0012, -0.0283,  0.0230],\n",
      "        [-0.0061, -0.0038, -0.0032,  0.0054,  0.0177],\n",
      "        [ 0.0020, -0.0076,  0.0200, -0.0109,  0.0077],\n",
      "        [ 0.0061, -0.0031,  0.0043, -0.0007, -0.0082],\n",
      "        [-0.0048, -0.0188,  0.0152,  0.0043,  0.0006],\n",
      "        [ 0.0111,  0.0402,  0.0402, -0.0333,  0.0113]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-1.6048e-02,  3.5849e-03, -1.4202e-02, -1.0421e-02, -7.7527e-03],\n",
      "        [-2.3969e-02, -1.9297e-02, -3.3688e-02,  2.5101e-02,  2.0375e-02],\n",
      "        [-4.7744e-02, -2.0559e-02, -1.6499e-02,  9.0871e-03, -3.5255e-02],\n",
      "        [ 3.6191e-03, -4.2225e-02, -5.2702e-03, -2.6927e-02, -2.6981e-02],\n",
      "        [-2.1499e-02, -2.1509e-02,  9.4898e-03, -6.6668e-05, -1.2284e-02],\n",
      "        [-4.7854e-04,  6.0206e-03,  6.8809e-03, -2.1506e-02, -4.8632e-03],\n",
      "        [-5.6680e-03,  1.3156e-03, -3.9911e-03, -3.9317e-03, -1.1399e-02],\n",
      "        [-2.2699e-02,  1.8188e-02, -2.2915e-02,  3.0451e-02, -1.2962e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0118, grad_fn=<MinBackward1>), tensor(0.8420, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08580959588289261\n",
      "@sample 1108: tensor([[-0.0017,  0.0022, -0.0010,  0.0178, -0.0078],\n",
      "        [-0.0238,  0.0219,  0.0232,  0.0203, -0.0243],\n",
      "        [-0.0029,  0.0129,  0.0140,  0.0075, -0.0107],\n",
      "        [ 0.0164,  0.0095, -0.0045, -0.0045,  0.0163],\n",
      "        [-0.0034, -0.0056,  0.0179, -0.0066,  0.0109],\n",
      "        [-0.0046, -0.0120,  0.0035,  0.0228, -0.0061],\n",
      "        [-0.0242, -0.0102,  0.0034, -0.0167,  0.0248],\n",
      "        [-0.0151,  0.0290,  0.0279,  0.0157, -0.0015]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0081,  0.0248,  0.0005, -0.0233, -0.0096],\n",
      "        [ 0.0332,  0.0300,  0.0294,  0.0259,  0.0264],\n",
      "        [-0.0378,  0.0039, -0.0763,  0.0242,  0.0300],\n",
      "        [ 0.0017, -0.0005,  0.0152,  0.0234, -0.0087],\n",
      "        [-0.0017,  0.0058, -0.0253, -0.0089, -0.0201],\n",
      "        [ 0.0282,  0.0012,  0.0145, -0.0114,  0.0003],\n",
      "        [-0.0061, -0.0599,  0.0164, -0.0019, -0.0111],\n",
      "        [-0.0206,  0.0318, -0.0550, -0.0034, -0.0335]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0126, grad_fn=<MinBackward1>), tensor(0.8556, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10322286933660507\n",
      "@sample 1109: tensor([[-3.0377e-02,  2.1914e-02,  1.8838e-02,  6.3762e-05, -5.9639e-03],\n",
      "        [-4.5876e-03, -1.2245e-02,  3.9721e-03,  1.0511e-03, -6.8156e-03],\n",
      "        [-2.0431e-03,  1.4712e-02,  9.7570e-03, -1.4480e-02,  1.5393e-02],\n",
      "        [-1.2801e-02,  1.8619e-02,  2.1146e-02, -2.1502e-02, -1.4433e-02],\n",
      "        [-1.2700e-02, -7.5897e-03,  3.4592e-02, -2.3991e-03,  2.7784e-02],\n",
      "        [ 1.4545e-02,  2.4408e-02,  1.4674e-02, -2.1546e-02, -6.4874e-03],\n",
      "        [ 1.6547e-02, -7.2265e-04,  4.4100e-04,  1.9443e-02, -6.2165e-03],\n",
      "        [-2.6467e-02,  5.9005e-03, -1.1156e-02, -3.3304e-04,  8.0475e-04]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-1.3539e-02, -4.2508e-02, -8.2852e-03,  2.9742e-03,  1.3253e-02],\n",
      "        [-5.8773e-03, -2.6274e-02,  9.2596e-03,  3.0621e-03, -2.8070e-03],\n",
      "        [ 8.0757e-04, -6.6869e-05, -3.4617e-02,  4.7479e-03, -2.4866e-02],\n",
      "        [-1.7799e-02, -1.0949e-02, -2.4391e-02,  3.1541e-03, -2.3760e-02],\n",
      "        [ 2.4129e-02,  1.9631e-02, -2.1345e-02,  2.1022e-02, -5.8579e-03],\n",
      "        [-2.8064e-03,  2.1074e-02, -2.6668e-03,  1.1548e-05,  6.6070e-03],\n",
      "        [ 1.0839e-02,  7.4617e-03, -8.4986e-03, -2.0645e-03, -4.2128e-03],\n",
      "        [-4.7224e-03, -2.3832e-02,  1.6276e-02, -2.6151e-02, -1.7007e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0127, grad_fn=<MinBackward1>), tensor(0.8596, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.096706822514534\n",
      "@sample 1110: tensor([[-0.0015,  0.0188,  0.0096, -0.0183,  0.0142],\n",
      "        [-0.0192, -0.0098,  0.0047, -0.0168,  0.0182],\n",
      "        [-0.0323, -0.0116,  0.0031, -0.0383,  0.0061],\n",
      "        [-0.0013,  0.0137,  0.0094, -0.0215,  0.0065],\n",
      "        [-0.0068,  0.0105,  0.0013, -0.0204,  0.0255],\n",
      "        [-0.0063,  0.0008,  0.0004, -0.0016,  0.0015],\n",
      "        [ 0.0020,  0.0146,  0.0074,  0.0086, -0.0408],\n",
      "        [ 0.0065, -0.0083, -0.0172,  0.0064, -0.0034]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-2.7972e-02, -4.2763e-04,  5.3882e-03, -3.8462e-02,  6.0691e-04],\n",
      "        [-1.8141e-03, -1.3333e-02,  1.4753e-02,  1.3143e-02, -1.2326e-02],\n",
      "        [-3.8453e-02, -4.5707e-02,  7.8570e-03, -2.2400e-02, -8.2519e-05],\n",
      "        [-1.5548e-02, -1.7835e-02, -1.0604e-02,  9.0727e-03,  4.4428e-03],\n",
      "        [-6.3608e-03,  1.1466e-02,  3.4238e-02, -1.1624e-02, -2.5302e-02],\n",
      "        [ 2.1288e-03, -1.2527e-02,  1.5075e-02, -5.7908e-03, -2.3107e-02],\n",
      "        [-8.2296e-03,  2.2874e-02, -4.6097e-04,  2.6599e-02,  3.2454e-02],\n",
      "        [ 7.5107e-03, -4.8771e-02, -1.0209e-03,  1.5301e-02,  2.1019e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0127, grad_fn=<MinBackward1>), tensor(0.8713, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09902619570493698\n",
      "@sample 1111: tensor([[-0.0070, -0.0159,  0.0222, -0.0116, -0.0002],\n",
      "        [-0.0004,  0.0097,  0.0086, -0.0096,  0.0069],\n",
      "        [-0.0218,  0.0054,  0.0189, -0.0018, -0.0271],\n",
      "        [-0.0103,  0.0235,  0.0267, -0.0474,  0.0249],\n",
      "        [ 0.0335, -0.0081, -0.0137, -0.0023, -0.0145],\n",
      "        [-0.0045, -0.0167,  0.0034, -0.0110,  0.0102],\n",
      "        [ 0.0029,  0.0153, -0.0050, -0.0295,  0.0146],\n",
      "        [ 0.0492,  0.0452,  0.0079, -0.0284,  0.0196]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0163, -0.0065, -0.0038,  0.0172, -0.0122],\n",
      "        [-0.0014, -0.0028, -0.0015,  0.0085, -0.0136],\n",
      "        [ 0.0055,  0.0290, -0.0020,  0.0085,  0.0154],\n",
      "        [-0.0360, -0.0263, -0.0148,  0.0166, -0.0008],\n",
      "        [-0.0170,  0.0095,  0.0286,  0.0045, -0.0270],\n",
      "        [-0.0031,  0.0054,  0.0058, -0.0017,  0.0010],\n",
      "        [-0.0106, -0.0234, -0.0100,  0.0320,  0.0091],\n",
      "        [-0.0302, -0.0352, -0.0830,  0.0541, -0.0310]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0130, grad_fn=<MinBackward1>), tensor(0.8318, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09193496406078339\n",
      "@sample 1112: tensor([[ 0.0186, -0.0043,  0.0328, -0.0243,  0.0067],\n",
      "        [ 0.0406, -0.0080, -0.0152,  0.0125,  0.0031],\n",
      "        [ 0.0083,  0.0096,  0.0124, -0.0093,  0.0136],\n",
      "        [-0.0161,  0.0067, -0.0236,  0.0101,  0.0059],\n",
      "        [ 0.0100, -0.0052, -0.0130, -0.0022, -0.0145],\n",
      "        [ 0.0089, -0.0079,  0.0224,  0.0095, -0.0075],\n",
      "        [-0.0119, -0.0195, -0.0206,  0.0117, -0.0228],\n",
      "        [ 0.0131,  0.0251, -0.0020, -0.0004, -0.0101]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0006,  0.0136,  0.0224,  0.0227,  0.0040],\n",
      "        [-0.0302, -0.0038, -0.0441,  0.0210, -0.0248],\n",
      "        [ 0.0086,  0.0165,  0.0016,  0.0109, -0.0056],\n",
      "        [-0.0053,  0.0440, -0.0163,  0.0053,  0.0084],\n",
      "        [ 0.0011,  0.0070, -0.0094,  0.0046,  0.0025],\n",
      "        [ 0.0036,  0.0111, -0.0070,  0.0184,  0.0037],\n",
      "        [-0.0008, -0.0137,  0.0439, -0.0352, -0.0136],\n",
      "        [-0.0146, -0.0157, -0.0487,  0.0157, -0.0188]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0107, grad_fn=<MinBackward1>), tensor(0.8723, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08345398306846619\n",
      "@sample 1113: tensor([[-0.0189,  0.0086,  0.0171, -0.0099,  0.0119],\n",
      "        [ 0.0111, -0.0035,  0.0123,  0.0233, -0.0063],\n",
      "        [-0.0104, -0.0086, -0.0188, -0.0091, -0.0038],\n",
      "        [ 0.0118, -0.0057, -0.0094,  0.0032, -0.0124],\n",
      "        [ 0.0031,  0.0478,  0.0014, -0.0274,  0.0038],\n",
      "        [ 0.0167, -0.0051,  0.0035, -0.0073,  0.0127],\n",
      "        [-0.0142, -0.0042, -0.0140, -0.0043,  0.0009],\n",
      "        [-0.0009,  0.0047,  0.0062, -0.0081, -0.0187]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0023, -0.0043,  0.0226, -0.0036, -0.0101],\n",
      "        [-0.0080,  0.0195,  0.0062,  0.0116, -0.0062],\n",
      "        [-0.0117, -0.0433,  0.0219, -0.0050,  0.0014],\n",
      "        [-0.0206,  0.0383, -0.0311,  0.0205,  0.0101],\n",
      "        [-0.0286, -0.0411, -0.0620,  0.0799, -0.0099],\n",
      "        [ 0.0174, -0.0005,  0.0017,  0.0338,  0.0002],\n",
      "        [-0.0015,  0.0142,  0.0123, -0.0044,  0.0008],\n",
      "        [-0.0051, -0.0183,  0.0167,  0.0022, -0.0034]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0151, grad_fn=<MinBackward1>), tensor(0.8365, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09187662601470947\n",
      "@sample 1114: tensor([[ 0.0070,  0.0042,  0.0076,  0.0302, -0.0220],\n",
      "        [ 0.0075,  0.0356, -0.0015, -0.0056, -0.0066],\n",
      "        [ 0.0174, -0.0104, -0.0118,  0.0185, -0.0066],\n",
      "        [ 0.0064,  0.0065,  0.0027, -0.0020, -0.0134],\n",
      "        [ 0.0070, -0.0172, -0.0138,  0.0286, -0.0153],\n",
      "        [-0.0008,  0.0103, -0.0136, -0.0125,  0.0015],\n",
      "        [-0.0095,  0.0051, -0.0042, -0.0013, -0.0029],\n",
      "        [-0.0081,  0.0342,  0.0198, -0.0066, -0.0028]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0463,  0.0158, -0.0285,  0.0347,  0.0046],\n",
      "        [-0.0142,  0.0105,  0.0039,  0.0246, -0.0112],\n",
      "        [-0.0089,  0.0063,  0.0512,  0.0067,  0.0116],\n",
      "        [ 0.0139,  0.0084,  0.0113, -0.0180,  0.0176],\n",
      "        [ 0.0165,  0.0171, -0.0206,  0.0221,  0.0049],\n",
      "        [-0.0248, -0.0068, -0.0074,  0.0278,  0.0028],\n",
      "        [ 0.0065, -0.0149, -0.0041, -0.0173, -0.0218],\n",
      "        [ 0.0032,  0.0025,  0.0245,  0.0187,  0.0229]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0138, grad_fn=<MinBackward1>), tensor(0.8240, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08679281920194626\n",
      "@sample 1115: tensor([[-0.0067,  0.0030, -0.0275,  0.0261, -0.0214],\n",
      "        [-0.0154,  0.0122,  0.0116,  0.0114,  0.0088],\n",
      "        [ 0.0212,  0.0185, -0.0021, -0.0375, -0.0086],\n",
      "        [ 0.0194,  0.0537, -0.0055, -0.0276,  0.0027],\n",
      "        [ 0.0020,  0.0011, -0.0101,  0.0177, -0.0030],\n",
      "        [-0.0033, -0.0091, -0.0273,  0.0343, -0.0062],\n",
      "        [-0.0265, -0.0090,  0.0013,  0.0104, -0.0086],\n",
      "        [-0.0161, -0.0180, -0.0015,  0.0015, -0.0199]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0116, -0.0018,  0.0235, -0.0136, -0.0062],\n",
      "        [-0.0163,  0.0008, -0.0396,  0.0230,  0.0224],\n",
      "        [ 0.0148, -0.0455,  0.0185,  0.0642, -0.0205],\n",
      "        [-0.0135, -0.0371, -0.0505,  0.0495, -0.0129],\n",
      "        [-0.0204,  0.0260, -0.0047,  0.0073, -0.0008],\n",
      "        [ 0.0311,  0.0167,  0.0007, -0.0107, -0.0023],\n",
      "        [-0.0186,  0.0154, -0.0339,  0.0091,  0.0013],\n",
      "        [-0.0031,  0.0138,  0.0438, -0.0197,  0.0010]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0129, grad_fn=<MinBackward1>), tensor(0.8927, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.0838538259267807\n",
      "@sample 1116: tensor([[-1.6815e-02, -2.0161e-02, -1.2561e-02,  1.9462e-02, -2.4466e-02],\n",
      "        [-1.1826e-04, -4.1580e-02, -2.8801e-02,  1.8263e-02,  1.8616e-03],\n",
      "        [ 1.2827e-02, -1.1560e-03, -3.0458e-02,  1.5809e-02,  3.1818e-02],\n",
      "        [ 8.5343e-03, -7.8858e-03,  8.4768e-03,  1.8895e-02, -9.0037e-03],\n",
      "        [ 1.9449e-02,  1.3722e-02,  1.9076e-02,  4.7731e-03, -3.5316e-03],\n",
      "        [ 1.1792e-02,  6.1198e-03, -8.7127e-03,  4.6603e-03,  1.5197e-02],\n",
      "        [-2.5259e-03, -1.6496e-05, -5.6054e-03, -4.9620e-03, -4.1181e-03],\n",
      "        [-1.7869e-03, -3.7397e-04, -3.1699e-03, -1.8924e-02,  2.7321e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0376, -0.0143,  0.0362,  0.0008,  0.0128],\n",
      "        [ 0.0046, -0.0224, -0.0072, -0.0037, -0.0035],\n",
      "        [ 0.0001, -0.0224, -0.0196,  0.0067, -0.0232],\n",
      "        [ 0.0221,  0.0465,  0.0018,  0.0067, -0.0021],\n",
      "        [-0.0112,  0.0208, -0.0045,  0.0287, -0.0106],\n",
      "        [-0.0032,  0.0229, -0.0073,  0.0309,  0.0126],\n",
      "        [-0.0019, -0.0152,  0.0145,  0.0105,  0.0110],\n",
      "        [-0.0021,  0.0203, -0.0194,  0.0259, -0.0055]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0165, grad_fn=<MinBackward1>), tensor(0.8830, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09538574516773224\n",
      "@sample 1117: tensor([[ 0.0057,  0.0057, -0.0056, -0.0283,  0.0139],\n",
      "        [ 0.0352,  0.0214,  0.0169, -0.0108, -0.0095],\n",
      "        [-0.0046, -0.0194, -0.0125, -0.0285,  0.0133],\n",
      "        [-0.0135,  0.0185, -0.0068,  0.0090,  0.0012],\n",
      "        [-0.0214,  0.0157, -0.0102,  0.0417, -0.0466],\n",
      "        [ 0.0167,  0.0153, -0.0065,  0.0006, -0.0059],\n",
      "        [ 0.0126,  0.0036, -0.0082,  0.0095,  0.0091],\n",
      "        [-0.0001, -0.0091,  0.0039,  0.0155, -0.0162]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-2.1647e-02, -2.0617e-03, -5.0366e-05, -1.9011e-02, -8.0849e-03],\n",
      "        [-6.7390e-02,  3.9630e-03, -5.9193e-02,  4.0420e-02,  4.2958e-03],\n",
      "        [-2.0818e-02, -2.5668e-02,  1.9581e-02, -1.7496e-02, -1.1329e-02],\n",
      "        [ 7.0826e-03,  3.7593e-03,  1.6690e-02,  6.6202e-03,  1.6446e-02],\n",
      "        [ 5.5492e-02,  5.0302e-02,  2.5392e-02, -1.2593e-02,  1.1860e-02],\n",
      "        [ 2.6178e-03,  5.6385e-03, -8.8968e-03,  1.1665e-02,  1.0746e-03],\n",
      "        [-1.2321e-02,  5.6885e-03, -2.1542e-02, -6.9064e-03, -2.2652e-02],\n",
      "        [ 5.7226e-03,  1.1964e-02,  1.0416e-02,  1.7031e-03, -3.3637e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0130, grad_fn=<MinBackward1>), tensor(0.8385, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10611268877983093\n",
      "@sample 1118: tensor([[ 0.0033, -0.0010, -0.0053, -0.0017,  0.0205],\n",
      "        [ 0.0067, -0.0092, -0.0107,  0.0513, -0.0327],\n",
      "        [ 0.0051,  0.0047,  0.0099, -0.0141,  0.0143],\n",
      "        [ 0.0187,  0.0175, -0.0011,  0.0022, -0.0265],\n",
      "        [ 0.0042, -0.0024, -0.0071, -0.0271,  0.0067],\n",
      "        [ 0.0020,  0.0094,  0.0007,  0.0117, -0.0248],\n",
      "        [-0.0060,  0.0344,  0.0049, -0.0105, -0.0143],\n",
      "        [-0.0203, -0.0067, -0.0232, -0.0046,  0.0186]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0275,  0.0073, -0.0282,  0.0287,  0.0278],\n",
      "        [ 0.0060, -0.0002,  0.0113,  0.0036, -0.0137],\n",
      "        [-0.0185, -0.0055, -0.0235,  0.0044, -0.0117],\n",
      "        [-0.0034,  0.0228, -0.0265, -0.0017, -0.0079],\n",
      "        [-0.0415, -0.0344, -0.0212,  0.0068,  0.0005],\n",
      "        [-0.0073,  0.0076,  0.0078, -0.0103,  0.0016],\n",
      "        [-0.0292,  0.0143,  0.0033,  0.0263,  0.0212],\n",
      "        [ 0.0056,  0.0100,  0.0298, -0.0489,  0.0157]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0171, grad_fn=<MinBackward1>), tensor(0.8877, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08631794154644012\n",
      "@sample 1119: tensor([[ 0.0113, -0.0023,  0.0338, -0.0007,  0.0143],\n",
      "        [-0.0186,  0.0426,  0.0132, -0.0140, -0.0117],\n",
      "        [ 0.0306, -0.0144, -0.0204,  0.0227, -0.0307],\n",
      "        [-0.0055,  0.0237, -0.0061, -0.0047,  0.0059],\n",
      "        [-0.0076,  0.0053, -0.0059,  0.0064,  0.0217],\n",
      "        [ 0.0137,  0.0122, -0.0286,  0.0145, -0.0290],\n",
      "        [ 0.0154,  0.0280, -0.0044,  0.0163, -0.0124],\n",
      "        [-0.0036, -0.0017, -0.0090,  0.0169,  0.0075]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0309,  0.0219, -0.0241,  0.0083,  0.0091],\n",
      "        [ 0.0088,  0.0387, -0.0208, -0.0139,  0.0234],\n",
      "        [ 0.0091,  0.0034,  0.0103, -0.0103, -0.0069],\n",
      "        [ 0.0059,  0.0167,  0.0078, -0.0008, -0.0029],\n",
      "        [-0.0057, -0.0022, -0.0248,  0.0159,  0.0081],\n",
      "        [ 0.0114, -0.0019, -0.0080, -0.0055,  0.0120],\n",
      "        [ 0.0135,  0.0083, -0.0063,  0.0068, -0.0076],\n",
      "        [ 0.0046,  0.0244,  0.0025, -0.0087, -0.0041]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0152, grad_fn=<MinBackward1>), tensor(0.8676, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09800122678279877\n",
      "@sample 1120: tensor([[ 0.0037, -0.0489, -0.0156, -0.0050, -0.0002],\n",
      "        [-0.0010, -0.0173,  0.0046,  0.0141,  0.0287],\n",
      "        [ 0.0042,  0.0145,  0.0112, -0.0149,  0.0122],\n",
      "        [-0.0002,  0.0043,  0.0110,  0.0179,  0.0011],\n",
      "        [-0.0301, -0.0019,  0.0022, -0.0041,  0.0024],\n",
      "        [-0.0143,  0.0112, -0.0064,  0.0110, -0.0031],\n",
      "        [-0.0224, -0.0100,  0.0355, -0.0035,  0.0121],\n",
      "        [-0.0060, -0.0231, -0.0195, -0.0105,  0.0322]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0052,  0.0038, -0.0303,  0.0004, -0.0020],\n",
      "        [-0.0013,  0.0044, -0.0202,  0.0264,  0.0179],\n",
      "        [-0.0102, -0.0143, -0.0412,  0.0148, -0.0006],\n",
      "        [ 0.0122,  0.0344,  0.0080, -0.0162, -0.0156],\n",
      "        [ 0.0186, -0.0177, -0.0102, -0.0091, -0.0182],\n",
      "        [ 0.0015, -0.0203, -0.0200, -0.0055,  0.0070],\n",
      "        [ 0.0010,  0.0322, -0.0184,  0.0137,  0.0370],\n",
      "        [-0.0123, -0.0171,  0.0463, -0.0156, -0.0343]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0144, grad_fn=<MinBackward1>), tensor(0.7941, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08811093866825104\n",
      "@sample 1121: tensor([[ 0.0119, -0.0229, -0.0108,  0.0121, -0.0099],\n",
      "        [-0.0214, -0.0183,  0.0176, -0.0115,  0.0041],\n",
      "        [-0.0240,  0.0077,  0.0277, -0.0282,  0.0139],\n",
      "        [-0.0007,  0.0061,  0.0108,  0.0050, -0.0112],\n",
      "        [ 0.0050, -0.0143, -0.0051,  0.0086, -0.0131],\n",
      "        [-0.0012, -0.0019, -0.0019, -0.0006, -0.0025],\n",
      "        [ 0.0014, -0.0158, -0.0193,  0.0242, -0.0018],\n",
      "        [ 0.0096,  0.0250,  0.0219, -0.0111, -0.0098]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0117,  0.0135,  0.0019, -0.0140,  0.0212],\n",
      "        [ 0.0139,  0.0075, -0.0165,  0.0387,  0.0309],\n",
      "        [-0.0104,  0.0290,  0.0097, -0.0099, -0.0079],\n",
      "        [ 0.0041,  0.0060,  0.0089, -0.0129,  0.0310],\n",
      "        [ 0.0164,  0.0069,  0.0018, -0.0090, -0.0001],\n",
      "        [ 0.0339, -0.0360,  0.0105, -0.0072, -0.0080],\n",
      "        [ 0.0114,  0.0186,  0.0067, -0.0134, -0.0082],\n",
      "        [-0.0253, -0.0181, -0.0377,  0.0230,  0.0192]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0123, grad_fn=<MinBackward1>), tensor(0.8414, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09098657220602036\n",
      "@sample 1122: tensor([[ 0.0017, -0.0280,  0.0007, -0.0162,  0.0138],\n",
      "        [-0.0030, -0.0159, -0.0013,  0.0112, -0.0091],\n",
      "        [ 0.0025, -0.0007,  0.0185, -0.0150,  0.0222],\n",
      "        [ 0.0114, -0.0099, -0.0166,  0.0189,  0.0140],\n",
      "        [ 0.0081,  0.0026, -0.0037, -0.0152,  0.0105],\n",
      "        [-0.0037, -0.0074,  0.0337,  0.0173, -0.0068],\n",
      "        [ 0.0023, -0.0025,  0.0063, -0.0082,  0.0025],\n",
      "        [-0.0113,  0.0038, -0.0041, -0.0277,  0.0042]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-1.8586e-03, -2.4111e-02,  4.5283e-02, -4.0598e-03, -9.5697e-04],\n",
      "        [ 1.1604e-02, -6.1218e-04, -1.6980e-02, -2.6440e-02, -9.1365e-04],\n",
      "        [-8.8442e-03, -1.0697e-02,  2.4870e-03, -1.8478e-03, -7.0431e-03],\n",
      "        [-1.3423e-02, -5.7866e-03,  9.1898e-03, -2.8600e-02,  4.1167e-03],\n",
      "        [-1.4354e-02, -5.9753e-03,  2.8012e-02, -1.2369e-03, -1.2221e-02],\n",
      "        [-1.7945e-03,  3.0239e-03, -9.2658e-03,  1.4986e-03,  5.1592e-03],\n",
      "        [ 1.9261e-04, -1.1048e-02, -1.3581e-03, -7.8356e-03,  1.5286e-02],\n",
      "        [ 2.3211e-02, -3.8033e-02,  4.2021e-05,  1.3346e-03,  4.2022e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0129, grad_fn=<MinBackward1>), tensor(0.8332, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09361723810434341\n",
      "@sample 1123: tensor([[ 0.0122, -0.0123, -0.0148,  0.0278, -0.0072],\n",
      "        [-0.0193, -0.0104,  0.0141, -0.0078, -0.0029],\n",
      "        [ 0.0140,  0.0071,  0.0198, -0.0236, -0.0171],\n",
      "        [ 0.0055, -0.0017, -0.0123, -0.0169, -0.0032],\n",
      "        [ 0.0265,  0.0380,  0.0150, -0.0465,  0.0164],\n",
      "        [-0.0006, -0.0076,  0.0096,  0.0033,  0.0031],\n",
      "        [-0.0012,  0.0231,  0.0106,  0.0047, -0.0276],\n",
      "        [-0.0270,  0.0077,  0.0097, -0.0058,  0.0075]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0206,  0.0052,  0.0165, -0.0064,  0.0074],\n",
      "        [-0.0029,  0.0503, -0.0364,  0.0409,  0.0404],\n",
      "        [-0.0039, -0.0039,  0.0082, -0.0129, -0.0163],\n",
      "        [-0.0018,  0.0076, -0.0080, -0.0063, -0.0049],\n",
      "        [-0.0448, -0.0090,  0.0047,  0.0512,  0.0112],\n",
      "        [-0.0014,  0.0027,  0.0208,  0.0094,  0.0085],\n",
      "        [ 0.0054,  0.0208,  0.0036, -0.0070, -0.0050],\n",
      "        [ 0.0207,  0.0049,  0.0334, -0.0268, -0.0169]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0177, grad_fn=<MinBackward1>), tensor(0.8374, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08643729984760284\n",
      "@sample 1124: tensor([[ 0.0288,  0.0027, -0.0395,  0.0178, -0.0148],\n",
      "        [ 0.0013, -0.0227,  0.0100,  0.0072,  0.0015],\n",
      "        [ 0.0073, -0.0037, -0.0049, -0.0108,  0.0167],\n",
      "        [ 0.0031, -0.0332, -0.0232, -0.0047,  0.0235],\n",
      "        [ 0.0066,  0.0099,  0.0084, -0.0203, -0.0117],\n",
      "        [ 0.0058, -0.0315,  0.0083,  0.0054,  0.0178],\n",
      "        [-0.0104,  0.0162,  0.0271, -0.0059, -0.0167],\n",
      "        [ 0.0210,  0.0174, -0.0070, -0.0113,  0.0067]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0350,  0.0316,  0.0022,  0.0008,  0.0322],\n",
      "        [ 0.0048,  0.0093,  0.0207, -0.0410, -0.0092],\n",
      "        [-0.0128,  0.0068, -0.0088, -0.0357, -0.0101],\n",
      "        [ 0.0174, -0.0566, -0.0279,  0.0205,  0.0023],\n",
      "        [-0.0124,  0.0012, -0.0324,  0.0210,  0.0251],\n",
      "        [-0.0104, -0.0154,  0.0365, -0.0267,  0.0040],\n",
      "        [-0.0214,  0.0014,  0.0038,  0.0159,  0.0218],\n",
      "        [-0.0207,  0.0037, -0.0350,  0.0038,  0.0080]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0171, grad_fn=<MinBackward1>), tensor(0.8740, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.0961042270064354\n",
      "@sample 1125: tensor([[-0.0149, -0.0097, -0.0106,  0.0004,  0.0302],\n",
      "        [-0.0041,  0.0007,  0.0106,  0.0081,  0.0019],\n",
      "        [-0.0114, -0.0288,  0.0084,  0.0155,  0.0160],\n",
      "        [ 0.0165,  0.0099,  0.0112, -0.0045,  0.0142],\n",
      "        [ 0.0030,  0.0067,  0.0028, -0.0065,  0.0330],\n",
      "        [-0.0050, -0.0020, -0.0145, -0.0266,  0.0266],\n",
      "        [-0.0071, -0.0062,  0.0062,  0.0106,  0.0005],\n",
      "        [-0.0203, -0.0185,  0.0083, -0.0157, -0.0050]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0236, -0.0279,  0.0238,  0.0004, -0.0338],\n",
      "        [-0.0109,  0.0144, -0.0099,  0.0142,  0.0362],\n",
      "        [ 0.0109,  0.0098,  0.0374, -0.0136, -0.0050],\n",
      "        [-0.0069,  0.0116,  0.0287, -0.0151,  0.0008],\n",
      "        [-0.0182,  0.0022, -0.0010, -0.0165, -0.0215],\n",
      "        [-0.0018, -0.0243,  0.0373, -0.0140, -0.0202],\n",
      "        [-0.0026,  0.0219,  0.0167, -0.0022,  0.0069],\n",
      "        [-0.0172,  0.0092,  0.0277, -0.0104, -0.0005]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0105, grad_fn=<MinBackward1>), tensor(0.8314, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08853181451559067\n",
      "@sample 1126: tensor([[-0.0017, -0.0244,  0.0181, -0.0083,  0.0193],\n",
      "        [ 0.0188, -0.0357, -0.0143,  0.0241, -0.0054],\n",
      "        [ 0.0106, -0.0111,  0.0128, -0.0036,  0.0238],\n",
      "        [-0.0023, -0.0099,  0.0079, -0.0223,  0.0163],\n",
      "        [-0.0294, -0.0054,  0.0167, -0.0033, -0.0091],\n",
      "        [ 0.0066, -0.0348, -0.0096, -0.0094,  0.0240],\n",
      "        [ 0.0173, -0.0170, -0.0218,  0.0386, -0.0213],\n",
      "        [-0.0392,  0.0068,  0.0151, -0.0262,  0.0149]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 1.6447e-04,  3.2891e-03,  1.1056e-02,  7.4570e-03,  3.3738e-02],\n",
      "        [ 5.1940e-03, -1.1960e-02,  3.6874e-02, -6.3414e-03,  2.8102e-04],\n",
      "        [-1.9205e-03, -1.6250e-05,  1.1235e-02, -8.0220e-03,  3.9032e-04],\n",
      "        [-2.4975e-02, -7.3306e-05, -5.2931e-02,  2.3653e-02, -2.1592e-03],\n",
      "        [ 2.7419e-02, -9.2004e-03,  2.6646e-02, -2.9598e-02, -9.2387e-04],\n",
      "        [-1.4230e-02, -5.5151e-02,  2.3565e-02, -1.5380e-02, -1.2549e-02],\n",
      "        [ 1.8277e-02,  8.2859e-03,  7.9628e-03, -2.6099e-02, -1.2691e-02],\n",
      "        [-1.1438e-03, -5.1299e-02,  3.2206e-02,  4.1494e-04, -2.4315e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0136, grad_fn=<MinBackward1>), tensor(0.8360, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10708197206258774\n",
      "@sample 1127: tensor([[ 1.9273e-03, -4.2833e-02, -1.4839e-02, -2.3393e-03, -2.5740e-03],\n",
      "        [-1.0952e-02,  1.9039e-02,  1.8345e-02, -2.9736e-03, -1.4603e-06],\n",
      "        [-8.6425e-03, -1.5496e-02,  1.0610e-02,  3.9960e-03,  6.0383e-03],\n",
      "        [-1.8377e-03, -2.2165e-02,  1.2284e-02,  1.0433e-02, -6.2663e-03],\n",
      "        [ 1.6735e-02, -1.1988e-03, -1.2562e-02,  2.4213e-02, -7.0823e-03],\n",
      "        [ 7.4006e-03,  2.2488e-03,  5.6934e-03, -2.6757e-03,  6.9825e-03],\n",
      "        [ 1.0913e-02,  1.7736e-03,  1.4464e-02,  4.2553e-03,  5.4384e-03],\n",
      "        [-1.6585e-02, -6.1356e-04,  5.9662e-03, -1.4490e-02,  2.5093e-04]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0135, -0.0159,  0.0420, -0.0161, -0.0057],\n",
      "        [-0.0028,  0.0199, -0.0293,  0.0012,  0.0267],\n",
      "        [-0.0009,  0.0079,  0.0212,  0.0101,  0.0050],\n",
      "        [-0.0032,  0.0070, -0.0023,  0.0020, -0.0285],\n",
      "        [ 0.0218,  0.0069, -0.0290, -0.0051,  0.0139],\n",
      "        [-0.0005, -0.0094, -0.0127,  0.0039,  0.0047],\n",
      "        [ 0.0085,  0.0134, -0.0016, -0.0001, -0.0011],\n",
      "        [-0.0128, -0.0184,  0.0085, -0.0315,  0.0052]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0149, grad_fn=<MinBackward1>), tensor(0.8315, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10125865042209625\n",
      "@sample 1128: tensor([[ 0.0197, -0.0103, -0.0148,  0.0235, -0.0175],\n",
      "        [-0.0077, -0.0129, -0.0110,  0.0070,  0.0040],\n",
      "        [ 0.0164, -0.0074, -0.0071, -0.0356,  0.0168],\n",
      "        [ 0.0027, -0.0213, -0.0088,  0.0126, -0.0055],\n",
      "        [ 0.0218, -0.0074, -0.0051,  0.0083, -0.0160],\n",
      "        [-0.0109,  0.0158, -0.0179, -0.0074,  0.0206],\n",
      "        [ 0.0026, -0.0023,  0.0112, -0.0038,  0.0007],\n",
      "        [ 0.0085,  0.0196, -0.0120,  0.0053,  0.0099]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0058,  0.0173, -0.0042,  0.0117,  0.0164],\n",
      "        [ 0.0014, -0.0360,  0.0100,  0.0054, -0.0020],\n",
      "        [-0.0307,  0.0116, -0.0288,  0.0031,  0.0194],\n",
      "        [ 0.0152, -0.0057,  0.0032, -0.0155, -0.0074],\n",
      "        [ 0.0090, -0.0107,  0.0179, -0.0270, -0.0021],\n",
      "        [ 0.0145, -0.0467, -0.0039,  0.0077, -0.0211],\n",
      "        [-0.0033,  0.0005,  0.0008, -0.0167, -0.0042],\n",
      "        [ 0.0020,  0.0291,  0.0093, -0.0180, -0.0048]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0106, grad_fn=<MinBackward1>), tensor(0.8185, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10612673312425613\n",
      "@sample 1129: tensor([[-0.0015, -0.0085, -0.0068,  0.0049,  0.0022],\n",
      "        [ 0.0184,  0.0194, -0.0295, -0.0167, -0.0003],\n",
      "        [ 0.0091, -0.0235, -0.0115,  0.0203, -0.0244],\n",
      "        [-0.0213,  0.0301, -0.0115, -0.0025,  0.0068],\n",
      "        [-0.0130,  0.0270, -0.0058, -0.0256, -0.0048],\n",
      "        [ 0.0397,  0.0376,  0.0318, -0.0267,  0.0287],\n",
      "        [-0.0001,  0.0057,  0.0036, -0.0261,  0.0107],\n",
      "        [ 0.0161, -0.0058, -0.0208,  0.0106, -0.0144]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0017, -0.0005,  0.0145, -0.0246, -0.0144],\n",
      "        [ 0.0080, -0.0025,  0.0151, -0.0008,  0.0220],\n",
      "        [ 0.0125,  0.0027,  0.0186, -0.0175,  0.0077],\n",
      "        [ 0.0117, -0.0037,  0.0276, -0.0181,  0.0014],\n",
      "        [-0.0159, -0.0403,  0.0323, -0.0360, -0.0231],\n",
      "        [-0.0148, -0.0301, -0.0395,  0.0699, -0.0140],\n",
      "        [ 0.0097, -0.0243,  0.0140,  0.0034, -0.0077],\n",
      "        [ 0.0171, -0.0023,  0.0004,  0.0069, -0.0024]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0169, grad_fn=<MinBackward1>), tensor(0.8607, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10268181562423706\n",
      "@sample 1130: tensor([[ 0.0153, -0.0180,  0.0069,  0.0013,  0.0073],\n",
      "        [-0.0025,  0.0109,  0.0037, -0.0181, -0.0004],\n",
      "        [ 0.0012, -0.0003,  0.0165, -0.0064, -0.0043],\n",
      "        [-0.0121,  0.0020, -0.0074, -0.0074, -0.0090],\n",
      "        [ 0.0105,  0.0149, -0.0072, -0.0064,  0.0025],\n",
      "        [-0.0004,  0.0252, -0.0031,  0.0041, -0.0162],\n",
      "        [ 0.0031, -0.0095, -0.0182,  0.0106, -0.0060],\n",
      "        [ 0.0140,  0.0164,  0.0249, -0.0026,  0.0237]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0038,  0.0026,  0.0096, -0.0116, -0.0067],\n",
      "        [-0.0168, -0.0310, -0.0109, -0.0076,  0.0108],\n",
      "        [-0.0262, -0.0147, -0.0072,  0.0121,  0.0043],\n",
      "        [ 0.0031, -0.0428,  0.0181, -0.0108, -0.0041],\n",
      "        [-0.0178, -0.0251, -0.0677,  0.0192, -0.0251],\n",
      "        [-0.0021, -0.0207, -0.0145, -0.0243, -0.0127],\n",
      "        [ 0.0052, -0.0074, -0.0029,  0.0019,  0.0110],\n",
      "        [-0.0128,  0.0163, -0.0382,  0.0290, -0.0015]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0085, grad_fn=<MinBackward1>), tensor(0.8325, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.07916688174009323\n",
      "@sample 1131: tensor([[ 0.0264,  0.0172, -0.0057,  0.0219,  0.0007],\n",
      "        [ 0.0125,  0.0097, -0.0175,  0.0113, -0.0138],\n",
      "        [ 0.0190,  0.0050, -0.0151, -0.0042,  0.0019],\n",
      "        [ 0.0255,  0.0018, -0.0150,  0.0037, -0.0167],\n",
      "        [ 0.0195,  0.0005,  0.0186, -0.0052,  0.0009],\n",
      "        [ 0.0029,  0.0054, -0.0193,  0.0023,  0.0030],\n",
      "        [ 0.0085,  0.0012, -0.0169, -0.0025,  0.0046],\n",
      "        [-0.0180,  0.0126, -0.0087, -0.0319,  0.0009]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 8.8359e-03,  3.3018e-02, -5.3597e-02, -1.7093e-02, -1.0917e-02],\n",
      "        [-2.0141e-02,  6.8410e-03, -3.9442e-03,  6.4404e-03,  1.3238e-02],\n",
      "        [-6.4149e-03, -3.0689e-03, -3.0999e-02,  1.0153e-02, -2.1561e-02],\n",
      "        [-5.5730e-03,  2.6275e-02, -2.5919e-02,  2.9969e-02,  2.5183e-03],\n",
      "        [ 6.9983e-03,  1.0020e-02, -1.2690e-02, -9.7767e-05,  2.4348e-02],\n",
      "        [ 8.0533e-03,  1.0370e-02, -1.3634e-02, -4.0162e-03, -4.5489e-03],\n",
      "        [ 4.8037e-03, -1.3430e-02, -1.4561e-02, -4.1245e-02,  2.0917e-02],\n",
      "        [-2.9942e-02, -1.5364e-02, -1.2317e-03,  1.1662e-02,  3.2912e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0084, grad_fn=<MinBackward1>), tensor(0.8725, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.0824570283293724\n",
      "@sample 1132: tensor([[-0.0144,  0.0001,  0.0006,  0.0031,  0.0009],\n",
      "        [ 0.0175, -0.0077, -0.0092,  0.0196, -0.0094],\n",
      "        [ 0.0237, -0.0090,  0.0134, -0.0007,  0.0016],\n",
      "        [ 0.0041, -0.0164, -0.0011,  0.0367, -0.0093],\n",
      "        [ 0.0187, -0.0031, -0.0048, -0.0057, -0.0026],\n",
      "        [-0.0218,  0.0022, -0.0119,  0.0027,  0.0219],\n",
      "        [-0.0123, -0.0037,  0.0107, -0.0070, -0.0138],\n",
      "        [-0.0135,  0.0144, -0.0215,  0.0042, -0.0041]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 4.2779e-03,  2.0214e-02,  1.2868e-02, -1.3124e-03, -1.2031e-03],\n",
      "        [ 2.8090e-02,  1.6404e-02, -1.9307e-02, -7.4421e-03,  1.8856e-02],\n",
      "        [-1.0777e-02,  1.6606e-02, -1.4838e-02,  4.4638e-03,  9.8949e-03],\n",
      "        [ 4.6249e-02,  1.6126e-02,  2.3720e-02, -3.9950e-02, -2.3678e-03],\n",
      "        [ 2.3079e-03, -8.1308e-05,  1.8267e-02,  5.5368e-04, -7.2200e-03],\n",
      "        [ 4.5463e-02,  8.9850e-03,  2.4493e-02,  1.4218e-02,  1.1441e-02],\n",
      "        [-5.0365e-03,  6.8583e-03, -3.4772e-03,  5.6212e-03,  1.3509e-02],\n",
      "        [ 1.6007e-02,  7.5547e-03, -3.8858e-02,  1.5139e-02,  1.9493e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0126, grad_fn=<MinBackward1>), tensor(0.8488, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08359555155038834\n",
      "@sample 1133: tensor([[-1.0568e-02,  4.3214e-03, -1.0396e-02,  2.9479e-02, -2.8522e-02],\n",
      "        [ 1.4975e-02,  8.8731e-03,  2.2858e-03, -1.1648e-02, -6.8117e-03],\n",
      "        [ 6.9583e-03,  5.9391e-02, -1.1993e-02, -2.9893e-02,  8.8009e-03],\n",
      "        [ 5.9528e-03,  1.5659e-02, -3.2689e-03, -3.4549e-03, -5.3974e-03],\n",
      "        [-2.9768e-02,  2.6791e-02, -6.1069e-03, -1.1927e-02, -1.1044e-02],\n",
      "        [-2.0447e-02, -2.6445e-02,  3.1878e-03,  2.5023e-02, -1.8710e-02],\n",
      "        [-1.5838e-02,  2.3362e-03, -1.3747e-02, -4.1656e-04,  8.7552e-03],\n",
      "        [-8.7282e-03,  1.7286e-02,  1.8158e-02, -1.8466e-02,  5.5753e-05]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0178,  0.0095, -0.0106, -0.0199,  0.0037],\n",
      "        [-0.0032,  0.0086,  0.0116, -0.0254, -0.0109],\n",
      "        [ 0.0078, -0.0045, -0.0527,  0.0281, -0.0085],\n",
      "        [-0.0202, -0.0030,  0.0093, -0.0034,  0.0177],\n",
      "        [ 0.0144, -0.0431, -0.0207, -0.0254,  0.0225],\n",
      "        [ 0.0204, -0.0180,  0.0040, -0.0232,  0.0293],\n",
      "        [-0.0019,  0.0242, -0.0237, -0.0135, -0.0214],\n",
      "        [-0.0131,  0.0231,  0.0157, -0.0074, -0.0207]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0125, grad_fn=<MinBackward1>), tensor(0.9209, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09427972137928009\n",
      "@sample 1134: tensor([[-0.0251, -0.0084,  0.0045,  0.0089, -0.0086],\n",
      "        [ 0.0046, -0.0033, -0.0057,  0.0310, -0.0116],\n",
      "        [-0.0121, -0.0089,  0.0108,  0.0032, -0.0060],\n",
      "        [-0.0055,  0.0148, -0.0011,  0.0231, -0.0147],\n",
      "        [ 0.0038,  0.0197,  0.0032,  0.0265, -0.0005],\n",
      "        [ 0.0293, -0.0097, -0.0134,  0.0049, -0.0115],\n",
      "        [ 0.0030,  0.0128,  0.0087, -0.0091,  0.0058],\n",
      "        [ 0.0035,  0.0085,  0.0038,  0.0370, -0.0355]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-4.6414e-03, -2.0163e-02,  2.6256e-02, -3.1049e-02, -2.1695e-02],\n",
      "        [ 4.8167e-03,  1.8074e-02,  9.5382e-05, -2.1482e-02,  1.6367e-02],\n",
      "        [-1.7335e-02, -2.1582e-02, -6.4140e-03, -1.1068e-03, -2.2360e-02],\n",
      "        [ 1.0265e-02,  3.4589e-02, -6.8780e-03,  1.3583e-02, -1.5532e-02],\n",
      "        [ 1.7209e-02,  5.3289e-02, -1.4839e-02,  1.8945e-02,  7.5256e-03],\n",
      "        [ 9.1121e-04, -2.3189e-03, -2.1185e-02,  1.7464e-02,  1.3418e-02],\n",
      "        [-5.0954e-03,  5.3250e-03,  2.4651e-02, -6.4764e-03, -3.2978e-02],\n",
      "        [ 1.3162e-02,  4.3153e-02, -1.1465e-02,  1.6019e-03,  4.2945e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0143, grad_fn=<MinBackward1>), tensor(0.8929, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10301033407449722\n",
      "@sample 1135: tensor([[ 0.0135, -0.0156,  0.0049,  0.0198, -0.0212],\n",
      "        [-0.0106,  0.0251,  0.0140,  0.0197,  0.0209],\n",
      "        [ 0.0013, -0.0248,  0.0026,  0.0299, -0.0116],\n",
      "        [ 0.0042,  0.0112,  0.0037,  0.0002,  0.0043],\n",
      "        [-0.0163,  0.0107, -0.0083,  0.0242, -0.0142],\n",
      "        [ 0.0043, -0.0092,  0.0132,  0.0006,  0.0091],\n",
      "        [-0.0074,  0.0103, -0.0051, -0.0044, -0.0101],\n",
      "        [ 0.0181,  0.0289, -0.0033, -0.0111, -0.0168]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0098, -0.0098, -0.0025, -0.0085, -0.0048],\n",
      "        [ 0.0451,  0.0378, -0.0245,  0.0345, -0.0131],\n",
      "        [-0.0180,  0.0014,  0.0086, -0.0333,  0.0072],\n",
      "        [ 0.0203, -0.0149, -0.0138,  0.0107,  0.0291],\n",
      "        [ 0.0279,  0.0264, -0.0107,  0.0015,  0.0080],\n",
      "        [-0.0192,  0.0204, -0.0452,  0.0219,  0.0324],\n",
      "        [-0.0189, -0.0093, -0.0142, -0.0055,  0.0187],\n",
      "        [-0.0162, -0.0016, -0.0344, -0.0030,  0.0023]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0119, grad_fn=<MinBackward1>), tensor(0.8188, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08649498224258423\n",
      "@sample 1136: tensor([[ 0.0102, -0.0251,  0.0308, -0.0100,  0.0024],\n",
      "        [ 0.0199, -0.0174,  0.0099,  0.0106, -0.0185],\n",
      "        [-0.0077, -0.0129,  0.0065, -0.0068,  0.0369],\n",
      "        [-0.0121, -0.0051, -0.0110, -0.0016,  0.0063],\n",
      "        [-0.0014,  0.0010, -0.0077,  0.0247,  0.0103],\n",
      "        [ 0.0194, -0.0193,  0.0161, -0.0112,  0.0023],\n",
      "        [-0.0044, -0.0197, -0.0040,  0.0156, -0.0033],\n",
      "        [ 0.0077,  0.0053,  0.0019,  0.0166,  0.0014]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0189,  0.0123,  0.0015, -0.0088, -0.0190],\n",
      "        [-0.0160, -0.0120, -0.0050,  0.0219, -0.0098],\n",
      "        [-0.0367,  0.0078, -0.0367,  0.0035,  0.0064],\n",
      "        [-0.0018,  0.0016,  0.0039,  0.0024, -0.0113],\n",
      "        [ 0.0260,  0.0289, -0.0188, -0.0204, -0.0023],\n",
      "        [-0.0156, -0.0113,  0.0116,  0.0164, -0.0113],\n",
      "        [-0.0183,  0.0180,  0.0107, -0.0017,  0.0300],\n",
      "        [-0.0038,  0.0012, -0.0046, -0.0254, -0.0192]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0092, grad_fn=<MinBackward1>), tensor(0.8420, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09746812283992767\n",
      "@sample 1137: tensor([[-0.0100, -0.0196,  0.0077, -0.0222,  0.0256],\n",
      "        [ 0.0112, -0.0169,  0.0043,  0.0035, -0.0127],\n",
      "        [-0.0111,  0.0127, -0.0110,  0.0067, -0.0133],\n",
      "        [-0.0192,  0.0219,  0.0354, -0.0286,  0.0185],\n",
      "        [-0.0045, -0.0374, -0.0089,  0.0386, -0.0019],\n",
      "        [-0.0113,  0.0013,  0.0127,  0.0149, -0.0209],\n",
      "        [-0.0175,  0.0218,  0.0201, -0.0075,  0.0021],\n",
      "        [-0.0155, -0.0037,  0.0098,  0.0083, -0.0204]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0068, -0.0066,  0.0379, -0.0117, -0.0090],\n",
      "        [ 0.0223,  0.0232, -0.0099,  0.0119,  0.0141],\n",
      "        [ 0.0104, -0.0013,  0.0160, -0.0122, -0.0121],\n",
      "        [-0.0202,  0.0072,  0.0073, -0.0192, -0.0195],\n",
      "        [ 0.0291,  0.0339,  0.0190, -0.0002,  0.0094],\n",
      "        [-0.0111,  0.0221, -0.0016, -0.0240, -0.0137],\n",
      "        [-0.0219,  0.0030,  0.0131,  0.0151, -0.0191],\n",
      "        [ 0.0063, -0.0196,  0.0068, -0.0048, -0.0178]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0118, grad_fn=<MinBackward1>), tensor(0.9007, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09555451571941376\n",
      "@sample 1138: tensor([[ 0.0029, -0.0076, -0.0185,  0.0168,  0.0048],\n",
      "        [-0.0094, -0.0015, -0.0003,  0.0033,  0.0104],\n",
      "        [-0.0089, -0.0038,  0.0035, -0.0123,  0.0039],\n",
      "        [-0.0005, -0.0050,  0.0324, -0.0004, -0.0417],\n",
      "        [-0.0136, -0.0026,  0.0084, -0.0131,  0.0038],\n",
      "        [-0.0127,  0.0477,  0.0445, -0.0561,  0.0580],\n",
      "        [-0.0128, -0.0023,  0.0116,  0.0067, -0.0095],\n",
      "        [ 0.0163,  0.0271,  0.0146, -0.0228, -0.0036]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 2.4690e-02,  2.2084e-02, -1.9947e-02,  3.1539e-02,  9.4687e-03],\n",
      "        [ 9.6472e-03,  1.7535e-02,  2.6384e-02, -1.9983e-02,  7.3074e-03],\n",
      "        [-1.0641e-02, -9.3603e-03, -1.9381e-02,  2.0322e-02, -6.6118e-03],\n",
      "        [ 2.0560e-02, -2.7605e-03, -4.9833e-02,  7.7672e-03,  1.3867e-02],\n",
      "        [-5.0217e-03,  9.7270e-04,  7.4380e-03,  3.6396e-03,  7.5672e-05],\n",
      "        [-3.3337e-02, -1.5556e-02, -9.7046e-02,  2.0690e-02, -3.4965e-02],\n",
      "        [-2.4261e-03,  7.1629e-03,  8.1994e-03, -1.7460e-02, -1.5131e-02],\n",
      "        [-2.2947e-02, -1.1231e-02, -5.4669e-02,  4.2062e-02,  1.3522e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0151, grad_fn=<MinBackward1>), tensor(0.8455, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09423870593309402\n",
      "@sample 1139: tensor([[ 0.0003, -0.0069,  0.0066, -0.0138, -0.0027],\n",
      "        [ 0.0176,  0.0189,  0.0041, -0.0090,  0.0045],\n",
      "        [ 0.0037,  0.0057,  0.0003,  0.0054,  0.0016],\n",
      "        [-0.0134, -0.0143, -0.0081,  0.0146, -0.0137],\n",
      "        [-0.0020, -0.0029,  0.0138, -0.0130, -0.0209],\n",
      "        [-0.0048,  0.0059,  0.0055, -0.0024,  0.0126],\n",
      "        [ 0.0130,  0.0280, -0.0048, -0.0107, -0.0160],\n",
      "        [ 0.0107, -0.0052, -0.0024,  0.0143, -0.0154]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0034, -0.0110,  0.0358, -0.0011, -0.0106],\n",
      "        [-0.0354,  0.0113, -0.0581,  0.0269,  0.0022],\n",
      "        [-0.0117, -0.0018, -0.0161, -0.0033, -0.0060],\n",
      "        [ 0.0132,  0.0085, -0.0022, -0.0201, -0.0154],\n",
      "        [ 0.0272,  0.0096, -0.0189, -0.0053,  0.0040],\n",
      "        [-0.0012,  0.0016, -0.0289,  0.0356, -0.0062],\n",
      "        [-0.0170,  0.0476, -0.0193,  0.0393, -0.0093],\n",
      "        [ 0.0190, -0.0030,  0.0038, -0.0118,  0.0052]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0163, grad_fn=<MinBackward1>), tensor(0.8461, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09862199425697327\n",
      "@sample 1140: tensor([[ 0.0063, -0.0081, -0.0102,  0.0131, -0.0019],\n",
      "        [-0.0133,  0.0013, -0.0140, -0.0319,  0.0249],\n",
      "        [-0.0164,  0.0011,  0.0103, -0.0044,  0.0368],\n",
      "        [-0.0165, -0.0081,  0.0009, -0.0271,  0.0124],\n",
      "        [ 0.0206, -0.0014,  0.0355, -0.0040,  0.0011],\n",
      "        [-0.0178,  0.0056,  0.0021, -0.0065, -0.0058],\n",
      "        [-0.0085,  0.0037, -0.0193, -0.0172,  0.0201],\n",
      "        [-0.0035, -0.0055,  0.0039, -0.0082,  0.0148]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 3.0512e-03,  8.7486e-03,  4.9554e-03, -1.5165e-03, -3.9913e-03],\n",
      "        [ 4.2345e-03, -5.5240e-02,  4.2487e-02, -2.6951e-02, -2.3110e-02],\n",
      "        [-1.0297e-02, -2.0837e-03, -2.0815e-03, -7.6458e-03, -1.2444e-02],\n",
      "        [-1.0783e-02, -1.0140e-02, -4.5508e-05,  1.0245e-02,  1.0308e-02],\n",
      "        [-9.1644e-04,  1.0614e-02, -2.1885e-02, -1.5946e-02, -3.0821e-02],\n",
      "        [ 2.3212e-02, -3.3701e-02,  3.5987e-02, -8.0850e-03, -1.8356e-02],\n",
      "        [-2.0991e-02,  3.7674e-03,  1.5512e-02, -1.2035e-02,  2.2415e-02],\n",
      "        [-1.3265e-02, -7.6196e-03,  1.9731e-02,  1.2927e-02, -1.6121e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0118, grad_fn=<MinBackward1>), tensor(0.8079, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09372974187135696\n",
      "@sample 1141: tensor([[-0.0049, -0.0139,  0.0037,  0.0063, -0.0138],\n",
      "        [ 0.0071, -0.0019,  0.0038, -0.0081,  0.0081],\n",
      "        [ 0.0013, -0.0297, -0.0002, -0.0301,  0.0163],\n",
      "        [-0.0092,  0.0319,  0.0147, -0.0268,  0.0184],\n",
      "        [-0.0141,  0.0010,  0.0005, -0.0042,  0.0168],\n",
      "        [-0.0128, -0.0035,  0.0173, -0.0150,  0.0016],\n",
      "        [ 0.0054, -0.0056,  0.0173, -0.0090,  0.0097],\n",
      "        [-0.0100,  0.0121, -0.0011, -0.0306,  0.0188]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 7.1835e-03, -2.3620e-02, -9.2730e-03,  1.1044e-02, -4.7616e-03],\n",
      "        [-2.6078e-02,  6.0001e-03, -5.7453e-02,  2.2184e-02,  2.5408e-02],\n",
      "        [-4.3014e-02, -2.1973e-02, -2.4543e-02,  4.5300e-05, -4.7625e-03],\n",
      "        [ 6.2532e-03, -3.5065e-02, -3.8964e-02,  1.3285e-02, -5.5174e-03],\n",
      "        [ 2.5581e-03,  1.0423e-02,  2.6845e-02, -9.4471e-03, -8.8962e-03],\n",
      "        [-2.7301e-02, -1.4555e-02, -2.5985e-02,  1.1814e-02,  5.3401e-03],\n",
      "        [-1.1192e-02, -8.8507e-03, -5.2285e-02, -9.2540e-03, -3.4559e-02],\n",
      "        [-2.1092e-02, -6.1900e-03, -2.2955e-02,  1.8252e-02,  2.7955e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0137, grad_fn=<MinBackward1>), tensor(0.8451, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08403630554676056\n",
      "@sample 1142: tensor([[ 0.0005, -0.0207, -0.0165, -0.0126,  0.0121],\n",
      "        [-0.0188, -0.0210,  0.0055, -0.0260,  0.0162],\n",
      "        [-0.0116,  0.0150,  0.0060,  0.0013, -0.0106],\n",
      "        [ 0.0014, -0.0087,  0.0160, -0.0253,  0.0037],\n",
      "        [-0.0019,  0.0006,  0.0103,  0.0090, -0.0030],\n",
      "        [-0.0032,  0.0101, -0.0041, -0.0258,  0.0263],\n",
      "        [-0.0045, -0.0115, -0.0213, -0.0106,  0.0148],\n",
      "        [-0.0180, -0.0171, -0.0088, -0.0108,  0.0036]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0153, -0.0252, -0.0164,  0.0233, -0.0250],\n",
      "        [-0.0110, -0.0191,  0.0012,  0.0037,  0.0254],\n",
      "        [-0.0253, -0.0053, -0.0376, -0.0007,  0.0049],\n",
      "        [-0.0231, -0.0287,  0.0030,  0.0359, -0.0074],\n",
      "        [ 0.0231,  0.0107, -0.0148, -0.0135, -0.0192],\n",
      "        [-0.0058, -0.0328,  0.0161, -0.0059, -0.0286],\n",
      "        [ 0.0118, -0.0167,  0.0168, -0.0024,  0.0088],\n",
      "        [-0.0181, -0.0436,  0.0085, -0.0189,  0.0059]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0122, grad_fn=<MinBackward1>), tensor(0.8400, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09923291206359863\n",
      "@sample 1143: tensor([[ 0.0078, -0.0161,  0.0183,  0.0155, -0.0154],\n",
      "        [-0.0368,  0.0242,  0.0039, -0.0017, -0.0086],\n",
      "        [-0.0064,  0.0054, -0.0101,  0.0163, -0.0335],\n",
      "        [-0.0025, -0.0006, -0.0003, -0.0100, -0.0037],\n",
      "        [ 0.0096,  0.0077, -0.0068,  0.0041,  0.0045],\n",
      "        [ 0.0157,  0.0332,  0.0264, -0.0230,  0.0182],\n",
      "        [ 0.0117,  0.0185, -0.0259, -0.0265, -0.0063],\n",
      "        [-0.0140,  0.0196,  0.0178, -0.0534,  0.0162]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0092,  0.0188, -0.0108, -0.0062,  0.0021],\n",
      "        [ 0.0133, -0.0005, -0.0263, -0.0157,  0.0126],\n",
      "        [-0.0065,  0.0107, -0.0190, -0.0424, -0.0138],\n",
      "        [-0.0126, -0.0330,  0.0173,  0.0079,  0.0019],\n",
      "        [-0.0002,  0.0011, -0.0188, -0.0166,  0.0047],\n",
      "        [-0.0254, -0.0158, -0.0461,  0.0102, -0.0262],\n",
      "        [-0.0338, -0.0287, -0.0429, -0.0014,  0.0024],\n",
      "        [-0.0175, -0.0539, -0.0736,  0.0333,  0.0194]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0141, grad_fn=<MinBackward1>), tensor(0.8518, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09780241549015045\n",
      "@sample 1144: tensor([[ 0.0060, -0.0365, -0.0015,  0.0274, -0.0159],\n",
      "        [ 0.0082, -0.0181, -0.0063,  0.0022,  0.0144],\n",
      "        [-0.0045,  0.0034, -0.0007,  0.0002, -0.0013],\n",
      "        [-0.0045,  0.0121, -0.0036,  0.0098,  0.0037],\n",
      "        [-0.0073,  0.0009,  0.0020, -0.0032,  0.0009],\n",
      "        [ 0.0249, -0.0007,  0.0129,  0.0069,  0.0042],\n",
      "        [ 0.0233, -0.0178,  0.0008,  0.0270, -0.0268],\n",
      "        [-0.0016, -0.0075,  0.0123, -0.0278,  0.0124]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0188,  0.0359, -0.0015,  0.0176,  0.0264],\n",
      "        [ 0.0038,  0.0183, -0.0471,  0.0094,  0.0035],\n",
      "        [ 0.0133,  0.0056,  0.0155,  0.0095, -0.0156],\n",
      "        [ 0.0263,  0.0152, -0.0268, -0.0095,  0.0060],\n",
      "        [ 0.0026,  0.0150, -0.0066, -0.0097,  0.0034],\n",
      "        [ 0.0163,  0.0264,  0.0233,  0.0025,  0.0076],\n",
      "        [ 0.0205,  0.0235, -0.0025,  0.0498,  0.0185],\n",
      "        [-0.0424, -0.0262,  0.0050,  0.0176, -0.0244]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0094, grad_fn=<MinBackward1>), tensor(0.8967, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09266389161348343\n",
      "@sample 1145: tensor([[ 0.0018,  0.0035, -0.0025, -0.0169,  0.0264],\n",
      "        [-0.0026, -0.0213,  0.0085,  0.0124, -0.0014],\n",
      "        [-0.0116,  0.0128, -0.0061,  0.0024, -0.0062],\n",
      "        [ 0.0008, -0.0209, -0.0137, -0.0009,  0.0085],\n",
      "        [-0.0138,  0.0041,  0.0090,  0.0125, -0.0095],\n",
      "        [-0.0110,  0.0283,  0.0151, -0.0315,  0.0404],\n",
      "        [ 0.0173,  0.0068,  0.0252, -0.0156, -0.0157],\n",
      "        [ 0.0004, -0.0082,  0.0112,  0.0022,  0.0182]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0128, -0.0182, -0.0404,  0.0150, -0.0304],\n",
      "        [ 0.0264, -0.0076,  0.0234, -0.0044, -0.0225],\n",
      "        [ 0.0161,  0.0227,  0.0382,  0.0064, -0.0152],\n",
      "        [-0.0028,  0.0125,  0.0197,  0.0024, -0.0007],\n",
      "        [ 0.0094,  0.0128,  0.0219, -0.0268, -0.0067],\n",
      "        [-0.0143, -0.0204, -0.0466,  0.0435, -0.0047],\n",
      "        [-0.0264, -0.0240, -0.0230,  0.0128, -0.0056],\n",
      "        [ 0.0084,  0.0061,  0.0075, -0.0032, -0.0017]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0151, grad_fn=<MinBackward1>), tensor(0.8432, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08064880967140198\n",
      "@sample 1146: tensor([[-0.0080,  0.0152,  0.0079, -0.0376,  0.0121],\n",
      "        [-0.0186,  0.0102,  0.0045,  0.0107, -0.0300],\n",
      "        [-0.0057,  0.0178, -0.0075, -0.0162, -0.0220],\n",
      "        [-0.0114,  0.0292,  0.0028,  0.0023, -0.0006],\n",
      "        [-0.0051,  0.0059, -0.0030, -0.0051, -0.0070],\n",
      "        [ 0.0126,  0.0323,  0.0066, -0.0045, -0.0139],\n",
      "        [-0.0026,  0.0118,  0.0109, -0.0045,  0.0015],\n",
      "        [-0.0056,  0.0090,  0.0058, -0.0050, -0.0051]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0252, -0.0156, -0.0432,  0.0203, -0.0036],\n",
      "        [-0.0136,  0.0049,  0.0247, -0.0201, -0.0055],\n",
      "        [-0.0016, -0.0472,  0.0191, -0.0121, -0.0123],\n",
      "        [-0.0028,  0.0103, -0.0178, -0.0050, -0.0360],\n",
      "        [-0.0292,  0.0091, -0.0204,  0.0153,  0.0160],\n",
      "        [-0.0117,  0.0008, -0.0206,  0.0158, -0.0113],\n",
      "        [ 0.0020,  0.0016, -0.0122, -0.0154,  0.0107],\n",
      "        [-0.0130, -0.0194, -0.0006, -0.0110, -0.0129]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0154, grad_fn=<MinBackward1>), tensor(0.8634, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08877521008253098\n",
      "@sample 1147: tensor([[ 0.0121, -0.0165, -0.0297, -0.0114, -0.0026],\n",
      "        [ 0.0101, -0.0215, -0.0266,  0.0181,  0.0045],\n",
      "        [ 0.0017, -0.0029, -0.0065, -0.0076, -0.0095],\n",
      "        [-0.0123, -0.0379,  0.0085, -0.0008,  0.0047],\n",
      "        [-0.0193,  0.0296, -0.0200,  0.0095, -0.0229],\n",
      "        [ 0.0044,  0.0103,  0.0058,  0.0120, -0.0293],\n",
      "        [-0.0045, -0.0068, -0.0139,  0.0091, -0.0103],\n",
      "        [ 0.0491, -0.0041,  0.0026,  0.0078, -0.0106]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0113, -0.0948, -0.0240,  0.0245, -0.0104],\n",
      "        [ 0.0199, -0.0126, -0.0057,  0.0297,  0.0117],\n",
      "        [-0.0028, -0.0231,  0.0216,  0.0062,  0.0073],\n",
      "        [-0.0108, -0.0014,  0.0095, -0.0108, -0.0259],\n",
      "        [ 0.0086, -0.0073,  0.0221, -0.0243, -0.0069],\n",
      "        [-0.0041, -0.0004,  0.0315, -0.0076, -0.0187],\n",
      "        [ 0.0077,  0.0166,  0.0016,  0.0015,  0.0069],\n",
      "        [-0.0081,  0.0270,  0.0008,  0.0047,  0.0140]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0098, grad_fn=<MinBackward1>), tensor(0.8792, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.11035339534282684\n",
      "@sample 1148: tensor([[-0.0057, -0.0203, -0.0058,  0.0290, -0.0184],\n",
      "        [ 0.0128, -0.0179, -0.0214,  0.0239,  0.0073],\n",
      "        [-0.0093,  0.0142, -0.0136,  0.0044,  0.0022],\n",
      "        [-0.0181,  0.0099, -0.0103,  0.0078, -0.0201],\n",
      "        [-0.0189,  0.0039, -0.0512,  0.0198, -0.0001],\n",
      "        [-0.0046,  0.0361, -0.0049,  0.0008,  0.0017],\n",
      "        [-0.0067,  0.0307, -0.0204,  0.0065,  0.0091],\n",
      "        [ 0.0113, -0.0109,  0.0070, -0.0061,  0.0060]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0231,  0.0078,  0.0503, -0.0208,  0.0058],\n",
      "        [ 0.0201,  0.0189, -0.0077, -0.0157, -0.0139],\n",
      "        [ 0.0054,  0.0191,  0.0105, -0.0271, -0.0263],\n",
      "        [ 0.0198, -0.0074,  0.0265, -0.0154, -0.0092],\n",
      "        [ 0.0234, -0.0089,  0.0140, -0.0117,  0.0030],\n",
      "        [ 0.0528,  0.0234, -0.0039,  0.0101,  0.0033],\n",
      "        [ 0.0241,  0.0261, -0.0287,  0.0034,  0.0088],\n",
      "        [ 0.0083,  0.0081, -0.0142, -0.0137, -0.0066]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0118, grad_fn=<MinBackward1>), tensor(0.8643, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10848142951726913\n",
      "@sample 1149: tensor([[ 0.0022,  0.0259, -0.0083, -0.0230,  0.0034],\n",
      "        [-0.0133, -0.0009, -0.0064,  0.0142,  0.0057],\n",
      "        [-0.0043,  0.0317,  0.0024, -0.0028,  0.0021],\n",
      "        [-0.0052,  0.0158,  0.0141, -0.0082,  0.0265],\n",
      "        [-0.0074, -0.0150,  0.0082, -0.0127,  0.0132],\n",
      "        [ 0.0031, -0.0225, -0.0245,  0.0342, -0.0053],\n",
      "        [ 0.0091, -0.0295, -0.0196,  0.0073, -0.0080],\n",
      "        [ 0.0014,  0.0485,  0.0015, -0.0227, -0.0003]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0028,  0.0049, -0.0448,  0.0075,  0.0145],\n",
      "        [ 0.0292,  0.0205,  0.0421, -0.0260, -0.0183],\n",
      "        [-0.0005,  0.0262, -0.0137, -0.0154, -0.0091],\n",
      "        [ 0.0141,  0.0002,  0.0017,  0.0131,  0.0134],\n",
      "        [-0.0092, -0.0031,  0.0297, -0.0188, -0.0172],\n",
      "        [ 0.0309,  0.0118, -0.0159,  0.0118,  0.0053],\n",
      "        [-0.0140,  0.0050,  0.0126, -0.0231, -0.0146],\n",
      "        [-0.0020, -0.0075, -0.0328,  0.0027,  0.0123]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0124, grad_fn=<MinBackward1>), tensor(0.8794, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08844417333602905\n",
      "@sample 1150: tensor([[-0.0085, -0.0263, -0.0176,  0.0079, -0.0184],\n",
      "        [-0.0074, -0.0166, -0.0080,  0.0117, -0.0009],\n",
      "        [-0.0005, -0.0095,  0.0139,  0.0049,  0.0104],\n",
      "        [ 0.0092, -0.0027,  0.0183,  0.0157, -0.0346],\n",
      "        [-0.0009,  0.0179,  0.0178, -0.0137, -0.0096],\n",
      "        [-0.0149,  0.0233, -0.0220,  0.0185, -0.0284],\n",
      "        [ 0.0152,  0.0001, -0.0073,  0.0059,  0.0067],\n",
      "        [ 0.0207,  0.0043, -0.0129,  0.0111, -0.0174]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0055, -0.0390,  0.0201,  0.0017,  0.0139],\n",
      "        [ 0.0058,  0.0101,  0.0196, -0.0103,  0.0140],\n",
      "        [ 0.0186,  0.0147, -0.0127,  0.0034, -0.0152],\n",
      "        [-0.0056,  0.0386, -0.0197,  0.0089, -0.0010],\n",
      "        [-0.0072,  0.0355, -0.0280,  0.0053, -0.0049],\n",
      "        [-0.0207,  0.0094, -0.0115, -0.0197, -0.0127],\n",
      "        [ 0.0099,  0.0001, -0.0211,  0.0110,  0.0050],\n",
      "        [-0.0036,  0.0266,  0.0048,  0.0153,  0.0344]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0120, grad_fn=<MinBackward1>), tensor(0.7995, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08822132647037506\n",
      "@sample 1151: tensor([[ 0.0066,  0.0170, -0.0153,  0.0042,  0.0194],\n",
      "        [ 0.0195, -0.0144, -0.0332,  0.0220,  0.0101],\n",
      "        [-0.0033,  0.0155, -0.0275, -0.0275,  0.0360],\n",
      "        [-0.0009, -0.0215,  0.0050, -0.0097,  0.0244],\n",
      "        [ 0.0175, -0.0010, -0.0080, -0.0096,  0.0023],\n",
      "        [ 0.0082,  0.0031, -0.0184,  0.0071,  0.0051],\n",
      "        [ 0.0146, -0.0324,  0.0012,  0.0175, -0.0136],\n",
      "        [ 0.0127,  0.0057, -0.0028,  0.0003, -0.0133]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 2.7935e-03,  1.5759e-02, -8.8253e-03, -7.0932e-03,  4.7685e-03],\n",
      "        [ 9.4256e-03,  2.7249e-03,  5.8750e-03,  2.8464e-02,  2.0625e-02],\n",
      "        [-1.7862e-02, -1.4542e-02, -2.0144e-02,  4.2839e-04,  4.0157e-03],\n",
      "        [-1.5576e-03, -2.1465e-02,  5.1441e-02, -4.9638e-02, -3.2873e-02],\n",
      "        [-1.7030e-02, -1.5230e-02,  1.9966e-02, -2.5352e-03,  1.1391e-04],\n",
      "        [ 4.5800e-03,  6.3838e-03, -2.4019e-02,  1.7068e-02,  1.6415e-02],\n",
      "        [ 6.6906e-05,  9.2553e-03,  1.0897e-02,  1.7976e-02,  1.3588e-02],\n",
      "        [ 2.7984e-03,  3.8550e-03, -1.2032e-02, -6.0095e-03,  4.9211e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0113, grad_fn=<MinBackward1>), tensor(0.8430, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09994275122880936\n",
      "@sample 1152: tensor([[-0.0135, -0.0106, -0.0093,  0.0038,  0.0014],\n",
      "        [ 0.0055,  0.0244, -0.0219,  0.0116, -0.0154],\n",
      "        [ 0.0035,  0.0279,  0.0111,  0.0223, -0.0170],\n",
      "        [ 0.0088,  0.0223, -0.0097,  0.0013, -0.0100],\n",
      "        [-0.0145, -0.0171, -0.0025,  0.0019,  0.0021],\n",
      "        [-0.0103,  0.0011, -0.0023, -0.0063, -0.0014],\n",
      "        [ 0.0064, -0.0046, -0.0157,  0.0184,  0.0075],\n",
      "        [ 0.0104,  0.0197, -0.0115,  0.0063, -0.0294]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0119, -0.0057,  0.0405, -0.0403, -0.0171],\n",
      "        [ 0.0239,  0.0283,  0.0220, -0.0127,  0.0134],\n",
      "        [ 0.0253,  0.0253,  0.0268, -0.0243, -0.0042],\n",
      "        [ 0.0138, -0.0128, -0.0203, -0.0032,  0.0127],\n",
      "        [-0.0150,  0.0088,  0.0401, -0.0326, -0.0111],\n",
      "        [ 0.0076, -0.0022,  0.0232, -0.0156, -0.0125],\n",
      "        [ 0.0298,  0.0066,  0.0120, -0.0010, -0.0106],\n",
      "        [ 0.0120,  0.0206, -0.0150,  0.0063,  0.0150]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0123, grad_fn=<MinBackward1>), tensor(0.8622, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.07892066985368729\n",
      "@sample 1153: tensor([[ 0.0091,  0.0068, -0.0172,  0.0075,  0.0073],\n",
      "        [-0.0039,  0.0201,  0.0193, -0.0241,  0.0021],\n",
      "        [ 0.0148, -0.0098,  0.0163, -0.0006, -0.0053],\n",
      "        [ 0.0180, -0.0184, -0.0008,  0.0092, -0.0214],\n",
      "        [-0.0265,  0.0037,  0.0314,  0.0058, -0.0190],\n",
      "        [ 0.0112,  0.0096,  0.0011,  0.0035, -0.0336],\n",
      "        [ 0.0021, -0.0009, -0.0176, -0.0185, -0.0056],\n",
      "        [-0.0024, -0.0028,  0.0165, -0.0106, -0.0031]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0324,  0.0133,  0.0412, -0.0036, -0.0429],\n",
      "        [-0.0336, -0.0010, -0.0307,  0.0110, -0.0090],\n",
      "        [-0.0139, -0.0014, -0.0004,  0.0020,  0.0094],\n",
      "        [-0.0163, -0.0069,  0.0018,  0.0234,  0.0092],\n",
      "        [-0.0037,  0.0050,  0.0118, -0.0086, -0.0144],\n",
      "        [ 0.0006,  0.0097,  0.0118, -0.0010,  0.0021],\n",
      "        [-0.0276, -0.0117, -0.0174,  0.0013, -0.0077],\n",
      "        [-0.0224, -0.0098,  0.0077,  0.0132,  0.0049]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0114, grad_fn=<MinBackward1>), tensor(0.8154, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09860184788703918\n",
      "@sample 1154: tensor([[-0.0063,  0.0089, -0.0245,  0.0128,  0.0039],\n",
      "        [ 0.0017, -0.0041,  0.0029,  0.0079, -0.0005],\n",
      "        [ 0.0010, -0.0092,  0.0050,  0.0018,  0.0028],\n",
      "        [ 0.0219, -0.0112, -0.0031, -0.0019,  0.0012],\n",
      "        [-0.0190,  0.0160,  0.0099,  0.0022,  0.0121],\n",
      "        [ 0.0051, -0.0115, -0.0035, -0.0066, -0.0045],\n",
      "        [ 0.0208,  0.0163,  0.0048, -0.0006,  0.0042],\n",
      "        [-0.0031, -0.0110,  0.0067,  0.0188,  0.0016]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0003,  0.0330,  0.0034,  0.0036,  0.0184],\n",
      "        [ 0.0026, -0.0050,  0.0310,  0.0284,  0.0033],\n",
      "        [ 0.0142,  0.0033,  0.0113,  0.0094, -0.0036],\n",
      "        [-0.0183, -0.0094, -0.0243,  0.0363,  0.0127],\n",
      "        [-0.0479,  0.0036, -0.0572,  0.0529,  0.0112],\n",
      "        [-0.0093, -0.0129,  0.0211, -0.0171,  0.0102],\n",
      "        [-0.0115,  0.0119, -0.0234,  0.0231, -0.0032],\n",
      "        [ 0.0162,  0.0061,  0.0029, -0.0217, -0.0174]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0120, grad_fn=<MinBackward1>), tensor(0.8378, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08521530032157898\n",
      "@sample 1155: tensor([[ 0.0311, -0.0021, -0.0071, -0.0002,  0.0164],\n",
      "        [-0.0115, -0.0085,  0.0023, -0.0119,  0.0150],\n",
      "        [ 0.0013, -0.0110, -0.0061,  0.0009, -0.0053],\n",
      "        [ 0.0122, -0.0146, -0.0158,  0.0075,  0.0087],\n",
      "        [-0.0137,  0.0175,  0.0471, -0.0285, -0.0040],\n",
      "        [ 0.0182, -0.0184,  0.0132,  0.0215, -0.0006],\n",
      "        [ 0.0108, -0.0034,  0.0090, -0.0140, -0.0089],\n",
      "        [-0.0192,  0.0111, -0.0143,  0.0032, -0.0177]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0094, -0.0064, -0.0033, -0.0123, -0.0027],\n",
      "        [ 0.0078, -0.0345, -0.0081, -0.0051, -0.0101],\n",
      "        [-0.0050,  0.0081,  0.0139, -0.0089,  0.0153],\n",
      "        [-0.0012,  0.0113,  0.0005,  0.0061,  0.0023],\n",
      "        [-0.0268, -0.0080, -0.0120,  0.0257, -0.0033],\n",
      "        [ 0.0437,  0.0116,  0.0443, -0.0222, -0.0485],\n",
      "        [-0.0293, -0.0149,  0.0326,  0.0030, -0.0133],\n",
      "        [ 0.0182,  0.0347, -0.0089, -0.0016,  0.0074]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0099, grad_fn=<MinBackward1>), tensor(0.8271, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09048690646886826\n",
      "@sample 1156: tensor([[-9.1347e-04,  2.3523e-02,  3.7426e-03, -1.8638e-02,  9.9817e-03],\n",
      "        [ 3.2466e-02,  1.1602e-03,  9.3774e-03,  4.9476e-04, -2.2872e-03],\n",
      "        [-2.7011e-02, -4.6341e-03,  2.0127e-02, -1.7254e-02,  1.5847e-02],\n",
      "        [ 1.3417e-02, -1.4253e-02,  1.0180e-02, -5.1445e-03, -2.9186e-03],\n",
      "        [ 8.6087e-03, -3.0187e-02, -5.5652e-03,  1.2436e-02,  7.0616e-03],\n",
      "        [-2.6034e-05, -3.5399e-03, -2.8805e-03,  7.6824e-03, -1.1276e-03],\n",
      "        [-6.2974e-03,  3.5837e-03,  1.4817e-02,  1.9048e-04,  2.4052e-03],\n",
      "        [ 1.0589e-03, -2.7344e-03, -2.4690e-03, -1.7053e-02, -2.0698e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0206,  0.0062, -0.0436,  0.0194, -0.0013],\n",
      "        [ 0.0062,  0.0077, -0.0571,  0.0218,  0.0273],\n",
      "        [-0.0169, -0.0042,  0.0058,  0.0021, -0.0025],\n",
      "        [-0.0038, -0.0164,  0.0052, -0.0097, -0.0058],\n",
      "        [-0.0115,  0.0077,  0.0020, -0.0179, -0.0188],\n",
      "        [-0.0023,  0.0006, -0.0085,  0.0016,  0.0340],\n",
      "        [-0.0002, -0.0170,  0.0281, -0.0104, -0.0149],\n",
      "        [-0.0227,  0.0104,  0.0255, -0.0086, -0.0055]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0155, grad_fn=<MinBackward1>), tensor(0.8300, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08690782636404037\n",
      "@sample 1157: tensor([[-0.0195, -0.0086, -0.0087, -0.0053,  0.0136],\n",
      "        [ 0.0051,  0.0030,  0.0202, -0.0003, -0.0109],\n",
      "        [ 0.0360, -0.0014, -0.0333,  0.0249, -0.0001],\n",
      "        [ 0.0063,  0.0011,  0.0012, -0.0078,  0.0071],\n",
      "        [-0.0059, -0.0225,  0.0041,  0.0073, -0.0171],\n",
      "        [ 0.0026,  0.0069,  0.0026, -0.0011,  0.0123],\n",
      "        [-0.0054,  0.0131,  0.0137, -0.0224,  0.0007],\n",
      "        [-0.0091, -0.0021, -0.0034, -0.0055,  0.0114]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0290,  0.0145,  0.0161, -0.0087,  0.0239],\n",
      "        [-0.0033,  0.0120,  0.0133, -0.0084, -0.0171],\n",
      "        [ 0.0202,  0.0329, -0.0112, -0.0144,  0.0133],\n",
      "        [-0.0261, -0.0072, -0.0138,  0.0045, -0.0079],\n",
      "        [-0.0367,  0.0077,  0.0021, -0.0123, -0.0024],\n",
      "        [ 0.0187, -0.0046,  0.0019,  0.0063,  0.0070],\n",
      "        [-0.0253,  0.0096,  0.0066, -0.0145, -0.0220],\n",
      "        [-0.0029, -0.0226,  0.0102,  0.0115,  0.0216]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0138, grad_fn=<MinBackward1>), tensor(0.8512, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08177966624498367\n",
      "@sample 1158: tensor([[ 0.0174,  0.0340, -0.0311, -0.0401, -0.0023],\n",
      "        [-0.0217, -0.0081,  0.0209, -0.0234,  0.0467],\n",
      "        [ 0.0090,  0.0128, -0.0021, -0.0210,  0.0016],\n",
      "        [ 0.0020, -0.0097,  0.0201, -0.0085,  0.0346],\n",
      "        [-0.0025, -0.0084,  0.0065, -0.0025, -0.0077],\n",
      "        [-0.0339,  0.0072,  0.0257, -0.0701,  0.0673],\n",
      "        [ 0.0084,  0.0138,  0.0054, -0.0277,  0.0069],\n",
      "        [ 0.0092,  0.0096, -0.0180, -0.0039,  0.0123]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0253, -0.0367, -0.0452,  0.0584,  0.0004],\n",
      "        [ 0.0268,  0.0235, -0.0192,  0.0239, -0.0039],\n",
      "        [-0.0347, -0.0136, -0.0547,  0.0109,  0.0280],\n",
      "        [-0.0195, -0.0434, -0.0186,  0.0086, -0.0423],\n",
      "        [-0.0051, -0.0137,  0.0249, -0.0053, -0.0024],\n",
      "        [-0.0304, -0.1059,  0.0058, -0.0100, -0.0200],\n",
      "        [-0.0314, -0.0196, -0.0305,  0.0113, -0.0087],\n",
      "        [ 0.0137, -0.0100, -0.0069,  0.0022, -0.0049]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0133, grad_fn=<MinBackward1>), tensor(0.8501, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10859418660402298\n",
      "@sample 1159: tensor([[ 0.0059, -0.0045, -0.0105,  0.0101, -0.0114],\n",
      "        [-0.0096,  0.0091,  0.0152, -0.0281,  0.0023],\n",
      "        [ 0.0010, -0.0233, -0.0067,  0.0012,  0.0099],\n",
      "        [-0.0062, -0.0196,  0.0056, -0.0122,  0.0236],\n",
      "        [-0.0172,  0.0067, -0.0008, -0.0008,  0.0096],\n",
      "        [ 0.0137, -0.0165,  0.0124, -0.0178,  0.0228],\n",
      "        [-0.0056, -0.0223,  0.0195, -0.0039, -0.0001],\n",
      "        [-0.0050, -0.0088,  0.0064, -0.0277,  0.0041]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0078, -0.0056,  0.0029, -0.0072,  0.0063],\n",
      "        [-0.0164,  0.0272, -0.0250,  0.0417,  0.0226],\n",
      "        [-0.0034, -0.0012,  0.0017, -0.0080, -0.0098],\n",
      "        [-0.0248,  0.0048, -0.0058,  0.0130, -0.0274],\n",
      "        [ 0.0166,  0.0488,  0.0089, -0.0137,  0.0352],\n",
      "        [-0.0165,  0.0107, -0.0088, -0.0090, -0.0100],\n",
      "        [ 0.0155,  0.0235, -0.0015, -0.0089,  0.0094],\n",
      "        [-0.0222, -0.0094, -0.0033, -0.0051,  0.0190]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0106, grad_fn=<MinBackward1>), tensor(0.8301, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.0934869647026062\n",
      "@sample 1160: tensor([[-0.0015,  0.0023, -0.0057,  0.0027, -0.0055],\n",
      "        [-0.0190, -0.0111, -0.0072,  0.0110, -0.0100],\n",
      "        [ 0.0041, -0.0025,  0.0132, -0.0091,  0.0103],\n",
      "        [-0.0035, -0.0258, -0.0253,  0.0059, -0.0045],\n",
      "        [ 0.0061,  0.0149,  0.0187, -0.0371,  0.0299],\n",
      "        [ 0.0080, -0.0108,  0.0280, -0.0361,  0.0150],\n",
      "        [-0.0022, -0.0233, -0.0095, -0.0023,  0.0126],\n",
      "        [-0.0362, -0.0033,  0.0002, -0.0208,  0.0059]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 1.8016e-03, -1.4459e-03, -6.1274e-03, -4.7828e-03,  2.0743e-02],\n",
      "        [ 1.4198e-02, -1.9864e-02,  7.1032e-04, -1.8824e-02, -2.2486e-02],\n",
      "        [-9.2622e-03, -9.4955e-03,  1.2621e-05, -4.5285e-03,  2.0317e-03],\n",
      "        [ 1.4417e-02, -2.4938e-02,  2.9673e-02,  1.2843e-04,  2.6278e-02],\n",
      "        [-1.7594e-02, -1.1800e-02, -1.9144e-02, -7.5631e-03,  1.1201e-03],\n",
      "        [ 1.8043e-02,  4.8647e-02,  1.5533e-02,  2.5920e-02,  3.2501e-02],\n",
      "        [ 2.8300e-02, -3.0073e-03,  5.4236e-02,  1.0244e-02,  6.0859e-03],\n",
      "        [ 9.1453e-03, -2.7241e-02, -1.3742e-02, -1.6050e-02,  1.6807e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0140, grad_fn=<MinBackward1>), tensor(0.8426, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09329156577587128\n",
      "@sample 1161: tensor([[ 0.0037, -0.0258, -0.0190, -0.0016,  0.0256],\n",
      "        [ 0.0010, -0.0033,  0.0114,  0.0049,  0.0021],\n",
      "        [ 0.0137,  0.0189,  0.0132, -0.0255,  0.0148],\n",
      "        [-0.0058, -0.0054,  0.0177, -0.0083,  0.0121],\n",
      "        [ 0.0015,  0.0082,  0.0067, -0.0099,  0.0104],\n",
      "        [-0.0030, -0.0104,  0.0015,  0.0118,  0.0184],\n",
      "        [ 0.0124, -0.0110,  0.0084, -0.0091,  0.0099],\n",
      "        [-0.0063, -0.0044,  0.0140,  0.0240, -0.0235]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0236, -0.0288,  0.0178,  0.0015, -0.0297],\n",
      "        [ 0.0112, -0.0161,  0.0058,  0.0166, -0.0265],\n",
      "        [-0.0150, -0.0043, -0.0163,  0.0317, -0.0088],\n",
      "        [ 0.0108,  0.0193,  0.0071, -0.0137, -0.0142],\n",
      "        [ 0.0346,  0.0233, -0.0033, -0.0057,  0.0009],\n",
      "        [ 0.0095,  0.0432,  0.0371,  0.0238,  0.0131],\n",
      "        [ 0.0222,  0.0089, -0.0178, -0.0078, -0.0091],\n",
      "        [ 0.0144,  0.0088, -0.0311,  0.0068,  0.0201]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0200, grad_fn=<MinBackward1>), tensor(0.8237, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09521805495023727\n",
      "@sample 1162: tensor([[-0.0037, -0.0106,  0.0076,  0.0085, -0.0054],\n",
      "        [ 0.0023,  0.0151, -0.0066, -0.0059, -0.0010],\n",
      "        [ 0.0049, -0.0185, -0.0058,  0.0260, -0.0128],\n",
      "        [-0.0018, -0.0098,  0.0297, -0.0015,  0.0008],\n",
      "        [-0.0069, -0.0021,  0.0152,  0.0130, -0.0096],\n",
      "        [ 0.0246, -0.0061, -0.0107,  0.0125,  0.0335],\n",
      "        [-0.0127, -0.0082, -0.0035, -0.0039,  0.0383],\n",
      "        [ 0.0014, -0.0017, -0.0093,  0.0046, -0.0224]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 2.0124e-02, -1.0569e-02,  1.2510e-02, -1.4111e-03,  2.8055e-03],\n",
      "        [-1.8474e-02, -1.3891e-03, -2.2748e-02,  7.1421e-05, -3.8278e-03],\n",
      "        [ 1.5128e-02,  5.4529e-03,  4.0357e-03, -5.3759e-03,  1.4717e-02],\n",
      "        [ 9.7888e-03,  3.3105e-02, -6.7881e-03,  1.6602e-02,  7.2438e-04],\n",
      "        [ 2.0077e-02, -1.0303e-02,  1.7341e-02,  7.2549e-04,  1.8700e-02],\n",
      "        [ 2.6243e-02,  3.3994e-02, -4.5244e-02,  2.7941e-02,  3.2011e-02],\n",
      "        [ 6.9635e-03,  4.9107e-03,  7.9198e-03,  3.4611e-02,  2.9752e-03],\n",
      "        [ 1.9319e-03,  2.1546e-02,  3.3469e-02,  3.9772e-03,  7.2862e-04]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0098, grad_fn=<MinBackward1>), tensor(0.7732, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08453762531280518\n",
      "@sample 1163: tensor([[ 0.0024, -0.0198, -0.0465,  0.0210,  0.0173],\n",
      "        [-0.0102,  0.0017, -0.0127,  0.0128,  0.0075],\n",
      "        [ 0.0048,  0.0106,  0.0007,  0.0160,  0.0034],\n",
      "        [-0.0018,  0.0229,  0.0117, -0.0078,  0.0361],\n",
      "        [-0.0087, -0.0096,  0.0007,  0.0108, -0.0170],\n",
      "        [-0.0264, -0.0336,  0.0272, -0.0152,  0.0115],\n",
      "        [ 0.0232,  0.0175, -0.0033,  0.0085, -0.0363],\n",
      "        [-0.0024, -0.0306,  0.0049,  0.0313, -0.0199]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0349,  0.0245, -0.0103, -0.0217, -0.0018],\n",
      "        [-0.0042, -0.0192, -0.0111, -0.0026,  0.0124],\n",
      "        [ 0.0152,  0.0144, -0.0282,  0.0060, -0.0119],\n",
      "        [ 0.0085,  0.0259,  0.0051,  0.0063, -0.0033],\n",
      "        [-0.0121, -0.0095, -0.0034, -0.0142,  0.0016],\n",
      "        [-0.0066, -0.0378,  0.0134, -0.0062, -0.0087],\n",
      "        [ 0.0009,  0.0245,  0.0370, -0.0168, -0.0058],\n",
      "        [ 0.0121, -0.0062,  0.0075, -0.0249,  0.0237]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0094, grad_fn=<MinBackward1>), tensor(0.8395, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.0998544916510582\n",
      "@sample 1164: tensor([[-0.0130,  0.0455, -0.0043, -0.0145,  0.0022],\n",
      "        [-0.0058,  0.0155,  0.0046,  0.0113,  0.0097],\n",
      "        [-0.0207,  0.0243,  0.0082,  0.0130,  0.0024],\n",
      "        [-0.0055, -0.0078,  0.0019,  0.0318,  0.0096],\n",
      "        [-0.0043, -0.0020,  0.0326, -0.0005, -0.0040],\n",
      "        [-0.0150, -0.0097, -0.0196,  0.0038,  0.0067],\n",
      "        [ 0.0062, -0.0217, -0.0085,  0.0179, -0.0045],\n",
      "        [ 0.0160, -0.0053,  0.0067,  0.0271,  0.0050]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0093,  0.0363,  0.0230,  0.0273,  0.0181],\n",
      "        [ 0.0006,  0.0209, -0.0017,  0.0112,  0.0083],\n",
      "        [-0.0031,  0.0313, -0.0120, -0.0065,  0.0108],\n",
      "        [ 0.0366,  0.0297, -0.0063,  0.0164,  0.0021],\n",
      "        [ 0.0060,  0.0277, -0.0020, -0.0140,  0.0025],\n",
      "        [-0.0115, -0.0339,  0.0041,  0.0005,  0.0220],\n",
      "        [ 0.0088, -0.0097,  0.0309, -0.0108,  0.0190],\n",
      "        [ 0.0043,  0.0101, -0.0186,  0.0146,  0.0114]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0106, grad_fn=<MinBackward1>), tensor(0.8875, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.0964161679148674\n",
      "@sample 1165: tensor([[ 2.2446e-03, -2.1296e-03,  1.2148e-02,  6.1589e-03, -1.0760e-02],\n",
      "        [ 1.7115e-02, -7.7144e-03, -6.8557e-03,  3.1305e-03,  1.6507e-03],\n",
      "        [-2.0802e-02,  7.7554e-03,  6.1364e-03, -1.6387e-02, -1.7479e-03],\n",
      "        [ 2.4209e-02,  2.0514e-02, -1.1673e-02, -7.0182e-03,  1.0980e-03],\n",
      "        [ 1.2840e-02, -1.3919e-02,  2.6680e-02,  1.1180e-02, -2.4034e-02],\n",
      "        [ 2.4552e-02,  4.2975e-03,  1.4443e-03,  1.8910e-02, -7.5176e-03],\n",
      "        [ 3.9107e-03,  1.1934e-02,  7.7151e-03,  9.3174e-03, -2.1398e-02],\n",
      "        [ 3.1814e-03,  2.7865e-06,  7.4582e-03,  4.8482e-03,  5.2271e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-3.2105e-05, -4.5464e-03,  1.2032e-02, -3.6544e-04, -7.7287e-03],\n",
      "        [-2.4500e-02, -2.7025e-02, -1.8960e-02,  9.9494e-04,  7.7840e-05],\n",
      "        [ 4.3536e-03,  1.5727e-02,  3.0955e-02, -2.3118e-03, -1.1440e-02],\n",
      "        [-2.9360e-02, -8.6287e-03, -2.5990e-02, -1.4152e-03, -1.3122e-03],\n",
      "        [ 8.4754e-03,  1.6134e-02,  3.7110e-04,  2.5655e-02,  6.9588e-03],\n",
      "        [ 1.5311e-02,  3.9189e-03,  4.0603e-02, -1.1039e-02, -3.1055e-02],\n",
      "        [-1.2921e-02,  1.7122e-02, -3.3470e-02, -1.5310e-02, -6.0800e-03],\n",
      "        [-4.2103e-03, -1.6450e-02, -4.5014e-03,  5.4517e-03, -1.1552e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0151, grad_fn=<MinBackward1>), tensor(0.8962, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09119552373886108\n",
      "@sample 1166: tensor([[ 0.0224,  0.0223, -0.0125,  0.0020, -0.0422],\n",
      "        [ 0.0018,  0.0123,  0.0202,  0.0006, -0.0007],\n",
      "        [ 0.0027, -0.0146,  0.0200, -0.0030,  0.0125],\n",
      "        [-0.0214, -0.0204, -0.0022, -0.0126,  0.0071],\n",
      "        [-0.0077, -0.0025, -0.0069,  0.0214, -0.0199],\n",
      "        [ 0.0084, -0.0085, -0.0039,  0.0171, -0.0116],\n",
      "        [ 0.0081, -0.0098, -0.0108,  0.0299, -0.0134],\n",
      "        [ 0.0332,  0.0118,  0.0011,  0.0250, -0.0076]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0069,  0.0071, -0.0089,  0.0011,  0.0310],\n",
      "        [ 0.0133,  0.0302, -0.0016, -0.0115, -0.0256],\n",
      "        [-0.0230, -0.0173,  0.0178,  0.0326,  0.0023],\n",
      "        [-0.0256,  0.0030, -0.0031,  0.0061,  0.0147],\n",
      "        [ 0.0093, -0.0141,  0.0091, -0.0163,  0.0246],\n",
      "        [ 0.0196,  0.0290, -0.0056,  0.0211,  0.0424],\n",
      "        [-0.0223,  0.0048, -0.0374, -0.0011,  0.0271],\n",
      "        [ 0.0017,  0.0452, -0.0083,  0.0196, -0.0055]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0154, grad_fn=<MinBackward1>), tensor(0.8724, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09834660589694977\n",
      "@sample 1167: tensor([[ 0.0129,  0.0083,  0.0023,  0.0104, -0.0112],\n",
      "        [ 0.0132,  0.0541,  0.0245, -0.0437,  0.0039],\n",
      "        [-0.0042,  0.0028,  0.0001,  0.0172, -0.0151],\n",
      "        [-0.0111, -0.0045,  0.0037,  0.0007, -0.0034],\n",
      "        [-0.0029, -0.0025, -0.0010,  0.0095, -0.0253],\n",
      "        [ 0.0275,  0.0136, -0.0120, -0.0104,  0.0163],\n",
      "        [ 0.0051,  0.0054,  0.0205,  0.0048, -0.0138],\n",
      "        [ 0.0037, -0.0166, -0.0146, -0.0014, -0.0139]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0030,  0.0145,  0.0109, -0.0099,  0.0075],\n",
      "        [-0.0132, -0.0068, -0.0558,  0.0057,  0.0137],\n",
      "        [-0.0250,  0.0071,  0.0129,  0.0268,  0.0146],\n",
      "        [-0.0157,  0.0013,  0.0064, -0.0004,  0.0083],\n",
      "        [-0.0209,  0.0263, -0.0126,  0.0144,  0.0354],\n",
      "        [-0.0129,  0.0172, -0.0166,  0.0082, -0.0052],\n",
      "        [-0.0127,  0.0074, -0.0011,  0.0054,  0.0218],\n",
      "        [-0.0086, -0.0498, -0.0233,  0.0029,  0.0370]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0131, grad_fn=<MinBackward1>), tensor(0.8677, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08574111759662628\n",
      "@sample 1168: tensor([[ 0.0464,  0.0037, -0.0100,  0.0379, -0.0100],\n",
      "        [-0.0090, -0.0064,  0.0083, -0.0345,  0.0163],\n",
      "        [ 0.0137, -0.0027, -0.0008, -0.0115,  0.0021],\n",
      "        [-0.0055, -0.0123, -0.0011, -0.0060,  0.0258],\n",
      "        [-0.0084,  0.0158, -0.0317,  0.0323, -0.0305],\n",
      "        [-0.0227, -0.0073,  0.0088, -0.0153,  0.0230],\n",
      "        [-0.0135,  0.0236,  0.0070, -0.0150, -0.0007],\n",
      "        [ 0.0100, -0.0170, -0.0232,  0.0183,  0.0112]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0226,  0.0293,  0.0003, -0.0416,  0.0001],\n",
      "        [-0.0074, -0.0218,  0.0108, -0.0108,  0.0054],\n",
      "        [-0.0132, -0.0114, -0.0137,  0.0134, -0.0046],\n",
      "        [ 0.0134, -0.0071, -0.0086, -0.0352, -0.0064],\n",
      "        [ 0.0045, -0.0293, -0.0224,  0.0086, -0.0380],\n",
      "        [ 0.0185,  0.0013, -0.0102,  0.0049,  0.0051],\n",
      "        [-0.0173,  0.0149,  0.0150,  0.0355,  0.0443],\n",
      "        [ 0.0343,  0.0030,  0.0008,  0.0172,  0.0198]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0089, grad_fn=<MinBackward1>), tensor(0.8525, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10146356374025345\n",
      "@sample 1169: tensor([[ 0.0150,  0.0260,  0.0006, -0.0213, -0.0009],\n",
      "        [ 0.0012, -0.0032, -0.0375, -0.0122, -0.0004],\n",
      "        [ 0.0021, -0.0048,  0.0094, -0.0326,  0.0133],\n",
      "        [ 0.0108,  0.0176,  0.0081, -0.0012,  0.0058],\n",
      "        [-0.0251,  0.0219,  0.0009, -0.0092,  0.0005],\n",
      "        [-0.0130, -0.0095, -0.0224, -0.0046,  0.0040],\n",
      "        [-0.0020, -0.0007,  0.0089,  0.0167, -0.0325],\n",
      "        [-0.0066,  0.0039,  0.0265,  0.0056,  0.0017]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-1.2284e-03, -2.5022e-02, -3.0571e-02,  3.6563e-02,  3.7230e-03],\n",
      "        [-8.3347e-03, -1.4102e-02, -7.3977e-03, -9.9853e-05,  8.2116e-03],\n",
      "        [-1.9167e-02, -1.9421e-02,  8.3158e-03,  3.2209e-03, -3.2357e-03],\n",
      "        [-1.8401e-03,  6.3408e-03, -1.7451e-02,  5.0098e-05,  9.4436e-03],\n",
      "        [ 1.1056e-03, -2.8011e-02,  4.1339e-02, -2.7159e-02, -2.3226e-02],\n",
      "        [-8.5850e-04, -3.5770e-02,  2.1595e-02,  1.5998e-02, -6.2518e-03],\n",
      "        [-3.2738e-02, -1.2674e-02, -2.1336e-02, -2.1932e-02, -1.9125e-02],\n",
      "        [-7.7160e-04,  2.1230e-02,  1.3934e-02,  1.0325e-02, -1.2530e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0127, grad_fn=<MinBackward1>), tensor(0.8240, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10203301906585693\n",
      "@sample 1170: tensor([[ 6.3035e-03,  4.9043e-03, -2.1181e-02,  1.3470e-02, -6.1589e-03],\n",
      "        [ 8.5025e-04,  1.6898e-02,  5.6806e-03,  2.0117e-02, -1.8357e-02],\n",
      "        [ 1.5807e-02, -6.6632e-04,  1.7358e-02, -2.1221e-02,  2.0246e-02],\n",
      "        [-4.7221e-03, -7.6342e-03, -9.1886e-03,  2.1782e-02, -1.3772e-03],\n",
      "        [ 6.6857e-03, -4.7326e-05, -2.1963e-02,  6.9222e-03, -2.6512e-02],\n",
      "        [ 4.5391e-03,  2.9766e-02,  1.1154e-02, -8.0501e-03, -1.1332e-02],\n",
      "        [-8.4229e-04,  2.0114e-02, -1.8924e-02,  1.5274e-02, -3.9481e-03],\n",
      "        [-1.7069e-02,  1.3078e-02, -9.1912e-04,  7.0795e-03,  5.1725e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0010, -0.0021, -0.0079, -0.0102, -0.0011],\n",
      "        [ 0.0114,  0.0116, -0.0117, -0.0402, -0.0207],\n",
      "        [-0.0034, -0.0099, -0.0383, -0.0099, -0.0126],\n",
      "        [ 0.0083,  0.0068,  0.0068,  0.0013,  0.0157],\n",
      "        [-0.0070,  0.0065, -0.0071, -0.0035,  0.0139],\n",
      "        [-0.0095,  0.0135,  0.0175, -0.0208, -0.0219],\n",
      "        [ 0.0175,  0.0212,  0.0083, -0.0229,  0.0128],\n",
      "        [-0.0051, -0.0185,  0.0087, -0.0075, -0.0005]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0108, grad_fn=<MinBackward1>), tensor(0.8260, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10177040845155716\n",
      "@sample 1171: tensor([[-0.0049,  0.0047,  0.0187, -0.0264,  0.0087],\n",
      "        [ 0.0133,  0.0056, -0.0120,  0.0186, -0.0097],\n",
      "        [-0.0144, -0.0093, -0.0100, -0.0040,  0.0047],\n",
      "        [ 0.0012, -0.0006, -0.0110,  0.0163,  0.0076],\n",
      "        [-0.0058, -0.0112, -0.0163, -0.0085, -0.0031],\n",
      "        [ 0.0007,  0.0111, -0.0154,  0.0088, -0.0192],\n",
      "        [ 0.0129, -0.0137, -0.0211,  0.0091, -0.0139],\n",
      "        [ 0.0090,  0.0433,  0.0170, -0.0199,  0.0271]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0032, -0.0043, -0.0033,  0.0225, -0.0095],\n",
      "        [ 0.0007, -0.0027,  0.0061, -0.0271,  0.0051],\n",
      "        [-0.0029, -0.0260,  0.0221, -0.0196, -0.0052],\n",
      "        [ 0.0191,  0.0140, -0.0270, -0.0361, -0.0225],\n",
      "        [ 0.0104, -0.0271,  0.0221,  0.0065,  0.0220],\n",
      "        [ 0.0056, -0.0121, -0.0199, -0.0008,  0.0089],\n",
      "        [-0.0105, -0.0311,  0.0079, -0.0076, -0.0117],\n",
      "        [-0.0007, -0.0154, -0.0452, -0.0049, -0.0112]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0109, grad_fn=<MinBackward1>), tensor(0.8285, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09274975210428238\n",
      "@sample 1172: tensor([[ 0.0052,  0.0104,  0.0207, -0.0191,  0.0250],\n",
      "        [ 0.0048,  0.0050,  0.0059, -0.0086,  0.0103],\n",
      "        [-0.0092, -0.0159,  0.0025,  0.0081, -0.0051],\n",
      "        [-0.0005,  0.0056,  0.0091, -0.0062,  0.0129],\n",
      "        [ 0.0200, -0.0021, -0.0001, -0.0403,  0.0068],\n",
      "        [-0.0026, -0.0003,  0.0073, -0.0075,  0.0068],\n",
      "        [ 0.0070,  0.0097,  0.0185, -0.0340, -0.0071],\n",
      "        [-0.0071, -0.0002,  0.0069,  0.0330, -0.0315]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0116, -0.0203, -0.0127,  0.0531,  0.0195],\n",
      "        [ 0.0307, -0.0209,  0.0200, -0.0183, -0.0250],\n",
      "        [ 0.0112,  0.0009,  0.0279, -0.0061,  0.0037],\n",
      "        [-0.0111, -0.0039, -0.0055,  0.0093,  0.0003],\n",
      "        [-0.0323,  0.0060, -0.0220, -0.0081,  0.0011],\n",
      "        [ 0.0177, -0.0279,  0.0197, -0.0199,  0.0231],\n",
      "        [-0.0340,  0.0143,  0.0265,  0.0294,  0.0104],\n",
      "        [ 0.0164, -0.0018,  0.0182, -0.0170, -0.0129]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0122, grad_fn=<MinBackward1>), tensor(0.8285, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09044192731380463\n",
      "@sample 1173: tensor([[ 1.2963e-02,  3.5762e-02,  3.4020e-02, -4.2264e-03, -1.4041e-02],\n",
      "        [-9.3083e-03,  5.2727e-03,  2.4186e-02, -3.1660e-02,  9.0028e-03],\n",
      "        [-3.5220e-03,  1.6583e-03, -5.2775e-04, -3.6036e-03,  1.2529e-02],\n",
      "        [-1.8643e-02,  5.2264e-02,  5.6073e-02, -3.9297e-02,  5.3664e-04],\n",
      "        [ 1.0188e-02,  1.6672e-02, -1.8996e-02, -5.3822e-03, -2.0268e-02],\n",
      "        [ 5.5899e-03,  9.5908e-03,  6.2822e-04, -1.0461e-02,  7.7322e-05],\n",
      "        [ 8.3294e-03, -2.9937e-03, -2.6169e-03,  8.1922e-03,  1.6298e-02],\n",
      "        [-1.8490e-03,  2.3345e-02, -1.2986e-02, -9.6150e-04, -3.1619e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0090,  0.0437, -0.0461,  0.0398,  0.0165],\n",
      "        [-0.0124,  0.0129,  0.0113, -0.0119, -0.0192],\n",
      "        [-0.0018,  0.0025,  0.0084,  0.0161,  0.0101],\n",
      "        [-0.0124,  0.0071, -0.0269,  0.0129, -0.0300],\n",
      "        [-0.0242, -0.0020, -0.0148,  0.0125,  0.0007],\n",
      "        [-0.0043, -0.0092, -0.0225,  0.0030, -0.0137],\n",
      "        [ 0.0211,  0.0074, -0.0411,  0.0158,  0.0036],\n",
      "        [-0.0050,  0.0363, -0.0282, -0.0488, -0.0096]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0127, grad_fn=<MinBackward1>), tensor(0.8340, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09255154430866241\n",
      "@sample 1174: tensor([[ 0.0083,  0.0090, -0.0090, -0.0082,  0.0097],\n",
      "        [-0.0304,  0.0216,  0.0213, -0.0133, -0.0096],\n",
      "        [-0.0056, -0.0143,  0.0190, -0.0124, -0.0136],\n",
      "        [-0.0058, -0.0176, -0.0193, -0.0025, -0.0220],\n",
      "        [-0.0118, -0.0004,  0.0161,  0.0009,  0.0007],\n",
      "        [-0.0047, -0.0094, -0.0370,  0.0054, -0.0153],\n",
      "        [-0.0201,  0.0168,  0.0010,  0.0012, -0.0032],\n",
      "        [ 0.0211, -0.0245,  0.0007, -0.0006, -0.0003]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0256, -0.0062, -0.0421, -0.0019, -0.0063],\n",
      "        [-0.0065, -0.0038, -0.0121, -0.0029,  0.0062],\n",
      "        [-0.0190, -0.0337, -0.0035, -0.0301, -0.0105],\n",
      "        [-0.0278, -0.0340, -0.0044, -0.0193, -0.0205],\n",
      "        [ 0.0125,  0.0036,  0.0132,  0.0149, -0.0086],\n",
      "        [ 0.0066, -0.0134,  0.0131,  0.0025,  0.0027],\n",
      "        [-0.0124, -0.0185,  0.0133, -0.0260, -0.0351],\n",
      "        [ 0.0313, -0.0120, -0.0232,  0.0014, -0.0072]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0195, grad_fn=<MinBackward1>), tensor(0.8557, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09264830499887466\n",
      "@sample 1175: tensor([[ 0.0012, -0.0009,  0.0042,  0.0008,  0.0008],\n",
      "        [-0.0255, -0.0085,  0.0126, -0.0176,  0.0253],\n",
      "        [ 0.0156, -0.0096,  0.0061,  0.0058,  0.0025],\n",
      "        [-0.0111,  0.0007,  0.0028,  0.0062, -0.0039],\n",
      "        [-0.0149, -0.0005,  0.0109, -0.0159,  0.0263],\n",
      "        [-0.0078,  0.0135, -0.0015, -0.0038, -0.0059],\n",
      "        [ 0.0129,  0.0057,  0.0092,  0.0069, -0.0019],\n",
      "        [-0.0119,  0.0112,  0.0115, -0.0047,  0.0023]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 7.5285e-03,  1.3818e-02, -1.8595e-02, -7.9211e-03, -4.3660e-03],\n",
      "        [-2.5249e-02, -1.1137e-02,  1.5278e-02, -1.3213e-02, -1.0363e-02],\n",
      "        [-1.8333e-03,  1.2405e-02, -8.6141e-03,  2.5700e-03, -7.5080e-05],\n",
      "        [-1.1198e-02, -1.6304e-02, -5.0404e-02,  2.9363e-02, -6.7792e-03],\n",
      "        [-1.7276e-02, -1.4303e-02, -3.7623e-03,  7.2903e-03, -2.8809e-02],\n",
      "        [ 2.7167e-03,  1.9699e-02, -1.1380e-02, -1.9862e-02,  9.9967e-03],\n",
      "        [-1.5347e-02,  1.2185e-02, -3.1942e-02,  2.6889e-02,  1.1423e-02],\n",
      "        [-2.0779e-02,  2.0936e-02,  5.2583e-04,  8.4854e-03, -8.7277e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0115, grad_fn=<MinBackward1>), tensor(0.8550, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.0760372057557106\n",
      "@sample 1176: tensor([[-0.0044, -0.0050, -0.0127, -0.0193,  0.0056],\n",
      "        [-0.0124,  0.0033,  0.0004, -0.0028, -0.0075],\n",
      "        [-0.0321, -0.0076, -0.0095, -0.0103,  0.0004],\n",
      "        [ 0.0258, -0.0121, -0.0295, -0.0155, -0.0086],\n",
      "        [-0.0079,  0.0010, -0.0021,  0.0172, -0.0112],\n",
      "        [-0.0064, -0.0018, -0.0102, -0.0230,  0.0031],\n",
      "        [ 0.0058,  0.0133, -0.0089, -0.0252,  0.0070],\n",
      "        [ 0.0020,  0.0008,  0.0149, -0.0013, -0.0112]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0041, -0.0036,  0.0055,  0.0103,  0.0012],\n",
      "        [ 0.0023, -0.0086,  0.0026, -0.0126, -0.0187],\n",
      "        [-0.0067, -0.0160, -0.0142,  0.0036,  0.0080],\n",
      "        [ 0.0123, -0.0020, -0.0354,  0.0292,  0.0411],\n",
      "        [-0.0076, -0.0044, -0.0022, -0.0372, -0.0123],\n",
      "        [-0.0124, -0.0146, -0.0215,  0.0535,  0.0137],\n",
      "        [-0.0452, -0.0030, -0.0208, -0.0091,  0.0097],\n",
      "        [-0.0339, -0.0026,  0.0172,  0.0028, -0.0130]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0035, grad_fn=<MinBackward1>), tensor(0.8377, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.0934934914112091\n",
      "@sample 1177: tensor([[ 0.0180,  0.0078, -0.0053,  0.0032,  0.0039],\n",
      "        [ 0.0178, -0.0094,  0.0030,  0.0223, -0.0179],\n",
      "        [-0.0016, -0.0083, -0.0154, -0.0329,  0.0307],\n",
      "        [-0.0254,  0.0135,  0.0026, -0.0139,  0.0138],\n",
      "        [ 0.0204,  0.0203,  0.0008,  0.0017, -0.0038],\n",
      "        [ 0.0159,  0.0150, -0.0050, -0.0192, -0.0120],\n",
      "        [ 0.0056, -0.0071,  0.0107,  0.0212, -0.0170],\n",
      "        [ 0.0079,  0.0095,  0.0101,  0.0011,  0.0061]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0086,  0.0208, -0.0553,  0.0204, -0.0128],\n",
      "        [-0.0439,  0.0090, -0.0291,  0.0026, -0.0190],\n",
      "        [-0.0108, -0.0089,  0.0177, -0.0222, -0.0030],\n",
      "        [ 0.0248, -0.0271,  0.0099, -0.0406, -0.0170],\n",
      "        [-0.0268, -0.0172, -0.0168,  0.0007, -0.0125],\n",
      "        [-0.0137, -0.0031,  0.0166, -0.0223, -0.0145],\n",
      "        [ 0.0090,  0.0149,  0.0247,  0.0002, -0.0038],\n",
      "        [ 0.0038,  0.0172, -0.0028,  0.0038, -0.0092]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0136, grad_fn=<MinBackward1>), tensor(0.8152, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08212187141180038\n",
      "@sample 1178: tensor([[-0.0017,  0.0183, -0.0017, -0.0231,  0.0117],\n",
      "        [ 0.0092,  0.0094,  0.0145, -0.0245,  0.0194],\n",
      "        [ 0.0042, -0.0052,  0.0148, -0.0138,  0.0108],\n",
      "        [-0.0151,  0.0006,  0.0025, -0.0202,  0.0038],\n",
      "        [-0.0149, -0.0332,  0.0341, -0.0196,  0.0117],\n",
      "        [ 0.0027,  0.0079,  0.0192, -0.0128,  0.0052],\n",
      "        [ 0.0052, -0.0100,  0.0034,  0.0099,  0.0043],\n",
      "        [-0.0046,  0.0038,  0.0206,  0.0149, -0.0378]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0080, -0.0165, -0.0313,  0.0073, -0.0229],\n",
      "        [ 0.0033, -0.0086, -0.0275,  0.0240,  0.0028],\n",
      "        [-0.0163, -0.0117,  0.0061, -0.0051, -0.0131],\n",
      "        [ 0.0204, -0.0246, -0.0278,  0.0148,  0.0150],\n",
      "        [ 0.0233, -0.0013, -0.0010,  0.0016, -0.0047],\n",
      "        [ 0.0093,  0.0060, -0.0106,  0.0179,  0.0235],\n",
      "        [ 0.0167,  0.0087,  0.0212, -0.0068, -0.0083],\n",
      "        [ 0.0017,  0.0301,  0.0210,  0.0360, -0.0013]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0096, grad_fn=<MinBackward1>), tensor(0.8274, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08785799145698547\n",
      "@sample 1179: tensor([[-0.0069, -0.0036,  0.0111,  0.0089,  0.0261],\n",
      "        [-0.0070, -0.0025,  0.0083, -0.0045, -0.0032],\n",
      "        [-0.0107,  0.0140,  0.0010, -0.0163,  0.0109],\n",
      "        [ 0.0157,  0.0256,  0.0250, -0.0134,  0.0259],\n",
      "        [-0.0087, -0.0159, -0.0064, -0.0118,  0.0162],\n",
      "        [ 0.0074,  0.0199,  0.0134, -0.0180,  0.0163],\n",
      "        [-0.0263, -0.0095,  0.0096, -0.0143, -0.0034],\n",
      "        [-0.0072,  0.0014,  0.0046, -0.0058,  0.0146]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0066, -0.0045, -0.0374, -0.0046, -0.0279],\n",
      "        [ 0.0032,  0.0115,  0.0165,  0.0166, -0.0071],\n",
      "        [ 0.0024,  0.0018,  0.0059,  0.0113, -0.0150],\n",
      "        [ 0.0002, -0.0009, -0.0412,  0.0315, -0.0006],\n",
      "        [-0.0016, -0.0049,  0.0076, -0.0058, -0.0126],\n",
      "        [ 0.0089, -0.0102, -0.0320,  0.0186,  0.0159],\n",
      "        [-0.0139, -0.0204,  0.0026,  0.0219,  0.0026],\n",
      "        [-0.0085,  0.0066, -0.0354, -0.0015, -0.0304]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0151, grad_fn=<MinBackward1>), tensor(0.8192, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08398093283176422\n",
      "@sample 1180: tensor([[-0.0234, -0.0115, -0.0027,  0.0127, -0.0066],\n",
      "        [ 0.0102,  0.0032,  0.0065,  0.0020,  0.0058],\n",
      "        [-0.0173, -0.0163,  0.0056,  0.0055, -0.0011],\n",
      "        [ 0.0045, -0.0175,  0.0138, -0.0013, -0.0038],\n",
      "        [ 0.0160, -0.0297, -0.0055,  0.0195, -0.0302],\n",
      "        [ 0.0096, -0.0159, -0.0207,  0.0172, -0.0037],\n",
      "        [-0.0165, -0.0011,  0.0055, -0.0025, -0.0071],\n",
      "        [ 0.0123, -0.0022,  0.0233, -0.0207,  0.0017]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 2.8224e-02, -3.2801e-03,  2.3171e-03,  2.9203e-03,  1.0680e-02],\n",
      "        [-2.4035e-03,  1.7568e-03, -1.4072e-02, -1.2237e-02, -9.2139e-03],\n",
      "        [ 2.6542e-03,  6.1885e-03,  2.1899e-02, -1.0467e-03, -2.9281e-03],\n",
      "        [-9.6955e-03,  5.9773e-03,  1.2472e-02, -1.9856e-03, -2.0256e-02],\n",
      "        [ 5.1556e-03,  1.9359e-02,  1.9809e-02, -1.1468e-02, -3.4538e-03],\n",
      "        [ 6.6538e-03, -1.5013e-05, -3.6115e-03,  1.2992e-03,  2.1817e-02],\n",
      "        [ 1.1481e-02,  8.1545e-03,  3.3595e-03,  1.9319e-03,  1.0150e-02],\n",
      "        [-1.5054e-02,  1.8321e-03,  1.1463e-02, -7.2609e-03, -1.5857e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0157, grad_fn=<MinBackward1>), tensor(0.8587, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.07945773005485535\n",
      "@sample 1181: tensor([[ 0.0063,  0.0245,  0.0046, -0.0266,  0.0199],\n",
      "        [ 0.0133,  0.0055, -0.0050,  0.0152,  0.0073],\n",
      "        [ 0.0033, -0.0162, -0.0028, -0.0159,  0.0285],\n",
      "        [ 0.0107, -0.0138,  0.0151,  0.0042, -0.0009],\n",
      "        [-0.0104,  0.0084, -0.0086,  0.0002,  0.0030],\n",
      "        [ 0.0181,  0.0275,  0.0398, -0.0415,  0.0512],\n",
      "        [ 0.0042,  0.0093, -0.0072,  0.0217, -0.0058],\n",
      "        [ 0.0003, -0.0290, -0.0148, -0.0089,  0.0059]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-1.8340e-02, -1.3687e-02, -3.1569e-02,  4.4189e-02,  3.2803e-03],\n",
      "        [ 4.2116e-02,  2.4692e-02,  3.7938e-02,  1.4910e-02,  1.2547e-02],\n",
      "        [ 1.0984e-03,  2.5773e-03, -2.0801e-03,  5.6113e-03,  4.6457e-03],\n",
      "        [ 5.7856e-03,  1.3292e-02,  8.1474e-03,  7.5186e-03, -1.9987e-03],\n",
      "        [-2.1576e-02, -1.5652e-02, -2.2664e-02,  2.0758e-02,  6.3395e-03],\n",
      "        [-2.2550e-02, -2.5202e-02, -6.7800e-02,  7.2299e-03, -3.1481e-02],\n",
      "        [ 2.1087e-02,  1.7767e-02,  1.6956e-02, -3.5565e-02, -2.6084e-02],\n",
      "        [-3.0941e-02, -1.7651e-03,  5.7951e-05,  1.8269e-02,  2.9383e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0131, grad_fn=<MinBackward1>), tensor(0.8444, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08485857397317886\n",
      "@sample 1182: tensor([[ 0.0023, -0.0012, -0.0066, -0.0230,  0.0170],\n",
      "        [-0.0064,  0.0153, -0.0062, -0.0048, -0.0107],\n",
      "        [-0.0221, -0.0181, -0.0352,  0.0187, -0.0018],\n",
      "        [ 0.0065,  0.0073, -0.0281, -0.0188,  0.0125],\n",
      "        [ 0.0031,  0.0004, -0.0165,  0.0390, -0.0208],\n",
      "        [ 0.0094,  0.0064,  0.0342, -0.0030, -0.0151],\n",
      "        [-0.0113, -0.0070, -0.0277,  0.0161,  0.0063],\n",
      "        [-0.0388,  0.0254,  0.0062, -0.0641,  0.0152]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0531, -0.0164, -0.0325,  0.0296, -0.0093],\n",
      "        [ 0.0307, -0.0133,  0.0462, -0.0086, -0.0292],\n",
      "        [ 0.0104, -0.0051,  0.0120, -0.0230, -0.0132],\n",
      "        [-0.0114,  0.0258, -0.0349,  0.0246,  0.0185],\n",
      "        [ 0.0075,  0.0222, -0.0285,  0.0007, -0.0020],\n",
      "        [-0.0014,  0.0100, -0.0280,  0.0548,  0.0079],\n",
      "        [ 0.0183,  0.0056, -0.0022, -0.0164, -0.0231],\n",
      "        [-0.0203,  0.0063, -0.0706,  0.0352,  0.0182]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0131, grad_fn=<MinBackward1>), tensor(0.8004, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09990992397069931\n",
      "@sample 1183: tensor([[-0.0120,  0.0119, -0.0026, -0.0163,  0.0122],\n",
      "        [ 0.0103, -0.0191,  0.0173, -0.0125,  0.0263],\n",
      "        [ 0.0067,  0.0206,  0.0264,  0.0010, -0.0032],\n",
      "        [ 0.0106, -0.0138,  0.0046,  0.0153,  0.0153],\n",
      "        [ 0.0125, -0.0082, -0.0108,  0.0092, -0.0075],\n",
      "        [ 0.0212,  0.0074,  0.0201,  0.0027, -0.0073],\n",
      "        [-0.0041, -0.0120,  0.0058, -0.0238,  0.0258],\n",
      "        [ 0.0172,  0.0172, -0.0071, -0.0183, -0.0011]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0023, -0.0079, -0.0142,  0.0102, -0.0061],\n",
      "        [-0.0105, -0.0117,  0.0031, -0.0006, -0.0226],\n",
      "        [-0.0266,  0.0125, -0.0217,  0.0195, -0.0036],\n",
      "        [ 0.0008, -0.0096, -0.0059,  0.0214, -0.0366],\n",
      "        [ 0.0173,  0.0173, -0.0292, -0.0075,  0.0233],\n",
      "        [ 0.0011, -0.0024, -0.0153,  0.0080, -0.0079],\n",
      "        [-0.0178, -0.0066,  0.0140,  0.0249,  0.0118],\n",
      "        [ 0.0154,  0.0030, -0.0386,  0.0133,  0.0034]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0158, grad_fn=<MinBackward1>), tensor(0.8077, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08894963562488556\n",
      "@sample 1184: tensor([[ 0.0134, -0.0130, -0.0120,  0.0031,  0.0045],\n",
      "        [-0.0051,  0.0380,  0.0074,  0.0172, -0.0097],\n",
      "        [-0.0154,  0.0111, -0.0038, -0.0015,  0.0286],\n",
      "        [ 0.0069, -0.0042, -0.0038,  0.0117,  0.0036],\n",
      "        [ 0.0190, -0.0215, -0.0049,  0.0061, -0.0224],\n",
      "        [ 0.0104,  0.0011, -0.0036,  0.0213, -0.0082],\n",
      "        [-0.0050, -0.0049, -0.0242,  0.0221, -0.0090],\n",
      "        [ 0.0129, -0.0036, -0.0023, -0.0017, -0.0017]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0354, -0.0230,  0.0227, -0.0115, -0.0037],\n",
      "        [ 0.0217,  0.0204, -0.0206, -0.0131, -0.0085],\n",
      "        [ 0.0519, -0.0136, -0.0173, -0.0123,  0.0081],\n",
      "        [ 0.0103,  0.0006,  0.0030,  0.0110,  0.0127],\n",
      "        [-0.0240, -0.0078, -0.0102,  0.0160,  0.0040],\n",
      "        [ 0.0114,  0.0226,  0.0111, -0.0054, -0.0050],\n",
      "        [-0.0009,  0.0066, -0.0105,  0.0089,  0.0102],\n",
      "        [ 0.0005, -0.0017,  0.0210,  0.0005, -0.0070]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0138, grad_fn=<MinBackward1>), tensor(0.8694, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08777759224176407\n",
      "@sample 1185: tensor([[ 0.0143,  0.0053,  0.0083, -0.0078,  0.0176],\n",
      "        [ 0.0024, -0.0047, -0.0225,  0.0175, -0.0149],\n",
      "        [-0.0066, -0.0098,  0.0098, -0.0112,  0.0330],\n",
      "        [ 0.0014,  0.0043,  0.0011,  0.0215,  0.0019],\n",
      "        [ 0.0151,  0.0049,  0.0085,  0.0029, -0.0121],\n",
      "        [-0.0148, -0.0118,  0.0064,  0.0072, -0.0038],\n",
      "        [-0.0110,  0.0343, -0.0064,  0.0141,  0.0188],\n",
      "        [-0.0011, -0.0180, -0.0052, -0.0037, -0.0090]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0112,  0.0053, -0.0198, -0.0115, -0.0401],\n",
      "        [ 0.0164,  0.0130, -0.0021, -0.0386, -0.0216],\n",
      "        [ 0.0140,  0.0261, -0.0058, -0.0148, -0.0136],\n",
      "        [ 0.0008,  0.0287, -0.0407,  0.0037, -0.0211],\n",
      "        [ 0.0019,  0.0079, -0.0022, -0.0079, -0.0092],\n",
      "        [ 0.0088,  0.0034,  0.0015, -0.0195, -0.0274],\n",
      "        [ 0.0316,  0.0622, -0.0330, -0.0126,  0.0177],\n",
      "        [ 0.0133, -0.0026,  0.0302, -0.0114, -0.0174]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0154, grad_fn=<MinBackward1>), tensor(0.8518, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08562866598367691\n",
      "@sample 1186: tensor([[-0.0030,  0.0046, -0.0072, -0.0090, -0.0007],\n",
      "        [-0.0025, -0.0254, -0.0006,  0.0273, -0.0045],\n",
      "        [ 0.0164, -0.0052,  0.0138, -0.0191, -0.0049],\n",
      "        [-0.0014, -0.0009, -0.0253,  0.0035,  0.0106],\n",
      "        [-0.0032, -0.0004, -0.0135, -0.0091,  0.0078],\n",
      "        [-0.0057,  0.0280,  0.0248, -0.0045, -0.0025],\n",
      "        [-0.0055,  0.0018, -0.0007,  0.0034,  0.0005],\n",
      "        [-0.0016,  0.0068,  0.0034, -0.0050, -0.0077]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0025, -0.0095, -0.0181,  0.0097,  0.0170],\n",
      "        [ 0.0178, -0.0120,  0.0242, -0.0326, -0.0183],\n",
      "        [ 0.0007,  0.0266, -0.0069, -0.0136,  0.0036],\n",
      "        [ 0.0436, -0.0159,  0.0380,  0.0046,  0.0138],\n",
      "        [ 0.0010, -0.0244,  0.0016,  0.0109,  0.0182],\n",
      "        [-0.0088,  0.0292, -0.0063, -0.0185,  0.0017],\n",
      "        [ 0.0212,  0.0062,  0.0273, -0.0025,  0.0036],\n",
      "        [-0.0160,  0.0067, -0.0116, -0.0043, -0.0267]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0090, grad_fn=<MinBackward1>), tensor(0.8462, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.0924471765756607\n",
      "@sample 1187: tensor([[ 0.0131,  0.0047, -0.0183,  0.0007,  0.0001],\n",
      "        [ 0.0059,  0.0023, -0.0180,  0.0376, -0.0480],\n",
      "        [ 0.0184,  0.0029, -0.0271,  0.0101, -0.0005],\n",
      "        [-0.0086, -0.0057, -0.0132,  0.0177, -0.0053],\n",
      "        [ 0.0148,  0.0284,  0.0177, -0.0358,  0.0056],\n",
      "        [ 0.0120,  0.0113,  0.0149, -0.0242,  0.0087],\n",
      "        [-0.0098, -0.0240,  0.0059,  0.0118,  0.0026],\n",
      "        [ 0.0155, -0.0102, -0.0238,  0.0237, -0.0082]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0267,  0.0054, -0.0094,  0.0153,  0.0047],\n",
      "        [-0.0101,  0.0342,  0.0046, -0.0048, -0.0014],\n",
      "        [ 0.0134,  0.0279, -0.0152, -0.0032,  0.0110],\n",
      "        [ 0.0201, -0.0030,  0.0073,  0.0131,  0.0030],\n",
      "        [-0.0508, -0.0423, -0.0641,  0.0245, -0.0140],\n",
      "        [-0.0030, -0.0258, -0.0317,  0.0426,  0.0228],\n",
      "        [-0.0021,  0.0235,  0.0083, -0.0114, -0.0133],\n",
      "        [ 0.0064, -0.0199,  0.0060, -0.0436, -0.0152]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0159, grad_fn=<MinBackward1>), tensor(0.9134, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08449152112007141\n",
      "@sample 1188: tensor([[-0.0071, -0.0135, -0.0215, -0.0018, -0.0115],\n",
      "        [-0.0225, -0.0090, -0.0044, -0.0186,  0.0034],\n",
      "        [ 0.0148, -0.0249, -0.0239,  0.0184, -0.0102],\n",
      "        [-0.0150,  0.0089,  0.0006, -0.0135,  0.0281],\n",
      "        [-0.0110,  0.0220, -0.0095, -0.0113, -0.0087],\n",
      "        [-0.0075, -0.0025, -0.0009, -0.0379,  0.0037],\n",
      "        [ 0.0140,  0.0028, -0.0123,  0.0119, -0.0055],\n",
      "        [-0.0039, -0.0025, -0.0162,  0.0234, -0.0335]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0144, -0.0135, -0.0066, -0.0015,  0.0165],\n",
      "        [-0.0062, -0.0363,  0.0119,  0.0063,  0.0083],\n",
      "        [ 0.0233,  0.0011,  0.0050, -0.0046,  0.0102],\n",
      "        [-0.0164,  0.0069,  0.0340,  0.0088, -0.0221],\n",
      "        [ 0.0175, -0.0080,  0.0140, -0.0056,  0.0068],\n",
      "        [-0.0010,  0.0086, -0.0460, -0.0119, -0.0082],\n",
      "        [ 0.0059,  0.0101,  0.0019, -0.0131, -0.0207],\n",
      "        [-0.0080,  0.0340,  0.0079, -0.0266,  0.0055]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0111, grad_fn=<MinBackward1>), tensor(0.8448, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08759976923465729\n",
      "@sample 1189: tensor([[ 0.0308,  0.0010, -0.0174,  0.0341, -0.0297],\n",
      "        [ 0.0073, -0.0203, -0.0074,  0.0055, -0.0176],\n",
      "        [ 0.0359, -0.0175,  0.0069,  0.0245, -0.0283],\n",
      "        [-0.0038, -0.0172, -0.0233,  0.0243, -0.0160],\n",
      "        [ 0.0083,  0.0011,  0.0031, -0.0059, -0.0336],\n",
      "        [ 0.0017,  0.0002, -0.0061,  0.0017, -0.0120],\n",
      "        [ 0.0013,  0.0122,  0.0124, -0.0101,  0.0144],\n",
      "        [-0.0011, -0.0025, -0.0133,  0.0072, -0.0081]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0088,  0.0006, -0.0118, -0.0010,  0.0076],\n",
      "        [-0.0034, -0.0128,  0.0396,  0.0073,  0.0042],\n",
      "        [-0.0334,  0.0520, -0.0098,  0.0139,  0.0391],\n",
      "        [ 0.0015,  0.0156,  0.0233, -0.0249, -0.0034],\n",
      "        [-0.0294, -0.0197,  0.0130,  0.0179,  0.0175],\n",
      "        [-0.0084, -0.0110,  0.0143, -0.0305, -0.0198],\n",
      "        [-0.0147, -0.0261,  0.0069,  0.0104, -0.0255],\n",
      "        [-0.0024, -0.0186,  0.0354, -0.0038, -0.0069]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0136, grad_fn=<MinBackward1>), tensor(0.8414, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09754905849695206\n",
      "@sample 1190: tensor([[-0.0176, -0.0418, -0.0189,  0.0066, -0.0071],\n",
      "        [ 0.0037, -0.0080, -0.0008, -0.0167,  0.0018],\n",
      "        [-0.0132,  0.0091,  0.0018, -0.0175,  0.0056],\n",
      "        [-0.0056,  0.0165,  0.0055, -0.0250, -0.0252],\n",
      "        [-0.0027,  0.0052,  0.0319, -0.0346, -0.0104],\n",
      "        [-0.0149,  0.0148,  0.0143, -0.0026,  0.0180],\n",
      "        [-0.0034,  0.0046, -0.0131, -0.0105, -0.0173],\n",
      "        [-0.0190,  0.0116, -0.0075,  0.0246, -0.0198]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0030, -0.0056,  0.0283, -0.0126, -0.0115],\n",
      "        [-0.0128,  0.0298, -0.0105,  0.0176,  0.0085],\n",
      "        [-0.0144,  0.0075, -0.0136, -0.0047,  0.0077],\n",
      "        [-0.0093, -0.0083, -0.0027, -0.0026,  0.0218],\n",
      "        [-0.0202, -0.0048,  0.0023,  0.0151, -0.0211],\n",
      "        [-0.0126, -0.0148, -0.0025, -0.0203, -0.0273],\n",
      "        [-0.0077, -0.0282,  0.0187, -0.0076,  0.0199],\n",
      "        [ 0.0069, -0.0081, -0.0158, -0.0012,  0.0225]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0145, grad_fn=<MinBackward1>), tensor(0.8739, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09077342599630356\n",
      "@sample 1191: tensor([[-1.5510e-02,  1.4597e-02, -8.7744e-03, -9.5051e-03,  2.5880e-02],\n",
      "        [-1.1667e-02,  1.9136e-03,  1.1090e-03,  4.7073e-05,  3.2839e-03],\n",
      "        [-1.9939e-02, -1.3332e-02, -2.4425e-03, -2.4833e-04, -7.5449e-03],\n",
      "        [ 7.5381e-03, -8.5163e-03,  1.8764e-02, -4.1521e-03, -2.0687e-03],\n",
      "        [ 1.5056e-03,  3.3647e-02,  1.4441e-02, -4.3121e-03, -9.5686e-03],\n",
      "        [ 1.1571e-02,  1.4514e-03, -2.5361e-02,  3.0980e-02, -2.2185e-02],\n",
      "        [-2.0179e-02,  2.3217e-03,  4.5664e-03, -1.0382e-02,  2.9071e-02],\n",
      "        [ 9.3103e-03, -4.4324e-03,  1.7288e-02, -5.8724e-03, -1.9208e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0096, -0.0318, -0.0012,  0.0014, -0.0288],\n",
      "        [ 0.0200, -0.0438,  0.0160, -0.0270, -0.0137],\n",
      "        [-0.0329, -0.0072,  0.0100, -0.0189, -0.0165],\n",
      "        [-0.0196, -0.0106,  0.0227,  0.0084,  0.0116],\n",
      "        [ 0.0070,  0.0215,  0.0043,  0.0110,  0.0187],\n",
      "        [ 0.0087, -0.0203,  0.0107, -0.0173,  0.0073],\n",
      "        [ 0.0002, -0.0304,  0.0249, -0.0073, -0.0372],\n",
      "        [-0.0012,  0.0030, -0.0207,  0.0211,  0.0234]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0126, grad_fn=<MinBackward1>), tensor(0.8287, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10074621438980103\n",
      "@sample 1192: tensor([[-2.7355e-02,  2.1488e-02,  1.6909e-03,  1.2854e-02,  3.9157e-03],\n",
      "        [ 1.3340e-02, -1.4008e-02,  2.9392e-03, -1.5871e-02,  2.5658e-02],\n",
      "        [-2.4365e-02, -8.6388e-03, -2.2546e-02, -2.8353e-02,  9.2140e-03],\n",
      "        [-2.3758e-03, -2.4133e-03,  7.8630e-03, -1.4249e-02,  1.3211e-03],\n",
      "        [-2.9079e-02, -6.3484e-03,  4.1963e-02, -4.0518e-02,  7.6529e-03],\n",
      "        [ 1.2283e-02, -9.7055e-03,  6.4470e-03, -6.9718e-03,  8.2921e-03],\n",
      "        [ 1.9695e-02,  1.7923e-02, -2.3856e-02, -7.9213e-03,  1.5084e-03],\n",
      "        [ 5.9938e-05,  1.5628e-02,  1.1803e-02, -1.1579e-02,  1.4891e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0078, -0.0166, -0.0155, -0.0041,  0.0016],\n",
      "        [-0.0011, -0.0253, -0.0128,  0.0057, -0.0175],\n",
      "        [ 0.0102, -0.0958, -0.0004,  0.0127,  0.0187],\n",
      "        [-0.0115, -0.0109, -0.0230, -0.0047, -0.0019],\n",
      "        [-0.0180, -0.0309,  0.0197,  0.0074,  0.0142],\n",
      "        [ 0.0145, -0.0052, -0.0064,  0.0192,  0.0128],\n",
      "        [-0.0134, -0.0109, -0.0290,  0.0117, -0.0056],\n",
      "        [ 0.0010, -0.0070, -0.0109,  0.0031, -0.0025]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0158, grad_fn=<MinBackward1>), tensor(0.7644, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09760080277919769\n",
      "@sample 1193: tensor([[ 0.0284,  0.0180,  0.0014, -0.0087, -0.0015],\n",
      "        [ 0.0141, -0.0329,  0.0013, -0.0178,  0.0117],\n",
      "        [ 0.0122,  0.0136, -0.0070,  0.0094,  0.0035],\n",
      "        [ 0.0066,  0.0048,  0.0017,  0.0100, -0.0014],\n",
      "        [ 0.0064,  0.0129, -0.0111,  0.0035, -0.0035],\n",
      "        [ 0.0031, -0.0195,  0.0100, -0.0066,  0.0270],\n",
      "        [ 0.0117, -0.0248,  0.0070, -0.0053, -0.0130],\n",
      "        [ 0.0050, -0.0126, -0.0058,  0.0141, -0.0055]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0129,  0.0138,  0.0306,  0.0081, -0.0011],\n",
      "        [-0.0096, -0.0062,  0.0187,  0.0294,  0.0276],\n",
      "        [-0.0087,  0.0091, -0.0112,  0.0291,  0.0101],\n",
      "        [ 0.0205, -0.0078,  0.0304, -0.0196,  0.0002],\n",
      "        [-0.0007, -0.0098, -0.0126, -0.0098, -0.0006],\n",
      "        [-0.0046, -0.0180, -0.0092, -0.0140, -0.0060],\n",
      "        [-0.0089, -0.0189,  0.0436, -0.0162, -0.0194],\n",
      "        [-0.0083,  0.0221, -0.0141, -0.0060,  0.0061]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0088, grad_fn=<MinBackward1>), tensor(0.8307, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.0841684564948082\n",
      "@sample 1194: tensor([[-1.1746e-02, -7.7689e-03,  5.2147e-03, -9.7956e-03, -1.4561e-03],\n",
      "        [ 2.3400e-02,  4.0869e-03, -2.5495e-03,  2.4032e-02, -2.8876e-03],\n",
      "        [ 1.4576e-02,  6.4929e-03,  2.6901e-03,  1.4207e-02, -1.4863e-02],\n",
      "        [-2.7034e-03, -3.5627e-02,  2.2350e-02, -6.0054e-03,  2.6861e-02],\n",
      "        [-2.8798e-03,  3.9351e-03,  2.4328e-03, -3.5942e-05,  1.2809e-02],\n",
      "        [ 2.6062e-02,  4.4537e-03, -2.9961e-03, -1.4703e-02,  8.8771e-03],\n",
      "        [ 2.6106e-02,  1.2652e-02,  1.5722e-03, -4.9953e-03, -1.7131e-03],\n",
      "        [-2.3729e-02, -1.8486e-03, -1.3633e-02,  2.3988e-02, -2.2803e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0137,  0.0036,  0.0270, -0.0165, -0.0276],\n",
      "        [-0.0072,  0.0546, -0.0243, -0.0080,  0.0053],\n",
      "        [-0.0045,  0.0267,  0.0137, -0.0308, -0.0117],\n",
      "        [ 0.0029, -0.0030,  0.0013, -0.0143, -0.0233],\n",
      "        [ 0.0044, -0.0132,  0.0048, -0.0069, -0.0174],\n",
      "        [-0.0001, -0.0202,  0.0319, -0.0048, -0.0216],\n",
      "        [-0.0006,  0.0023, -0.0138, -0.0113,  0.0025],\n",
      "        [-0.0071,  0.0209, -0.0206,  0.0093, -0.0144]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0096, grad_fn=<MinBackward1>), tensor(0.8880, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10255381464958191\n",
      "@sample 1195: tensor([[ 0.0119, -0.0003,  0.0144,  0.0197, -0.0115],\n",
      "        [ 0.0076, -0.0234,  0.0028,  0.0183, -0.0299],\n",
      "        [-0.0017, -0.0181,  0.0040,  0.0276, -0.0081],\n",
      "        [ 0.0021,  0.0092,  0.0418, -0.0228,  0.0132],\n",
      "        [ 0.0144,  0.0101,  0.0030,  0.0136, -0.0049],\n",
      "        [-0.0106,  0.0162,  0.0016,  0.0301, -0.0202],\n",
      "        [ 0.0088, -0.0102,  0.0106, -0.0012,  0.0112],\n",
      "        [ 0.0169,  0.0354, -0.0066, -0.0259,  0.0221]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0040,  0.0013, -0.0380, -0.0156,  0.0157],\n",
      "        [ 0.0084,  0.0011, -0.0047,  0.0289,  0.0320],\n",
      "        [ 0.0112,  0.0232,  0.0004, -0.0012,  0.0072],\n",
      "        [-0.0133,  0.0231,  0.0147, -0.0009,  0.0163],\n",
      "        [ 0.0028,  0.0299,  0.0031, -0.0157, -0.0071],\n",
      "        [ 0.0133,  0.0334,  0.0526, -0.0156,  0.0051],\n",
      "        [ 0.0062,  0.0021,  0.0106, -0.0078, -0.0165],\n",
      "        [-0.0065, -0.0089, -0.0819,  0.0112, -0.0036]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0142, grad_fn=<MinBackward1>), tensor(0.8847, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.0926189050078392\n",
      "@sample 1196: tensor([[ 0.0294, -0.0027,  0.0039, -0.0044,  0.0163],\n",
      "        [-0.0112,  0.0010, -0.0043,  0.0066,  0.0016],\n",
      "        [ 0.0171,  0.0084,  0.0167, -0.0122,  0.0077],\n",
      "        [-0.0020, -0.0100,  0.0193,  0.0017,  0.0042],\n",
      "        [ 0.0016,  0.0337, -0.0003, -0.0579,  0.0147],\n",
      "        [ 0.0231,  0.0025,  0.0055, -0.0002, -0.0069],\n",
      "        [-0.0071, -0.0148, -0.0026,  0.0264,  0.0119],\n",
      "        [-0.0048,  0.0039,  0.0036,  0.0169, -0.0044]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0364, -0.0111, -0.0151,  0.0214,  0.0307],\n",
      "        [-0.0174, -0.0081,  0.0269, -0.0148, -0.0207],\n",
      "        [ 0.0029,  0.0184,  0.0663, -0.0465, -0.0539],\n",
      "        [-0.0087,  0.0193, -0.0087,  0.0062,  0.0297],\n",
      "        [-0.0281, -0.0325, -0.0626,  0.0329, -0.0061],\n",
      "        [ 0.0001,  0.0025,  0.0090,  0.0107,  0.0048],\n",
      "        [-0.0016, -0.0108,  0.0140, -0.0174, -0.0207],\n",
      "        [ 0.0036,  0.0332, -0.0221,  0.0120,  0.0017]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0113, grad_fn=<MinBackward1>), tensor(0.8768, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09017287939786911\n",
      "@sample 1197: tensor([[-1.5795e-02,  1.0303e-02,  2.2228e-03, -1.3449e-03,  5.8119e-03],\n",
      "        [ 6.5306e-03, -9.6469e-03, -8.5338e-03,  2.0954e-02, -1.4346e-02],\n",
      "        [-4.5861e-03,  6.8411e-05, -7.0171e-03,  1.1321e-03, -9.5172e-03],\n",
      "        [ 1.1686e-02, -2.8434e-03,  1.0470e-02,  1.5110e-02, -1.3007e-02],\n",
      "        [-4.5141e-03,  9.1264e-03, -8.6043e-03,  7.4372e-03, -9.1347e-03],\n",
      "        [-4.5221e-03, -1.0694e-02,  3.1105e-02, -1.2917e-02,  1.5503e-02],\n",
      "        [ 9.6944e-03,  5.4683e-03,  2.8964e-02,  9.6345e-03, -2.8053e-02],\n",
      "        [ 2.1912e-02, -1.0020e-02,  1.8418e-02,  3.0346e-02, -4.5343e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0200, -0.0295,  0.0038, -0.0196, -0.0303],\n",
      "        [ 0.0211, -0.0296,  0.0012,  0.0052,  0.0025],\n",
      "        [-0.0051,  0.0026, -0.0122,  0.0085,  0.0162],\n",
      "        [ 0.0005,  0.0227, -0.0081,  0.0131,  0.0086],\n",
      "        [-0.0086, -0.0170, -0.0166, -0.0183, -0.0072],\n",
      "        [-0.0082,  0.0077,  0.0257,  0.0119,  0.0108],\n",
      "        [-0.0161,  0.0288, -0.0270,  0.0020,  0.0125],\n",
      "        [ 0.0145,  0.0023, -0.0021,  0.0058,  0.0066]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0149, grad_fn=<MinBackward1>), tensor(0.7900, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09331218153238297\n",
      "@sample 1198: tensor([[-0.0020, -0.0100,  0.0150, -0.0167,  0.0140],\n",
      "        [ 0.0041,  0.0422, -0.0102, -0.0152,  0.0115],\n",
      "        [ 0.0025, -0.0006,  0.0054,  0.0138, -0.0082],\n",
      "        [-0.0036,  0.0027,  0.0094, -0.0029,  0.0068],\n",
      "        [ 0.0225, -0.0257, -0.0194, -0.0295, -0.0017],\n",
      "        [-0.0253, -0.0002,  0.0046,  0.0107,  0.0064],\n",
      "        [ 0.0122, -0.0070,  0.0110, -0.0206,  0.0162],\n",
      "        [-0.0075, -0.0053, -0.0260,  0.0203, -0.0066]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0098,  0.0126, -0.0024,  0.0168,  0.0235],\n",
      "        [-0.0278,  0.0082, -0.0768,  0.0297,  0.0074],\n",
      "        [-0.0125,  0.0339, -0.0201,  0.0403,  0.0153],\n",
      "        [-0.0072, -0.0017, -0.0145, -0.0120, -0.0054],\n",
      "        [-0.0027, -0.0354,  0.0051, -0.0044, -0.0014],\n",
      "        [ 0.0086,  0.0103,  0.0066, -0.0092,  0.0126],\n",
      "        [ 0.0168,  0.0139, -0.0273,  0.0227,  0.0335],\n",
      "        [-0.0003,  0.0098,  0.0049, -0.0177, -0.0019]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0113, grad_fn=<MinBackward1>), tensor(0.8327, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08801069110631943\n",
      "@sample 1199: tensor([[ 0.0125,  0.0330, -0.0137,  0.0123,  0.0105],\n",
      "        [-0.0111, -0.0211, -0.0121,  0.0061,  0.0139],\n",
      "        [ 0.0115,  0.0172,  0.0016, -0.0126, -0.0002],\n",
      "        [-0.0124,  0.0140,  0.0028, -0.0212,  0.0179],\n",
      "        [-0.0127,  0.0268,  0.0172, -0.0156,  0.0067],\n",
      "        [-0.0126,  0.0038,  0.0032,  0.0009,  0.0035],\n",
      "        [-0.0007,  0.0264,  0.0052, -0.0235,  0.0054],\n",
      "        [-0.0059,  0.0010, -0.0066, -0.0067,  0.0214]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0033,  0.0099, -0.0300,  0.0080, -0.0056],\n",
      "        [ 0.0021, -0.0003,  0.0150, -0.0123,  0.0425],\n",
      "        [-0.0305,  0.0011, -0.0304,  0.0027, -0.0199],\n",
      "        [-0.0007, -0.0045, -0.0223,  0.0463,  0.0115],\n",
      "        [-0.0249, -0.0093, -0.0550,  0.0134,  0.0115],\n",
      "        [-0.0192,  0.0132,  0.0347, -0.0280,  0.0161],\n",
      "        [-0.0020,  0.0346, -0.0189,  0.0164, -0.0222],\n",
      "        [-0.0212,  0.0189, -0.0362,  0.0073,  0.0174]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0117, grad_fn=<MinBackward1>), tensor(0.8391, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08957602828741074\n",
      "@sample 1200: tensor([[ 0.0044,  0.0314, -0.0159, -0.0059, -0.0224],\n",
      "        [-0.0126,  0.0067, -0.0148,  0.0174, -0.0010],\n",
      "        [-0.0012, -0.0054,  0.0108,  0.0064, -0.0169],\n",
      "        [-0.0114,  0.0103,  0.0128, -0.0235,  0.0277],\n",
      "        [-0.0150,  0.0119, -0.0276,  0.0169, -0.0118],\n",
      "        [-0.0279,  0.0013, -0.0047,  0.0003,  0.0111],\n",
      "        [ 0.0023, -0.0068,  0.0129, -0.0013, -0.0127],\n",
      "        [-0.0072,  0.0037, -0.0041, -0.0096,  0.0155]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0075,  0.0242, -0.0343,  0.0162,  0.0254],\n",
      "        [ 0.0291,  0.0160, -0.0303,  0.0033,  0.0092],\n",
      "        [ 0.0015, -0.0003,  0.0313, -0.0027,  0.0042],\n",
      "        [ 0.0027, -0.0224,  0.0144, -0.0226, -0.0144],\n",
      "        [ 0.0095,  0.0294, -0.0065,  0.0156,  0.0045],\n",
      "        [ 0.0090, -0.0307,  0.0105, -0.0192,  0.0025],\n",
      "        [ 0.0080, -0.0159,  0.0220,  0.0039, -0.0146],\n",
      "        [-0.0241,  0.0062,  0.0313,  0.0220,  0.0046]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0064, grad_fn=<MinBackward1>), tensor(0.8406, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09783196449279785\n",
      "@sample 1201: tensor([[ 1.7355e-02, -2.4256e-04, -4.6150e-03,  8.8908e-03,  4.7430e-03],\n",
      "        [-1.2177e-02,  8.6749e-03, -1.6412e-02,  8.0183e-05, -2.1862e-03],\n",
      "        [ 7.6140e-03, -1.2110e-02, -7.7836e-04,  7.6759e-04,  2.8813e-02],\n",
      "        [-5.2584e-03, -1.7133e-02, -2.1839e-02, -8.7225e-03,  2.4650e-02],\n",
      "        [-3.3237e-03,  2.7119e-02,  1.4269e-02, -3.1333e-02,  1.6609e-02],\n",
      "        [ 3.8888e-03, -5.3130e-03,  2.8819e-02,  1.2401e-03,  1.4821e-02],\n",
      "        [-1.7527e-02, -7.3923e-03, -2.1491e-02, -1.2626e-02,  1.3330e-02],\n",
      "        [-1.8261e-02,  8.5465e-03, -1.2320e-02,  2.6912e-02, -4.0702e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0073,  0.0051,  0.0055,  0.0009,  0.0150],\n",
      "        [-0.0038, -0.0039,  0.0269, -0.0204,  0.0052],\n",
      "        [-0.0003, -0.0226, -0.0105,  0.0184,  0.0105],\n",
      "        [ 0.0040, -0.0077,  0.0060, -0.0045,  0.0298],\n",
      "        [ 0.0088, -0.0015, -0.0110,  0.0256,  0.0189],\n",
      "        [ 0.0001, -0.0235, -0.0115,  0.0056, -0.0060],\n",
      "        [ 0.0072, -0.0435,  0.0252,  0.0075,  0.0117],\n",
      "        [ 0.0142,  0.0030, -0.0277,  0.0100,  0.0127]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0115, grad_fn=<MinBackward1>), tensor(0.8202, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09452405571937561\n",
      "@sample 1202: tensor([[ 1.2129e-02,  1.9375e-02, -7.9071e-03, -7.7227e-04,  1.0026e-02],\n",
      "        [ 1.3533e-03,  2.7076e-03,  1.7920e-03, -2.0792e-03,  4.9332e-03],\n",
      "        [-1.0873e-02,  1.6945e-03,  3.5974e-03,  3.1908e-02, -2.0492e-02],\n",
      "        [ 8.6601e-05, -4.1781e-03,  5.3335e-03, -1.4665e-02,  2.1309e-02],\n",
      "        [-1.0002e-03,  5.2920e-03, -2.3512e-02, -1.4478e-03,  2.0452e-02],\n",
      "        [-2.3167e-02, -9.0229e-03, -2.2626e-02,  2.3363e-02, -6.0177e-03],\n",
      "        [-2.1242e-02,  8.0537e-03, -2.3759e-03,  4.7461e-03,  3.6005e-04],\n",
      "        [-3.2727e-04,  4.6888e-03, -1.2344e-03, -1.5876e-02,  1.4310e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-1.4862e-02,  1.4981e-02, -2.2747e-02,  3.9176e-02, -2.6227e-02],\n",
      "        [-2.4141e-02, -1.4469e-02, -1.3135e-02,  1.7064e-02,  7.7013e-03],\n",
      "        [-4.6866e-03,  3.1507e-02, -3.4632e-03, -2.9998e-02, -7.4935e-04],\n",
      "        [ 6.3054e-03, -1.0915e-02,  4.4851e-04, -2.5750e-02, -7.7799e-03],\n",
      "        [ 7.9864e-03,  1.9178e-02, -3.3602e-02, -2.0608e-02, -9.2793e-05],\n",
      "        [ 2.2902e-02,  7.7341e-03, -1.2669e-02, -4.0112e-04,  1.4045e-02],\n",
      "        [-5.4153e-03, -2.4193e-02,  1.3117e-02, -2.3715e-02, -1.2447e-02],\n",
      "        [-2.8887e-02,  5.0491e-04, -2.3499e-02,  3.9402e-02,  6.8747e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0093, grad_fn=<MinBackward1>), tensor(0.8780, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09059970080852509\n",
      "@sample 1203: tensor([[ 0.0083,  0.0173,  0.0121, -0.0067,  0.0021],\n",
      "        [-0.0079,  0.0080,  0.0155, -0.0215,  0.0198],\n",
      "        [-0.0045,  0.0199,  0.0045,  0.0120, -0.0162],\n",
      "        [-0.0056, -0.0065,  0.0172, -0.0007,  0.0054],\n",
      "        [-0.0050,  0.0181,  0.0312, -0.0057, -0.0236],\n",
      "        [ 0.0090,  0.0323,  0.0068, -0.0241,  0.0009],\n",
      "        [-0.0026,  0.0108,  0.0074, -0.0109,  0.0112],\n",
      "        [ 0.0057,  0.0009,  0.0052, -0.0008, -0.0062]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0050,  0.0075,  0.0036, -0.0074, -0.0051],\n",
      "        [-0.0253, -0.0040, -0.0327,  0.0357,  0.0103],\n",
      "        [-0.0149,  0.0257, -0.0006, -0.0177, -0.0047],\n",
      "        [-0.0143, -0.0120,  0.0017, -0.0145, -0.0186],\n",
      "        [-0.0332,  0.0072,  0.0089,  0.0120, -0.0148],\n",
      "        [-0.0287, -0.0061,  0.0050,  0.0128, -0.0203],\n",
      "        [-0.0328,  0.0001, -0.0243, -0.0059, -0.0159],\n",
      "        [ 0.0137,  0.0042,  0.0065, -0.0192, -0.0154]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0137, grad_fn=<MinBackward1>), tensor(0.8806, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08349893987178802\n",
      "@sample 1204: tensor([[-3.0101e-03,  2.7919e-03, -2.2854e-02,  9.5260e-03, -4.3755e-03],\n",
      "        [ 8.5171e-03,  1.8303e-02, -9.3885e-05,  8.0686e-03, -3.3114e-03],\n",
      "        [ 1.5747e-02,  9.1870e-03,  1.0680e-02, -2.6986e-02,  2.6870e-02],\n",
      "        [ 1.3935e-02, -1.2774e-02, -1.0684e-03,  1.1004e-02,  2.1379e-03],\n",
      "        [-1.3144e-02, -1.2355e-02,  9.8360e-03, -2.3588e-03,  2.1384e-04],\n",
      "        [-3.7368e-03, -1.2958e-02,  1.3797e-02,  1.6986e-03, -5.0609e-03],\n",
      "        [ 1.0151e-02, -8.9770e-03,  1.3574e-02, -2.4379e-03,  4.2377e-03],\n",
      "        [-1.8624e-02,  1.2475e-02, -2.5437e-02, -5.1108e-04,  1.2207e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 7.8643e-03, -7.7598e-05,  1.1690e-02, -1.5386e-02,  4.8702e-03],\n",
      "        [-1.1869e-02,  1.5869e-02, -1.9817e-02, -3.7342e-03, -2.5733e-03],\n",
      "        [-2.3254e-02, -1.8243e-02, -3.2150e-02,  5.5637e-03, -2.0785e-02],\n",
      "        [ 1.4539e-03,  2.5423e-02,  4.3881e-03, -1.1066e-02, -2.1868e-02],\n",
      "        [-1.4928e-02,  5.2936e-03, -3.0662e-03,  8.8016e-03,  1.1075e-02],\n",
      "        [-5.3941e-03, -5.4417e-03,  1.7082e-02, -2.1646e-03,  1.1938e-04],\n",
      "        [-5.0515e-05, -1.5935e-02, -1.3186e-02,  2.8305e-02,  1.8212e-02],\n",
      "        [ 7.2161e-03,  1.6413e-02, -4.2845e-02,  2.2024e-04,  8.1919e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0136, grad_fn=<MinBackward1>), tensor(0.8485, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.07769828289747238\n",
      "@sample 1205: tensor([[ 0.0103, -0.0112, -0.0045, -0.0032,  0.0027],\n",
      "        [-0.0228,  0.0073, -0.0139,  0.0075,  0.0061],\n",
      "        [-0.0064,  0.0056, -0.0024,  0.0143, -0.0055],\n",
      "        [-0.0029, -0.0026,  0.0019,  0.0197, -0.0138],\n",
      "        [ 0.0110, -0.0082, -0.0179,  0.0045,  0.0112],\n",
      "        [ 0.0176, -0.0141,  0.0036,  0.0184, -0.0074],\n",
      "        [ 0.0013, -0.0139, -0.0085,  0.0187,  0.0056],\n",
      "        [ 0.0104,  0.0277, -0.0059, -0.0101, -0.0009]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0018, -0.0087, -0.0015, -0.0225, -0.0018],\n",
      "        [ 0.0272, -0.0157,  0.0155, -0.0174, -0.0125],\n",
      "        [ 0.0086,  0.0196,  0.0104, -0.0110,  0.0170],\n",
      "        [-0.0011,  0.0301,  0.0139, -0.0006,  0.0124],\n",
      "        [-0.0098,  0.0053, -0.0311,  0.0048,  0.0003],\n",
      "        [-0.0035, -0.0005,  0.0115, -0.0294, -0.0074],\n",
      "        [ 0.0177,  0.0066,  0.0193, -0.0141, -0.0046],\n",
      "        [-0.0194,  0.0215,  0.0208, -0.0444,  0.0094]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0151, grad_fn=<MinBackward1>), tensor(0.9107, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09823258221149445\n",
      "@sample 1206: tensor([[ 0.0136, -0.0136,  0.0122,  0.0059,  0.0223],\n",
      "        [-0.0045, -0.0011,  0.0139, -0.0237, -0.0052],\n",
      "        [ 0.0090,  0.0245, -0.0245, -0.0285, -0.0100],\n",
      "        [ 0.0107, -0.0131, -0.0006, -0.0094,  0.0079],\n",
      "        [-0.0034,  0.0320,  0.0171, -0.0133, -0.0363],\n",
      "        [ 0.0022,  0.0257,  0.0206, -0.0254, -0.0019],\n",
      "        [ 0.0058,  0.0045, -0.0207, -0.0165,  0.0018],\n",
      "        [-0.0074, -0.0008, -0.0171,  0.0062,  0.0032]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0262, -0.0110,  0.0183, -0.0016, -0.0303],\n",
      "        [-0.0286,  0.0217,  0.0182, -0.0132, -0.0135],\n",
      "        [-0.0108, -0.0218, -0.0873,  0.0637,  0.0159],\n",
      "        [-0.0187,  0.0094, -0.0165, -0.0125, -0.0176],\n",
      "        [-0.0031, -0.0034, -0.0152, -0.0178,  0.0194],\n",
      "        [-0.0298, -0.0006, -0.0286,  0.0517, -0.0322],\n",
      "        [-0.0182, -0.0462, -0.0328, -0.0078, -0.0096],\n",
      "        [-0.0190, -0.0183, -0.0055, -0.0197,  0.0115]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0134, grad_fn=<MinBackward1>), tensor(0.8737, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09268941730260849\n",
      "@sample 1207: tensor([[-0.0098, -0.0058, -0.0154, -0.0044, -0.0003],\n",
      "        [-0.0003, -0.0107,  0.0123, -0.0064,  0.0071],\n",
      "        [ 0.0049,  0.0052, -0.0070,  0.0205,  0.0025],\n",
      "        [ 0.0038, -0.0298, -0.0081,  0.0055, -0.0069],\n",
      "        [ 0.0048,  0.0037,  0.0109, -0.0226,  0.0078],\n",
      "        [ 0.0237,  0.0126,  0.0122,  0.0145,  0.0082],\n",
      "        [-0.0087, -0.0077, -0.0049, -0.0026,  0.0038],\n",
      "        [-0.0153,  0.0083, -0.0068,  0.0016, -0.0071]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0146, -0.0261,  0.0241, -0.0177, -0.0139],\n",
      "        [-0.0127,  0.0107, -0.0126,  0.0439, -0.0164],\n",
      "        [-0.0128,  0.0120,  0.0002, -0.0215, -0.0035],\n",
      "        [-0.0083,  0.0068,  0.0140,  0.0187,  0.0053],\n",
      "        [-0.0166,  0.0026,  0.0037, -0.0144, -0.0073],\n",
      "        [ 0.0314,  0.0354, -0.0198,  0.0037,  0.0195],\n",
      "        [-0.0067,  0.0036, -0.0061,  0.0005,  0.0144],\n",
      "        [ 0.0027, -0.0075,  0.0062, -0.0091, -0.0036]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0134, grad_fn=<MinBackward1>), tensor(0.8449, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08679316937923431\n",
      "@sample 1208: tensor([[-0.0087,  0.0114, -0.0025,  0.0083, -0.0080],\n",
      "        [ 0.0146, -0.0074, -0.0070,  0.0341, -0.0268],\n",
      "        [-0.0013,  0.0074, -0.0057, -0.0055, -0.0181],\n",
      "        [ 0.0008,  0.0370,  0.0031, -0.0300,  0.0002],\n",
      "        [ 0.0130, -0.0017,  0.0157, -0.0039,  0.0118],\n",
      "        [-0.0071,  0.0002, -0.0119,  0.0006, -0.0063],\n",
      "        [ 0.0184, -0.0112, -0.0055,  0.0047,  0.0135],\n",
      "        [-0.0074, -0.0055, -0.0081,  0.0048,  0.0029]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0204,  0.0129,  0.0022, -0.0263, -0.0196],\n",
      "        [ 0.0211, -0.0015, -0.0264,  0.0184,  0.0304],\n",
      "        [ 0.0066, -0.0422,  0.0448, -0.0120,  0.0178],\n",
      "        [-0.0026, -0.0101, -0.0387,  0.0031, -0.0293],\n",
      "        [ 0.0006,  0.0053,  0.0316,  0.0279, -0.0020],\n",
      "        [-0.0095,  0.0148,  0.0271, -0.0018,  0.0172],\n",
      "        [ 0.0207,  0.0041,  0.0368,  0.0023, -0.0142],\n",
      "        [ 0.0044, -0.0385, -0.0138, -0.0126, -0.0102]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0094, grad_fn=<MinBackward1>), tensor(0.8737, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09354448318481445\n",
      "@sample 1209: tensor([[-0.0248, -0.0074,  0.0134, -0.0256,  0.0067],\n",
      "        [ 0.0085, -0.0164,  0.0090,  0.0143, -0.0154],\n",
      "        [ 0.0112, -0.0205,  0.0059,  0.0017, -0.0179],\n",
      "        [ 0.0141, -0.0364, -0.0217, -0.0026, -0.0077],\n",
      "        [-0.0060, -0.0058,  0.0094,  0.0207, -0.0226],\n",
      "        [-0.0086, -0.0002,  0.0051,  0.0039, -0.0085],\n",
      "        [-0.0247, -0.0079, -0.0022,  0.0008, -0.0007],\n",
      "        [-0.0026, -0.0123,  0.0069,  0.0085,  0.0005]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0146, -0.0648, -0.0003,  0.0078, -0.0010],\n",
      "        [ 0.0082,  0.0036,  0.0260, -0.0257,  0.0033],\n",
      "        [-0.0112, -0.0012,  0.0291,  0.0253,  0.0499],\n",
      "        [-0.0270, -0.0216,  0.0326, -0.0359, -0.0001],\n",
      "        [-0.0149, -0.0044,  0.0054,  0.0223,  0.0303],\n",
      "        [ 0.0156,  0.0049,  0.0111, -0.0026, -0.0156],\n",
      "        [ 0.0031,  0.0139,  0.0118, -0.0199, -0.0038],\n",
      "        [ 0.0185,  0.0154,  0.0139, -0.0167,  0.0111]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0108, grad_fn=<MinBackward1>), tensor(0.9147, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09458772093057632\n",
      "@sample 1210: tensor([[ 1.8470e-02, -6.7760e-03, -3.8164e-04,  2.3248e-03,  2.1358e-03],\n",
      "        [-9.1153e-05, -2.7758e-03, -1.5848e-02,  6.3725e-03, -2.9120e-02],\n",
      "        [ 7.7092e-03,  4.4519e-03,  2.9147e-03, -1.1759e-03,  1.0641e-02],\n",
      "        [ 1.1803e-02,  6.8112e-03,  1.7494e-02, -2.2400e-02,  6.4536e-03],\n",
      "        [ 2.7387e-03,  1.6655e-02, -1.1476e-02,  1.7794e-03,  4.2908e-03],\n",
      "        [ 2.6818e-02,  1.9915e-02,  2.6280e-02, -4.1839e-02,  7.4399e-03],\n",
      "        [-1.0699e-02, -5.5102e-04, -2.9079e-02,  1.2341e-02,  1.0692e-02],\n",
      "        [-1.5843e-03,  4.6307e-03, -2.1235e-02, -5.7646e-03, -2.1276e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 8.4677e-04,  1.0340e-02, -1.2811e-03,  1.1598e-02, -2.3057e-03],\n",
      "        [-3.5726e-03, -2.7093e-02,  2.0371e-02, -3.4394e-02,  1.4584e-02],\n",
      "        [ 1.7667e-04,  2.6251e-02, -1.7249e-02, -1.0450e-02,  1.2958e-02],\n",
      "        [-4.6730e-05,  2.7758e-02, -8.6331e-03,  9.6886e-03,  3.5522e-04],\n",
      "        [ 9.7239e-03, -8.0799e-03, -1.5728e-02, -5.8731e-03, -6.2637e-03],\n",
      "        [-1.9149e-02,  2.2568e-02,  2.9706e-02, -4.9710e-03,  4.0918e-03],\n",
      "        [ 1.9030e-02, -2.9174e-02,  3.0306e-02, -5.2405e-03, -1.0305e-02],\n",
      "        [ 5.8277e-03,  5.3885e-03,  3.8342e-02, -1.4695e-02,  1.4120e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0116, grad_fn=<MinBackward1>), tensor(0.8393, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08061361312866211\n",
      "@sample 1211: tensor([[ 0.0102,  0.0135, -0.0011,  0.0285, -0.0081],\n",
      "        [ 0.0076, -0.0009, -0.0053, -0.0079,  0.0018],\n",
      "        [ 0.0039, -0.0131, -0.0031,  0.0264, -0.0105],\n",
      "        [ 0.0179, -0.0199, -0.0015,  0.0198,  0.0026],\n",
      "        [ 0.0152, -0.0097, -0.0071,  0.0079, -0.0092],\n",
      "        [-0.0020, -0.0152,  0.0008,  0.0040, -0.0017],\n",
      "        [-0.0024, -0.0286,  0.0025,  0.0215, -0.0133],\n",
      "        [ 0.0191,  0.0159,  0.0180, -0.0087, -0.0020]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0218,  0.0513,  0.0091, -0.0212,  0.0091],\n",
      "        [ 0.0067, -0.0024, -0.0115,  0.0028,  0.0032],\n",
      "        [ 0.0155,  0.0191,  0.0292, -0.0171, -0.0012],\n",
      "        [ 0.0077,  0.0123,  0.0114, -0.0002, -0.0014],\n",
      "        [ 0.0148, -0.0364, -0.0142, -0.0076, -0.0007],\n",
      "        [-0.0235, -0.0295,  0.0220, -0.0352, -0.0290],\n",
      "        [-0.0105, -0.0185,  0.0141, -0.0195, -0.0030],\n",
      "        [-0.0109,  0.0101, -0.0353,  0.0180,  0.0074]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0142, grad_fn=<MinBackward1>), tensor(0.8123, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08725489675998688\n",
      "@sample 1212: tensor([[-0.0037,  0.0423,  0.0107, -0.0193,  0.0152],\n",
      "        [ 0.0198, -0.0045, -0.0149,  0.0239, -0.0017],\n",
      "        [-0.0158,  0.0082,  0.0085, -0.0016, -0.0156],\n",
      "        [-0.0067, -0.0278, -0.0055,  0.0330, -0.0199],\n",
      "        [-0.0084, -0.0005,  0.0086, -0.0032, -0.0056],\n",
      "        [ 0.0110, -0.0170,  0.0129,  0.0175,  0.0126],\n",
      "        [-0.0004,  0.0142,  0.0241, -0.0042,  0.0102],\n",
      "        [-0.0104,  0.0055, -0.0120,  0.0064, -0.0267]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0028, -0.0233, -0.0548,  0.0193, -0.0198],\n",
      "        [ 0.0212,  0.0385, -0.0109,  0.0071,  0.0092],\n",
      "        [-0.0144, -0.0145, -0.0228, -0.0060,  0.0155],\n",
      "        [ 0.0204, -0.0146, -0.0179, -0.0159, -0.0018],\n",
      "        [ 0.0043,  0.0037,  0.0231,  0.0020,  0.0075],\n",
      "        [-0.0281, -0.0222, -0.0047,  0.0063, -0.0113],\n",
      "        [ 0.0031,  0.0051, -0.0218,  0.0180, -0.0044],\n",
      "        [ 0.0229, -0.0002,  0.0335, -0.0271, -0.0055]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0137, grad_fn=<MinBackward1>), tensor(0.8123, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09279052913188934\n",
      "@sample 1213: tensor([[ 1.7081e-02,  1.3936e-02,  2.6413e-02, -2.2325e-02, -1.3549e-03],\n",
      "        [-1.1400e-02,  1.3618e-02, -3.0487e-04, -4.0865e-03, -1.3218e-03],\n",
      "        [ 1.3706e-02,  2.4262e-03, -1.1661e-02, -6.2013e-04, -2.4451e-03],\n",
      "        [-5.2424e-03,  2.0000e-02, -3.2582e-04, -2.8120e-02,  5.0520e-02],\n",
      "        [-3.2616e-03, -1.4784e-02,  2.2294e-03,  1.0424e-02,  9.5212e-03],\n",
      "        [-3.0668e-03, -2.2087e-02,  1.4171e-02,  9.6185e-03, -1.9374e-02],\n",
      "        [-2.5803e-03,  1.2416e-02,  6.3173e-03, -1.3114e-02,  4.7840e-05],\n",
      "        [-1.4903e-02,  5.3918e-03, -1.9925e-03, -4.5454e-03, -2.1366e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0045,  0.0246, -0.0488,  0.0002,  0.0294],\n",
      "        [-0.0025, -0.0001, -0.0157, -0.0162, -0.0406],\n",
      "        [-0.0060, -0.0160, -0.0189, -0.0009,  0.0011],\n",
      "        [-0.0071,  0.0110, -0.0369,  0.0319, -0.0062],\n",
      "        [ 0.0210,  0.0004,  0.0167, -0.0134, -0.0188],\n",
      "        [-0.0039,  0.0175,  0.0254, -0.0036,  0.0070],\n",
      "        [-0.0024,  0.0165, -0.0058, -0.0124, -0.0186],\n",
      "        [ 0.0004,  0.0103,  0.0071, -0.0126,  0.0251]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0131, grad_fn=<MinBackward1>), tensor(0.8492, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.0837729275226593\n",
      "@sample 1214: tensor([[-0.0074, -0.0102, -0.0037, -0.0086,  0.0102],\n",
      "        [ 0.0115, -0.0132,  0.0185,  0.0171, -0.0177],\n",
      "        [-0.0001, -0.0009,  0.0052, -0.0193, -0.0042],\n",
      "        [ 0.0093, -0.0129,  0.0095, -0.0068, -0.0055],\n",
      "        [ 0.0113, -0.0067,  0.0172,  0.0112, -0.0070],\n",
      "        [-0.0189,  0.0020,  0.0415,  0.0013,  0.0170],\n",
      "        [ 0.0006, -0.0207,  0.0101,  0.0020,  0.0179],\n",
      "        [ 0.0007,  0.0086, -0.0012,  0.0114, -0.0042]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0126, -0.0004, -0.0092,  0.0042,  0.0018],\n",
      "        [-0.0012,  0.0136, -0.0087,  0.0044,  0.0110],\n",
      "        [-0.0085,  0.0002,  0.0207, -0.0081, -0.0122],\n",
      "        [-0.0143, -0.0080,  0.0010,  0.0114, -0.0190],\n",
      "        [ 0.0008, -0.0025,  0.0207, -0.0074,  0.0146],\n",
      "        [ 0.0063,  0.0042, -0.0148,  0.0251, -0.0058],\n",
      "        [ 0.0215,  0.0033,  0.0386, -0.0110, -0.0082],\n",
      "        [ 0.0160,  0.0099,  0.0105, -0.0005, -0.0114]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0183, grad_fn=<MinBackward1>), tensor(0.8753, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08397699892520905\n",
      "@sample 1215: tensor([[-9.3910e-03, -6.6843e-03,  3.0217e-03, -1.1541e-02,  6.9267e-03],\n",
      "        [ 2.3420e-02, -9.8481e-03, -2.5830e-02,  6.8737e-03, -2.4143e-03],\n",
      "        [-8.0621e-03,  2.5836e-04,  3.7574e-02, -1.7313e-03, -2.6605e-03],\n",
      "        [-4.3037e-04, -1.8604e-02,  1.5140e-02, -1.1537e-02,  2.4011e-02],\n",
      "        [ 1.4673e-02, -1.0238e-02,  2.0673e-03,  7.9003e-03, -8.3386e-03],\n",
      "        [-1.4749e-03, -4.2379e-05,  9.3222e-04, -3.6644e-02,  2.5143e-03],\n",
      "        [-3.5852e-03, -4.7591e-02,  1.5013e-05,  7.7544e-03, -5.5432e-04],\n",
      "        [ 1.2997e-02, -1.6387e-02,  2.0966e-02,  4.7711e-03,  4.1578e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-4.7530e-03, -1.8650e-02,  2.7083e-02,  4.7806e-03, -1.5124e-02],\n",
      "        [ 1.9874e-02,  3.1419e-05, -1.8722e-02,  3.4151e-02,  2.7125e-02],\n",
      "        [ 1.4086e-02,  1.4026e-02,  4.5874e-02,  2.7962e-03, -1.8294e-02],\n",
      "        [-9.4863e-03,  4.2003e-03, -1.8544e-02,  1.9602e-02, -2.1462e-02],\n",
      "        [-1.0599e-02,  7.1768e-03, -1.4527e-02,  1.7625e-02, -9.4268e-05],\n",
      "        [ 3.7313e-02,  2.8527e-03, -8.7483e-03,  9.0560e-03,  8.4807e-03],\n",
      "        [ 1.7809e-02,  1.3225e-02,  3.7909e-02, -2.8647e-02, -5.3546e-04],\n",
      "        [-1.0077e-02, -7.0922e-03, -2.3293e-02,  2.7448e-02,  2.0187e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0124, grad_fn=<MinBackward1>), tensor(0.8404, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09580778330564499\n",
      "@sample 1216: tensor([[ 0.0065, -0.0141,  0.0020, -0.0085,  0.0020],\n",
      "        [ 0.0260, -0.0005, -0.0078,  0.0003,  0.0056],\n",
      "        [ 0.0222,  0.0437,  0.0100, -0.0265,  0.0140],\n",
      "        [-0.0139, -0.0126,  0.0143,  0.0254,  0.0071],\n",
      "        [ 0.0016, -0.0108,  0.0023, -0.0173,  0.0114],\n",
      "        [ 0.0029,  0.0102, -0.0047, -0.0021, -0.0069],\n",
      "        [ 0.0201, -0.0095,  0.0093, -0.0065,  0.0005],\n",
      "        [ 0.0027,  0.0020, -0.0005,  0.0067,  0.0164]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0004,  0.0152,  0.0238,  0.0085,  0.0134],\n",
      "        [ 0.0052, -0.0054, -0.0172,  0.0364,  0.0115],\n",
      "        [-0.0203,  0.0182, -0.0289,  0.0107,  0.0101],\n",
      "        [ 0.0242,  0.0407,  0.0138, -0.0078, -0.0105],\n",
      "        [-0.0057, -0.0108,  0.0145,  0.0180,  0.0018],\n",
      "        [ 0.0114,  0.0020,  0.0234,  0.0017,  0.0140],\n",
      "        [-0.0002, -0.0149,  0.0002,  0.0155, -0.0044],\n",
      "        [ 0.0364,  0.0182, -0.0199,  0.0114, -0.0054]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0140, grad_fn=<MinBackward1>), tensor(0.8491, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09167493134737015\n",
      "@sample 1217: tensor([[ 0.0061, -0.0097,  0.0124, -0.0049,  0.0004],\n",
      "        [-0.0329,  0.0341,  0.0055, -0.0128, -0.0019],\n",
      "        [ 0.0061, -0.0298,  0.0060,  0.0125, -0.0233],\n",
      "        [ 0.0094, -0.0150, -0.0140,  0.0020,  0.0010],\n",
      "        [ 0.0136,  0.0111,  0.0221,  0.0119, -0.0288],\n",
      "        [ 0.0045, -0.0141,  0.0053,  0.0089, -0.0122],\n",
      "        [ 0.0028,  0.0211, -0.0009, -0.0087,  0.0054],\n",
      "        [-0.0173, -0.0112,  0.0237, -0.0178, -0.0101]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0193,  0.0071, -0.0112,  0.0283, -0.0131],\n",
      "        [-0.0142,  0.0268, -0.0260, -0.0013, -0.0121],\n",
      "        [ 0.0021, -0.0244, -0.0037, -0.0032,  0.0148],\n",
      "        [-0.0070,  0.0041, -0.0240,  0.0195,  0.0190],\n",
      "        [-0.0002, -0.0032,  0.0050, -0.0080, -0.0194],\n",
      "        [ 0.0041,  0.0021,  0.0230, -0.0042,  0.0073],\n",
      "        [-0.0090,  0.0116, -0.0164, -0.0019, -0.0052],\n",
      "        [-0.0029, -0.0416,  0.0419,  0.0054, -0.0061]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0130, grad_fn=<MinBackward1>), tensor(0.8174, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08638159930706024\n",
      "@sample 1218: tensor([[-0.0032,  0.0158, -0.0244, -0.0125,  0.0098],\n",
      "        [-0.0070, -0.0054,  0.0116, -0.0334, -0.0003],\n",
      "        [-0.0264,  0.0143,  0.0120, -0.0061, -0.0112],\n",
      "        [-0.0013, -0.0093, -0.0239, -0.0054,  0.0285],\n",
      "        [-0.0045,  0.0040, -0.0097,  0.0029, -0.0177],\n",
      "        [ 0.0182, -0.0203, -0.0123,  0.0076, -0.0075],\n",
      "        [-0.0228,  0.0040,  0.0112,  0.0037, -0.0011],\n",
      "        [-0.0197,  0.0055,  0.0202, -0.0299,  0.0005]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0217, -0.0063, -0.0472,  0.0508,  0.0321],\n",
      "        [-0.0143,  0.0021,  0.0049,  0.0033, -0.0106],\n",
      "        [ 0.0013, -0.0236,  0.0084, -0.0109, -0.0053],\n",
      "        [-0.0009,  0.0130,  0.0055,  0.0042,  0.0178],\n",
      "        [-0.0181,  0.0213, -0.0127,  0.0060,  0.0017],\n",
      "        [ 0.0357,  0.0126, -0.0066,  0.0124,  0.0535],\n",
      "        [-0.0042,  0.0136,  0.0049,  0.0216, -0.0203],\n",
      "        [-0.0164, -0.0030, -0.0412,  0.0537,  0.0517]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0107, grad_fn=<MinBackward1>), tensor(0.8133, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08826150000095367\n",
      "@sample 1219: tensor([[-0.0200, -0.0223, -0.0025,  0.0089,  0.0093],\n",
      "        [-0.0281,  0.0048, -0.0169, -0.0039,  0.0116],\n",
      "        [-0.0227, -0.0009,  0.0012, -0.0084,  0.0034],\n",
      "        [-0.0056,  0.0080, -0.0050, -0.0008, -0.0009],\n",
      "        [ 0.0003, -0.0100, -0.0018,  0.0158, -0.0024],\n",
      "        [-0.0244,  0.0243,  0.0047, -0.0124,  0.0225],\n",
      "        [ 0.0179,  0.0034, -0.0061, -0.0126,  0.0173],\n",
      "        [ 0.0153, -0.0145,  0.0122,  0.0095,  0.0057]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0147, -0.0070,  0.0194, -0.0085,  0.0055],\n",
      "        [-0.0040, -0.0374,  0.0088, -0.0312, -0.0114],\n",
      "        [ 0.0042, -0.0032,  0.0183, -0.0115, -0.0098],\n",
      "        [-0.0048,  0.0216, -0.0064, -0.0118, -0.0072],\n",
      "        [ 0.0143, -0.0186,  0.0007,  0.0272,  0.0106],\n",
      "        [-0.0039,  0.0280, -0.0280,  0.0213, -0.0028],\n",
      "        [-0.0146, -0.0365, -0.0343,  0.0396, -0.0064],\n",
      "        [ 0.0114,  0.0169,  0.0100, -0.0067,  0.0089]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0100, grad_fn=<MinBackward1>), tensor(0.8437, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09470252692699432\n",
      "@sample 1220: tensor([[-0.0124,  0.0080,  0.0286, -0.0191,  0.0069],\n",
      "        [ 0.0016, -0.0070,  0.0022,  0.0139, -0.0093],\n",
      "        [ 0.0111, -0.0108, -0.0152,  0.0075, -0.0067],\n",
      "        [ 0.0180, -0.0054, -0.0157,  0.0157, -0.0119],\n",
      "        [ 0.0153, -0.0059, -0.0139,  0.0077, -0.0035],\n",
      "        [-0.0070,  0.0121, -0.0018,  0.0167,  0.0138],\n",
      "        [ 0.0116, -0.0098, -0.0271, -0.0098, -0.0050],\n",
      "        [-0.0172, -0.0088, -0.0092,  0.0118,  0.0058]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-2.1427e-02,  2.3900e-02, -6.5226e-02,  2.2317e-02,  5.2319e-03],\n",
      "        [ 1.0612e-02,  4.0439e-03,  2.9263e-02, -2.5755e-02, -8.8226e-03],\n",
      "        [ 1.5393e-02,  1.4708e-03, -8.0554e-03,  1.8871e-02,  1.6630e-02],\n",
      "        [ 1.8759e-02,  1.4000e-02, -2.1260e-02, -2.0886e-03,  2.0609e-02],\n",
      "        [ 9.4007e-03,  6.8611e-03,  5.0083e-05,  5.9049e-03,  4.0371e-04],\n",
      "        [ 1.6904e-02, -1.5635e-02, -1.5968e-02, -4.2408e-03, -2.0455e-02],\n",
      "        [-1.6441e-02, -7.4525e-03, -3.3002e-02,  1.0146e-02,  1.3738e-02],\n",
      "        [ 4.2439e-03,  4.9995e-03, -6.5095e-03,  1.9821e-02,  6.7451e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0138, grad_fn=<MinBackward1>), tensor(0.8445, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08862114697694778\n",
      "@sample 1221: tensor([[-0.0053, -0.0065, -0.0182, -0.0080, -0.0079],\n",
      "        [-0.0214,  0.0227, -0.0026, -0.0120,  0.0061],\n",
      "        [-0.0087, -0.0150,  0.0105, -0.0019,  0.0094],\n",
      "        [-0.0212, -0.0023, -0.0255, -0.0043,  0.0105],\n",
      "        [-0.0207, -0.0006,  0.0045, -0.0087,  0.0170],\n",
      "        [ 0.0214,  0.0258,  0.0082, -0.0159, -0.0005],\n",
      "        [-0.0034, -0.0019, -0.0121, -0.0190,  0.0010],\n",
      "        [-0.0084,  0.0116, -0.0121,  0.0033,  0.0153]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0205, -0.0029, -0.0134,  0.0026,  0.0057],\n",
      "        [ 0.0152, -0.0503, -0.0192, -0.0082,  0.0188],\n",
      "        [-0.0144, -0.0133, -0.0052,  0.0348,  0.0016],\n",
      "        [-0.0187, -0.0050,  0.0023,  0.0082, -0.0122],\n",
      "        [ 0.0003, -0.0194,  0.0041, -0.0113, -0.0383],\n",
      "        [-0.0417, -0.0020, -0.0079,  0.0131,  0.0066],\n",
      "        [-0.0321, -0.0180, -0.0209,  0.0129, -0.0065],\n",
      "        [ 0.0071,  0.0324,  0.0283, -0.0060,  0.0013]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0087, grad_fn=<MinBackward1>), tensor(0.8225, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09009616076946259\n",
      "@sample 1222: tensor([[-0.0083,  0.0201, -0.0261,  0.0147, -0.0109],\n",
      "        [-0.0008, -0.0106, -0.0108,  0.0046, -0.0192],\n",
      "        [ 0.0273, -0.0284, -0.0145,  0.0229, -0.0028],\n",
      "        [-0.0026,  0.0027, -0.0042, -0.0135,  0.0056],\n",
      "        [ 0.0069,  0.0328, -0.0225, -0.0337,  0.0295],\n",
      "        [ 0.0201,  0.0232, -0.0165, -0.0139, -0.0016],\n",
      "        [-0.0108,  0.0045, -0.0112, -0.0223,  0.0150],\n",
      "        [ 0.0006,  0.0139,  0.0157, -0.0174,  0.0064]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 2.1695e-03,  5.9724e-05, -1.9162e-02,  1.8153e-03,  2.9996e-02],\n",
      "        [-1.4791e-02, -4.1507e-03,  2.5833e-02, -1.6084e-02, -3.3789e-03],\n",
      "        [ 1.5094e-02,  1.1781e-02, -1.1194e-03, -1.1639e-02,  1.0844e-02],\n",
      "        [-1.3933e-02, -1.2988e-02, -2.0078e-02, -2.1142e-02, -1.7085e-02],\n",
      "        [-1.0152e-02, -2.2629e-02, -5.6524e-02,  1.5319e-02, -1.9545e-02],\n",
      "        [-6.9619e-03, -3.7426e-02, -3.4892e-02,  2.1851e-02,  1.4036e-02],\n",
      "        [-2.7839e-02, -3.5802e-03, -2.1129e-02,  1.5970e-02,  1.1523e-02],\n",
      "        [-3.1873e-02, -5.5130e-03, -3.7163e-02,  2.6850e-02, -1.3519e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0116, grad_fn=<MinBackward1>), tensor(0.8611, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08165339380502701\n",
      "@sample 1223: tensor([[-0.0084,  0.0147, -0.0160,  0.0120, -0.0095],\n",
      "        [ 0.0195, -0.0055, -0.0066,  0.0082,  0.0106],\n",
      "        [-0.0123, -0.0168,  0.0018,  0.0014,  0.0068],\n",
      "        [-0.0091, -0.0047, -0.0144,  0.0051, -0.0026],\n",
      "        [ 0.0082, -0.0116, -0.0251, -0.0016,  0.0092],\n",
      "        [-0.0136,  0.0144,  0.0208, -0.0020, -0.0041],\n",
      "        [ 0.0120, -0.0034, -0.0330,  0.0106,  0.0048],\n",
      "        [-0.0313,  0.0243, -0.0132,  0.0143,  0.0043]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0049,  0.0108, -0.0283,  0.0085, -0.0156],\n",
      "        [ 0.0143, -0.0092, -0.0161,  0.0260,  0.0270],\n",
      "        [ 0.0036, -0.0235,  0.0172, -0.0055,  0.0112],\n",
      "        [ 0.0168,  0.0130,  0.0288, -0.0227, -0.0085],\n",
      "        [-0.0148, -0.0199, -0.0132,  0.0348,  0.0042],\n",
      "        [-0.0069, -0.0060, -0.0287,  0.0312,  0.0055],\n",
      "        [-0.0131,  0.0169, -0.0011,  0.0124,  0.0260],\n",
      "        [-0.0043,  0.0144, -0.0095, -0.0213, -0.0349]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0119, grad_fn=<MinBackward1>), tensor(0.8294, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09997532516717911\n",
      "@sample 1224: tensor([[-5.7879e-03,  9.6410e-03, -3.4571e-03, -4.7375e-03, -1.3006e-02],\n",
      "        [ 3.3431e-03, -1.8165e-05,  4.8928e-03, -2.4100e-02,  6.4942e-03],\n",
      "        [-4.9393e-03,  7.7221e-03,  3.9596e-03, -1.2367e-03,  1.5170e-02],\n",
      "        [ 1.3308e-03,  1.7691e-02, -1.7299e-02, -1.2733e-02,  1.5464e-03],\n",
      "        [ 1.3928e-02,  1.4085e-02, -3.2677e-03, -2.8497e-03,  3.3942e-03],\n",
      "        [-2.7060e-03, -3.6847e-03,  7.5973e-03, -1.0054e-02,  8.2379e-03],\n",
      "        [-1.3930e-02, -1.4255e-02, -8.5682e-07, -5.2102e-03,  2.0021e-02],\n",
      "        [ 6.7594e-03,  1.3551e-02, -1.2451e-02,  1.6034e-03,  1.7247e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0158,  0.0076, -0.0162,  0.0103, -0.0026],\n",
      "        [ 0.0005, -0.0163,  0.0007, -0.0098,  0.0118],\n",
      "        [ 0.0173,  0.0115,  0.0055,  0.0019, -0.0233],\n",
      "        [ 0.0124,  0.0233,  0.0162,  0.0113,  0.0089],\n",
      "        [-0.0054,  0.0193,  0.0209,  0.0087,  0.0137],\n",
      "        [ 0.0016, -0.0140, -0.0594, -0.0186, -0.0239],\n",
      "        [-0.0134, -0.0396,  0.0267, -0.0203, -0.0129],\n",
      "        [ 0.0191,  0.0241,  0.0119,  0.0065, -0.0015]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0115, grad_fn=<MinBackward1>), tensor(0.8181, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08756303787231445\n",
      "@sample 1225: tensor([[ 0.0004,  0.0168,  0.0038, -0.0136,  0.0106],\n",
      "        [ 0.0005, -0.0003, -0.0085,  0.0091,  0.0132],\n",
      "        [ 0.0195,  0.0016, -0.0087, -0.0179, -0.0027],\n",
      "        [-0.0014,  0.0084, -0.0298,  0.0321,  0.0004],\n",
      "        [-0.0123, -0.0029, -0.0195, -0.0025,  0.0054],\n",
      "        [ 0.0122,  0.0387,  0.0381, -0.0158,  0.0044],\n",
      "        [ 0.0023, -0.0022,  0.0102, -0.0051, -0.0238],\n",
      "        [-0.0151,  0.0108,  0.0003, -0.0037,  0.0093]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0085,  0.0185,  0.0232, -0.0017, -0.0008],\n",
      "        [ 0.0244,  0.0062,  0.0019, -0.0084,  0.0059],\n",
      "        [ 0.0078, -0.0051,  0.0130, -0.0069, -0.0092],\n",
      "        [ 0.0475,  0.0316, -0.0332, -0.0221,  0.0209],\n",
      "        [ 0.0042, -0.0143, -0.0177, -0.0114, -0.0011],\n",
      "        [-0.0194,  0.0206, -0.0059, -0.0254, -0.0471],\n",
      "        [ 0.0025,  0.0178, -0.0130,  0.0118,  0.0308],\n",
      "        [-0.0460, -0.0574, -0.0161,  0.0077, -0.0423]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0168, grad_fn=<MinBackward1>), tensor(0.8741, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09777801483869553\n",
      "@sample 1226: tensor([[ 0.0147, -0.0001,  0.0060,  0.0067, -0.0234],\n",
      "        [ 0.0138, -0.0042,  0.0054, -0.0156,  0.0048],\n",
      "        [ 0.0137, -0.0139, -0.0230,  0.0154,  0.0022],\n",
      "        [-0.0078,  0.0054, -0.0089,  0.0048, -0.0029],\n",
      "        [-0.0037, -0.0114, -0.0021,  0.0232, -0.0063],\n",
      "        [-0.0039, -0.0030, -0.0181, -0.0132,  0.0012],\n",
      "        [-0.0046, -0.0032, -0.0025, -0.0038,  0.0263],\n",
      "        [ 0.0077,  0.0067,  0.0324, -0.0124,  0.0133]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0092,  0.0203,  0.0199, -0.0304, -0.0251],\n",
      "        [-0.0083,  0.0227,  0.0084, -0.0132, -0.0082],\n",
      "        [ 0.0035,  0.0130, -0.0021,  0.0126, -0.0069],\n",
      "        [ 0.0112,  0.0290, -0.0381,  0.0134,  0.0149],\n",
      "        [ 0.0255,  0.0166,  0.0283, -0.0191, -0.0217],\n",
      "        [ 0.0001, -0.0017,  0.0015, -0.0011,  0.0050],\n",
      "        [ 0.0060, -0.0030,  0.0423, -0.0406, -0.0125],\n",
      "        [ 0.0049,  0.0042, -0.0197, -0.0078, -0.0311]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0102, grad_fn=<MinBackward1>), tensor(0.7988, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09319538623094559\n",
      "@sample 1227: tensor([[-0.0159,  0.0055, -0.0089,  0.0064, -0.0076],\n",
      "        [ 0.0100,  0.0190, -0.0027, -0.0148,  0.0181],\n",
      "        [-0.0041,  0.0152,  0.0096, -0.0028,  0.0029],\n",
      "        [-0.0090,  0.0031, -0.0035, -0.0150,  0.0241],\n",
      "        [ 0.0190, -0.0136, -0.0092,  0.0117, -0.0012],\n",
      "        [ 0.0065, -0.0176,  0.0143,  0.0250, -0.0096],\n",
      "        [-0.0028, -0.0199,  0.0055,  0.0074,  0.0051],\n",
      "        [-0.0054,  0.0421, -0.0050, -0.0028, -0.0044]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0274,  0.0036,  0.0102,  0.0080, -0.0195],\n",
      "        [-0.0008, -0.0184, -0.0288,  0.0090, -0.0202],\n",
      "        [-0.0053,  0.0128, -0.0092, -0.0108,  0.0016],\n",
      "        [-0.0047, -0.0203,  0.0285, -0.0133, -0.0238],\n",
      "        [-0.0080, -0.0023,  0.0033, -0.0072,  0.0062],\n",
      "        [ 0.0032,  0.0318, -0.0060,  0.0072,  0.0012],\n",
      "        [ 0.0244, -0.0109,  0.0045,  0.0242,  0.0052],\n",
      "        [ 0.0279,  0.0345,  0.0020, -0.0031, -0.0148]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0120, grad_fn=<MinBackward1>), tensor(0.8345, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08640455454587936\n",
      "@sample 1228: tensor([[-0.0062,  0.0043, -0.0055,  0.0142,  0.0214],\n",
      "        [ 0.0048, -0.0254, -0.0135,  0.0012,  0.0049],\n",
      "        [ 0.0167, -0.0044,  0.0057,  0.0061, -0.0242],\n",
      "        [ 0.0114,  0.0079,  0.0050,  0.0178, -0.0040],\n",
      "        [-0.0064,  0.0104,  0.0038, -0.0009, -0.0102],\n",
      "        [ 0.0028,  0.0041,  0.0037,  0.0016,  0.0035],\n",
      "        [-0.0028,  0.0090,  0.0170, -0.0018,  0.0064],\n",
      "        [ 0.0026,  0.0158,  0.0126,  0.0157, -0.0173]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0256,  0.0187,  0.0301,  0.0145,  0.0101],\n",
      "        [-0.0025,  0.0187, -0.0121,  0.0017, -0.0085],\n",
      "        [ 0.0008,  0.0252, -0.0211,  0.0160,  0.0157],\n",
      "        [ 0.0033, -0.0046,  0.0084, -0.0235, -0.0244],\n",
      "        [-0.0339,  0.0228, -0.0081,  0.0106,  0.0023],\n",
      "        [ 0.0039, -0.0132, -0.0049, -0.0049, -0.0299],\n",
      "        [-0.0033,  0.0245, -0.0101, -0.0221, -0.0242],\n",
      "        [ 0.0134, -0.0007,  0.0036, -0.0117, -0.0180]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0158, grad_fn=<MinBackward1>), tensor(0.8612, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08950389176607132\n",
      "@sample 1229: tensor([[ 6.3400e-03,  1.4267e-02,  3.3600e-03, -2.3972e-02,  4.7284e-03],\n",
      "        [ 3.6680e-02, -7.3670e-03,  2.0788e-03,  9.1346e-03, -1.8719e-02],\n",
      "        [ 7.0759e-04, -1.5838e-02, -5.0954e-03, -1.4273e-03,  1.7944e-02],\n",
      "        [ 2.1406e-02, -1.3444e-02,  3.3565e-02, -6.2961e-03, -5.9050e-03],\n",
      "        [-4.5082e-03, -1.3755e-02, -2.6629e-03, -3.2502e-03, -3.7648e-03],\n",
      "        [ 4.1720e-02, -1.0006e-02,  1.7611e-02,  1.0803e-02,  4.8049e-05],\n",
      "        [ 2.2665e-02,  2.3955e-02,  1.3722e-02, -4.2927e-02,  2.9494e-02],\n",
      "        [-1.4704e-02,  1.7839e-02,  1.8297e-02,  2.7469e-03, -2.1431e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0202, -0.0090,  0.0136,  0.0081, -0.0010],\n",
      "        [ 0.0088,  0.0121, -0.0172,  0.0173,  0.0233],\n",
      "        [-0.0148,  0.0171, -0.0116, -0.0065, -0.0186],\n",
      "        [ 0.0125,  0.0096, -0.0071, -0.0203, -0.0366],\n",
      "        [-0.0017, -0.0115, -0.0118, -0.0466, -0.0064],\n",
      "        [-0.0166,  0.0218, -0.0332,  0.0350,  0.0119],\n",
      "        [-0.0242, -0.0342, -0.0545,  0.0300, -0.0159],\n",
      "        [-0.0059, -0.0003, -0.0118, -0.0440, -0.0300]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0106, grad_fn=<MinBackward1>), tensor(0.8476, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08244096487760544\n",
      "@sample 1230: tensor([[ 0.0009,  0.0067,  0.0473, -0.0083, -0.0101],\n",
      "        [ 0.0049, -0.0024, -0.0044, -0.0171,  0.0157],\n",
      "        [ 0.0061,  0.0115, -0.0173,  0.0035,  0.0003],\n",
      "        [-0.0034,  0.0077, -0.0027,  0.0206,  0.0091],\n",
      "        [-0.0065,  0.0048, -0.0044,  0.0056, -0.0065],\n",
      "        [ 0.0122,  0.0122, -0.0071, -0.0231,  0.0036],\n",
      "        [-0.0171, -0.0083, -0.0103,  0.0035, -0.0021],\n",
      "        [ 0.0053,  0.0157,  0.0277,  0.0031, -0.0129]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0174,  0.0299,  0.0132,  0.0149,  0.0265],\n",
      "        [-0.0141,  0.0056, -0.0277, -0.0166,  0.0150],\n",
      "        [ 0.0055,  0.0118, -0.0242,  0.0115,  0.0030],\n",
      "        [ 0.0279,  0.0432,  0.0118, -0.0194, -0.0126],\n",
      "        [ 0.0129,  0.0135, -0.0169, -0.0035, -0.0099],\n",
      "        [-0.0300,  0.0301, -0.0640,  0.0303,  0.0105],\n",
      "        [ 0.0152, -0.0048,  0.0272, -0.0084,  0.0137],\n",
      "        [ 0.0226,  0.0033,  0.0291, -0.0032, -0.0037]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0141, grad_fn=<MinBackward1>), tensor(0.8091, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09674172848463058\n",
      "@sample 1231: tensor([[ 0.0045,  0.0063, -0.0006,  0.0051, -0.0167],\n",
      "        [ 0.0008,  0.0160,  0.0229, -0.0074,  0.0161],\n",
      "        [ 0.0020,  0.0141, -0.0125, -0.0050, -0.0116],\n",
      "        [ 0.0122, -0.0120, -0.0327,  0.0259, -0.0092],\n",
      "        [ 0.0021, -0.0050, -0.0023, -0.0147,  0.0068],\n",
      "        [ 0.0114, -0.0281, -0.0181,  0.0161, -0.0177],\n",
      "        [-0.0001, -0.0093,  0.0047, -0.0116,  0.0080],\n",
      "        [ 0.0038, -0.0064,  0.0105,  0.0282, -0.0333]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0160,  0.0092,  0.0196,  0.0009,  0.0282],\n",
      "        [ 0.0007,  0.0026,  0.0165, -0.0167, -0.0379],\n",
      "        [ 0.0101,  0.0045,  0.0053, -0.0133, -0.0059],\n",
      "        [-0.0013,  0.0302,  0.0056, -0.0216,  0.0221],\n",
      "        [-0.0210, -0.0356,  0.0205, -0.0230, -0.0378],\n",
      "        [-0.0006,  0.0124,  0.0271, -0.0258,  0.0167],\n",
      "        [-0.0040, -0.0037, -0.0165, -0.0103, -0.0073],\n",
      "        [ 0.0067,  0.0128,  0.0180,  0.0125,  0.0009]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0121, grad_fn=<MinBackward1>), tensor(0.8300, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09617164731025696\n",
      "@sample 1232: tensor([[ 0.0203, -0.0107, -0.0235,  0.0336, -0.0298],\n",
      "        [ 0.0128,  0.0036,  0.0094,  0.0002, -0.0142],\n",
      "        [-0.0102, -0.0026,  0.0015,  0.0071, -0.0125],\n",
      "        [ 0.0061, -0.0160,  0.0110, -0.0142, -0.0088],\n",
      "        [ 0.0126, -0.0144,  0.0076,  0.0024, -0.0013],\n",
      "        [ 0.0039, -0.0073, -0.0126,  0.0135, -0.0141],\n",
      "        [ 0.0059, -0.0054, -0.0011, -0.0063, -0.0010],\n",
      "        [ 0.0041, -0.0213,  0.0038,  0.0286, -0.0393]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0025,  0.0152,  0.0100,  0.0018,  0.0074],\n",
      "        [-0.0066, -0.0154,  0.0163, -0.0003,  0.0017],\n",
      "        [ 0.0049,  0.0214,  0.0134,  0.0003,  0.0141],\n",
      "        [-0.0254, -0.0002, -0.0148,  0.0131,  0.0141],\n",
      "        [-0.0052,  0.0169, -0.0383,  0.0143,  0.0158],\n",
      "        [ 0.0025,  0.0108, -0.0160,  0.0007,  0.0294],\n",
      "        [ 0.0222, -0.0326,  0.0120, -0.0284, -0.0102],\n",
      "        [ 0.0029,  0.0085, -0.0220, -0.0212,  0.0111]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0117, grad_fn=<MinBackward1>), tensor(0.8126, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09108470380306244\n",
      "@sample 1233: tensor([[-0.0044,  0.0092,  0.0174, -0.0067,  0.0035],\n",
      "        [ 0.0050, -0.0140,  0.0056, -0.0039, -0.0077],\n",
      "        [ 0.0067, -0.0145, -0.0148, -0.0123, -0.0005],\n",
      "        [ 0.0049, -0.0122,  0.0132, -0.0097,  0.0098],\n",
      "        [ 0.0113, -0.0354, -0.0017,  0.0130, -0.0110],\n",
      "        [-0.0030, -0.0098,  0.0060, -0.0010, -0.0160],\n",
      "        [-0.0060, -0.0181,  0.0116, -0.0135, -0.0013],\n",
      "        [ 0.0078, -0.0067, -0.0062,  0.0059, -0.0116]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0090, -0.0131,  0.0079, -0.0195, -0.0187],\n",
      "        [ 0.0255, -0.0044,  0.0446, -0.0190,  0.0002],\n",
      "        [-0.0073, -0.0184,  0.0053, -0.0117,  0.0078],\n",
      "        [ 0.0014,  0.0087, -0.0007, -0.0214, -0.0339],\n",
      "        [-0.0016,  0.0063,  0.0364, -0.0212,  0.0084],\n",
      "        [-0.0188,  0.0045,  0.0085, -0.0149,  0.0075],\n",
      "        [-0.0146,  0.0172,  0.0117, -0.0077, -0.0164],\n",
      "        [ 0.0032, -0.0126,  0.0307,  0.0078, -0.0020]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0131, grad_fn=<MinBackward1>), tensor(0.8277, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08171717822551727\n",
      "@sample 1234: tensor([[-0.0102,  0.0081,  0.0018,  0.0127, -0.0199],\n",
      "        [-0.0109, -0.0207, -0.0102,  0.0027,  0.0118],\n",
      "        [-0.0010, -0.0136,  0.0129,  0.0119, -0.0042],\n",
      "        [-0.0010, -0.0155,  0.0149, -0.0106, -0.0104],\n",
      "        [-0.0126, -0.0104, -0.0007, -0.0113,  0.0060],\n",
      "        [-0.0014,  0.0166,  0.0084, -0.0120, -0.0043],\n",
      "        [ 0.0060, -0.0030, -0.0263,  0.0090, -0.0020],\n",
      "        [-0.0071,  0.0038,  0.0116,  0.0035, -0.0201]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0009,  0.0192,  0.0201, -0.0308, -0.0274],\n",
      "        [ 0.0087, -0.0075, -0.0073,  0.0004,  0.0086],\n",
      "        [ 0.0028,  0.0306,  0.0139, -0.0211, -0.0301],\n",
      "        [-0.0002, -0.0405,  0.0274, -0.0189, -0.0132],\n",
      "        [-0.0143, -0.0349, -0.0197,  0.0159,  0.0443],\n",
      "        [ 0.0147,  0.0358, -0.0197,  0.0141, -0.0026],\n",
      "        [ 0.0256,  0.0159,  0.0141, -0.0224,  0.0094],\n",
      "        [-0.0206,  0.0292, -0.0151,  0.0081,  0.0256]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0122, grad_fn=<MinBackward1>), tensor(0.8381, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09632454067468643\n",
      "@sample 1235: tensor([[-0.0085, -0.0248, -0.0090,  0.0096, -0.0103],\n",
      "        [-0.0007,  0.0135,  0.0032, -0.0218,  0.0141],\n",
      "        [-0.0008, -0.0212,  0.0038, -0.0257,  0.0316],\n",
      "        [ 0.0004, -0.0119,  0.0058, -0.0221, -0.0011],\n",
      "        [-0.0075, -0.0341, -0.0022, -0.0060, -0.0059],\n",
      "        [-0.0003,  0.0080,  0.0025, -0.0206,  0.0004],\n",
      "        [ 0.0093, -0.0107, -0.0148, -0.0069, -0.0021],\n",
      "        [ 0.0144,  0.0088,  0.0247,  0.0115, -0.0149]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0157, -0.0315,  0.0092,  0.0007,  0.0284],\n",
      "        [-0.0188, -0.0116, -0.0424, -0.0038, -0.0135],\n",
      "        [-0.0312, -0.0191, -0.0141, -0.0061, -0.0173],\n",
      "        [-0.0342, -0.0324,  0.0184,  0.0010,  0.0154],\n",
      "        [ 0.0124, -0.0274,  0.0094, -0.0028,  0.0029],\n",
      "        [-0.0186, -0.0143,  0.0317,  0.0041,  0.0155],\n",
      "        [-0.0084, -0.0282, -0.0065,  0.0067,  0.0145],\n",
      "        [ 0.0048,  0.0132,  0.0190, -0.0101, -0.0140]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0132, grad_fn=<MinBackward1>), tensor(0.8489, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09537193179130554\n",
      "@sample 1236: tensor([[ 0.0165,  0.0232,  0.0293, -0.0318,  0.0194],\n",
      "        [ 0.0128,  0.0345,  0.0105, -0.0383,  0.0095],\n",
      "        [-0.0057,  0.0103,  0.0087, -0.0131, -0.0105],\n",
      "        [ 0.0034,  0.0050,  0.0032, -0.0171, -0.0025],\n",
      "        [-0.0072,  0.0017,  0.0150, -0.0019,  0.0133],\n",
      "        [-0.0171, -0.0093,  0.0011,  0.0005,  0.0123],\n",
      "        [ 0.0181,  0.0055, -0.0034, -0.0088, -0.0054],\n",
      "        [-0.0058, -0.0120,  0.0037, -0.0294, -0.0019]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0097,  0.0011, -0.0220,  0.0426, -0.0080],\n",
      "        [-0.0299, -0.0205, -0.0536,  0.0304, -0.0091],\n",
      "        [-0.0034, -0.0114,  0.0192, -0.0007,  0.0226],\n",
      "        [-0.0157, -0.0037,  0.0328, -0.0072,  0.0106],\n",
      "        [-0.0029,  0.0087, -0.0056,  0.0028,  0.0036],\n",
      "        [-0.0115, -0.0750, -0.0080,  0.0133,  0.0025],\n",
      "        [ 0.0052,  0.0054,  0.0203,  0.0224,  0.0082],\n",
      "        [-0.0045,  0.0081, -0.0167,  0.0043, -0.0056]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0089, grad_fn=<MinBackward1>), tensor(0.8054, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.0794353187084198\n",
      "@sample 1237: tensor([[-0.0212,  0.0029,  0.0115, -0.0105,  0.0057],\n",
      "        [-0.0099, -0.0032, -0.0077,  0.0037, -0.0103],\n",
      "        [-0.0074,  0.0043,  0.0218, -0.0006,  0.0061],\n",
      "        [-0.0077,  0.0161, -0.0141,  0.0056,  0.0080],\n",
      "        [-0.0088, -0.0069, -0.0064,  0.0167, -0.0134],\n",
      "        [-0.0412, -0.0065, -0.0062,  0.0039, -0.0034],\n",
      "        [ 0.0041, -0.0008,  0.0443, -0.0204, -0.0039],\n",
      "        [-0.0139, -0.0112, -0.0121,  0.0027,  0.0118]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-9.1522e-03,  9.0978e-03,  1.5551e-02, -4.3679e-03,  1.0831e-03],\n",
      "        [-2.3576e-02, -2.0075e-02, -1.7465e-02,  7.4622e-03,  1.0556e-02],\n",
      "        [-2.9085e-02, -5.3435e-03, -1.9956e-02,  3.8672e-02, -2.9936e-05],\n",
      "        [ 9.2667e-03, -2.5849e-02,  1.8582e-03, -2.9214e-02,  1.3636e-02],\n",
      "        [ 4.2567e-03, -1.8798e-02, -5.5263e-03, -6.7796e-03, -1.2106e-02],\n",
      "        [-1.4142e-02, -5.3242e-02,  2.3127e-03, -1.5006e-02,  1.0431e-02],\n",
      "        [-1.3510e-02, -2.6549e-02, -4.9845e-03, -1.4730e-03, -2.2809e-02],\n",
      "        [ 8.1891e-03,  1.2092e-02, -1.1234e-02,  1.2057e-02,  6.0377e-03]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0071, grad_fn=<MinBackward1>), tensor(0.8550, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09589488804340363\n",
      "@sample 1238: tensor([[-0.0194, -0.0004, -0.0076, -0.0116,  0.0153],\n",
      "        [-0.0174,  0.0134, -0.0418, -0.0208,  0.0081],\n",
      "        [-0.0076, -0.0209, -0.0103,  0.0239, -0.0225],\n",
      "        [ 0.0084,  0.0020, -0.0032, -0.0009, -0.0141],\n",
      "        [-0.0006, -0.0177, -0.0150, -0.0026, -0.0098],\n",
      "        [ 0.0102, -0.0064, -0.0014,  0.0021, -0.0263],\n",
      "        [-0.0083, -0.0169, -0.0020, -0.0050,  0.0232],\n",
      "        [-0.0134, -0.0026,  0.0141, -0.0069, -0.0140]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0163, -0.0248,  0.0400, -0.0270, -0.0273],\n",
      "        [ 0.0089, -0.0448,  0.0062,  0.0163,  0.0067],\n",
      "        [-0.0173, -0.0260,  0.0101, -0.0127,  0.0067],\n",
      "        [-0.0131,  0.0059,  0.0033, -0.0021, -0.0162],\n",
      "        [-0.0194, -0.0122,  0.0454, -0.0058,  0.0214],\n",
      "        [-0.0057, -0.0046,  0.0039,  0.0196,  0.0191],\n",
      "        [ 0.0044, -0.0197,  0.0491, -0.0173, -0.0132],\n",
      "        [ 0.0081, -0.0329,  0.0004, -0.0096, -0.0240]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0115, grad_fn=<MinBackward1>), tensor(0.7922, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.0991993322968483\n",
      "@sample 1239: tensor([[-8.1505e-03,  1.9162e-02,  2.4383e-02, -1.0066e-02,  3.0305e-03],\n",
      "        [-2.8555e-02,  1.4124e-02,  4.6984e-04, -2.4909e-02,  5.0911e-03],\n",
      "        [-1.3856e-02,  5.4295e-03, -1.1117e-02,  9.7581e-03,  9.4109e-03],\n",
      "        [-6.2548e-03,  1.7476e-02, -1.0644e-02, -1.0531e-02, -3.2793e-03],\n",
      "        [-2.3967e-03, -2.3678e-05,  5.6766e-03, -1.6558e-02,  1.6359e-02],\n",
      "        [-8.2356e-03,  6.1497e-03,  1.3228e-04, -1.0875e-02,  6.9547e-03],\n",
      "        [-2.1479e-02,  5.9272e-03, -1.6396e-02, -1.8395e-02,  1.3298e-02],\n",
      "        [-2.5030e-02,  1.8290e-02,  5.9521e-03, -3.5369e-03, -3.8541e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0205, -0.0203,  0.0244,  0.0337,  0.0099],\n",
      "        [-0.0102, -0.0152, -0.0242,  0.0270,  0.0173],\n",
      "        [ 0.0058,  0.0043, -0.0071, -0.0028,  0.0005],\n",
      "        [ 0.0016, -0.0011, -0.0199,  0.0078, -0.0185],\n",
      "        [-0.0151, -0.0294, -0.0195,  0.0267,  0.0064],\n",
      "        [-0.0059,  0.0058,  0.0067,  0.0222,  0.0089],\n",
      "        [-0.0093, -0.0233,  0.0266, -0.0128, -0.0379],\n",
      "        [ 0.0085,  0.0200,  0.0126, -0.0156, -0.0232]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0057, grad_fn=<MinBackward1>), tensor(0.8712, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.07747375965118408\n",
      "@sample 1240: tensor([[-0.0188,  0.0076,  0.0036, -0.0293,  0.0241],\n",
      "        [-0.0240,  0.0062, -0.0082,  0.0132, -0.0002],\n",
      "        [ 0.0106, -0.0358, -0.0182,  0.0098,  0.0075],\n",
      "        [-0.0211,  0.0098, -0.0080, -0.0051, -0.0047],\n",
      "        [-0.0128, -0.0108,  0.0212,  0.0009,  0.0122],\n",
      "        [-0.0120, -0.0078, -0.0061, -0.0236,  0.0192],\n",
      "        [ 0.0100,  0.0335,  0.0253, -0.0424,  0.0215],\n",
      "        [-0.0014, -0.0077,  0.0176, -0.0030,  0.0052]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0054,  0.0211,  0.0241, -0.0074,  0.0037],\n",
      "        [ 0.0315,  0.0125, -0.0217,  0.0175,  0.0116],\n",
      "        [ 0.0173, -0.0005,  0.0048, -0.0222, -0.0157],\n",
      "        [ 0.0136, -0.0133,  0.0197,  0.0049,  0.0092],\n",
      "        [ 0.0123, -0.0152,  0.0098, -0.0134, -0.0152],\n",
      "        [-0.0366, -0.0512, -0.0096,  0.0018, -0.0022],\n",
      "        [-0.0249, -0.0184, -0.0490,  0.0590, -0.0117],\n",
      "        [-0.0024,  0.0146, -0.0289,  0.0183,  0.0016]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0133, grad_fn=<MinBackward1>), tensor(0.8906, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.103491872549057\n",
      "@sample 1241: tensor([[ 0.0042,  0.0163, -0.0052, -0.0188,  0.0021],\n",
      "        [-0.0006,  0.0244, -0.0024, -0.0026,  0.0018],\n",
      "        [-0.0027, -0.0206,  0.0087,  0.0021, -0.0122],\n",
      "        [-0.0104,  0.0102,  0.0049, -0.0036,  0.0091],\n",
      "        [ 0.0071, -0.0056,  0.0213,  0.0118,  0.0249],\n",
      "        [-0.0037,  0.0051,  0.0152,  0.0317, -0.0260],\n",
      "        [ 0.0237, -0.0114, -0.0181,  0.0208,  0.0061],\n",
      "        [-0.0128,  0.0263, -0.0166,  0.0030, -0.0275]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0018,  0.0110,  0.0148, -0.0571, -0.0462],\n",
      "        [ 0.0042,  0.0026, -0.0088,  0.0030,  0.0049],\n",
      "        [-0.0057, -0.0006,  0.0183, -0.0001, -0.0055],\n",
      "        [-0.0119,  0.0076, -0.0179, -0.0006, -0.0035],\n",
      "        [ 0.0175,  0.0002, -0.0351, -0.0076, -0.0254],\n",
      "        [ 0.0116, -0.0089,  0.0032,  0.0160,  0.0073],\n",
      "        [ 0.0124, -0.0226,  0.0026,  0.0095, -0.0074],\n",
      "        [-0.0038, -0.0020, -0.0022,  0.0247,  0.0108]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0102, grad_fn=<MinBackward1>), tensor(0.8206, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08784616738557816\n",
      "@sample 1242: tensor([[-0.0075,  0.0200,  0.0031,  0.0139,  0.0032],\n",
      "        [ 0.0008, -0.0002,  0.0069,  0.0444, -0.0135],\n",
      "        [ 0.0029, -0.0254, -0.0013,  0.0155,  0.0154],\n",
      "        [ 0.0076, -0.0081,  0.0164,  0.0070, -0.0099],\n",
      "        [-0.0079,  0.0117, -0.0114, -0.0157,  0.0036],\n",
      "        [-0.0160,  0.0037, -0.0048,  0.0224, -0.0100],\n",
      "        [-0.0074,  0.0055, -0.0100,  0.0107,  0.0182],\n",
      "        [ 0.0155,  0.0246,  0.0058,  0.0027, -0.0039]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0002,  0.0277, -0.0106,  0.0163, -0.0094],\n",
      "        [-0.0055, -0.0204, -0.0310, -0.0049,  0.0068],\n",
      "        [ 0.0200, -0.0198,  0.0243,  0.0328, -0.0022],\n",
      "        [-0.0012,  0.0326,  0.0312,  0.0124,  0.0172],\n",
      "        [-0.0283, -0.0020, -0.0375,  0.0004, -0.0069],\n",
      "        [-0.0036,  0.0377, -0.0063, -0.0380, -0.0330],\n",
      "        [ 0.0336, -0.0055, -0.0115,  0.0075, -0.0033],\n",
      "        [ 0.0146,  0.0117, -0.0050,  0.0014,  0.0125]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0106, grad_fn=<MinBackward1>), tensor(0.8693, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.1067945659160614\n",
      "@sample 1243: tensor([[-0.0124,  0.0101,  0.0100,  0.0069,  0.0159],\n",
      "        [ 0.0063, -0.0095, -0.0217,  0.0027, -0.0115],\n",
      "        [ 0.0011,  0.0009, -0.0019,  0.0302, -0.0228],\n",
      "        [ 0.0058,  0.0098, -0.0020,  0.0162,  0.0049],\n",
      "        [ 0.0095,  0.0153, -0.0084,  0.0301, -0.0138],\n",
      "        [ 0.0267,  0.0216, -0.0158, -0.0234,  0.0149],\n",
      "        [-0.0103,  0.0115, -0.0115,  0.0225, -0.0060],\n",
      "        [ 0.0096, -0.0175, -0.0160,  0.0042,  0.0080]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0005,  0.0194, -0.0230,  0.0323, -0.0097],\n",
      "        [-0.0200,  0.0363, -0.0405,  0.0252,  0.0254],\n",
      "        [-0.0144, -0.0005, -0.0001, -0.0187, -0.0145],\n",
      "        [-0.0033,  0.0068, -0.0112, -0.0020, -0.0077],\n",
      "        [ 0.0062,  0.0375, -0.0064,  0.0155,  0.0008],\n",
      "        [-0.0402, -0.0279, -0.0624,  0.0345,  0.0099],\n",
      "        [-0.0088, -0.0013, -0.0258,  0.0020, -0.0128],\n",
      "        [ 0.0110, -0.0147, -0.0112,  0.0144,  0.0123]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0117, grad_fn=<MinBackward1>), tensor(0.7831, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.0860871821641922\n",
      "@sample 1244: tensor([[-1.1015e-02, -1.1632e-03,  5.1890e-03,  1.2679e-02,  5.9338e-03],\n",
      "        [ 8.2592e-03,  9.0471e-04, -2.6337e-02,  2.6466e-02,  3.0613e-04],\n",
      "        [-6.3019e-03, -8.0202e-03, -1.4719e-02, -5.6528e-04,  4.6404e-03],\n",
      "        [-1.0536e-02, -5.0954e-03, -6.7341e-03,  1.2212e-02, -2.5979e-03],\n",
      "        [ 1.5581e-02,  2.1325e-02, -3.4885e-03,  8.1242e-03, -1.0964e-02],\n",
      "        [ 3.9223e-02,  2.1226e-03, -2.4876e-04,  1.9135e-02, -1.8730e-02],\n",
      "        [ 5.0776e-05,  1.3013e-04, -8.3100e-03,  4.5066e-02, -1.4652e-03],\n",
      "        [ 1.4636e-02,  1.0434e-02, -2.7912e-03, -2.1332e-02,  1.9276e-02]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0002,  0.0378, -0.0229,  0.0164,  0.0062],\n",
      "        [ 0.0426,  0.0227,  0.0040, -0.0059, -0.0012],\n",
      "        [ 0.0144, -0.0100,  0.0074, -0.0220, -0.0017],\n",
      "        [ 0.0090,  0.0558, -0.0077,  0.0017,  0.0064],\n",
      "        [ 0.0264,  0.0144, -0.0101, -0.0094, -0.0037],\n",
      "        [ 0.0114,  0.0384, -0.0059,  0.0069, -0.0221],\n",
      "        [ 0.0417,  0.0373, -0.0207,  0.0113, -0.0028],\n",
      "        [-0.0342, -0.0126, -0.0377, -0.0006, -0.0225]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0146, grad_fn=<MinBackward1>), tensor(0.8091, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08332519233226776\n",
      "@sample 1245: tensor([[ 0.0016,  0.0066,  0.0089, -0.0039,  0.0062],\n",
      "        [ 0.0057, -0.0142, -0.0179,  0.0422, -0.0150],\n",
      "        [ 0.0080,  0.0199,  0.0188, -0.0058, -0.0009],\n",
      "        [-0.0114,  0.0095, -0.0246, -0.0001,  0.0184],\n",
      "        [-0.0004,  0.0394,  0.0160, -0.0306,  0.0171],\n",
      "        [ 0.0093,  0.0044, -0.0228,  0.0168, -0.0184],\n",
      "        [ 0.0105,  0.0314,  0.0052, -0.0281, -0.0006],\n",
      "        [ 0.0057,  0.0300, -0.0036,  0.0226, -0.0071]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0002,  0.0115, -0.0320,  0.0052,  0.0023],\n",
      "        [ 0.0295,  0.0183, -0.0119,  0.0030, -0.0022],\n",
      "        [-0.0013,  0.0068, -0.0111,  0.0116,  0.0036],\n",
      "        [-0.0036,  0.0211, -0.0022,  0.0018,  0.0173],\n",
      "        [ 0.0178,  0.0042, -0.0160,  0.0136, -0.0065],\n",
      "        [ 0.0220,  0.0129,  0.0233, -0.0189,  0.0060],\n",
      "        [-0.0299, -0.0206, -0.0793,  0.0313, -0.0093],\n",
      "        [ 0.0022,  0.0398, -0.0158, -0.0098, -0.0074]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0116, grad_fn=<MinBackward1>), tensor(0.8112, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.07871614396572113\n",
      "@sample 1246: tensor([[-0.0127,  0.0121,  0.0207,  0.0117, -0.0012],\n",
      "        [ 0.0007,  0.0017, -0.0125,  0.0153,  0.0033],\n",
      "        [ 0.0302,  0.0360,  0.0083, -0.0242,  0.0157],\n",
      "        [-0.0196, -0.0154,  0.0198, -0.0031,  0.0249],\n",
      "        [ 0.0128,  0.0351,  0.0313, -0.0052,  0.0014],\n",
      "        [ 0.0090,  0.0153,  0.0174,  0.0039,  0.0153],\n",
      "        [ 0.0163,  0.0186, -0.0063,  0.0027, -0.0019],\n",
      "        [ 0.0037, -0.0066, -0.0053,  0.0051,  0.0060]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0261,  0.0021,  0.0047, -0.0035,  0.0122],\n",
      "        [ 0.0222,  0.0207, -0.0122,  0.0089, -0.0086],\n",
      "        [-0.0109, -0.0030, -0.0469,  0.0261,  0.0084],\n",
      "        [ 0.0158, -0.0352, -0.0230,  0.0103, -0.0046],\n",
      "        [-0.0358,  0.0130, -0.0547,  0.0359, -0.0098],\n",
      "        [ 0.0003,  0.0283, -0.0335,  0.0275,  0.0211],\n",
      "        [-0.0050,  0.0140, -0.0192,  0.0189,  0.0113],\n",
      "        [ 0.0024,  0.0093,  0.0023, -0.0142, -0.0065]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0133, grad_fn=<MinBackward1>), tensor(0.7838, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08516804873943329\n",
      "@sample 1247: tensor([[ 5.3382e-03,  1.2957e-02, -1.6708e-02, -1.7466e-02,  6.4060e-03],\n",
      "        [ 2.5371e-02, -1.4261e-02, -7.4710e-03, -4.8216e-03,  7.7060e-03],\n",
      "        [-4.0614e-03,  3.6295e-02,  4.5222e-03, -3.8504e-02,  2.2331e-02],\n",
      "        [ 2.0366e-02,  1.0615e-02, -4.3498e-04,  7.1254e-03, -1.4857e-02],\n",
      "        [-5.8983e-05,  5.1931e-02,  5.2041e-02, -9.5990e-03, -1.0742e-02],\n",
      "        [-2.0159e-03,  2.1905e-02,  4.8381e-03,  9.8761e-03, -1.3185e-02],\n",
      "        [ 1.7770e-02,  7.2996e-03,  4.8268e-03,  8.3260e-03, -3.4877e-02],\n",
      "        [ 5.7345e-03,  2.5991e-03, -8.5188e-03,  1.9852e-03,  1.1865e-03]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0329, -0.0094,  0.0291, -0.0154,  0.0054],\n",
      "        [-0.0063,  0.0219, -0.0015, -0.0046, -0.0148],\n",
      "        [-0.0020, -0.0202, -0.1156,  0.0436, -0.0196],\n",
      "        [ 0.0222,  0.0471,  0.0054,  0.0027,  0.0379],\n",
      "        [-0.0599,  0.0090, -0.0460,  0.0310, -0.0171],\n",
      "        [-0.0025,  0.0117, -0.0045, -0.0035, -0.0124],\n",
      "        [-0.0029,  0.0166, -0.0436,  0.0092,  0.0125],\n",
      "        [-0.0120,  0.0084, -0.0207,  0.0204,  0.0122]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0145, grad_fn=<MinBackward1>), tensor(0.8238, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09943418949842453\n",
      "@sample 1248: tensor([[ 0.0001, -0.0080, -0.0085,  0.0031, -0.0053],\n",
      "        [-0.0179, -0.0035,  0.0041, -0.0027,  0.0023],\n",
      "        [-0.0047,  0.0172,  0.0024,  0.0075,  0.0088],\n",
      "        [-0.0235,  0.0160,  0.0004, -0.0020,  0.0056],\n",
      "        [ 0.0080, -0.0065,  0.0032,  0.0066,  0.0110],\n",
      "        [-0.0048,  0.0079, -0.0061, -0.0020, -0.0069],\n",
      "        [ 0.0141,  0.0063,  0.0128,  0.0262, -0.0077],\n",
      "        [ 0.0018, -0.0061, -0.0041,  0.0065,  0.0123]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0019,  0.0074, -0.0073, -0.0097, -0.0095],\n",
      "        [-0.0031,  0.0208, -0.0108,  0.0186,  0.0181],\n",
      "        [-0.0047,  0.0385, -0.0191,  0.0170,  0.0203],\n",
      "        [-0.0373,  0.0059, -0.0418,  0.0022, -0.0047],\n",
      "        [ 0.0100, -0.0178,  0.0088,  0.0062,  0.0184],\n",
      "        [-0.0147, -0.0070,  0.0139,  0.0153, -0.0143],\n",
      "        [ 0.0307,  0.0201,  0.0110, -0.0056,  0.0057],\n",
      "        [-0.0210, -0.0198, -0.0134, -0.0082, -0.0105]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0127, grad_fn=<MinBackward1>), tensor(0.8151, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08538250625133514\n",
      "@sample 1249: tensor([[ 0.0093,  0.0004, -0.0186, -0.0319,  0.0036],\n",
      "        [ 0.0025, -0.0140,  0.0018,  0.0150,  0.0100],\n",
      "        [-0.0089, -0.0109, -0.0182,  0.0225, -0.0090],\n",
      "        [-0.0181,  0.0043,  0.0110, -0.0021, -0.0015],\n",
      "        [ 0.0073,  0.0103,  0.0028, -0.0099, -0.0028],\n",
      "        [-0.0124, -0.0011, -0.0046, -0.0038,  0.0047],\n",
      "        [ 0.0266,  0.0144,  0.0075,  0.0119, -0.0046],\n",
      "        [-0.0012, -0.0050, -0.0052, -0.0118,  0.0002]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0270, -0.0319, -0.0229,  0.0100, -0.0038],\n",
      "        [ 0.0137,  0.0151, -0.0206,  0.0112,  0.0299],\n",
      "        [ 0.0125,  0.0151,  0.0316, -0.0195,  0.0066],\n",
      "        [-0.0082,  0.0068,  0.0317, -0.0244, -0.0129],\n",
      "        [-0.0091,  0.0113,  0.0172,  0.0106,  0.0163],\n",
      "        [-0.0112, -0.0073,  0.0069, -0.0145,  0.0401],\n",
      "        [ 0.0062,  0.0254, -0.0009, -0.0022,  0.0031],\n",
      "        [-0.0218, -0.0129, -0.0171,  0.0152,  0.0159]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0092, grad_fn=<MinBackward1>), tensor(0.8890, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08049087226390839\n",
      "@sample 1250: tensor([[ 0.0115, -0.0192,  0.0112, -0.0007,  0.0088],\n",
      "        [ 0.0036, -0.0123,  0.0059, -0.0077,  0.0005],\n",
      "        [ 0.0229, -0.0127,  0.0061,  0.0037, -0.0029],\n",
      "        [ 0.0151,  0.0355, -0.0004, -0.0184,  0.0051],\n",
      "        [ 0.0138, -0.0197, -0.0014, -0.0082,  0.0048],\n",
      "        [ 0.0087,  0.0273, -0.0055, -0.0427,  0.0293],\n",
      "        [ 0.0223,  0.0240, -0.0371, -0.0141, -0.0005],\n",
      "        [ 0.0140, -0.0081,  0.0072,  0.0084,  0.0023]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0104,  0.0238, -0.0113,  0.0105, -0.0063],\n",
      "        [ 0.0029,  0.0034,  0.0092, -0.0152, -0.0122],\n",
      "        [ 0.0150,  0.0133,  0.0117,  0.0010,  0.0150],\n",
      "        [-0.0018, -0.0276, -0.0291,  0.0259, -0.0047],\n",
      "        [ 0.0073, -0.0025,  0.0276, -0.0144, -0.0064],\n",
      "        [ 0.0091, -0.0151, -0.0024,  0.0462, -0.0070],\n",
      "        [-0.0192, -0.0166, -0.0114,  0.0246, -0.0043],\n",
      "        [ 0.0075,  0.0110,  0.0218, -0.0011,  0.0045]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0117, grad_fn=<MinBackward1>), tensor(0.8796, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08554980903863907\n",
      "@sample 1251: tensor([[ 0.0075, -0.0075,  0.0051,  0.0005, -0.0079],\n",
      "        [-0.0021, -0.0076,  0.0215,  0.0039,  0.0074],\n",
      "        [ 0.0052, -0.0194, -0.0019,  0.0012, -0.0020],\n",
      "        [ 0.0060, -0.0329, -0.0040, -0.0011,  0.0274],\n",
      "        [-0.0126, -0.0151,  0.0421, -0.0205,  0.0109],\n",
      "        [-0.0041, -0.0073,  0.0047,  0.0056, -0.0079],\n",
      "        [ 0.0048, -0.0042,  0.0034, -0.0120,  0.0015],\n",
      "        [-0.0071,  0.0023, -0.0075, -0.0029,  0.0268]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0095,  0.0134, -0.0022, -0.0020, -0.0040],\n",
      "        [-0.0078,  0.0066,  0.0376, -0.0289, -0.0020],\n",
      "        [ 0.0067,  0.0265, -0.0178,  0.0076,  0.0277],\n",
      "        [-0.0252, -0.0636, -0.0157, -0.0086, -0.0212],\n",
      "        [ 0.0071, -0.0149,  0.0217, -0.0082, -0.0072],\n",
      "        [ 0.0021,  0.0074,  0.0078, -0.0033,  0.0085],\n",
      "        [-0.0054,  0.0363,  0.0070, -0.0058,  0.0201],\n",
      "        [ 0.0152,  0.0128, -0.0342,  0.0230, -0.0055]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0120, grad_fn=<MinBackward1>), tensor(0.8439, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08485577255487442\n",
      "@sample 1252: tensor([[ 0.0094, -0.0066,  0.0155,  0.0098,  0.0042],\n",
      "        [ 0.0075, -0.0149, -0.0006,  0.0035,  0.0094],\n",
      "        [-0.0084, -0.0129,  0.0193, -0.0068,  0.0101],\n",
      "        [-0.0060, -0.0009,  0.0089, -0.0150, -0.0028],\n",
      "        [ 0.0156,  0.0059,  0.0160, -0.0168,  0.0192],\n",
      "        [-0.0009, -0.0184,  0.0136, -0.0057, -0.0016],\n",
      "        [ 0.0249, -0.0353,  0.0191, -0.0116, -0.0195],\n",
      "        [-0.0113, -0.0039,  0.0004, -0.0114,  0.0189]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0240,  0.0272,  0.0042, -0.0009, -0.0010],\n",
      "        [ 0.0109, -0.0195,  0.0136,  0.0017, -0.0200],\n",
      "        [ 0.0020, -0.0346,  0.0309, -0.0193,  0.0059],\n",
      "        [ 0.0010, -0.0155,  0.0110, -0.0233,  0.0143],\n",
      "        [-0.0166, -0.0015, -0.0143,  0.0341,  0.0002],\n",
      "        [-0.0095, -0.0025, -0.0161,  0.0040,  0.0342],\n",
      "        [-0.0102, -0.0003,  0.0194,  0.0198,  0.0110],\n",
      "        [-0.0050, -0.0005,  0.0141, -0.0104, -0.0095]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0118, grad_fn=<MinBackward1>), tensor(0.8386, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.09349333494901657\n",
      "@sample 1253: tensor([[-0.0004, -0.0137,  0.0060, -0.0143, -0.0118],\n",
      "        [ 0.0221,  0.0135, -0.0191, -0.0301,  0.0063],\n",
      "        [ 0.0248,  0.0253, -0.0095, -0.0229,  0.0160],\n",
      "        [ 0.0005, -0.0134,  0.0413, -0.0512,  0.0326],\n",
      "        [ 0.0108, -0.0301, -0.0060,  0.0022,  0.0051],\n",
      "        [-0.0140, -0.0079,  0.0032, -0.0089,  0.0185],\n",
      "        [-0.0009, -0.0090, -0.0062, -0.0171,  0.0102],\n",
      "        [-0.0227, -0.0255, -0.0036, -0.0087,  0.0169]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[-0.0164, -0.0073,  0.0224, -0.0331, -0.0152],\n",
      "        [ 0.0010,  0.0034, -0.0572,  0.0166,  0.0086],\n",
      "        [-0.0282, -0.0193, -0.0758,  0.0440, -0.0083],\n",
      "        [-0.0236, -0.0438, -0.0465, -0.0060, -0.0070],\n",
      "        [ 0.0081, -0.0371,  0.0335,  0.0026, -0.0225],\n",
      "        [-0.0005, -0.0200, -0.0019,  0.0106,  0.0023],\n",
      "        [-0.0083, -0.0160,  0.0091, -0.0228, -0.0052],\n",
      "        [-0.0143, -0.0254, -0.0220,  0.0010, -0.0211]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0165, grad_fn=<MinBackward1>), tensor(0.8567, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.08973231166601181\n",
      "@sample 1254: tensor([[-0.0070,  0.0244,  0.0234, -0.0249,  0.0164],\n",
      "        [-0.0043, -0.0106,  0.0122, -0.0167,  0.0064],\n",
      "        [ 0.0026, -0.0119,  0.0022, -0.0068,  0.0130],\n",
      "        [ 0.0223,  0.0440,  0.0378, -0.0083, -0.0257],\n",
      "        [ 0.0028, -0.0169,  0.0036, -0.0048,  0.0102],\n",
      "        [ 0.0093, -0.0069,  0.0085, -0.0138,  0.0300],\n",
      "        [-0.0119, -0.0056,  0.0249, -0.0048,  0.0211],\n",
      "        [ 0.0039,  0.0018,  0.0093,  0.0064,  0.0037]],\n",
      "       grad_fn=<AddmmBackward0>), tensor([[ 0.0040, -0.0304, -0.0243, -0.0053, -0.0041],\n",
      "        [-0.0060, -0.0189,  0.0318, -0.0437, -0.0134],\n",
      "        [ 0.0070, -0.0029,  0.0066, -0.0103,  0.0095],\n",
      "        [-0.0581,  0.0009,  0.0011,  0.0161, -0.0094],\n",
      "        [-0.0325, -0.0212,  0.0041, -0.0061, -0.0008],\n",
      "        [ 0.0010, -0.0425, -0.0142, -0.0073, -0.0344],\n",
      "        [ 0.0271, -0.0157, -0.0203, -0.0225, -0.0349],\n",
      "        [-0.0083, -0.0009,  0.0084, -0.0340, -0.0107]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x_hat: (tensor(0.), tensor(1.), tensor(0.0110, grad_fn=<MinBackward1>), tensor(0.8467, grad_fn=<MaxBackward1>)) \n",
      "Loss @ this stage: 0.10455577075481415\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvae\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 34\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epochs, vae, dataloader)\u001b[0m\n\u001b[1;32m     31\u001b[0m loss: Tensor \u001b[38;5;241m=\u001b[39m criterion(x\u001b[38;5;241m.\u001b[39mflatten(start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), x_hat, enc_mu_sigma[\u001b[38;5;241m0\u001b[39m], enc_mu_sigma[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss @ this stage: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(vae\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)    \n\u001b[1;32m     36\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/KompAKI-Backend-CwZxqLfI/lib/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/KompAKI-Backend-CwZxqLfI/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
=======
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
>>>>>>> f633652cb673a0ee980853e0855a256bcab7b7ee
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 32,
=======
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VAE import train, VAE, Encoder, Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(MNIST_SIZE)\n",
    "dec = Decoder()\n",
    "vae = VAE(enc, dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VAE import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:07<00:00,  6.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Iteration no 0 with loss: 3889069.255859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:05<00:00,  7.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Iteration no 1 with loss: 2628078.473388672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:05<00:00,  7.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Iteration no 2 with loss: 2445978.725341797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:05<00:00,  7.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Iteration no 3 with loss: 2379396.4841308594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:06<00:00,  7.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Iteration no 4 with loss: 2349982.7568359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:05<00:00,  7.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Iteration no 5 with loss: 2326187.5856933594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:05<00:00,  7.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Iteration no 6 with loss: 2304967.6564941406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:05<00:00,  7.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Iteration no 7 with loss: 2291031.9077148438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:04<00:00,  7.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Iteration no 8 with loss: 2280179.6958007812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:03<00:00,  7.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Iteration no 9 with loss: 2274654.3115234375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:03<00:00,  7.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Iteration no 10 with loss: 2271074.0732421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:04<00:00,  7.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Iteration no 11 with loss: 2270577.810546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:03<00:00,  7.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Iteration no 12 with loss: 2266758.0927734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:03<00:00,  7.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Iteration no 13 with loss: 2262867.8759765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:04<00:00,  7.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Iteration no 14 with loss: 2263878.3227539062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:03<00:00,  7.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Iteration no 15 with loss: 2264349.8627929688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:04<00:00,  7.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Iteration no 16 with loss: 2263631.1928710938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:04<00:00,  7.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Iteration no 17 with loss: 2261260.313232422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:03<00:00,  7.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Iteration no 18 with loss: 2265272.3662109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:03<00:00,  7.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Iteration no 19 with loss: 2259350.7724609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:03<00:00,  7.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Iteration no 20 with loss: 2255863.833984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:03<00:00,  7.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Iteration no 21 with loss: 2259862.533203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:03<00:00,  7.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Iteration no 22 with loss: 2258949.5805664062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:03<00:00,  7.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Iteration no 23 with loss: 2258874.5661621094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:04<00:00,  7.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Iteration no 24 with loss: 2255229.946044922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:05<00:00,  7.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Iteration no 25 with loss: 2257246.0571289062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:05<00:00,  7.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Iteration no 26 with loss: 2260245.4946289062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:05<00:00,  7.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Iteration no 27 with loss: 2255037.7368164062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:04<00:00,  7.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Iteration no 28 with loss: 2257766.4995117188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:04<00:00,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Iteration no 29 with loss: 2254419.822265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(epochs=30, vae=vae, dataloader=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
>>>>>>> f633652cb673a0ee980853e0855a256bcab7b7ee
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
<<<<<<< HEAD
     "execution_count": 32,
=======
     "execution_count": 42,
>>>>>>> f633652cb673a0ee980853e0855a256bcab7b7ee
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "flatten() takes from 0 to 1 positional arguments but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m16\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(np\u001b[38;5;241m.\u001b[39mconcatenate(vae(np\u001b[38;5;241m.\u001b[39marray([random\u001b[38;5;241m.\u001b[39mchoice(mnist_data)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m)])), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/ML-Practice/VAE/VAE.py:97\u001b[0m, in \u001b[0;36mVAE.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor):\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__forward__(x)\n",
      "File \u001b[0;32m~/ML-Practice/VAE/VAE.py:82\u001b[0m, in \u001b[0;36mVAE.__forward__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     79\u001b[0m mu_dec: Tensor\n\u001b[1;32m     80\u001b[0m log_sigma_square_dec: Tensor\n\u001b[0;32m---> 82\u001b[0m mu_enc, log_sigma_square_enc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc(x)\n\u001b[1;32m     83\u001b[0m latent_distr: torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mMultivariateNormal \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mMultivariateNormal(\n\u001b[1;32m     84\u001b[0m     loc\u001b[38;5;241m=\u001b[39mmu_enc,\n\u001b[1;32m     85\u001b[0m     covariance_matrix\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdiag_embed(torch\u001b[38;5;241m.\u001b[39mexp(log_sigma_square_enc))\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     87\u001b[0m z: Tensor \u001b[38;5;241m=\u001b[39m latent_distr\u001b[38;5;241m.\u001b[39mrsample()\n",
      "File \u001b[0;32m~/ML-Practice/VAE/VAE.py:42\u001b[0m, in \u001b[0;36mEncoder.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Tensor]:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__forward__(x)\n",
      "File \u001b[0;32m~/ML-Practice/VAE/VAE.py:34\u001b[0m, in \u001b[0;36mEncoder.__forward__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__forward__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Tensor]:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# encoder returns mu and log sigma^2 where sigma (both 1D vectors)\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten_inp(x)\n\u001b[1;32m     35\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minp_layer(x)\n\u001b[1;32m     36\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtanh(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/ML-Practice/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ML-Practice/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/ML-Practice/lib/python3.12/site-packages/torch/nn/modules/flatten.py:53\u001b[0m, in \u001b[0;36mFlatten.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_dim)\n",
      "\u001b[0;31mTypeError\u001b[0m: flatten() takes from 0 to 1 positional arguments but 2 were given"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x1600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "import numpy as np\n",
    "plt.imshow(np.concatenate(vae(np.array([random.choice(mnist_data)[0] for i in range(10)])), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n",
       "          0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n",
       "          0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n",
       "          0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "          0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n",
       "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n",
       "          0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n",
       "          0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n",
       "          0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n",
       "          0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n",
       "          0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n",
       "          0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n",
       "          0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n",
       "          0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n",
       "          0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n",
       "          0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n",
       "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n",
       "          0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n",
       "          0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(sample[0].detach().numpy().reshape(28,28))\n",
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(2, 10, figsize=(15,5))\n",
    "\n",
    "sample_idx = [random.randint(0, len(mnist_data)) for i in range(10)]\n",
    "for i in range(10):\n",
    "\n",
    "\n",
    "    ax[0, i].imshow(vae.to(\"cpu\")(mnist_data[sample_idx[i]][0]).detach().numpy().reshape(28,28))\n",
    "    ax[1, i].imshow(mnist_data[sample_idx[i]][0].detach().numpy().reshape(28,28))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig, ax = plt.subplots(2, 10, figsize=(15,10))\n",
    "\n",
    "nm_z = torch.distributions.MultivariateNormal(torch.zeros(25), torch.diag_embed(torch.ones(25))).rsample(sample_shape=(10,))\n",
    "params = [vae.to(\"cpu\").dec(nm_z[i]) for i in range(10)]\n",
    "\n",
    "for i in range(10):\n",
    "    mu, log_sig = params[i]\n",
    "    sfromlat = torch.distributions.MultivariateNormal(mu, torch.diag_embed(torch.exp(log_sig))).rsample()\n",
    "    sfromlat = sfromlat.reshape(28,28)\n",
    "    ax[1, i].imshow(nm_z[i].reshape(5,5))\n",
    "    ax[0, i].imshow(sfromlat.detach().numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x31fa71250>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.imshow(vae(sample[0]).detach().numpy().reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x31fd24650>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(vae(sample[0]).detach().numpy().reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x376335670>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(vae(sample[0]).detach().numpy().reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x329748230>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.imshow(vae(sample[0]).detach().numpy().reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msample_batch\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_batch' is not defined"
     ]
    }
   ],
   "source": [
    "sample_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 784])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vae(sample_batch[0]).size()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1284674c0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
=======
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(vae(\u001b[43msample_batch\u001b[49m[\u001b[38;5;241m0\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m28\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# plt.imshow(sample_batch[0][3].reshape(28,28))\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_batch' is not defined"
     ]
>>>>>>> f633652cb673a0ee980853e0855a256bcab7b7ee
    }
   ],
   "source": [
    "plt.imshow(vae(sample_batch[0])[0].detach().numpy().reshape(28,28))\n",
    "# plt.imshow(sample_batch[0][3].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[284], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMultivariateNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcovariance_matrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiag_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[144], line 62\u001b[0m, in \u001b[0;36mDecoder.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor):\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__forward__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[144], line 51\u001b[0m, in \u001b[0;36mDecoder.__forward__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__forward__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor):\n\u001b[0;32m---> 51\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     52\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minp_layer(x)\n\u001b[1;32m     53\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(x\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "plt.imshow(vae.dec(torch.distributions.MultivariateNormal(torch.ones(5), covariance_matrix=torch.diag_embed(torch.ones(5))).rsample().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": "KompAKI-Backend-CwZxqLfI",
=======
   "display_name": "ML-Practice",
>>>>>>> f633652cb673a0ee980853e0855a256bcab7b7ee
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
